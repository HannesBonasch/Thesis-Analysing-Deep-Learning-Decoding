{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5227a5a",
   "metadata": {},
   "source": [
    "# Model Comparision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfba4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataLoader, Training, HelperFunctions, os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35f030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5210\u001b[0m        \u001b[32m1.0424\u001b[0m                     \u001b[35m0.5569\u001b[0m        \u001b[31m0.7331\u001b[0m  0.0100  1.4316\n",
      "      2                     \u001b[36m0.5332\u001b[0m        \u001b[32m0.7908\u001b[0m                     \u001b[35m0.5831\u001b[0m        \u001b[31m0.6729\u001b[0m  0.0100  0.4505\n",
      "      3                     \u001b[36m0.5594\u001b[0m        \u001b[32m0.7779\u001b[0m                     \u001b[35m0.6034\u001b[0m        0.7361  0.0098  0.4508\n",
      "      4                     \u001b[36m0.5696\u001b[0m        0.7883                     \u001b[35m0.6163\u001b[0m        \u001b[31m0.6492\u001b[0m  0.0096  0.4488\n",
      "      5                     \u001b[36m0.6079\u001b[0m        \u001b[32m0.7074\u001b[0m                     \u001b[35m0.6401\u001b[0m        \u001b[31m0.6381\u001b[0m  0.0093  0.4499\n",
      "      6                     \u001b[36m0.6324\u001b[0m        \u001b[32m0.6860\u001b[0m                     \u001b[35m0.6531\u001b[0m        \u001b[31m0.6257\u001b[0m  0.0090  0.4499\n",
      "      7                     \u001b[36m0.6498\u001b[0m        \u001b[32m0.6682\u001b[0m                     \u001b[35m0.7074\u001b[0m        \u001b[31m0.5642\u001b[0m  0.0085  0.4489\n",
      "      8                     \u001b[36m0.6727\u001b[0m        \u001b[32m0.6394\u001b[0m                     0.7068        \u001b[31m0.5631\u001b[0m  0.0080  0.4488\n",
      "      9                     \u001b[36m0.6854\u001b[0m        \u001b[32m0.6124\u001b[0m                     \u001b[35m0.7165\u001b[0m        \u001b[31m0.5489\u001b[0m  0.0075  0.4488\n",
      "     10                     0.6722        0.6267                     \u001b[35m0.7307\u001b[0m        \u001b[31m0.5398\u001b[0m  0.0069  0.4478\n",
      "     11                     0.6852        \u001b[32m0.6117\u001b[0m                     0.7012        0.5723  0.0063  0.4498\n",
      "     12                     \u001b[36m0.7118\u001b[0m        \u001b[32m0.5883\u001b[0m                     0.6998        0.5553  0.0057  0.4488\n",
      "     13                     0.6994        \u001b[32m0.5771\u001b[0m                     0.7281        \u001b[31m0.5325\u001b[0m  0.0050  0.4498\n",
      "     14                     \u001b[36m0.7157\u001b[0m        \u001b[32m0.5649\u001b[0m                     0.7050        0.5624  0.0043  0.4479\n",
      "     15                     \u001b[36m0.7261\u001b[0m        \u001b[32m0.5613\u001b[0m                     \u001b[35m0.7359\u001b[0m        0.5328  0.0037  0.4498\n",
      "     16                     \u001b[36m0.7319\u001b[0m        \u001b[32m0.5516\u001b[0m                     \u001b[35m0.7409\u001b[0m        \u001b[31m0.5208\u001b[0m  0.0031  0.4478\n",
      "     17                     \u001b[36m0.7356\u001b[0m        \u001b[32m0.5348\u001b[0m                     \u001b[35m0.7417\u001b[0m        \u001b[31m0.5187\u001b[0m  0.0025  0.4462\n",
      "     18                     \u001b[36m0.7419\u001b[0m        \u001b[32m0.5313\u001b[0m                     0.7325        0.5222  0.0020  0.4489\n",
      "     19                     \u001b[36m0.7448\u001b[0m        \u001b[32m0.5229\u001b[0m                     0.7323        \u001b[31m0.5169\u001b[0m  0.0015  0.4498\n",
      "     20                     \u001b[36m0.7507\u001b[0m        \u001b[32m0.5149\u001b[0m                     \u001b[35m0.7453\u001b[0m        0.5176  0.0010  0.4472\n",
      "     21                     0.7494        0.5201                     0.7418        \u001b[31m0.5157\u001b[0m  0.0007  0.4498\n",
      "     22                     \u001b[36m0.7539\u001b[0m        \u001b[32m0.5109\u001b[0m                     0.7418        \u001b[31m0.5124\u001b[0m  0.0004  0.4488\n",
      "     23                     \u001b[36m0.7602\u001b[0m        \u001b[32m0.5081\u001b[0m                     0.7394        0.5141  0.0002  0.4488\n",
      "     24                     \u001b[36m0.7607\u001b[0m        \u001b[32m0.5002\u001b[0m                     0.7413        0.5157  0.0000  0.4478\n",
      "     25                     0.7564        \u001b[32m0.4986\u001b[0m                     \u001b[35m0.7503\u001b[0m        \u001b[31m0.5117\u001b[0m  0.0000  0.4498\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5121\u001b[0m        \u001b[32m1.2649\u001b[0m                     \u001b[35m0.5628\u001b[0m        \u001b[31m0.6833\u001b[0m  0.0100  0.4418\n",
      "      2                     \u001b[36m0.5404\u001b[0m        \u001b[32m0.7862\u001b[0m                     0.5094        0.7901  0.0100  0.4498\n",
      "      3                     0.5220        1.0413                     0.5314        0.8190  0.0098  0.4487\n",
      "      4                     \u001b[36m0.5498\u001b[0m        0.7972                     \u001b[35m0.5831\u001b[0m        0.6837  0.0096  0.4498\n",
      "      5                     \u001b[36m0.5881\u001b[0m        \u001b[32m0.7156\u001b[0m                     \u001b[35m0.6110\u001b[0m        \u001b[31m0.6590\u001b[0m  0.0093  0.4499\n",
      "      6                     \u001b[36m0.5886\u001b[0m        \u001b[32m0.7156\u001b[0m                     \u001b[35m0.6414\u001b[0m        \u001b[31m0.6230\u001b[0m  0.0090  0.4510\n",
      "      7                     \u001b[36m0.6183\u001b[0m        \u001b[32m0.6985\u001b[0m                     0.5759        0.7635  0.0085  0.4498\n",
      "      8                     \u001b[36m0.6291\u001b[0m        \u001b[32m0.6881\u001b[0m                     \u001b[35m0.6642\u001b[0m        \u001b[31m0.6203\u001b[0m  0.0080  0.4478\n",
      "      9                     \u001b[36m0.6600\u001b[0m        \u001b[32m0.6580\u001b[0m                     0.6366        0.6769  0.0075  0.4508\n",
      "     10                     \u001b[36m0.6712\u001b[0m        \u001b[32m0.6463\u001b[0m                     \u001b[35m0.7031\u001b[0m        \u001b[31m0.5742\u001b[0m  0.0069  0.4488\n",
      "     11                     \u001b[36m0.6726\u001b[0m        \u001b[32m0.6349\u001b[0m                     0.6925        \u001b[31m0.5737\u001b[0m  0.0063  0.4500\n",
      "     12                     \u001b[36m0.6826\u001b[0m        \u001b[32m0.6126\u001b[0m                     \u001b[35m0.7137\u001b[0m        \u001b[31m0.5622\u001b[0m  0.0057  0.4499\n",
      "     13                     \u001b[36m0.7027\u001b[0m        \u001b[32m0.5902\u001b[0m                     0.6928        \u001b[31m0.5604\u001b[0m  0.0050  0.4546\n",
      "     14                     \u001b[36m0.7113\u001b[0m        \u001b[32m0.5665\u001b[0m                     \u001b[35m0.7204\u001b[0m        \u001b[31m0.5432\u001b[0m  0.0043  0.4489\n",
      "     15                     \u001b[36m0.7136\u001b[0m        0.5766                     0.7086        0.5563  0.0037  0.4498\n",
      "     16                     \u001b[36m0.7204\u001b[0m        \u001b[32m0.5511\u001b[0m                     0.6811        0.6123  0.0031  0.4475\n",
      "     17                     0.7194        0.5602                     \u001b[35m0.7384\u001b[0m        \u001b[31m0.5339\u001b[0m  0.0025  0.4506\n",
      "     18                     \u001b[36m0.7432\u001b[0m        \u001b[32m0.5251\u001b[0m                     0.7340        \u001b[31m0.5201\u001b[0m  0.0020  0.4501\n",
      "     19                     \u001b[36m0.7514\u001b[0m        \u001b[32m0.5238\u001b[0m                     0.7304        0.5205  0.0015  0.4488\n",
      "     20                     0.7507        \u001b[32m0.5167\u001b[0m                     \u001b[35m0.7434\u001b[0m        \u001b[31m0.5176\u001b[0m  0.0010  0.4499\n",
      "     21                     \u001b[36m0.7559\u001b[0m        \u001b[32m0.5044\u001b[0m                     0.7322        0.5260  0.0007  0.4509\n",
      "     22                     0.7524        0.5094                     0.7280        0.5259  0.0004  0.4498\n",
      "     23                     \u001b[36m0.7600\u001b[0m        0.5069                     0.7354        0.5191  0.0002  0.4479\n",
      "     24                     0.7514        \u001b[32m0.5043\u001b[0m                     0.7275        0.5352  0.0000  0.4478\n",
      "     25                     0.7581        \u001b[32m0.5036\u001b[0m                     0.7266        0.5347  0.0000  0.4498\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5279\u001b[0m        \u001b[32m1.0066\u001b[0m                     \u001b[35m0.5215\u001b[0m        \u001b[31m0.7746\u001b[0m  0.0100  0.4438\n",
      "      2                     \u001b[36m0.5599\u001b[0m        \u001b[32m0.7669\u001b[0m                     \u001b[35m0.5612\u001b[0m        \u001b[31m0.7438\u001b[0m  0.0100  0.4492\n",
      "      3                     \u001b[36m0.5693\u001b[0m        \u001b[32m0.7584\u001b[0m                     \u001b[35m0.6190\u001b[0m        \u001b[31m0.6589\u001b[0m  0.0098  0.4488\n",
      "      4                     \u001b[36m0.6066\u001b[0m        \u001b[32m0.7102\u001b[0m                     \u001b[35m0.6318\u001b[0m        \u001b[31m0.6587\u001b[0m  0.0096  0.4499\n",
      "      5                     \u001b[36m0.6365\u001b[0m        \u001b[32m0.6860\u001b[0m                     \u001b[35m0.6668\u001b[0m        \u001b[31m0.6064\u001b[0m  0.0093  0.4488\n",
      "      6                     0.6305        0.7053                     \u001b[35m0.6714\u001b[0m        0.6080  0.0090  0.4486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7                     \u001b[36m0.6623\u001b[0m        \u001b[32m0.6495\u001b[0m                     0.6236        0.7352  0.0085  0.4469\n",
      "      8                     \u001b[36m0.6780\u001b[0m        \u001b[32m0.6392\u001b[0m                     \u001b[35m0.6995\u001b[0m        \u001b[31m0.5820\u001b[0m  0.0080  0.4488\n",
      "      9                     \u001b[36m0.6895\u001b[0m        \u001b[32m0.6187\u001b[0m                     0.6937        \u001b[31m0.5809\u001b[0m  0.0075  0.4488\n",
      "     10                     \u001b[36m0.6934\u001b[0m        \u001b[32m0.6020\u001b[0m                     0.6740        0.6180  0.0069  0.4488\n",
      "     11                     0.6930        0.6054                     \u001b[35m0.7110\u001b[0m        \u001b[31m0.5583\u001b[0m  0.0063  0.4488\n",
      "     12                     \u001b[36m0.7023\u001b[0m        \u001b[32m0.5821\u001b[0m                     0.7072        0.5767  0.0057  0.4490\n",
      "     13                     \u001b[36m0.7079\u001b[0m        \u001b[32m0.5776\u001b[0m                     \u001b[35m0.7275\u001b[0m        \u001b[31m0.5517\u001b[0m  0.0050  0.4488\n",
      "     14                     \u001b[36m0.7260\u001b[0m        \u001b[32m0.5600\u001b[0m                     0.7124        0.5787  0.0043  0.4478\n",
      "     15                     \u001b[36m0.7369\u001b[0m        \u001b[32m0.5407\u001b[0m                     0.7129        0.5715  0.0037  0.4488\n",
      "     16                     0.7340        \u001b[32m0.5384\u001b[0m                     \u001b[35m0.7314\u001b[0m        \u001b[31m0.5383\u001b[0m  0.0031  0.4498\n",
      "     17                     \u001b[36m0.7494\u001b[0m        \u001b[32m0.5176\u001b[0m                     \u001b[35m0.7460\u001b[0m        \u001b[31m0.5347\u001b[0m  0.0025  0.4489\n",
      "     18                     \u001b[36m0.7505\u001b[0m        \u001b[32m0.5097\u001b[0m                     0.7130        0.5616  0.0020  0.4492\n",
      "     19                     \u001b[36m0.7582\u001b[0m        \u001b[32m0.5079\u001b[0m                     \u001b[35m0.7470\u001b[0m        \u001b[31m0.5242\u001b[0m  0.0015  0.4503\n",
      "     20                     \u001b[36m0.7727\u001b[0m        \u001b[32m0.4864\u001b[0m                     \u001b[35m0.7507\u001b[0m        0.5246  0.0010  0.4496\n",
      "     21                     \u001b[36m0.7734\u001b[0m        \u001b[32m0.4817\u001b[0m                     0.7311        0.5547  0.0007  0.4488\n",
      "     22                     0.7668        0.4822                     0.7460        0.5248  0.0004  0.4478\n",
      "     23                     \u001b[36m0.7804\u001b[0m        \u001b[32m0.4774\u001b[0m                     \u001b[35m0.7530\u001b[0m        \u001b[31m0.5223\u001b[0m  0.0002  0.4499\n",
      "     24                     0.7788        \u001b[32m0.4747\u001b[0m                     \u001b[35m0.7547\u001b[0m        \u001b[31m0.5221\u001b[0m  0.0000  0.4491\n",
      "     25                     \u001b[36m0.7831\u001b[0m        \u001b[32m0.4641\u001b[0m                     0.7243        0.5570  0.0000  0.4498\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5372\u001b[0m        \u001b[32m0.9951\u001b[0m                     \u001b[35m0.5644\u001b[0m        \u001b[31m0.7117\u001b[0m  0.0100  0.4439\n",
      "      2                     \u001b[36m0.5420\u001b[0m        \u001b[32m0.8980\u001b[0m                     \u001b[35m0.5747\u001b[0m        \u001b[31m0.6909\u001b[0m  0.0100  0.4489\n",
      "      3                     \u001b[36m0.5676\u001b[0m        \u001b[32m0.8124\u001b[0m                     \u001b[35m0.6211\u001b[0m        \u001b[31m0.6616\u001b[0m  0.0098  0.4499\n",
      "      4                     \u001b[36m0.6122\u001b[0m        \u001b[32m0.7063\u001b[0m                     \u001b[35m0.6656\u001b[0m        \u001b[31m0.6183\u001b[0m  0.0096  0.4478\n",
      "      5                     \u001b[36m0.6368\u001b[0m        \u001b[32m0.6722\u001b[0m                     \u001b[35m0.6971\u001b[0m        \u001b[31m0.5823\u001b[0m  0.0093  0.4488\n",
      "      6                     \u001b[36m0.6585\u001b[0m        \u001b[32m0.6482\u001b[0m                     0.6364        0.6398  0.0090  0.4488\n",
      "      7                     0.6528        0.6568                     0.6718        0.6201  0.0085  0.4498\n",
      "      8                     \u001b[36m0.6735\u001b[0m        \u001b[32m0.6400\u001b[0m                     0.6832        0.5986  0.0080  0.4502\n",
      "      9                     \u001b[36m0.6821\u001b[0m        \u001b[32m0.6273\u001b[0m                     0.6674        0.6270  0.0075  0.4508\n",
      "     10                     \u001b[36m0.6859\u001b[0m        \u001b[32m0.6104\u001b[0m                     0.6666        0.6519  0.0069  0.4528\n",
      "     11                     \u001b[36m0.6938\u001b[0m        \u001b[32m0.6017\u001b[0m                     \u001b[35m0.7045\u001b[0m        0.5901  0.0063  0.4536\n",
      "     12                     \u001b[36m0.7083\u001b[0m        \u001b[32m0.5820\u001b[0m                     \u001b[35m0.7209\u001b[0m        \u001b[31m0.5644\u001b[0m  0.0057  0.4488\n",
      "     13                     \u001b[36m0.7268\u001b[0m        \u001b[32m0.5712\u001b[0m                     \u001b[35m0.7285\u001b[0m        \u001b[31m0.5607\u001b[0m  0.0050  0.4498\n",
      "     14                     0.7244        \u001b[32m0.5496\u001b[0m                     \u001b[35m0.7327\u001b[0m        \u001b[31m0.5372\u001b[0m  0.0043  0.4558\n",
      "     15                     \u001b[36m0.7360\u001b[0m        \u001b[32m0.5466\u001b[0m                     \u001b[35m0.7394\u001b[0m        0.5385  0.0037  0.4508\n",
      "     16                     0.7341        \u001b[32m0.5229\u001b[0m                     0.7375        0.5380  0.0031  0.4518\n",
      "     17                     \u001b[36m0.7423\u001b[0m        0.5254                     0.7310        \u001b[31m0.5345\u001b[0m  0.0025  0.4508\n",
      "     18                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5209\u001b[0m                     \u001b[35m0.7460\u001b[0m        \u001b[31m0.5294\u001b[0m  0.0020  0.4509\n",
      "     19                     0.7460        \u001b[32m0.5196\u001b[0m                     \u001b[35m0.7475\u001b[0m        0.5309  0.0015  0.4508\n",
      "     20                     \u001b[36m0.7609\u001b[0m        \u001b[32m0.5018\u001b[0m                     \u001b[35m0.7521\u001b[0m        \u001b[31m0.5248\u001b[0m  0.0010  0.4498\n",
      "     21                     \u001b[36m0.7676\u001b[0m        \u001b[32m0.4899\u001b[0m                     0.7455        0.5253  0.0007  0.4483\n",
      "     22                     0.7655        \u001b[32m0.4889\u001b[0m                     \u001b[35m0.7529\u001b[0m        \u001b[31m0.5236\u001b[0m  0.0004  0.4515\n",
      "     23                     \u001b[36m0.7683\u001b[0m        \u001b[32m0.4815\u001b[0m                     0.7510        \u001b[31m0.5234\u001b[0m  0.0002  0.4538\n",
      "     24                     \u001b[36m0.7691\u001b[0m        0.4955                     0.7504        0.5235  0.0000  0.4505\n",
      "     25                     0.7662        0.4965                     0.7510        0.5234  0.0000  0.4508\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5253\u001b[0m        \u001b[32m1.0483\u001b[0m                     \u001b[35m0.5594\u001b[0m        \u001b[31m0.7143\u001b[0m  0.0100  0.4428\n",
      "      2                     \u001b[36m0.5341\u001b[0m        \u001b[32m0.9542\u001b[0m                     0.5570        0.7896  0.0100  0.4509\n",
      "      3                     \u001b[36m0.5721\u001b[0m        \u001b[32m0.7692\u001b[0m                     0.5386        0.7904  0.0098  0.4520\n",
      "      4                     \u001b[36m0.5839\u001b[0m        \u001b[32m0.7432\u001b[0m                     \u001b[35m0.6041\u001b[0m        \u001b[31m0.6670\u001b[0m  0.0096  0.4488\n",
      "      5                     \u001b[36m0.6062\u001b[0m        \u001b[32m0.7266\u001b[0m                     \u001b[35m0.6065\u001b[0m        0.7097  0.0093  0.4488\n",
      "      6                     \u001b[36m0.6216\u001b[0m        \u001b[32m0.7016\u001b[0m                     \u001b[35m0.6763\u001b[0m        \u001b[31m0.5974\u001b[0m  0.0090  0.4495\n",
      "      7                     \u001b[36m0.6395\u001b[0m        \u001b[32m0.6637\u001b[0m                     0.6714        0.6050  0.0085  0.4498\n",
      "      8                     \u001b[36m0.6594\u001b[0m        \u001b[32m0.6466\u001b[0m                     \u001b[35m0.6925\u001b[0m        \u001b[31m0.5803\u001b[0m  0.0080  0.4499\n",
      "      9                     \u001b[36m0.6784\u001b[0m        \u001b[32m0.6285\u001b[0m                     \u001b[35m0.7284\u001b[0m        \u001b[31m0.5608\u001b[0m  0.0075  0.4518\n",
      "     10                     \u001b[36m0.6897\u001b[0m        \u001b[32m0.6141\u001b[0m                     0.6718        0.6418  0.0069  0.4506\n",
      "     11                     0.6780        0.6350                     0.7147        \u001b[31m0.5578\u001b[0m  0.0063  0.4498\n",
      "     12                     \u001b[36m0.6969\u001b[0m        \u001b[32m0.5930\u001b[0m                     \u001b[35m0.7374\u001b[0m        \u001b[31m0.5465\u001b[0m  0.0057  0.4478\n",
      "     13                     \u001b[36m0.7070\u001b[0m        \u001b[32m0.5753\u001b[0m                     0.7270        0.5590  0.0050  0.4508\n",
      "     14                     \u001b[36m0.7099\u001b[0m        0.5780                     \u001b[35m0.7460\u001b[0m        \u001b[31m0.5260\u001b[0m  0.0043  0.4489\n",
      "     15                     \u001b[36m0.7216\u001b[0m        \u001b[32m0.5539\u001b[0m                     0.7339        0.5393  0.0037  0.4498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16                     \u001b[36m0.7288\u001b[0m        \u001b[32m0.5449\u001b[0m                     0.7313        0.5443  0.0031  0.4511\n",
      "     17                     \u001b[36m0.7299\u001b[0m        0.5486                     0.7425        0.5307  0.0025  0.4499\n",
      "     18                     \u001b[36m0.7407\u001b[0m        \u001b[32m0.5339\u001b[0m                     0.7383        0.5324  0.0020  0.4502\n",
      "     19                     0.7391        \u001b[32m0.5291\u001b[0m                     \u001b[35m0.7466\u001b[0m        0.5266  0.0015  0.4498\n",
      "     20                     \u001b[36m0.7449\u001b[0m        \u001b[32m0.5185\u001b[0m                     \u001b[35m0.7515\u001b[0m        \u001b[31m0.5258\u001b[0m  0.0010  0.4499\n",
      "     21                     \u001b[36m0.7557\u001b[0m        \u001b[32m0.5042\u001b[0m                     0.7436        \u001b[31m0.5225\u001b[0m  0.0007  0.4500\n",
      "     22                     0.7525        0.5097                     0.7454        \u001b[31m0.5205\u001b[0m  0.0004  0.4508\n",
      "     23                     \u001b[36m0.7598\u001b[0m        0.5065                     \u001b[35m0.7545\u001b[0m        \u001b[31m0.5204\u001b[0m  0.0002  0.4509\n",
      "     24                     \u001b[36m0.7601\u001b[0m        \u001b[32m0.5013\u001b[0m                     0.7475        0.5215  0.0000  0.4505\n",
      "     25                     0.7511        0.5071                     0.7477        \u001b[31m0.5200\u001b[0m  0.0000  0.4518\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5135\u001b[0m        \u001b[32m1.0474\u001b[0m                     \u001b[35m0.5284\u001b[0m        \u001b[31m0.7114\u001b[0m  0.0100  0.4438\n",
      "      2                     \u001b[36m0.5471\u001b[0m        \u001b[32m0.7694\u001b[0m                     0.4978        1.9860  0.0100  0.4499\n",
      "      3                     \u001b[36m0.5521\u001b[0m        0.9219                     \u001b[35m0.5379\u001b[0m        1.0028  0.0098  0.4479\n",
      "      4                     \u001b[36m0.6048\u001b[0m        \u001b[32m0.7183\u001b[0m                     \u001b[35m0.6339\u001b[0m        \u001b[31m0.6402\u001b[0m  0.0096  0.4499\n",
      "      5                     \u001b[36m0.6326\u001b[0m        \u001b[32m0.6996\u001b[0m                     \u001b[35m0.6510\u001b[0m        0.6420  0.0093  0.4492\n",
      "      6                     \u001b[36m0.6333\u001b[0m        0.7002                     \u001b[35m0.6986\u001b[0m        \u001b[31m0.5822\u001b[0m  0.0090  0.4498\n",
      "      7                     \u001b[36m0.6659\u001b[0m        \u001b[32m0.6399\u001b[0m                     0.6866        0.5838  0.0085  0.4498\n",
      "      8                     \u001b[36m0.6782\u001b[0m        \u001b[32m0.6322\u001b[0m                     0.6870        0.5926  0.0080  0.4503\n",
      "      9                     0.6729        0.6396                     0.6760        0.6202  0.0075  0.4499\n",
      "     10                     0.6752        \u001b[32m0.6257\u001b[0m                     \u001b[35m0.7082\u001b[0m        \u001b[31m0.5624\u001b[0m  0.0069  0.4488\n",
      "     11                     \u001b[36m0.6979\u001b[0m        \u001b[32m0.5966\u001b[0m                     \u001b[35m0.7343\u001b[0m        \u001b[31m0.5406\u001b[0m  0.0063  0.4494\n",
      "     12                     \u001b[36m0.7064\u001b[0m        \u001b[32m0.5788\u001b[0m                     0.7174        0.5500  0.0057  0.4498\n",
      "     13                     \u001b[36m0.7168\u001b[0m        \u001b[32m0.5738\u001b[0m                     0.7245        0.5520  0.0050  0.4508\n",
      "     14                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.5734\u001b[0m                     \u001b[35m0.7419\u001b[0m        \u001b[31m0.5359\u001b[0m  0.0043  0.4498\n",
      "     15                     \u001b[36m0.7216\u001b[0m        \u001b[32m0.5627\u001b[0m                     0.7301        0.5477  0.0037  0.4518\n",
      "     16                     \u001b[36m0.7335\u001b[0m        \u001b[32m0.5391\u001b[0m                     0.7359        \u001b[31m0.5304\u001b[0m  0.0031  0.4508\n",
      "     17                     \u001b[36m0.7362\u001b[0m        \u001b[32m0.5369\u001b[0m                     0.7353        0.5342  0.0025  0.4495\n",
      "     18                     \u001b[36m0.7436\u001b[0m        \u001b[32m0.5224\u001b[0m                     0.7282        0.5436  0.0020  0.4498\n",
      "     19                     \u001b[36m0.7580\u001b[0m        \u001b[32m0.5075\u001b[0m                     0.7344        \u001b[31m0.5200\u001b[0m  0.0015  0.4487\n",
      "     20                     0.7572        \u001b[32m0.5009\u001b[0m                     \u001b[35m0.7424\u001b[0m        \u001b[31m0.5198\u001b[0m  0.0010  0.4500\n",
      "     21                     \u001b[36m0.7622\u001b[0m        \u001b[32m0.4961\u001b[0m                     0.7236        0.5450  0.0007  0.4506\n",
      "     22                     \u001b[36m0.7642\u001b[0m        \u001b[32m0.4944\u001b[0m                     0.7337        0.5215  0.0004  0.4518\n",
      "     23                     \u001b[36m0.7702\u001b[0m        \u001b[32m0.4905\u001b[0m                     0.7367        \u001b[31m0.5163\u001b[0m  0.0002  0.4498\n",
      "     24                     \u001b[36m0.7758\u001b[0m        \u001b[32m0.4786\u001b[0m                     0.7399        \u001b[31m0.5149\u001b[0m  0.0000  0.4528\n",
      "     25                     0.7740        0.4859                     0.7344        0.5205  0.0000  0.4544\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5161\u001b[0m        \u001b[32m0.9963\u001b[0m                     \u001b[35m0.5553\u001b[0m        \u001b[31m0.6929\u001b[0m  0.0100  0.4438\n",
      "      2                     \u001b[36m0.5526\u001b[0m        \u001b[32m0.7807\u001b[0m                     \u001b[35m0.5902\u001b[0m        \u001b[31m0.6723\u001b[0m  0.0100  0.4488\n",
      "      3                     \u001b[36m0.5729\u001b[0m        \u001b[32m0.7781\u001b[0m                     0.5865        0.8061  0.0098  0.4488\n",
      "      4                     \u001b[36m0.5770\u001b[0m        0.8258                     \u001b[35m0.6120\u001b[0m        \u001b[31m0.6654\u001b[0m  0.0096  0.4488\n",
      "      5                     \u001b[36m0.5991\u001b[0m        \u001b[32m0.7447\u001b[0m                     0.5834        0.7276  0.0093  0.4535\n",
      "      6                     \u001b[36m0.6172\u001b[0m        0.7647                     \u001b[35m0.6557\u001b[0m        \u001b[31m0.6165\u001b[0m  0.0090  0.4498\n",
      "      7                     \u001b[36m0.6333\u001b[0m        \u001b[32m0.6714\u001b[0m                     \u001b[35m0.6839\u001b[0m        \u001b[31m0.5887\u001b[0m  0.0085  0.4508\n",
      "      8                     \u001b[36m0.6519\u001b[0m        \u001b[32m0.6504\u001b[0m                     \u001b[35m0.6977\u001b[0m        \u001b[31m0.5748\u001b[0m  0.0080  0.4488\n",
      "      9                     \u001b[36m0.6802\u001b[0m        \u001b[32m0.6222\u001b[0m                     0.6859        0.6360  0.0075  0.4488\n",
      "     10                     \u001b[36m0.6818\u001b[0m        \u001b[32m0.6166\u001b[0m                     \u001b[35m0.7188\u001b[0m        \u001b[31m0.5576\u001b[0m  0.0069  0.4478\n",
      "     11                     0.6810        \u001b[32m0.6159\u001b[0m                     0.7156        0.5587  0.0063  0.4508\n",
      "     12                     \u001b[36m0.6998\u001b[0m        \u001b[32m0.5883\u001b[0m                     \u001b[35m0.7247\u001b[0m        \u001b[31m0.5444\u001b[0m  0.0057  0.4498\n",
      "     13                     \u001b[36m0.7066\u001b[0m        \u001b[32m0.5738\u001b[0m                     0.6627        0.6032  0.0050  0.4488\n",
      "     14                     0.7062        0.5753                     0.6827        0.5849  0.0043  0.4507\n",
      "     15                     \u001b[36m0.7163\u001b[0m        0.5764                     \u001b[35m0.7327\u001b[0m        \u001b[31m0.5364\u001b[0m  0.0037  0.4498\n",
      "     16                     \u001b[36m0.7283\u001b[0m        \u001b[32m0.5526\u001b[0m                     0.7313        \u001b[31m0.5292\u001b[0m  0.0031  0.4508\n",
      "     17                     \u001b[36m0.7311\u001b[0m        \u001b[32m0.5412\u001b[0m                     0.7324        0.5387  0.0025  0.4498\n",
      "     18                     \u001b[36m0.7377\u001b[0m        \u001b[32m0.5382\u001b[0m                     0.7183        0.5755  0.0020  0.4489\n",
      "     19                     \u001b[36m0.7482\u001b[0m        \u001b[32m0.5202\u001b[0m                     \u001b[35m0.7404\u001b[0m        \u001b[31m0.5205\u001b[0m  0.0015  0.4518\n",
      "     20                     \u001b[36m0.7548\u001b[0m        \u001b[32m0.5181\u001b[0m                     \u001b[35m0.7416\u001b[0m        0.5265  0.0010  0.4508\n",
      "     21                     0.7507        \u001b[32m0.5104\u001b[0m                     \u001b[35m0.7445\u001b[0m        \u001b[31m0.5193\u001b[0m  0.0007  0.4524\n",
      "     22                     \u001b[36m0.7566\u001b[0m        \u001b[32m0.5082\u001b[0m                     0.7442        \u001b[31m0.5172\u001b[0m  0.0004  0.4489\n",
      "     23                     \u001b[36m0.7578\u001b[0m        0.5085                     \u001b[35m0.7481\u001b[0m        0.5176  0.0002  0.4508\n",
      "     24                     0.7548        0.5090                     0.7320        0.5460  0.0000  0.4498\n",
      "     25                     \u001b[36m0.7581\u001b[0m        \u001b[32m0.5036\u001b[0m                     0.7385        0.5330  0.0000  0.4498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5172\u001b[0m        \u001b[32m1.0335\u001b[0m                     \u001b[35m0.5396\u001b[0m        \u001b[31m0.7104\u001b[0m  0.0100  0.4458\n",
      "      2                     \u001b[36m0.5436\u001b[0m        \u001b[32m0.7816\u001b[0m                     \u001b[35m0.5678\u001b[0m        0.7150  0.0100  0.4511\n",
      "      3                     \u001b[36m0.5709\u001b[0m        \u001b[32m0.7723\u001b[0m                     \u001b[35m0.5875\u001b[0m        0.7209  0.0098  0.4508\n",
      "      4                     \u001b[36m0.5967\u001b[0m        \u001b[32m0.7226\u001b[0m                     0.5686        \u001b[31m0.6801\u001b[0m  0.0096  0.4508\n",
      "      5                     \u001b[36m0.6027\u001b[0m        \u001b[32m0.7153\u001b[0m                     \u001b[35m0.5903\u001b[0m        0.7140  0.0093  0.4498\n",
      "      6                     \u001b[36m0.6331\u001b[0m        \u001b[32m0.6927\u001b[0m                     \u001b[35m0.6492\u001b[0m        \u001b[31m0.6373\u001b[0m  0.0090  0.4508\n",
      "      7                     \u001b[36m0.6394\u001b[0m        \u001b[32m0.6766\u001b[0m                     \u001b[35m0.7016\u001b[0m        \u001b[31m0.5787\u001b[0m  0.0085  0.4499\n",
      "      8                     \u001b[36m0.6596\u001b[0m        \u001b[32m0.6558\u001b[0m                     0.6842        0.5979  0.0080  0.4498\n",
      "      9                     \u001b[36m0.6654\u001b[0m        \u001b[32m0.6375\u001b[0m                     \u001b[35m0.7162\u001b[0m        \u001b[31m0.5694\u001b[0m  0.0075  0.4479\n",
      "     10                     \u001b[36m0.6916\u001b[0m        \u001b[32m0.6098\u001b[0m                     0.6754        0.6227  0.0069  0.4509\n",
      "     11                     \u001b[36m0.6971\u001b[0m        \u001b[32m0.6083\u001b[0m                     \u001b[35m0.7225\u001b[0m        \u001b[31m0.5459\u001b[0m  0.0063  0.4489\n",
      "     12                     \u001b[36m0.7010\u001b[0m        \u001b[32m0.5837\u001b[0m                     0.7173        0.5606  0.0057  0.4508\n",
      "     13                     \u001b[36m0.7198\u001b[0m        \u001b[32m0.5672\u001b[0m                     0.7223        0.5585  0.0050  0.4486\n",
      "     14                     \u001b[36m0.7247\u001b[0m        \u001b[32m0.5551\u001b[0m                     \u001b[35m0.7239\u001b[0m        0.5581  0.0043  0.4488\n",
      "     15                     \u001b[36m0.7255\u001b[0m        0.5566                     \u001b[35m0.7327\u001b[0m        \u001b[31m0.5328\u001b[0m  0.0037  0.4498\n",
      "     16                     \u001b[36m0.7325\u001b[0m        \u001b[32m0.5392\u001b[0m                     \u001b[35m0.7342\u001b[0m        \u001b[31m0.5250\u001b[0m  0.0031  0.4490\n",
      "     17                     \u001b[36m0.7387\u001b[0m        \u001b[32m0.5382\u001b[0m                     \u001b[35m0.7466\u001b[0m        \u001b[31m0.5243\u001b[0m  0.0025  0.4478\n",
      "     18                     \u001b[36m0.7478\u001b[0m        \u001b[32m0.5218\u001b[0m                     0.7441        \u001b[31m0.5225\u001b[0m  0.0020  0.4499\n",
      "     19                     \u001b[36m0.7523\u001b[0m        0.5249                     0.7375        0.5225  0.0015  0.4479\n",
      "     20                     \u001b[36m0.7634\u001b[0m        \u001b[32m0.5002\u001b[0m                     0.7442        \u001b[31m0.5114\u001b[0m  0.0010  0.4502\n",
      "     21                     \u001b[36m0.7661\u001b[0m        \u001b[32m0.4951\u001b[0m                     \u001b[35m0.7477\u001b[0m        \u001b[31m0.5067\u001b[0m  0.0007  0.4515\n",
      "     22                     0.7642        \u001b[32m0.4868\u001b[0m                     \u001b[35m0.7537\u001b[0m        0.5082  0.0004  0.4518\n",
      "     23                     0.7646        0.4888                     \u001b[35m0.7547\u001b[0m        \u001b[31m0.5056\u001b[0m  0.0002  0.4508\n",
      "     24                     \u001b[36m0.7696\u001b[0m        \u001b[32m0.4855\u001b[0m                     \u001b[35m0.7589\u001b[0m        0.5061  0.0000  0.4498\n",
      "     25                     0.7576        0.4941                     0.7579        0.5060  0.0000  0.4498\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5077\u001b[0m        \u001b[32m1.0130\u001b[0m                     \u001b[35m0.5759\u001b[0m        \u001b[31m0.6884\u001b[0m  0.0100  0.4438\n",
      "      2                     \u001b[36m0.5328\u001b[0m        \u001b[32m0.8396\u001b[0m                     0.5674        0.8255  0.0100  0.4521\n",
      "      3                     \u001b[36m0.5836\u001b[0m        \u001b[32m0.7692\u001b[0m                     \u001b[35m0.6162\u001b[0m        \u001b[31m0.6753\u001b[0m  0.0098  0.4508\n",
      "      4                     \u001b[36m0.6193\u001b[0m        \u001b[32m0.7212\u001b[0m                     \u001b[35m0.6591\u001b[0m        \u001b[31m0.6388\u001b[0m  0.0096  0.4508\n",
      "      5                     0.6186        \u001b[32m0.7024\u001b[0m                     0.6375        \u001b[31m0.6303\u001b[0m  0.0093  0.4505\n",
      "      6                     \u001b[36m0.6493\u001b[0m        \u001b[32m0.6741\u001b[0m                     0.6437        0.7064  0.0090  0.4509\n",
      "      7                     \u001b[36m0.6546\u001b[0m        \u001b[32m0.6724\u001b[0m                     0.6391        0.6799  0.0085  0.4518\n",
      "      8                     \u001b[36m0.6735\u001b[0m        \u001b[32m0.6271\u001b[0m                     \u001b[35m0.6733\u001b[0m        0.6350  0.0080  0.4488\n",
      "      9                     0.6691        0.6357                     \u001b[35m0.7051\u001b[0m        \u001b[31m0.5808\u001b[0m  0.0075  0.4498\n",
      "     10                     \u001b[36m0.6997\u001b[0m        \u001b[32m0.6097\u001b[0m                     \u001b[35m0.7123\u001b[0m        \u001b[31m0.5676\u001b[0m  0.0069  0.4508\n",
      "     11                     0.6843        0.6268                     0.6825        0.6061  0.0063  0.4503\n",
      "     12                     \u001b[36m0.7088\u001b[0m        \u001b[32m0.5952\u001b[0m                     \u001b[35m0.7303\u001b[0m        \u001b[31m0.5427\u001b[0m  0.0057  0.4498\n",
      "     13                     0.7042        \u001b[32m0.5782\u001b[0m                     0.7230        0.5776  0.0050  0.4508\n",
      "     14                     \u001b[36m0.7265\u001b[0m        \u001b[32m0.5540\u001b[0m                     \u001b[35m0.7404\u001b[0m        \u001b[31m0.5338\u001b[0m  0.0043  0.4528\n",
      "     15                     \u001b[36m0.7328\u001b[0m        \u001b[32m0.5420\u001b[0m                     0.7297        \u001b[31m0.5299\u001b[0m  0.0037  0.4520\n",
      "     16                     \u001b[36m0.7344\u001b[0m        \u001b[32m0.5289\u001b[0m                     0.7227        0.5413  0.0031  0.4499\n",
      "     17                     \u001b[36m0.7473\u001b[0m        0.5347                     \u001b[35m0.7481\u001b[0m        \u001b[31m0.5279\u001b[0m  0.0025  0.4508\n",
      "     18                     \u001b[36m0.7488\u001b[0m        \u001b[32m0.5166\u001b[0m                     0.7437        \u001b[31m0.5278\u001b[0m  0.0020  0.4637\n",
      "     19                     0.7466        0.5230                     0.7465        \u001b[31m0.5230\u001b[0m  0.0015  0.4520\n",
      "     20                     \u001b[36m0.7510\u001b[0m        \u001b[32m0.5062\u001b[0m                     0.7404        \u001b[31m0.5197\u001b[0m  0.0010  0.4519\n",
      "     21                     \u001b[36m0.7622\u001b[0m        \u001b[32m0.5011\u001b[0m                     0.7455        0.5228  0.0007  0.4516\n",
      "     22                     0.7570        \u001b[32m0.4975\u001b[0m                     0.7451        0.5243  0.0004  0.4529\n",
      "     23                     \u001b[36m0.7700\u001b[0m        \u001b[32m0.4848\u001b[0m                     0.7477        \u001b[31m0.5192\u001b[0m  0.0002  0.4498\n",
      "     24                     0.7693        0.4972                     0.7442        \u001b[31m0.5192\u001b[0m  0.0000  0.4498\n",
      "     25                     0.7624        0.4927                     0.7442        0.5193  0.0000  0.4488\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5274\u001b[0m        \u001b[32m0.9998\u001b[0m                     \u001b[35m0.5328\u001b[0m        \u001b[31m0.7247\u001b[0m  0.0100  0.4469\n",
      "      2                     \u001b[36m0.5426\u001b[0m        \u001b[32m0.8850\u001b[0m                     \u001b[35m0.5497\u001b[0m        0.8817  0.0100  0.4528\n",
      "      3                     \u001b[36m0.5818\u001b[0m        \u001b[32m0.7540\u001b[0m                     \u001b[35m0.6080\u001b[0m        \u001b[31m0.6796\u001b[0m  0.0098  0.4532\n",
      "      4                     \u001b[36m0.5860\u001b[0m        0.7561                     \u001b[35m0.6440\u001b[0m        \u001b[31m0.6341\u001b[0m  0.0096  0.4528\n",
      "      5                     \u001b[36m0.6336\u001b[0m        \u001b[32m0.6954\u001b[0m                     0.6414        \u001b[31m0.6178\u001b[0m  0.0093  0.4528\n",
      "      6                     \u001b[36m0.6404\u001b[0m        \u001b[32m0.6948\u001b[0m                     0.6138        0.6963  0.0090  0.4528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7                     \u001b[36m0.6490\u001b[0m        \u001b[32m0.6657\u001b[0m                     \u001b[35m0.6706\u001b[0m        \u001b[31m0.5909\u001b[0m  0.0085  0.4517\n",
      "      8                     \u001b[36m0.6673\u001b[0m        \u001b[32m0.6465\u001b[0m                     0.6449        0.6544  0.0080  0.4508\n",
      "      9                     \u001b[36m0.6752\u001b[0m        \u001b[32m0.6293\u001b[0m                     \u001b[35m0.6884\u001b[0m        0.6009  0.0075  0.4528\n",
      "     10                     0.6740        0.6324                     0.6726        0.5951  0.0069  0.4512\n",
      "     11                     \u001b[36m0.6754\u001b[0m        \u001b[32m0.6247\u001b[0m                     \u001b[35m0.7098\u001b[0m        \u001b[31m0.5713\u001b[0m  0.0063  0.4519\n",
      "     12                     \u001b[36m0.6978\u001b[0m        \u001b[32m0.5935\u001b[0m                     0.7070        0.5740  0.0057  0.4526\n",
      "     13                     \u001b[36m0.7059\u001b[0m        \u001b[32m0.5794\u001b[0m                     \u001b[35m0.7152\u001b[0m        0.5742  0.0050  0.4539\n",
      "     14                     \u001b[36m0.7120\u001b[0m        \u001b[32m0.5782\u001b[0m                     \u001b[35m0.7166\u001b[0m        \u001b[31m0.5586\u001b[0m  0.0043  0.4538\n",
      "     15                     \u001b[36m0.7296\u001b[0m        \u001b[32m0.5445\u001b[0m                     \u001b[35m0.7188\u001b[0m        \u001b[31m0.5530\u001b[0m  0.0037  0.4520\n",
      "     16                     \u001b[36m0.7420\u001b[0m        \u001b[32m0.5383\u001b[0m                     \u001b[35m0.7361\u001b[0m        \u001b[31m0.5445\u001b[0m  0.0031  0.4508\n",
      "     17                     0.7285        0.5427                     0.7278        0.5503  0.0025  0.4817\n",
      "     18                     \u001b[36m0.7507\u001b[0m        \u001b[32m0.5184\u001b[0m                     \u001b[35m0.7411\u001b[0m        \u001b[31m0.5379\u001b[0m  0.0020  0.4518\n",
      "     19                     0.7443        \u001b[32m0.5175\u001b[0m                     \u001b[35m0.7441\u001b[0m        0.5405  0.0015  0.4528\n",
      "     20                     \u001b[36m0.7542\u001b[0m        \u001b[32m0.5047\u001b[0m                     0.7427        \u001b[31m0.5377\u001b[0m  0.0010  0.4528\n",
      "     21                     \u001b[36m0.7635\u001b[0m        \u001b[32m0.4997\u001b[0m                     0.7253        0.5449  0.0007  0.4716\n",
      "     22                     0.7593        \u001b[32m0.4993\u001b[0m                     \u001b[35m0.7476\u001b[0m        0.5381  0.0004  0.4518\n",
      "     23                     0.7548        0.5060                     0.7433        \u001b[31m0.5352\u001b[0m  0.0002  0.4527\n",
      "     24                     0.7464        0.5070                     0.7262        0.5447  0.0000  0.4524\n",
      "     25                     0.7594        0.5060                     0.7468        0.5354  0.0000  0.4498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 1 1 ... 1 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5572\u001b[0m        \u001b[32m0.9904\u001b[0m                     \u001b[35m0.6490\u001b[0m        \u001b[31m0.7088\u001b[0m  0.0100  0.5223\n",
      "      2                     \u001b[36m0.6453\u001b[0m        \u001b[32m0.7653\u001b[0m                     \u001b[35m0.6555\u001b[0m        \u001b[31m0.6958\u001b[0m  0.0100  0.2723\n",
      "      3                     \u001b[36m0.6604\u001b[0m        \u001b[32m0.7346\u001b[0m                     \u001b[35m0.6828\u001b[0m        \u001b[31m0.6234\u001b[0m  0.0098  0.2701\n",
      "      4                     \u001b[36m0.6808\u001b[0m        \u001b[32m0.6988\u001b[0m                     0.6791        \u001b[31m0.5974\u001b[0m  0.0096  0.2713\n",
      "      5                     0.6513        0.7381                     \u001b[35m0.7080\u001b[0m        \u001b[31m0.5776\u001b[0m  0.0093  0.2713\n",
      "      6                     \u001b[36m0.6871\u001b[0m        \u001b[32m0.6408\u001b[0m                     \u001b[35m0.7147\u001b[0m        0.6312  0.0090  0.2719\n",
      "      7                     \u001b[36m0.7119\u001b[0m        \u001b[32m0.6230\u001b[0m                     0.7124        0.6103  0.0085  0.2713\n",
      "      8                     0.6970        0.6536                     0.7143        0.5802  0.0080  0.2713\n",
      "      9                     \u001b[36m0.7230\u001b[0m        \u001b[32m0.5920\u001b[0m                     \u001b[35m0.7159\u001b[0m        \u001b[31m0.5385\u001b[0m  0.0075  0.2703\n",
      "     10                     \u001b[36m0.7361\u001b[0m        \u001b[32m0.5557\u001b[0m                     \u001b[35m0.7272\u001b[0m        0.5438  0.0069  0.2723\n",
      "     11                     \u001b[36m0.7397\u001b[0m        0.5650                     \u001b[35m0.7367\u001b[0m        0.5478  0.0063  0.2694\n",
      "     12                     \u001b[36m0.7495\u001b[0m        \u001b[32m0.5093\u001b[0m                     \u001b[35m0.7380\u001b[0m        \u001b[31m0.5297\u001b[0m  0.0057  0.2713\n",
      "     13                     0.7456        0.5166                     0.7182        0.5668  0.0050  0.2702\n",
      "     14                     \u001b[36m0.7574\u001b[0m        \u001b[32m0.5088\u001b[0m                     0.7317        0.5519  0.0043  0.2703\n",
      "     15                     \u001b[36m0.7759\u001b[0m        \u001b[32m0.4822\u001b[0m                     0.7207        0.5538  0.0037  0.2723\n",
      "     16                     0.7721        0.4825                     \u001b[35m0.7396\u001b[0m        \u001b[31m0.5011\u001b[0m  0.0031  0.2693\n",
      "     17                     \u001b[36m0.7786\u001b[0m        0.4831                     \u001b[35m0.7490\u001b[0m        0.5283  0.0025  0.2733\n",
      "     18                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4592\u001b[0m                     0.7433        0.5126  0.0020  0.2703\n",
      "     19                     \u001b[36m0.7880\u001b[0m        \u001b[32m0.4536\u001b[0m                     0.7442        0.5164  0.0015  0.2715\n",
      "     20                     \u001b[36m0.8019\u001b[0m        \u001b[32m0.4411\u001b[0m                     \u001b[35m0.7512\u001b[0m        0.5112  0.0010  0.2693\n",
      "     21                     0.7959        0.4424                     0.7490        0.5151  0.0007  0.2715\n",
      "     22                     0.7998        \u001b[32m0.4267\u001b[0m                     0.7456        0.5156  0.0004  0.2703\n",
      "     23                     \u001b[36m0.8046\u001b[0m        0.4305                     0.7486        0.5152  0.0002  0.2713\n",
      "     24                     \u001b[36m0.8052\u001b[0m        \u001b[32m0.4174\u001b[0m                     0.7505        0.5147  0.0000  0.2710\n",
      "     25                     \u001b[36m0.8082\u001b[0m        0.4229                     0.7473        0.5147  0.0000  0.2713\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5670\u001b[0m        \u001b[32m1.0897\u001b[0m                     \u001b[35m0.6254\u001b[0m        \u001b[31m0.7602\u001b[0m  0.0100  0.2643\n",
      "      2                     \u001b[36m0.6264\u001b[0m        \u001b[32m0.8207\u001b[0m                     \u001b[35m0.6446\u001b[0m        \u001b[31m0.6750\u001b[0m  0.0100  0.2723\n",
      "      3                     \u001b[36m0.6440\u001b[0m        \u001b[32m0.8143\u001b[0m                     \u001b[35m0.6842\u001b[0m        0.6934  0.0098  0.2689\n",
      "      4                     \u001b[36m0.6855\u001b[0m        \u001b[32m0.6875\u001b[0m                     0.6518        0.7414  0.0096  0.2713\n",
      "      5                     \u001b[36m0.6917\u001b[0m        \u001b[32m0.6752\u001b[0m                     0.6773        0.6926  0.0093  0.2703\n",
      "      6                     \u001b[36m0.7076\u001b[0m        \u001b[32m0.6367\u001b[0m                     \u001b[35m0.7316\u001b[0m        \u001b[31m0.5647\u001b[0m  0.0090  0.2704\n",
      "      7                     \u001b[36m0.7093\u001b[0m        \u001b[32m0.5898\u001b[0m                     0.7040        0.5751  0.0085  0.2696\n",
      "      8                     \u001b[36m0.7300\u001b[0m        \u001b[32m0.5626\u001b[0m                     0.7071        \u001b[31m0.5624\u001b[0m  0.0080  0.2708\n",
      "      9                     0.7201        0.5948                     0.7181        0.5654  0.0075  0.2743\n",
      "     10                     \u001b[36m0.7490\u001b[0m        \u001b[32m0.5485\u001b[0m                     0.7188        \u001b[31m0.5576\u001b[0m  0.0069  0.2703\n",
      "     11                     0.7200        0.6557                     \u001b[35m0.7420\u001b[0m        \u001b[31m0.5344\u001b[0m  0.0063  0.2714\n",
      "     12                     0.7452        0.5551                     0.6682        0.6653  0.0057  0.2697\n",
      "     13                     0.7405        0.5583                     0.7269        0.5693  0.0050  0.2723\n",
      "     14                     0.7463        \u001b[32m0.5185\u001b[0m                     0.7317        0.5551  0.0043  0.2703\n",
      "     15                     \u001b[36m0.7843\u001b[0m        \u001b[32m0.4738\u001b[0m                     0.7234        0.5565  0.0037  0.2723\n",
      "     16                     0.7657        0.4970                     0.7231        0.5618  0.0031  0.2703\n",
      "     17                     0.7823        \u001b[32m0.4679\u001b[0m                     0.7107        0.5844  0.0025  0.2723\n",
      "     18                     \u001b[36m0.7847\u001b[0m        \u001b[32m0.4658\u001b[0m                     0.7116        0.5451  0.0020  0.2718\n",
      "     19                     \u001b[36m0.7947\u001b[0m        \u001b[32m0.4578\u001b[0m                     0.7096        0.5497  0.0015  0.2723\n",
      "     20                     0.7938        \u001b[32m0.4437\u001b[0m                     0.7209        0.5512  0.0010  0.2716\n",
      "     21                     \u001b[36m0.7992\u001b[0m        \u001b[32m0.4375\u001b[0m                     0.7238        0.5405  0.0007  0.2723\n",
      "     22                     0.7939        0.4437                     0.7261        0.5418  0.0004  0.2703\n",
      "     23                     \u001b[36m0.8060\u001b[0m        \u001b[32m0.4211\u001b[0m                     0.7253        0.5420  0.0002  0.2733\n",
      "     24                     0.7949        0.4392                     0.7253        0.5429  0.0000  0.2713\n",
      "     25                     0.8041        0.4270                     0.7292        0.5427  0.0000  0.2703\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5664\u001b[0m        \u001b[32m0.9967\u001b[0m                     \u001b[35m0.6422\u001b[0m        \u001b[31m0.7032\u001b[0m  0.0100  0.2663\n",
      "      2                     \u001b[36m0.6309\u001b[0m        \u001b[32m0.7414\u001b[0m                     \u001b[35m0.6764\u001b[0m        \u001b[31m0.6715\u001b[0m  0.0100  0.2703\n",
      "      3                     \u001b[36m0.6830\u001b[0m        \u001b[32m0.6657\u001b[0m                     0.6682        \u001b[31m0.6328\u001b[0m  0.0098  0.2733\n",
      "      4                     0.6704        0.6757                     \u001b[35m0.6913\u001b[0m        \u001b[31m0.5833\u001b[0m  0.0096  0.2703\n",
      "      5                     \u001b[36m0.6874\u001b[0m        0.6658                     0.6844        0.6366  0.0093  0.2713\n",
      "      6                     \u001b[36m0.7026\u001b[0m        \u001b[32m0.6462\u001b[0m                     \u001b[35m0.6994\u001b[0m        0.5837  0.0090  0.2693\n",
      "      7                     0.6992        0.6475                     \u001b[35m0.7225\u001b[0m        \u001b[31m0.5573\u001b[0m  0.0085  0.2723\n",
      "      8                     \u001b[36m0.7164\u001b[0m        \u001b[32m0.6076\u001b[0m                     0.7210        0.5672  0.0080  0.2703\n",
      "      9                     \u001b[36m0.7275\u001b[0m        \u001b[32m0.5869\u001b[0m                     0.7065        0.6887  0.0075  0.2723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     \u001b[36m0.7484\u001b[0m        \u001b[32m0.5537\u001b[0m                     \u001b[35m0.7333\u001b[0m        \u001b[31m0.5489\u001b[0m  0.0069  0.2713\n",
      "     11                     0.7326        0.5558                     0.7119        0.5549  0.0063  0.2731\n",
      "     12                     \u001b[36m0.7620\u001b[0m        \u001b[32m0.5107\u001b[0m                     0.7172        0.5909  0.0057  0.2703\n",
      "     13                     0.7549        0.5598                     0.7090        0.5594  0.0050  0.2723\n",
      "     14                     0.7609        \u001b[32m0.4960\u001b[0m                     0.7096        0.6666  0.0043  0.2703\n",
      "     15                     \u001b[36m0.7665\u001b[0m        0.5011                     0.7285        0.5523  0.0037  0.2726\n",
      "     16                     \u001b[36m0.7804\u001b[0m        \u001b[32m0.4738\u001b[0m                     \u001b[35m0.7610\u001b[0m        0.5768  0.0031  0.2713\n",
      "     17                     \u001b[36m0.7921\u001b[0m        \u001b[32m0.4633\u001b[0m                     0.7363        \u001b[31m0.5324\u001b[0m  0.0025  0.2703\n",
      "     18                     \u001b[36m0.8004\u001b[0m        \u001b[32m0.4484\u001b[0m                     \u001b[35m0.7644\u001b[0m        \u001b[31m0.5250\u001b[0m  0.0020  0.2713\n",
      "     19                     \u001b[36m0.8069\u001b[0m        \u001b[32m0.4331\u001b[0m                     0.7389        \u001b[31m0.5228\u001b[0m  0.0015  0.2708\n",
      "     20                     0.8009        \u001b[32m0.4249\u001b[0m                     0.7374        \u001b[31m0.5212\u001b[0m  0.0010  0.2723\n",
      "     21                     \u001b[36m0.8126\u001b[0m        \u001b[32m0.4188\u001b[0m                     0.7445        0.5241  0.0007  0.2703\n",
      "     22                     0.8047        0.4216                     0.7456        0.5247  0.0004  0.2723\n",
      "     23                     \u001b[36m0.8156\u001b[0m        \u001b[32m0.4143\u001b[0m                     0.7407        \u001b[31m0.5207\u001b[0m  0.0002  0.2694\n",
      "     24                     0.8134        0.4181                     0.7442        0.5212  0.0000  0.2723\n",
      "     25                     0.8094        0.4189                     0.7445        0.5218  0.0000  0.2713\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5700\u001b[0m        \u001b[32m1.0230\u001b[0m                     \u001b[35m0.5704\u001b[0m        \u001b[31m0.7234\u001b[0m  0.0100  0.2663\n",
      "      2                     \u001b[36m0.6155\u001b[0m        \u001b[32m0.8132\u001b[0m                     \u001b[35m0.6754\u001b[0m        \u001b[31m0.6143\u001b[0m  0.0100  0.2704\n",
      "      3                     \u001b[36m0.6471\u001b[0m        \u001b[32m0.7292\u001b[0m                     \u001b[35m0.7023\u001b[0m        \u001b[31m0.6051\u001b[0m  0.0098  0.2721\n",
      "      4                     \u001b[36m0.6662\u001b[0m        \u001b[32m0.6893\u001b[0m                     \u001b[35m0.7497\u001b[0m        \u001b[31m0.5325\u001b[0m  0.0096  0.2703\n",
      "      5                     \u001b[36m0.6940\u001b[0m        \u001b[32m0.6316\u001b[0m                     0.7103        0.6470  0.0093  0.2713\n",
      "      6                     0.6795        0.7012                     0.7245        0.5379  0.0090  0.2695\n",
      "      7                     \u001b[36m0.7171\u001b[0m        \u001b[32m0.6038\u001b[0m                     0.7437        \u001b[31m0.5212\u001b[0m  0.0085  0.2713\n",
      "      8                     \u001b[36m0.7207\u001b[0m        \u001b[32m0.5826\u001b[0m                     0.7283        0.5247  0.0080  0.2713\n",
      "      9                     \u001b[36m0.7345\u001b[0m        \u001b[32m0.5746\u001b[0m                     \u001b[35m0.7524\u001b[0m        \u001b[31m0.5142\u001b[0m  0.0075  0.2703\n",
      "     10                     0.7329        \u001b[32m0.5670\u001b[0m                     \u001b[35m0.7612\u001b[0m        \u001b[31m0.5040\u001b[0m  0.0069  0.2733\n",
      "     11                     \u001b[36m0.7371\u001b[0m        \u001b[32m0.5590\u001b[0m                     \u001b[35m0.7663\u001b[0m        0.5207  0.0063  0.2703\n",
      "     12                     \u001b[36m0.7504\u001b[0m        \u001b[32m0.5329\u001b[0m                     0.7445        0.5757  0.0057  0.2723\n",
      "     13                     0.7484        0.5403                     0.7217        0.5726  0.0050  0.2703\n",
      "     14                     \u001b[36m0.7661\u001b[0m        \u001b[32m0.5020\u001b[0m                     0.7520        0.5205  0.0043  0.2723\n",
      "     15                     0.7656        \u001b[32m0.4906\u001b[0m                     0.7625        0.5086  0.0037  0.2703\n",
      "     16                     \u001b[36m0.7898\u001b[0m        \u001b[32m0.4459\u001b[0m                     \u001b[35m0.7754\u001b[0m        0.5043  0.0031  0.2746\n",
      "     17                     0.7871        0.4495                     0.7685        \u001b[31m0.5020\u001b[0m  0.0025  0.2703\n",
      "     18                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4316\u001b[0m                     0.7580        0.5122  0.0020  0.2733\n",
      "     19                     0.7886        0.4480                     0.7631        \u001b[31m0.5016\u001b[0m  0.0015  0.2713\n",
      "     20                     0.8027        \u001b[32m0.4237\u001b[0m                     0.7613        \u001b[31m0.4930\u001b[0m  0.0010  0.2724\n",
      "     21                     \u001b[36m0.8136\u001b[0m        \u001b[32m0.4022\u001b[0m                     0.7659        0.4931  0.0007  0.2713\n",
      "     22                     \u001b[36m0.8164\u001b[0m        0.4082                     0.7606        0.4937  0.0004  0.2720\n",
      "     23                     0.8067        0.4180                     0.7695        \u001b[31m0.4924\u001b[0m  0.0002  0.2703\n",
      "     24                     \u001b[36m0.8189\u001b[0m        \u001b[32m0.3940\u001b[0m                     0.7619        \u001b[31m0.4918\u001b[0m  0.0000  0.2723\n",
      "     25                     0.8167        0.3995                     0.7659        0.4921  0.0000  0.2713\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5719\u001b[0m        \u001b[32m0.9665\u001b[0m                     \u001b[35m0.6279\u001b[0m        \u001b[31m0.7863\u001b[0m  0.0100  0.2633\n",
      "      2                     \u001b[36m0.6267\u001b[0m        \u001b[32m0.8665\u001b[0m                     \u001b[35m0.7121\u001b[0m        \u001b[31m0.5733\u001b[0m  0.0100  0.2744\n",
      "      3                     \u001b[36m0.6513\u001b[0m        \u001b[32m0.7491\u001b[0m                     \u001b[35m0.7134\u001b[0m        0.5907  0.0098  0.2703\n",
      "      4                     \u001b[36m0.6812\u001b[0m        \u001b[32m0.6586\u001b[0m                     0.7094        0.5805  0.0096  0.2723\n",
      "      5                     \u001b[36m0.6955\u001b[0m        \u001b[32m0.6310\u001b[0m                     0.7130        0.6078  0.0093  0.2703\n",
      "      6                     \u001b[36m0.7005\u001b[0m        0.6502                     0.7089        0.6219  0.0090  0.2713\n",
      "      7                     \u001b[36m0.7189\u001b[0m        \u001b[32m0.5742\u001b[0m                     \u001b[35m0.7159\u001b[0m        0.6495  0.0085  0.2703\n",
      "      8                     0.7039        0.6475                     \u001b[35m0.7345\u001b[0m        \u001b[31m0.5628\u001b[0m  0.0080  0.2723\n",
      "      9                     \u001b[36m0.7202\u001b[0m        0.6183                     0.7149        \u001b[31m0.5515\u001b[0m  0.0075  0.2703\n",
      "     10                     \u001b[36m0.7294\u001b[0m        \u001b[32m0.5712\u001b[0m                     \u001b[35m0.7615\u001b[0m        \u001b[31m0.5169\u001b[0m  0.0069  0.2733\n",
      "     11                     0.7250        \u001b[32m0.5656\u001b[0m                     0.7301        0.5447  0.0063  0.2703\n",
      "     12                     \u001b[36m0.7382\u001b[0m        \u001b[32m0.5504\u001b[0m                     0.7345        0.5812  0.0057  0.2733\n",
      "     13                     \u001b[36m0.7629\u001b[0m        \u001b[32m0.5163\u001b[0m                     0.7536        0.5236  0.0050  0.2703\n",
      "     14                     \u001b[36m0.7637\u001b[0m        \u001b[32m0.5140\u001b[0m                     0.7346        0.5315  0.0043  0.2713\n",
      "     15                     \u001b[36m0.7658\u001b[0m        \u001b[32m0.5027\u001b[0m                     0.7358        0.5240  0.0037  0.2703\n",
      "     16                     \u001b[36m0.7806\u001b[0m        \u001b[32m0.4732\u001b[0m                     0.7530        0.5263  0.0031  0.2723\n",
      "     17                     \u001b[36m0.7873\u001b[0m        \u001b[32m0.4486\u001b[0m                     \u001b[35m0.7624\u001b[0m        0.5296  0.0025  0.2713\n",
      "     18                     \u001b[36m0.7964\u001b[0m        \u001b[32m0.4416\u001b[0m                     \u001b[35m0.7736\u001b[0m        \u001b[31m0.5048\u001b[0m  0.0020  0.2733\n",
      "     19                     0.7917        0.4460                     0.7625        0.5061  0.0015  0.2713\n",
      "     20                     \u001b[36m0.7999\u001b[0m        \u001b[32m0.4369\u001b[0m                     0.7517        0.5173  0.0010  0.2691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21                     \u001b[36m0.8032\u001b[0m        \u001b[32m0.4249\u001b[0m                     0.7725        0.5095  0.0007  0.2730\n",
      "     22                     0.7960        0.4287                     0.7723        \u001b[31m0.5040\u001b[0m  0.0004  0.2703\n",
      "     23                     0.7989        \u001b[32m0.4161\u001b[0m                     0.7722        0.5044  0.0002  0.2723\n",
      "     24                     0.8016        0.4196                     0.7713        \u001b[31m0.5031\u001b[0m  0.0000  0.2703\n",
      "     25                     \u001b[36m0.8067\u001b[0m        0.4187                     0.7719        0.5048  0.0000  0.2723\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5673\u001b[0m        \u001b[32m1.0510\u001b[0m                     \u001b[35m0.6559\u001b[0m        \u001b[31m0.6425\u001b[0m  0.0100  0.2633\n",
      "      2                     \u001b[36m0.6328\u001b[0m        \u001b[32m0.7830\u001b[0m                     \u001b[35m0.6564\u001b[0m        \u001b[31m0.6290\u001b[0m  0.0100  0.2733\n",
      "      3                     \u001b[36m0.6490\u001b[0m        \u001b[32m0.7489\u001b[0m                     \u001b[35m0.7027\u001b[0m        \u001b[31m0.5644\u001b[0m  0.0098  0.2703\n",
      "      4                     \u001b[36m0.6728\u001b[0m        \u001b[32m0.6799\u001b[0m                     0.6757        0.6535  0.0096  0.2733\n",
      "      5                     0.6717        0.6926                     0.6307        0.7553  0.0093  0.2703\n",
      "      6                     \u001b[36m0.6827\u001b[0m        0.7048                     \u001b[35m0.7094\u001b[0m        0.5862  0.0090  0.2733\n",
      "      7                     \u001b[36m0.7139\u001b[0m        \u001b[32m0.6385\u001b[0m                     0.6748        0.7950  0.0085  0.2703\n",
      "      8                     0.7138        \u001b[32m0.6321\u001b[0m                     \u001b[35m0.7338\u001b[0m        \u001b[31m0.5382\u001b[0m  0.0080  0.2723\n",
      "      9                     \u001b[36m0.7379\u001b[0m        \u001b[32m0.5580\u001b[0m                     0.7085        0.5695  0.0075  0.2703\n",
      "     10                     0.7234        0.5756                     0.6769        0.6825  0.0069  0.2703\n",
      "     11                     0.7166        0.5953                     \u001b[35m0.7361\u001b[0m        \u001b[31m0.5327\u001b[0m  0.0063  0.2723\n",
      "     12                     0.7359        \u001b[32m0.5577\u001b[0m                     0.7119        0.5606  0.0057  0.2713\n",
      "     13                     \u001b[36m0.7481\u001b[0m        \u001b[32m0.5107\u001b[0m                     \u001b[35m0.7446\u001b[0m        \u001b[31m0.5155\u001b[0m  0.0050  0.2714\n",
      "     14                     \u001b[36m0.7509\u001b[0m        0.5122                     \u001b[35m0.7492\u001b[0m        0.5281  0.0043  0.2703\n",
      "     15                     \u001b[36m0.7730\u001b[0m        \u001b[32m0.4850\u001b[0m                     \u001b[35m0.7514\u001b[0m        \u001b[31m0.5095\u001b[0m  0.0037  0.2713\n",
      "     16                     0.7716        0.4937                     \u001b[35m0.7575\u001b[0m        \u001b[31m0.5032\u001b[0m  0.0031  0.2683\n",
      "     17                     0.7711        \u001b[32m0.4715\u001b[0m                     0.7556        0.5228  0.0025  0.2703\n",
      "     18                     0.7693        0.4715                     \u001b[35m0.7590\u001b[0m        \u001b[31m0.4987\u001b[0m  0.0020  0.2693\n",
      "     19                     \u001b[36m0.7967\u001b[0m        \u001b[32m0.4445\u001b[0m                     0.7562        0.5112  0.0015  0.2723\n",
      "     20                     0.7865        0.4473                     0.7462        0.5016  0.0010  0.2713\n",
      "     21                     \u001b[36m0.8095\u001b[0m        \u001b[32m0.4327\u001b[0m                     \u001b[35m0.7600\u001b[0m        0.4990  0.0007  0.2714\n",
      "     22                     0.7960        0.4450                     0.7580        \u001b[31m0.4957\u001b[0m  0.0004  0.2693\n",
      "     23                     0.8031        \u001b[32m0.4259\u001b[0m                     0.7593        0.4994  0.0002  0.2710\n",
      "     24                     0.8004        \u001b[32m0.4253\u001b[0m                     \u001b[35m0.7656\u001b[0m        \u001b[31m0.4928\u001b[0m  0.0000  0.2703\n",
      "     25                     0.8065        0.4257                     0.7572        0.4975  0.0000  0.2733\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5702\u001b[0m        \u001b[32m0.9812\u001b[0m                     \u001b[35m0.6997\u001b[0m        \u001b[31m0.5699\u001b[0m  0.0100  0.2633\n",
      "      2                     \u001b[36m0.6321\u001b[0m        \u001b[32m0.7873\u001b[0m                     0.6707        0.6000  0.0100  0.2703\n",
      "      3                     \u001b[36m0.6590\u001b[0m        \u001b[32m0.7037\u001b[0m                     0.6115        0.9695  0.0098  0.2718\n",
      "      4                     0.6357        0.9874                     \u001b[35m0.7114\u001b[0m        0.5793  0.0096  0.2730\n",
      "      5                     \u001b[36m0.6737\u001b[0m        0.7337                     0.7106        \u001b[31m0.5664\u001b[0m  0.0093  0.2712\n",
      "      6                     \u001b[36m0.6898\u001b[0m        \u001b[32m0.6554\u001b[0m                     \u001b[35m0.7204\u001b[0m        \u001b[31m0.5662\u001b[0m  0.0090  0.2703\n",
      "      7                     \u001b[36m0.6994\u001b[0m        \u001b[32m0.6129\u001b[0m                     \u001b[35m0.7454\u001b[0m        \u001b[31m0.4974\u001b[0m  0.0085  0.2713\n",
      "      8                     0.6818        0.6573                     0.7328        0.5261  0.0080  0.2713\n",
      "      9                     \u001b[36m0.7033\u001b[0m        0.6137                     0.7276        0.5434  0.0075  0.2753\n",
      "     10                     \u001b[36m0.7117\u001b[0m        0.6289                     \u001b[35m0.7550\u001b[0m        0.5218  0.0069  0.2703\n",
      "     11                     \u001b[36m0.7396\u001b[0m        \u001b[32m0.5572\u001b[0m                     0.7547        0.4987  0.0063  0.2723\n",
      "     12                     \u001b[36m0.7421\u001b[0m        \u001b[32m0.5374\u001b[0m                     \u001b[35m0.7649\u001b[0m        \u001b[31m0.4945\u001b[0m  0.0057  0.2713\n",
      "     13                     0.7278        0.5520                     0.7222        0.5531  0.0050  0.2723\n",
      "     14                     0.7395        \u001b[32m0.5304\u001b[0m                     0.7540        0.4981  0.0043  0.2703\n",
      "     15                     \u001b[36m0.7518\u001b[0m        \u001b[32m0.5173\u001b[0m                     0.7512        0.4997  0.0037  0.2721\n",
      "     16                     0.7447        0.5394                     0.7591        \u001b[31m0.4844\u001b[0m  0.0031  0.2703\n",
      "     17                     \u001b[36m0.7573\u001b[0m        \u001b[32m0.5108\u001b[0m                     \u001b[35m0.7731\u001b[0m        \u001b[31m0.4816\u001b[0m  0.0025  0.2714\n",
      "     18                     \u001b[36m0.7615\u001b[0m        \u001b[32m0.5075\u001b[0m                     0.7597        \u001b[31m0.4709\u001b[0m  0.0020  0.2713\n",
      "     19                     \u001b[36m0.7772\u001b[0m        \u001b[32m0.4782\u001b[0m                     \u001b[35m0.7775\u001b[0m        0.4733  0.0015  0.2723\n",
      "     20                     \u001b[36m0.7785\u001b[0m        0.4794                     0.7754        \u001b[31m0.4676\u001b[0m  0.0010  0.2711\n",
      "     21                     \u001b[36m0.7835\u001b[0m        \u001b[32m0.4723\u001b[0m                     \u001b[35m0.7811\u001b[0m        0.4697  0.0007  0.2712\n",
      "     22                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.4647\u001b[0m                     0.7805        \u001b[31m0.4654\u001b[0m  0.0004  0.2723\n",
      "     23                     \u001b[36m0.7888\u001b[0m        \u001b[32m0.4616\u001b[0m                     \u001b[35m0.7829\u001b[0m        0.4659  0.0002  0.2713\n",
      "     24                     0.7800        0.4634                     0.7772        \u001b[31m0.4652\u001b[0m  0.0000  0.2723\n",
      "     25                     \u001b[36m0.7891\u001b[0m        \u001b[32m0.4507\u001b[0m                     0.7813        \u001b[31m0.4646\u001b[0m  0.0000  0.2703\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5779\u001b[0m        \u001b[32m1.0366\u001b[0m                     \u001b[35m0.6319\u001b[0m        \u001b[31m0.8869\u001b[0m  0.0100  0.2663\n",
      "      2                     \u001b[36m0.6308\u001b[0m        \u001b[32m0.9125\u001b[0m                     \u001b[35m0.6542\u001b[0m        \u001b[31m0.7258\u001b[0m  0.0100  0.2719\n",
      "      3                     \u001b[36m0.6813\u001b[0m        \u001b[32m0.7486\u001b[0m                     \u001b[35m0.7049\u001b[0m        \u001b[31m0.5958\u001b[0m  0.0098  0.2713\n",
      "      4                     \u001b[36m0.6861\u001b[0m        \u001b[32m0.6738\u001b[0m                     0.6830        \u001b[31m0.5877\u001b[0m  0.0096  0.2713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5                     \u001b[36m0.7035\u001b[0m        \u001b[32m0.6262\u001b[0m                     0.6313        0.7347  0.0093  0.2733\n",
      "      6                     0.6940        0.6540                     0.6720        0.6434  0.0090  0.2713\n",
      "      7                     \u001b[36m0.7076\u001b[0m        \u001b[32m0.6082\u001b[0m                     0.7020        0.6013  0.0085  0.2723\n",
      "      8                     \u001b[36m0.7386\u001b[0m        \u001b[32m0.5530\u001b[0m                     0.7046        0.6795  0.0080  0.2703\n",
      "      9                     0.7155        0.6154                     0.6829        0.6162  0.0075  0.2723\n",
      "     10                     \u001b[36m0.7424\u001b[0m        0.5822                     0.6892        0.6272  0.0069  0.2704\n",
      "     11                     0.7391        \u001b[32m0.5441\u001b[0m                     \u001b[35m0.7066\u001b[0m        \u001b[31m0.5726\u001b[0m  0.0063  0.2733\n",
      "     12                     0.7420        \u001b[32m0.5283\u001b[0m                     0.6967        0.6033  0.0057  0.2713\n",
      "     13                     \u001b[36m0.7606\u001b[0m        \u001b[32m0.5028\u001b[0m                     \u001b[35m0.7103\u001b[0m        \u001b[31m0.5699\u001b[0m  0.0050  0.2703\n",
      "     14                     \u001b[36m0.7681\u001b[0m        \u001b[32m0.4849\u001b[0m                     \u001b[35m0.7106\u001b[0m        \u001b[31m0.5611\u001b[0m  0.0043  0.2733\n",
      "     15                     0.7671        0.4887                     \u001b[35m0.7173\u001b[0m        0.5629  0.0037  0.2708\n",
      "     16                     \u001b[36m0.7780\u001b[0m        \u001b[32m0.4763\u001b[0m                     \u001b[35m0.7194\u001b[0m        0.5759  0.0031  0.2713\n",
      "     17                     0.7672        0.5102                     0.7165        0.6080  0.0025  0.2693\n",
      "     18                     \u001b[36m0.7832\u001b[0m        0.4792                     \u001b[35m0.7231\u001b[0m        0.5661  0.0020  0.2714\n",
      "     19                     \u001b[36m0.7870\u001b[0m        \u001b[32m0.4624\u001b[0m                     0.7206        0.5657  0.0015  0.2703\n",
      "     20                     \u001b[36m0.8011\u001b[0m        \u001b[32m0.4399\u001b[0m                     \u001b[35m0.7329\u001b[0m        \u001b[31m0.5505\u001b[0m  0.0010  0.2723\n",
      "     21                     \u001b[36m0.8064\u001b[0m        \u001b[32m0.4330\u001b[0m                     0.7285        0.5568  0.0007  0.2699\n",
      "     22                     0.7933        0.4443                     0.7260        \u001b[31m0.5490\u001b[0m  0.0004  0.2715\n",
      "     23                     0.8015        \u001b[32m0.4219\u001b[0m                     0.7258        0.5490  0.0002  0.2702\n",
      "     24                     \u001b[36m0.8085\u001b[0m        0.4304                     0.7256        \u001b[31m0.5487\u001b[0m  0.0000  0.2713\n",
      "     25                     \u001b[36m0.8105\u001b[0m        \u001b[32m0.4190\u001b[0m                     0.7254        0.5496  0.0000  0.2708\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5727\u001b[0m        \u001b[32m1.0271\u001b[0m                     \u001b[35m0.6830\u001b[0m        \u001b[31m0.6022\u001b[0m  0.0100  0.2673\n",
      "      2                     \u001b[36m0.6436\u001b[0m        \u001b[32m0.7101\u001b[0m                     0.6500        0.6433  0.0100  0.2693\n",
      "      3                     \u001b[36m0.6787\u001b[0m        \u001b[32m0.6743\u001b[0m                     \u001b[35m0.7109\u001b[0m        \u001b[31m0.5601\u001b[0m  0.0098  0.2703\n",
      "      4                     \u001b[36m0.6794\u001b[0m        0.6856                     0.6238        0.7660  0.0096  0.2703\n",
      "      5                     0.6763        0.7117                     0.6945        0.6088  0.0093  0.2703\n",
      "      6                     \u001b[36m0.6996\u001b[0m        \u001b[32m0.6723\u001b[0m                     0.7003        0.6240  0.0090  0.2733\n",
      "      7                     \u001b[36m0.7011\u001b[0m        \u001b[32m0.6216\u001b[0m                     \u001b[35m0.7430\u001b[0m        \u001b[31m0.5509\u001b[0m  0.0085  0.2703\n",
      "      8                     \u001b[36m0.7205\u001b[0m        \u001b[32m0.5790\u001b[0m                     0.6682        0.6928  0.0080  0.2723\n",
      "      9                     0.6908        0.6719                     \u001b[35m0.7581\u001b[0m        \u001b[31m0.5298\u001b[0m  0.0075  0.2713\n",
      "     10                     \u001b[36m0.7428\u001b[0m        \u001b[32m0.5387\u001b[0m                     0.7138        0.6581  0.0069  0.2723\n",
      "     11                     0.7244        0.6045                     0.6927        0.6243  0.0063  0.2703\n",
      "     12                     0.7395        0.5414                     0.7533        0.5359  0.0057  0.2713\n",
      "     13                     \u001b[36m0.7622\u001b[0m        \u001b[32m0.5081\u001b[0m                     0.7468        \u001b[31m0.5224\u001b[0m  0.0050  0.2713\n",
      "     14                     0.7497        0.5142                     0.7410        0.5330  0.0043  0.2723\n",
      "     15                     \u001b[36m0.7645\u001b[0m        \u001b[32m0.4946\u001b[0m                     0.7464        0.5560  0.0037  0.2703\n",
      "     16                     \u001b[36m0.7722\u001b[0m        \u001b[32m0.4823\u001b[0m                     0.7478        0.5550  0.0031  0.2713\n",
      "     17                     \u001b[36m0.7817\u001b[0m        \u001b[32m0.4745\u001b[0m                     0.7376        \u001b[31m0.5182\u001b[0m  0.0025  0.2717\n",
      "     18                     \u001b[36m0.7893\u001b[0m        \u001b[32m0.4519\u001b[0m                     0.7452        \u001b[31m0.5165\u001b[0m  0.0020  0.2731\n",
      "     19                     \u001b[36m0.7941\u001b[0m        \u001b[32m0.4390\u001b[0m                     0.7442        \u001b[31m0.5150\u001b[0m  0.0015  0.2711\n",
      "     20                     \u001b[36m0.8062\u001b[0m        \u001b[32m0.4196\u001b[0m                     0.7540        0.5212  0.0010  0.2735\n",
      "     21                     \u001b[36m0.8093\u001b[0m        0.4200                     0.7421        0.5248  0.0007  0.2712\n",
      "     22                     0.7917        0.4325                     0.7423        0.5270  0.0004  0.2723\n",
      "     23                     0.8049        \u001b[32m0.4174\u001b[0m                     0.7386        0.5232  0.0002  0.2714\n",
      "     24                     \u001b[36m0.8126\u001b[0m        0.4191                     0.7418        0.5234  0.0000  0.2713\n",
      "     25                     \u001b[36m0.8159\u001b[0m        \u001b[32m0.4118\u001b[0m                     0.7467        0.5230  0.0000  0.2734\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5825\u001b[0m        \u001b[32m0.9566\u001b[0m                     \u001b[35m0.6031\u001b[0m        \u001b[31m0.7523\u001b[0m  0.0100  0.2643\n",
      "      2                     \u001b[36m0.6206\u001b[0m        \u001b[32m0.8207\u001b[0m                     0.6029        0.8924  0.0100  0.2723\n",
      "      3                     \u001b[36m0.6828\u001b[0m        \u001b[32m0.7022\u001b[0m                     \u001b[35m0.6697\u001b[0m        \u001b[31m0.6582\u001b[0m  0.0098  0.2703\n",
      "      4                     0.6700        \u001b[32m0.6979\u001b[0m                     0.6258        0.8367  0.0096  0.2713\n",
      "      5                     0.6658        0.7168                     \u001b[35m0.6958\u001b[0m        \u001b[31m0.6206\u001b[0m  0.0093  0.2703\n",
      "      6                     \u001b[36m0.6986\u001b[0m        \u001b[32m0.6299\u001b[0m                     \u001b[35m0.7015\u001b[0m        \u001b[31m0.5733\u001b[0m  0.0090  0.2713\n",
      "      7                     0.6949        0.6647                     0.6650        0.6415  0.0085  0.2733\n",
      "      8                     \u001b[36m0.7048\u001b[0m        \u001b[32m0.6230\u001b[0m                     0.6959        0.5896  0.0080  0.2733\n",
      "      9                     0.6955        0.6675                     \u001b[35m0.7298\u001b[0m        \u001b[31m0.5540\u001b[0m  0.0075  0.2703\n",
      "     10                     \u001b[36m0.7373\u001b[0m        \u001b[32m0.5810\u001b[0m                     \u001b[35m0.7345\u001b[0m        \u001b[31m0.5395\u001b[0m  0.0069  0.2713\n",
      "     11                     0.7222        \u001b[32m0.5743\u001b[0m                     0.7138        0.5725  0.0063  0.2713\n",
      "     12                     \u001b[36m0.7418\u001b[0m        \u001b[32m0.5303\u001b[0m                     \u001b[35m0.7405\u001b[0m        \u001b[31m0.5294\u001b[0m  0.0057  0.2713\n",
      "     13                     \u001b[36m0.7602\u001b[0m        \u001b[32m0.5124\u001b[0m                     0.7143        0.5803  0.0050  0.2703\n",
      "     14                     0.7582        0.5260                     0.7341        0.5325  0.0043  0.2723\n",
      "     15                     \u001b[36m0.7659\u001b[0m        \u001b[32m0.4957\u001b[0m                     \u001b[35m0.7439\u001b[0m        \u001b[31m0.5149\u001b[0m  0.0037  0.2713\n",
      "     16                     \u001b[36m0.7744\u001b[0m        \u001b[32m0.4682\u001b[0m                     \u001b[35m0.7470\u001b[0m        \u001b[31m0.5092\u001b[0m  0.0031  0.2705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17                     \u001b[36m0.7851\u001b[0m        \u001b[32m0.4584\u001b[0m                     0.7418        0.5228  0.0025  0.2733\n",
      "     18                     0.7801        \u001b[32m0.4540\u001b[0m                     \u001b[35m0.7536\u001b[0m        0.5125  0.0020  0.2713\n",
      "     19                     \u001b[36m0.7936\u001b[0m        \u001b[32m0.4429\u001b[0m                     0.7354        0.5310  0.0015  0.2713\n",
      "     20                     \u001b[36m0.7964\u001b[0m        \u001b[32m0.4403\u001b[0m                     \u001b[35m0.7552\u001b[0m        0.5155  0.0010  0.2705\n",
      "     21                     0.7916        0.4445                     0.7503        0.5113  0.0007  0.2713\n",
      "     22                     \u001b[36m0.7986\u001b[0m        \u001b[32m0.4306\u001b[0m                     0.7499        0.5093  0.0004  0.2703\n",
      "     23                     \u001b[36m0.8018\u001b[0m        \u001b[32m0.4269\u001b[0m                     \u001b[35m0.7561\u001b[0m        0.5123  0.0002  0.2734\n",
      "     24                     \u001b[36m0.8066\u001b[0m        \u001b[32m0.4234\u001b[0m                     0.7559        0.5101  0.0000  0.2704\n",
      "     25                     0.8051        \u001b[32m0.4178\u001b[0m                     0.7515        0.5110  0.0000  0.2723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[1 0 0 ... 0 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5735\u001b[0m        \u001b[32m1.0393\u001b[0m                     \u001b[35m0.6633\u001b[0m        \u001b[31m0.5980\u001b[0m  0.0100  0.7865\n",
      "      2                     \u001b[36m0.6334\u001b[0m        \u001b[32m0.7479\u001b[0m                     0.5719        0.9891  0.0100  0.5890\n",
      "      3                     0.6299        0.7758                     0.6631        0.6786  0.0098  0.5872\n",
      "      4                     \u001b[36m0.6699\u001b[0m        \u001b[32m0.6961\u001b[0m                     \u001b[35m0.6758\u001b[0m        0.7226  0.0096  0.5876\n",
      "      5                     0.6669        0.7191                     \u001b[35m0.6851\u001b[0m        0.6206  0.0093  0.5865\n",
      "      6                     \u001b[36m0.6955\u001b[0m        \u001b[32m0.6186\u001b[0m                     \u001b[35m0.7322\u001b[0m        \u001b[31m0.5361\u001b[0m  0.0090  0.5865\n",
      "      7                     0.6920        0.6366                     0.7295        0.5588  0.0085  0.5874\n",
      "      8                     \u001b[36m0.7068\u001b[0m        0.6242                     \u001b[35m0.7462\u001b[0m        \u001b[31m0.5190\u001b[0m  0.0080  0.5861\n",
      "      9                     \u001b[36m0.7101\u001b[0m        \u001b[32m0.5696\u001b[0m                     0.7346        0.5480  0.0075  0.5875\n",
      "     10                     0.7088        0.6095                     0.7040        0.5801  0.0069  0.5873\n",
      "     11                     \u001b[36m0.7249\u001b[0m        \u001b[32m0.5604\u001b[0m                     0.7182        0.5541  0.0063  0.5864\n",
      "     12                     \u001b[36m0.7495\u001b[0m        \u001b[32m0.5300\u001b[0m                     \u001b[35m0.7565\u001b[0m        \u001b[31m0.5017\u001b[0m  0.0057  0.5865\n",
      "     13                     \u001b[36m0.7497\u001b[0m        \u001b[32m0.5193\u001b[0m                     0.7532        \u001b[31m0.4984\u001b[0m  0.0050  0.5864\n",
      "     14                     \u001b[36m0.7516\u001b[0m        \u001b[32m0.5056\u001b[0m                     0.7518        0.5288  0.0043  0.5875\n",
      "     15                     \u001b[36m0.7608\u001b[0m        \u001b[32m0.5020\u001b[0m                     0.7512        \u001b[31m0.4908\u001b[0m  0.0037  0.5865\n",
      "     16                     \u001b[36m0.7689\u001b[0m        \u001b[32m0.4916\u001b[0m                     \u001b[35m0.7580\u001b[0m        0.4940  0.0031  0.5865\n",
      "     17                     0.7662        0.4920                     0.7531        0.5007  0.0025  0.5852\n",
      "     18                     \u001b[36m0.7750\u001b[0m        \u001b[32m0.4735\u001b[0m                     \u001b[35m0.7634\u001b[0m        0.4970  0.0020  0.5870\n",
      "     19                     \u001b[36m0.7811\u001b[0m        \u001b[32m0.4610\u001b[0m                     0.7397        0.4986  0.0015  0.5854\n",
      "     20                     \u001b[36m0.7895\u001b[0m        0.4615                     0.7433        0.4952  0.0010  0.5875\n",
      "     21                     0.7872        \u001b[32m0.4514\u001b[0m                     0.7529        0.4921  0.0007  0.5865\n",
      "     22                     \u001b[36m0.7947\u001b[0m        \u001b[32m0.4362\u001b[0m                     0.7462        0.4928  0.0004  0.5875\n",
      "     23                     \u001b[36m0.7994\u001b[0m        0.4368                     0.7490        0.4966  0.0002  0.5884\n",
      "     24                     \u001b[36m0.8011\u001b[0m        \u001b[32m0.4276\u001b[0m                     0.7492        0.4964  0.0000  0.5857\n",
      "     25                     0.7989        0.4335                     0.7519        0.4928  0.0000  0.5865\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5699\u001b[0m        \u001b[32m1.0447\u001b[0m                     \u001b[35m0.6220\u001b[0m        \u001b[31m0.7174\u001b[0m  0.0100  0.5815\n",
      "      2                     \u001b[36m0.6228\u001b[0m        \u001b[32m0.8073\u001b[0m                     \u001b[35m0.6690\u001b[0m        \u001b[31m0.6712\u001b[0m  0.0100  0.5855\n",
      "      3                     \u001b[36m0.6466\u001b[0m        \u001b[32m0.7407\u001b[0m                     0.6545        0.7224  0.0098  0.5865\n",
      "      4                     \u001b[36m0.6784\u001b[0m        \u001b[32m0.7034\u001b[0m                     \u001b[35m0.7056\u001b[0m        \u001b[31m0.5701\u001b[0m  0.0096  0.5863\n",
      "      5                     \u001b[36m0.6871\u001b[0m        \u001b[32m0.6790\u001b[0m                     0.7029        0.5834  0.0093  0.5865\n",
      "      6                     \u001b[36m0.6955\u001b[0m        \u001b[32m0.6571\u001b[0m                     \u001b[35m0.7089\u001b[0m        0.5786  0.0090  0.5855\n",
      "      7                     0.6912        \u001b[32m0.6481\u001b[0m                     0.7038        \u001b[31m0.5689\u001b[0m  0.0085  0.5854\n",
      "      8                     \u001b[36m0.7020\u001b[0m        \u001b[32m0.6288\u001b[0m                     0.6775        0.6326  0.0080  0.5863\n",
      "      9                     \u001b[36m0.7102\u001b[0m        \u001b[32m0.5999\u001b[0m                     0.6912        0.6756  0.0075  0.5856\n",
      "     10                     \u001b[36m0.7237\u001b[0m        \u001b[32m0.5694\u001b[0m                     \u001b[35m0.7109\u001b[0m        0.5860  0.0069  0.5855\n",
      "     11                     \u001b[36m0.7252\u001b[0m        \u001b[32m0.5613\u001b[0m                     \u001b[35m0.7308\u001b[0m        \u001b[31m0.5579\u001b[0m  0.0063  0.5895\n",
      "     12                     \u001b[36m0.7418\u001b[0m        \u001b[32m0.5324\u001b[0m                     0.7253        0.5620  0.0057  0.5862\n",
      "     13                     \u001b[36m0.7520\u001b[0m        \u001b[32m0.5320\u001b[0m                     \u001b[35m0.7332\u001b[0m        0.5658  0.0050  0.5864\n",
      "     14                     \u001b[36m0.7552\u001b[0m        \u001b[32m0.5133\u001b[0m                     0.7042        0.6029  0.0043  0.5875\n",
      "     15                     \u001b[36m0.7564\u001b[0m        \u001b[32m0.5002\u001b[0m                     0.7239        \u001b[31m0.5475\u001b[0m  0.0037  0.5878\n",
      "     16                     0.7544        0.5066                     0.7265        \u001b[31m0.5467\u001b[0m  0.0031  0.5874\n",
      "     17                     \u001b[36m0.7651\u001b[0m        \u001b[32m0.4823\u001b[0m                     \u001b[35m0.7501\u001b[0m        \u001b[31m0.5407\u001b[0m  0.0025  0.5875\n",
      "     18                     \u001b[36m0.7718\u001b[0m        \u001b[32m0.4798\u001b[0m                     0.7405        \u001b[31m0.5349\u001b[0m  0.0020  0.5885\n",
      "     19                     \u001b[36m0.7779\u001b[0m        \u001b[32m0.4671\u001b[0m                     0.7457        \u001b[31m0.5324\u001b[0m  0.0015  0.5878\n",
      "     20                     0.7752        \u001b[32m0.4604\u001b[0m                     0.7430        0.5346  0.0010  0.5885\n",
      "     21                     \u001b[36m0.7820\u001b[0m        0.4675                     0.7451        0.5351  0.0007  0.5875\n",
      "     22                     \u001b[36m0.7892\u001b[0m        \u001b[32m0.4521\u001b[0m                     0.7402        0.5366  0.0004  0.5864\n",
      "     23                     \u001b[36m0.7894\u001b[0m        \u001b[32m0.4491\u001b[0m                     0.7411        0.5333  0.0002  0.5886\n",
      "     24                     0.7813        0.4533                     0.7392        0.5362  0.0000  0.5875\n",
      "     25                     \u001b[36m0.7907\u001b[0m        \u001b[32m0.4481\u001b[0m                     0.7397        0.5353  0.0000  0.5875\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5760\u001b[0m        \u001b[32m1.0044\u001b[0m                     \u001b[35m0.6149\u001b[0m        \u001b[31m0.6967\u001b[0m  0.0100  0.5804\n",
      "      2                     \u001b[36m0.6353\u001b[0m        \u001b[32m0.7808\u001b[0m                     \u001b[35m0.6476\u001b[0m        \u001b[31m0.6822\u001b[0m  0.0100  0.5859\n",
      "      3                     \u001b[36m0.6537\u001b[0m        \u001b[32m0.7254\u001b[0m                     0.6382        0.8369  0.0098  0.5885\n",
      "      4                     0.6458        0.8865                     \u001b[35m0.6972\u001b[0m        \u001b[31m0.5906\u001b[0m  0.0096  0.5885\n",
      "      5                     \u001b[36m0.6902\u001b[0m        \u001b[32m0.6565\u001b[0m                     0.6606        0.6714  0.0093  0.5868\n",
      "      6                     0.6718        0.7131                     \u001b[35m0.7057\u001b[0m        \u001b[31m0.5820\u001b[0m  0.0090  0.5895\n",
      "      7                     \u001b[36m0.7022\u001b[0m        \u001b[32m0.6178\u001b[0m                     \u001b[35m0.7063\u001b[0m        0.5889  0.0085  0.5884\n",
      "      8                     0.6998        \u001b[32m0.6135\u001b[0m                     0.7041        \u001b[31m0.5698\u001b[0m  0.0080  0.5864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.7256\u001b[0m        \u001b[32m0.5784\u001b[0m                     \u001b[35m0.7185\u001b[0m        \u001b[31m0.5413\u001b[0m  0.0075  0.5854\n",
      "     10                     \u001b[36m0.7378\u001b[0m        \u001b[32m0.5476\u001b[0m                     \u001b[35m0.7232\u001b[0m        0.5501  0.0069  0.5865\n",
      "     11                     \u001b[36m0.7395\u001b[0m        \u001b[32m0.5329\u001b[0m                     \u001b[35m0.7416\u001b[0m        \u001b[31m0.5376\u001b[0m  0.0063  0.5888\n",
      "     12                     \u001b[36m0.7454\u001b[0m        \u001b[32m0.5323\u001b[0m                     0.7361        \u001b[31m0.5360\u001b[0m  0.0057  0.5865\n",
      "     13                     \u001b[36m0.7479\u001b[0m        \u001b[32m0.5205\u001b[0m                     0.7389        \u001b[31m0.5236\u001b[0m  0.0050  0.5875\n",
      "     14                     0.7398        0.5288                     0.7335        0.5362  0.0043  0.5875\n",
      "     15                     \u001b[36m0.7530\u001b[0m        \u001b[32m0.5194\u001b[0m                     \u001b[35m0.7442\u001b[0m        \u001b[31m0.5229\u001b[0m  0.0037  0.5895\n",
      "     16                     \u001b[36m0.7627\u001b[0m        \u001b[32m0.4911\u001b[0m                     \u001b[35m0.7576\u001b[0m        0.5239  0.0031  0.5875\n",
      "     17                     \u001b[36m0.7688\u001b[0m        \u001b[32m0.4871\u001b[0m                     0.7375        0.5231  0.0025  0.5875\n",
      "     18                     \u001b[36m0.7712\u001b[0m        \u001b[32m0.4778\u001b[0m                     \u001b[35m0.7607\u001b[0m        \u001b[31m0.5093\u001b[0m  0.0020  0.5876\n",
      "     19                     \u001b[36m0.7837\u001b[0m        \u001b[32m0.4690\u001b[0m                     0.7502        \u001b[31m0.5092\u001b[0m  0.0015  0.5878\n",
      "     20                     \u001b[36m0.7924\u001b[0m        \u001b[32m0.4513\u001b[0m                     0.7522        \u001b[31m0.5062\u001b[0m  0.0010  0.5874\n",
      "     21                     0.7837        0.4602                     0.7472        \u001b[31m0.5055\u001b[0m  0.0007  0.5856\n",
      "     22                     0.7788        0.4537                     0.7503        0.5071  0.0004  0.5875\n",
      "     23                     \u001b[36m0.7939\u001b[0m        \u001b[32m0.4458\u001b[0m                     0.7513        0.5074  0.0002  0.5869\n",
      "     24                     0.7922        0.4503                     0.7523        0.5077  0.0000  0.6164\n",
      "     25                     0.7862        0.4522                     0.7531        0.5091  0.0000  0.5868\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5800\u001b[0m        \u001b[32m1.0540\u001b[0m                     \u001b[35m0.7049\u001b[0m        \u001b[31m0.5917\u001b[0m  0.0100  0.5787\n",
      "      2                     \u001b[36m0.6169\u001b[0m        \u001b[32m0.8508\u001b[0m                     0.6741        0.6556  0.0100  0.6054\n",
      "      3                     \u001b[36m0.6363\u001b[0m        \u001b[32m0.7833\u001b[0m                     0.5979        0.9508  0.0098  0.5885\n",
      "      4                     \u001b[36m0.6363\u001b[0m        0.8184                     0.6938        0.6321  0.0096  0.5864\n",
      "      5                     \u001b[36m0.6777\u001b[0m        \u001b[32m0.6639\u001b[0m                     \u001b[35m0.7256\u001b[0m        \u001b[31m0.5607\u001b[0m  0.0093  0.5864\n",
      "      6                     0.6727        0.6818                     0.7130        0.5965  0.0090  0.5859\n",
      "      7                     \u001b[36m0.6982\u001b[0m        \u001b[32m0.6182\u001b[0m                     \u001b[35m0.7577\u001b[0m        \u001b[31m0.5149\u001b[0m  0.0085  0.5859\n",
      "      8                     \u001b[36m0.7104\u001b[0m        \u001b[32m0.5804\u001b[0m                     \u001b[35m0.7649\u001b[0m        0.5395  0.0080  0.5864\n",
      "      9                     \u001b[36m0.7125\u001b[0m        0.5829                     0.7639        0.5191  0.0075  0.5855\n",
      "     10                     \u001b[36m0.7205\u001b[0m        \u001b[32m0.5802\u001b[0m                     \u001b[35m0.7717\u001b[0m        0.5255  0.0069  0.5865\n",
      "     11                     \u001b[36m0.7299\u001b[0m        \u001b[32m0.5533\u001b[0m                     0.7480        0.5455  0.0063  0.5858\n",
      "     12                     \u001b[36m0.7330\u001b[0m        \u001b[32m0.5521\u001b[0m                     0.7681        \u001b[31m0.4996\u001b[0m  0.0057  0.5865\n",
      "     13                     0.7313        \u001b[32m0.5488\u001b[0m                     \u001b[35m0.7738\u001b[0m        \u001b[31m0.4811\u001b[0m  0.0050  0.5861\n",
      "     14                     0.7266        \u001b[32m0.5346\u001b[0m                     0.7698        0.5212  0.0043  0.5875\n",
      "     15                     \u001b[36m0.7364\u001b[0m        \u001b[32m0.5155\u001b[0m                     0.7694        0.4994  0.0037  0.5924\n",
      "     16                     \u001b[36m0.7604\u001b[0m        \u001b[32m0.5005\u001b[0m                     0.7639        0.5054  0.0031  0.5865\n",
      "     17                     \u001b[36m0.7632\u001b[0m        \u001b[32m0.4895\u001b[0m                     0.7721        0.4914  0.0025  0.5878\n",
      "     18                     0.7606        \u001b[32m0.4865\u001b[0m                     \u001b[35m0.7762\u001b[0m        0.4945  0.0020  0.5884\n",
      "     19                     \u001b[36m0.7681\u001b[0m        \u001b[32m0.4778\u001b[0m                     0.7717        0.4859  0.0015  0.5885\n",
      "     20                     \u001b[36m0.7733\u001b[0m        \u001b[32m0.4723\u001b[0m                     \u001b[35m0.7826\u001b[0m        0.4865  0.0010  0.5854\n",
      "     21                     \u001b[36m0.7772\u001b[0m        \u001b[32m0.4636\u001b[0m                     0.7753        0.4918  0.0007  0.5874\n",
      "     22                     \u001b[36m0.7821\u001b[0m        0.4665                     0.7825        0.4834  0.0004  0.5865\n",
      "     23                     \u001b[36m0.7886\u001b[0m        \u001b[32m0.4580\u001b[0m                     0.7808        0.4864  0.0002  0.5904\n",
      "     24                     0.7765        0.4589                     0.7769        0.4850  0.0000  0.5875\n",
      "     25                     \u001b[36m0.7902\u001b[0m        \u001b[32m0.4556\u001b[0m                     0.7805        0.4837  0.0000  0.5875\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5882\u001b[0m        \u001b[32m1.0047\u001b[0m                     \u001b[35m0.6448\u001b[0m        \u001b[31m0.6641\u001b[0m  0.0100  0.5815\n",
      "      2                     \u001b[36m0.6371\u001b[0m        \u001b[32m0.7616\u001b[0m                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5569\u001b[0m  0.0100  0.5854\n",
      "      3                     \u001b[36m0.6521\u001b[0m        0.7834                     0.6653        0.6687  0.0098  0.5865\n",
      "      4                     \u001b[36m0.6687\u001b[0m        \u001b[32m0.6992\u001b[0m                     0.6706        0.7252  0.0096  0.5854\n",
      "      5                     \u001b[36m0.6730\u001b[0m        \u001b[32m0.6878\u001b[0m                     0.6663        0.7067  0.0093  0.5865\n",
      "      6                     \u001b[36m0.6889\u001b[0m        \u001b[32m0.6255\u001b[0m                     0.6616        0.7064  0.0090  0.5865\n",
      "      7                     \u001b[36m0.6949\u001b[0m        \u001b[32m0.6196\u001b[0m                     \u001b[35m0.7298\u001b[0m        0.5813  0.0085  0.5884\n",
      "      8                     \u001b[36m0.6950\u001b[0m        0.6256                     0.7017        0.5938  0.0080  0.5865\n",
      "      9                     \u001b[36m0.7029\u001b[0m        \u001b[32m0.6180\u001b[0m                     0.6703        0.6556  0.0075  0.5866\n",
      "     10                     \u001b[36m0.7258\u001b[0m        \u001b[32m0.5586\u001b[0m                     0.7285        \u001b[31m0.5458\u001b[0m  0.0069  0.5865\n",
      "     11                     \u001b[36m0.7331\u001b[0m        \u001b[32m0.5502\u001b[0m                     0.7286        0.5557  0.0063  0.5864\n",
      "     12                     \u001b[36m0.7333\u001b[0m        \u001b[32m0.5494\u001b[0m                     0.7227        0.5544  0.0057  0.5875\n",
      "     13                     \u001b[36m0.7392\u001b[0m        \u001b[32m0.5383\u001b[0m                     0.7207        0.5666  0.0050  0.5876\n",
      "     14                     \u001b[36m0.7479\u001b[0m        \u001b[32m0.5181\u001b[0m                     \u001b[35m0.7436\u001b[0m        \u001b[31m0.5405\u001b[0m  0.0043  0.5875\n",
      "     15                     \u001b[36m0.7604\u001b[0m        \u001b[32m0.5068\u001b[0m                     0.7381        \u001b[31m0.5372\u001b[0m  0.0037  0.5885\n",
      "     16                     0.7526        \u001b[32m0.5006\u001b[0m                     \u001b[35m0.7462\u001b[0m        0.5408  0.0031  0.5870\n",
      "     17                     0.7563        \u001b[32m0.4898\u001b[0m                     \u001b[35m0.7513\u001b[0m        0.5434  0.0025  0.5865\n",
      "     18                     \u001b[36m0.7702\u001b[0m        \u001b[32m0.4748\u001b[0m                     0.7320        0.5486  0.0020  0.5872\n",
      "     19                     0.7594        0.4911                     0.7313        \u001b[31m0.5343\u001b[0m  0.0015  0.5865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                     \u001b[36m0.7774\u001b[0m        \u001b[32m0.4614\u001b[0m                     0.7442        \u001b[31m0.5340\u001b[0m  0.0010  0.5873\n",
      "     21                     0.7732        0.4657                     0.7374        0.5396  0.0007  0.5874\n",
      "     22                     \u001b[36m0.7814\u001b[0m        \u001b[32m0.4595\u001b[0m                     0.7386        \u001b[31m0.5334\u001b[0m  0.0004  0.5879\n",
      "     23                     \u001b[36m0.7872\u001b[0m        \u001b[32m0.4462\u001b[0m                     0.7334        0.5362  0.0002  0.5867\n",
      "     24                     0.7853        0.4483                     0.7369        0.5338  0.0000  0.5870\n",
      "     25                     0.7839        0.4534                     0.7406        0.5352  0.0000  0.5865\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5778\u001b[0m        \u001b[32m1.2520\u001b[0m                     \u001b[35m0.6704\u001b[0m        \u001b[31m0.7150\u001b[0m  0.0100  0.5815\n",
      "      2                     \u001b[36m0.6292\u001b[0m        \u001b[32m0.8179\u001b[0m                     0.6419        \u001b[31m0.6869\u001b[0m  0.0100  0.5875\n",
      "      3                     \u001b[36m0.6659\u001b[0m        \u001b[32m0.6756\u001b[0m                     0.5979        0.8434  0.0098  0.5885\n",
      "      4                     0.6448        0.7985                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.5908\u001b[0m  0.0096  0.5865\n",
      "      5                     \u001b[36m0.6749\u001b[0m        0.6981                     0.6887        \u001b[31m0.5865\u001b[0m  0.0093  0.5854\n",
      "      6                     \u001b[36m0.6835\u001b[0m        \u001b[32m0.6637\u001b[0m                     0.6862        0.6408  0.0090  0.5857\n",
      "      7                     \u001b[36m0.7041\u001b[0m        \u001b[32m0.6215\u001b[0m                     \u001b[35m0.7048\u001b[0m        \u001b[31m0.5721\u001b[0m  0.0085  0.5864\n",
      "      8                     0.6809        0.6729                     \u001b[35m0.7289\u001b[0m        \u001b[31m0.5555\u001b[0m  0.0080  0.5854\n",
      "      9                     \u001b[36m0.7189\u001b[0m        \u001b[32m0.5909\u001b[0m                     0.7256        0.5671  0.0075  0.5864\n",
      "     10                     0.7130        \u001b[32m0.5811\u001b[0m                     0.7105        \u001b[31m0.5522\u001b[0m  0.0069  0.5855\n",
      "     11                     \u001b[36m0.7345\u001b[0m        \u001b[32m0.5534\u001b[0m                     0.7117        0.5797  0.0063  0.5862\n",
      "     12                     0.7308        \u001b[32m0.5463\u001b[0m                     0.7287        \u001b[31m0.5506\u001b[0m  0.0057  0.5865\n",
      "     13                     \u001b[36m0.7395\u001b[0m        \u001b[32m0.5372\u001b[0m                     0.7246        0.5547  0.0050  0.5858\n",
      "     14                     0.7394        \u001b[32m0.5262\u001b[0m                     0.7096        \u001b[31m0.5386\u001b[0m  0.0043  0.5867\n",
      "     15                     \u001b[36m0.7523\u001b[0m        \u001b[32m0.5123\u001b[0m                     \u001b[35m0.7353\u001b[0m        \u001b[31m0.5313\u001b[0m  0.0037  0.5865\n",
      "     16                     \u001b[36m0.7570\u001b[0m        \u001b[32m0.5044\u001b[0m                     0.7279        \u001b[31m0.5146\u001b[0m  0.0031  0.5875\n",
      "     17                     \u001b[36m0.7597\u001b[0m        \u001b[32m0.4917\u001b[0m                     \u001b[35m0.7491\u001b[0m        \u001b[31m0.5135\u001b[0m  0.0025  0.5864\n",
      "     18                     \u001b[36m0.7602\u001b[0m        0.4970                     0.7485        \u001b[31m0.5108\u001b[0m  0.0020  0.5874\n",
      "     19                     \u001b[36m0.7752\u001b[0m        \u001b[32m0.4845\u001b[0m                     0.7278        0.5216  0.0015  0.5865\n",
      "     20                     \u001b[36m0.7830\u001b[0m        \u001b[32m0.4656\u001b[0m                     0.7479        0.5150  0.0010  0.5864\n",
      "     21                     \u001b[36m0.7871\u001b[0m        0.4703                     0.7450        0.5149  0.0007  0.5865\n",
      "     22                     0.7842        \u001b[32m0.4635\u001b[0m                     0.7418        0.5150  0.0004  0.5855\n",
      "     23                     0.7856        0.4655                     0.7422        0.5123  0.0002  0.5894\n",
      "     24                     0.7865        \u001b[32m0.4556\u001b[0m                     0.7421        0.5141  0.0000  0.5855\n",
      "     25                     0.7853        \u001b[32m0.4502\u001b[0m                     0.7409        0.5152  0.0000  0.5894\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5815\u001b[0m        \u001b[32m0.9935\u001b[0m                     \u001b[35m0.5775\u001b[0m        \u001b[31m1.0957\u001b[0m  0.0100  0.5785\n",
      "      2                     \u001b[36m0.6269\u001b[0m        \u001b[32m0.8071\u001b[0m                     \u001b[35m0.6902\u001b[0m        \u001b[31m0.6299\u001b[0m  0.0100  0.5931\n",
      "      3                     \u001b[36m0.6429\u001b[0m        \u001b[32m0.7449\u001b[0m                     0.6688        0.6819  0.0098  0.5914\n",
      "      4                     \u001b[36m0.6648\u001b[0m        \u001b[32m0.7345\u001b[0m                     0.6374        0.7410  0.0096  0.5890\n",
      "      5                     \u001b[36m0.6739\u001b[0m        \u001b[32m0.6724\u001b[0m                     \u001b[35m0.6943\u001b[0m        \u001b[31m0.6082\u001b[0m  0.0093  0.5885\n",
      "      6                     0.6685        0.7040                     0.6817        0.6427  0.0090  0.5873\n",
      "      7                     \u001b[36m0.7048\u001b[0m        \u001b[32m0.6114\u001b[0m                     \u001b[35m0.7099\u001b[0m        \u001b[31m0.5655\u001b[0m  0.0085  0.5875\n",
      "      8                     \u001b[36m0.7128\u001b[0m        \u001b[32m0.5951\u001b[0m                     \u001b[35m0.7149\u001b[0m        0.6293  0.0080  0.5875\n",
      "      9                     0.7068        0.6186                     0.7089        0.6086  0.0075  0.5865\n",
      "     10                     \u001b[36m0.7166\u001b[0m        \u001b[32m0.5793\u001b[0m                     \u001b[35m0.7333\u001b[0m        \u001b[31m0.5441\u001b[0m  0.0069  0.5875\n",
      "     11                     0.7151        \u001b[32m0.5730\u001b[0m                     0.7229        0.5663  0.0063  0.5865\n",
      "     12                     \u001b[36m0.7193\u001b[0m        \u001b[32m0.5681\u001b[0m                     \u001b[35m0.7419\u001b[0m        0.5542  0.0057  0.5873\n",
      "     13                     \u001b[36m0.7313\u001b[0m        \u001b[32m0.5426\u001b[0m                     0.7286        0.5678  0.0050  0.5875\n",
      "     14                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5299\u001b[0m                     \u001b[35m0.7558\u001b[0m        \u001b[31m0.5283\u001b[0m  0.0043  0.5874\n",
      "     15                     \u001b[36m0.7524\u001b[0m        \u001b[32m0.5224\u001b[0m                     0.7474        \u001b[31m0.5268\u001b[0m  0.0037  0.5865\n",
      "     16                     0.7481        \u001b[32m0.5112\u001b[0m                     0.7462        \u001b[31m0.5219\u001b[0m  0.0031  0.5864\n",
      "     17                     0.7467        0.5129                     \u001b[35m0.7572\u001b[0m        \u001b[31m0.5146\u001b[0m  0.0025  0.5865\n",
      "     18                     0.7515        \u001b[32m0.5006\u001b[0m                     0.7452        0.5357  0.0020  0.5873\n",
      "     19                     \u001b[36m0.7641\u001b[0m        \u001b[32m0.4853\u001b[0m                     0.7431        0.5219  0.0015  0.5876\n",
      "     20                     0.7617        \u001b[32m0.4731\u001b[0m                     0.7489        0.5197  0.0010  0.5881\n",
      "     21                     \u001b[36m0.7853\u001b[0m        \u001b[32m0.4707\u001b[0m                     0.7494        0.5254  0.0007  0.5875\n",
      "     22                     0.7748        0.4754                     0.7531        0.5173  0.0004  0.5876\n",
      "     23                     0.7816        \u001b[32m0.4669\u001b[0m                     \u001b[35m0.7586\u001b[0m        0.5172  0.0002  0.5897\n",
      "     24                     0.7815        \u001b[32m0.4641\u001b[0m                     0.7571        \u001b[31m0.5132\u001b[0m  0.0000  0.5874\n",
      "     25                     0.7752        0.4773                     0.7568        0.5167  0.0000  0.5875\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5742\u001b[0m        \u001b[32m1.1260\u001b[0m                     \u001b[35m0.6137\u001b[0m        \u001b[31m0.7661\u001b[0m  0.0100  0.5797\n",
      "      2                     \u001b[36m0.6176\u001b[0m        \u001b[32m0.8264\u001b[0m                     \u001b[35m0.6390\u001b[0m        \u001b[31m0.7085\u001b[0m  0.0100  0.5866\n",
      "      3                     \u001b[36m0.6636\u001b[0m        \u001b[32m0.7363\u001b[0m                     \u001b[35m0.6429\u001b[0m        \u001b[31m0.6658\u001b[0m  0.0098  0.5855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4                     \u001b[36m0.6749\u001b[0m        \u001b[32m0.6975\u001b[0m                     \u001b[35m0.6542\u001b[0m        0.6850  0.0096  0.5865\n",
      "      5                     \u001b[36m0.6757\u001b[0m        \u001b[32m0.6812\u001b[0m                     \u001b[35m0.7182\u001b[0m        \u001b[31m0.5726\u001b[0m  0.0093  0.5858\n",
      "      6                     \u001b[36m0.6871\u001b[0m        \u001b[32m0.6526\u001b[0m                     0.6528        0.6691  0.0090  0.5865\n",
      "      7                     0.6756        0.6726                     0.6555        0.6374  0.0085  0.5852\n",
      "      8                     \u001b[36m0.6874\u001b[0m        0.6576                     0.7004        0.5750  0.0080  0.5865\n",
      "      9                     \u001b[36m0.6934\u001b[0m        \u001b[32m0.6120\u001b[0m                     0.6789        0.6393  0.0075  0.5860\n",
      "     10                     \u001b[36m0.7275\u001b[0m        \u001b[32m0.5773\u001b[0m                     \u001b[35m0.7214\u001b[0m        \u001b[31m0.5460\u001b[0m  0.0069  0.5875\n",
      "     11                     \u001b[36m0.7299\u001b[0m        \u001b[32m0.5706\u001b[0m                     \u001b[35m0.7261\u001b[0m        \u001b[31m0.5428\u001b[0m  0.0063  0.5864\n",
      "     12                     \u001b[36m0.7365\u001b[0m        \u001b[32m0.5413\u001b[0m                     0.7143        0.5671  0.0057  0.5854\n",
      "     13                     0.7288        0.5615                     \u001b[35m0.7361\u001b[0m        \u001b[31m0.5307\u001b[0m  0.0050  0.5875\n",
      "     14                     \u001b[36m0.7486\u001b[0m        \u001b[32m0.5258\u001b[0m                     0.7271        0.5432  0.0043  0.5864\n",
      "     15                     0.7422        0.5306                     0.7002        0.5599  0.0037  0.5855\n",
      "     16                     \u001b[36m0.7597\u001b[0m        \u001b[32m0.4955\u001b[0m                     \u001b[35m0.7440\u001b[0m        \u001b[31m0.5273\u001b[0m  0.0031  0.5865\n",
      "     17                     \u001b[36m0.7606\u001b[0m        0.4986                     0.7416        0.5285  0.0025  0.5865\n",
      "     18                     \u001b[36m0.7694\u001b[0m        \u001b[32m0.4775\u001b[0m                     \u001b[35m0.7464\u001b[0m        \u001b[31m0.5212\u001b[0m  0.0020  0.5875\n",
      "     19                     0.7640        0.4827                     0.7401        \u001b[31m0.5210\u001b[0m  0.0015  0.5874\n",
      "     20                     \u001b[36m0.7724\u001b[0m        \u001b[32m0.4690\u001b[0m                     0.7360        0.5304  0.0010  0.5854\n",
      "     21                     \u001b[36m0.7835\u001b[0m        \u001b[32m0.4639\u001b[0m                     \u001b[35m0.7548\u001b[0m        0.5236  0.0007  0.5865\n",
      "     22                     0.7818        0.4672                     0.7507        0.5220  0.0004  0.5862\n",
      "     23                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4545\u001b[0m                     0.7415        0.5279  0.0002  0.5875\n",
      "     24                     \u001b[36m0.7967\u001b[0m        0.4575                     0.7516        \u001b[31m0.5210\u001b[0m  0.0000  0.5865\n",
      "     25                     0.7870        \u001b[32m0.4491\u001b[0m                     0.7467        0.5229  0.0000  0.5855\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5803\u001b[0m        \u001b[32m1.0773\u001b[0m                     \u001b[35m0.5619\u001b[0m        \u001b[31m1.4061\u001b[0m  0.0100  0.5795\n",
      "      2                     \u001b[36m0.6317\u001b[0m        \u001b[32m0.8005\u001b[0m                     \u001b[35m0.6391\u001b[0m        \u001b[31m0.7039\u001b[0m  0.0100  0.6204\n",
      "      3                     \u001b[36m0.6493\u001b[0m        \u001b[32m0.7400\u001b[0m                     \u001b[35m0.6639\u001b[0m        \u001b[31m0.6611\u001b[0m  0.0098  0.5857\n",
      "      4                     \u001b[36m0.6649\u001b[0m        \u001b[32m0.7119\u001b[0m                     0.6597        \u001b[31m0.6278\u001b[0m  0.0096  0.5875\n",
      "      5                     \u001b[36m0.6727\u001b[0m        \u001b[32m0.6686\u001b[0m                     0.6512        0.6846  0.0093  0.5875\n",
      "      6                     \u001b[36m0.6820\u001b[0m        \u001b[32m0.6455\u001b[0m                     \u001b[35m0.6830\u001b[0m        \u001b[31m0.5981\u001b[0m  0.0090  0.5879\n",
      "      7                     0.6820        0.6579                     0.6776        0.6368  0.0085  0.5892\n",
      "      8                     \u001b[36m0.7052\u001b[0m        \u001b[32m0.6056\u001b[0m                     \u001b[35m0.7178\u001b[0m        \u001b[31m0.5516\u001b[0m  0.0080  0.5878\n",
      "      9                     \u001b[36m0.7091\u001b[0m        \u001b[32m0.5807\u001b[0m                     0.7097        0.5811  0.0075  0.5866\n",
      "     10                     \u001b[36m0.7159\u001b[0m        0.5839                     0.7151        0.5572  0.0069  0.5864\n",
      "     11                     \u001b[36m0.7300\u001b[0m        \u001b[32m0.5741\u001b[0m                     \u001b[35m0.7193\u001b[0m        0.5587  0.0063  0.5874\n",
      "     12                     0.7239        \u001b[32m0.5578\u001b[0m                     \u001b[35m0.7220\u001b[0m        0.5532  0.0057  0.5864\n",
      "     13                     \u001b[36m0.7345\u001b[0m        \u001b[32m0.5561\u001b[0m                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5275\u001b[0m  0.0050  0.5874\n",
      "     14                     \u001b[36m0.7524\u001b[0m        \u001b[32m0.5160\u001b[0m                     0.6978        0.6018  0.0043  0.5893\n",
      "     15                     0.7509        0.5278                     \u001b[35m0.7402\u001b[0m        \u001b[31m0.5144\u001b[0m  0.0037  0.5858\n",
      "     16                     \u001b[36m0.7621\u001b[0m        \u001b[32m0.4989\u001b[0m                     \u001b[35m0.7435\u001b[0m        \u001b[31m0.5138\u001b[0m  0.0031  0.5865\n",
      "     17                     0.7557        \u001b[32m0.4930\u001b[0m                     0.7089        0.5391  0.0025  0.5864\n",
      "     18                     0.7611        \u001b[32m0.4929\u001b[0m                     0.7249        0.5269  0.0020  0.5874\n",
      "     19                     \u001b[36m0.7739\u001b[0m        \u001b[32m0.4741\u001b[0m                     0.7283        0.5200  0.0015  0.5874\n",
      "     20                     \u001b[36m0.7752\u001b[0m        0.4774                     0.7274        0.5185  0.0010  0.5875\n",
      "     21                     \u001b[36m0.7824\u001b[0m        \u001b[32m0.4687\u001b[0m                     0.7313        0.5140  0.0007  0.5875\n",
      "     22                     0.7788        \u001b[32m0.4622\u001b[0m                     0.7255        0.5199  0.0004  0.5885\n",
      "     23                     \u001b[36m0.7883\u001b[0m        \u001b[32m0.4619\u001b[0m                     0.7295        0.5178  0.0002  0.5866\n",
      "     24                     0.7862        \u001b[32m0.4575\u001b[0m                     0.7289        0.5166  0.0000  0.5865\n",
      "     25                     0.7878        \u001b[32m0.4516\u001b[0m                     0.7300        0.5148  0.0000  0.5865\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5741\u001b[0m        \u001b[32m1.1176\u001b[0m                     \u001b[35m0.5990\u001b[0m        \u001b[31m0.9029\u001b[0m  0.0100  0.5815\n",
      "      2                     \u001b[36m0.6365\u001b[0m        \u001b[32m0.7605\u001b[0m                     \u001b[35m0.6297\u001b[0m        \u001b[31m0.7365\u001b[0m  0.0100  0.5885\n",
      "      3                     \u001b[36m0.6604\u001b[0m        \u001b[32m0.6996\u001b[0m                     \u001b[35m0.6579\u001b[0m        \u001b[31m0.6562\u001b[0m  0.0098  0.5865\n",
      "      4                     \u001b[36m0.6817\u001b[0m        \u001b[32m0.6524\u001b[0m                     0.6356        0.8082  0.0096  0.5865\n",
      "      5                     \u001b[36m0.6890\u001b[0m        0.6757                     \u001b[35m0.6798\u001b[0m        \u001b[31m0.6042\u001b[0m  0.0093  0.5855\n",
      "      6                     \u001b[36m0.6936\u001b[0m        \u001b[32m0.6478\u001b[0m                     \u001b[35m0.7128\u001b[0m        0.6140  0.0090  0.5865\n",
      "      7                     \u001b[36m0.6963\u001b[0m        \u001b[32m0.6124\u001b[0m                     0.6957        0.6210  0.0085  0.5865\n",
      "      8                     \u001b[36m0.7078\u001b[0m        0.6137                     \u001b[35m0.7138\u001b[0m        \u001b[31m0.5994\u001b[0m  0.0080  0.5867\n",
      "      9                     \u001b[36m0.7238\u001b[0m        \u001b[32m0.5681\u001b[0m                     0.7029        \u001b[31m0.5807\u001b[0m  0.0075  0.5875\n",
      "     10                     0.7187        0.5866                     \u001b[35m0.7202\u001b[0m        \u001b[31m0.5748\u001b[0m  0.0069  0.5878\n",
      "     11                     \u001b[36m0.7407\u001b[0m        \u001b[32m0.5402\u001b[0m                     \u001b[35m0.7291\u001b[0m        0.5997  0.0063  0.5866\n",
      "     12                     0.7349        0.5584                     0.6804        0.6133  0.0057  0.5858\n",
      "     13                     0.7151        0.5985                     0.7174        0.6069  0.0050  0.5855\n",
      "     14                     0.7390        \u001b[32m0.5284\u001b[0m                     0.7191        0.5784  0.0043  0.5855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15                     \u001b[36m0.7621\u001b[0m        \u001b[32m0.5037\u001b[0m                     0.7245        \u001b[31m0.5598\u001b[0m  0.0037  0.5864\n",
      "     16                     \u001b[36m0.7627\u001b[0m        \u001b[32m0.4901\u001b[0m                     \u001b[35m0.7369\u001b[0m        0.5624  0.0031  0.5875\n",
      "     17                     \u001b[36m0.7637\u001b[0m        \u001b[32m0.4829\u001b[0m                     0.7204        \u001b[31m0.5596\u001b[0m  0.0025  0.5865\n",
      "     18                     \u001b[36m0.7773\u001b[0m        \u001b[32m0.4687\u001b[0m                     0.7160        0.5599  0.0020  0.5886\n",
      "     19                     0.7755        \u001b[32m0.4576\u001b[0m                     0.7208        0.5638  0.0015  0.5875\n",
      "     20                     \u001b[36m0.7913\u001b[0m        \u001b[32m0.4514\u001b[0m                     \u001b[35m0.7417\u001b[0m        \u001b[31m0.5574\u001b[0m  0.0010  0.5875\n",
      "     21                     0.7823        0.4602                     0.7370        0.5618  0.0007  0.5864\n",
      "     22                     \u001b[36m0.7954\u001b[0m        \u001b[32m0.4385\u001b[0m                     0.7389        0.5645  0.0004  0.5874\n",
      "     23                     \u001b[36m0.7981\u001b[0m        0.4451                     \u001b[35m0.7420\u001b[0m        0.5596  0.0002  0.5865\n",
      "     24                     \u001b[36m0.8034\u001b[0m        \u001b[32m0.4349\u001b[0m                     0.7361        0.5620  0.0000  0.5885\n",
      "     25                     0.7992        0.4375                     0.7377        0.5581  0.0000  0.5885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 0 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5395\u001b[0m        \u001b[32m0.9143\u001b[0m                     \u001b[35m0.5654\u001b[0m        \u001b[31m0.7065\u001b[0m  0.0100  1.1230\n",
      "      2                     \u001b[36m0.5586\u001b[0m        \u001b[32m0.7494\u001b[0m                     \u001b[35m0.6038\u001b[0m        \u001b[31m0.6778\u001b[0m  0.0100  0.8418\n",
      "      3                     \u001b[36m0.5822\u001b[0m        0.7618                     \u001b[35m0.6041\u001b[0m        0.7005  0.0098  0.8388\n",
      "      4                     \u001b[36m0.6021\u001b[0m        \u001b[32m0.7150\u001b[0m                     \u001b[35m0.6093\u001b[0m        0.6842  0.0096  0.8398\n",
      "      5                     0.6019        0.7168                     \u001b[35m0.6221\u001b[0m        \u001b[31m0.6538\u001b[0m  0.0093  0.8395\n",
      "      6                     \u001b[36m0.6121\u001b[0m        \u001b[32m0.7103\u001b[0m                     0.6152        0.7008  0.0090  0.8416\n",
      "      7                     \u001b[36m0.6297\u001b[0m        \u001b[32m0.6817\u001b[0m                     \u001b[35m0.6261\u001b[0m        0.6552  0.0085  0.8403\n",
      "      8                     0.6197        0.6883                     \u001b[35m0.6264\u001b[0m        0.6767  0.0080  0.8398\n",
      "      9                     \u001b[36m0.6369\u001b[0m        \u001b[32m0.6687\u001b[0m                     \u001b[35m0.6705\u001b[0m        \u001b[31m0.6261\u001b[0m  0.0075  0.8418\n",
      "     10                     \u001b[36m0.6394\u001b[0m        \u001b[32m0.6535\u001b[0m                     0.6175        0.6965  0.0069  0.8416\n",
      "     11                     \u001b[36m0.6470\u001b[0m        \u001b[32m0.6532\u001b[0m                     0.6492        0.6344  0.0063  0.8405\n",
      "     12                     \u001b[36m0.6511\u001b[0m        \u001b[32m0.6393\u001b[0m                     0.6555        0.6297  0.0057  0.8428\n",
      "     13                     \u001b[36m0.6621\u001b[0m        \u001b[32m0.6305\u001b[0m                     0.6651        \u001b[31m0.6142\u001b[0m  0.0050  0.8398\n",
      "     14                     \u001b[36m0.6636\u001b[0m        \u001b[32m0.6214\u001b[0m                     \u001b[35m0.6812\u001b[0m        \u001b[31m0.6008\u001b[0m  0.0043  0.8656\n",
      "     15                     \u001b[36m0.6747\u001b[0m        \u001b[32m0.6097\u001b[0m                     0.6669        0.6205  0.0037  0.8421\n",
      "     16                     0.6728        0.6113                     0.6699        0.6113  0.0031  0.8398\n",
      "     17                     \u001b[36m0.6757\u001b[0m        \u001b[32m0.5973\u001b[0m                     0.6757        0.6010  0.0025  0.8615\n",
      "     18                     \u001b[36m0.6838\u001b[0m        \u001b[32m0.5942\u001b[0m                     0.6707        0.6025  0.0020  0.8409\n",
      "     19                     \u001b[36m0.6920\u001b[0m        \u001b[32m0.5853\u001b[0m                     0.6760        0.6042  0.0015  0.8389\n",
      "     20                     \u001b[36m0.6983\u001b[0m        \u001b[32m0.5808\u001b[0m                     0.6799        0.6024  0.0010  0.8438\n",
      "     21                     0.6973        \u001b[32m0.5777\u001b[0m                     \u001b[35m0.6846\u001b[0m        \u001b[31m0.5920\u001b[0m  0.0007  0.8409\n",
      "     22                     0.6965        0.5792                     \u001b[35m0.6930\u001b[0m        \u001b[31m0.5910\u001b[0m  0.0004  0.8399\n",
      "     23                     \u001b[36m0.7032\u001b[0m        \u001b[32m0.5727\u001b[0m                     0.6889        \u001b[31m0.5904\u001b[0m  0.0002  0.8435\n",
      "     24                     0.7026        \u001b[32m0.5712\u001b[0m                     0.6880        0.5916  0.0000  0.8408\n",
      "     25                     \u001b[36m0.7043\u001b[0m        \u001b[32m0.5705\u001b[0m                     0.6890        0.5913  0.0000  0.8423\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5301\u001b[0m        \u001b[32m0.9336\u001b[0m                     \u001b[35m0.5503\u001b[0m        \u001b[31m0.7090\u001b[0m  0.0100  0.8348\n",
      "      2                     \u001b[36m0.5599\u001b[0m        \u001b[32m0.8031\u001b[0m                     \u001b[35m0.6075\u001b[0m        \u001b[31m0.6803\u001b[0m  0.0100  0.8398\n",
      "      3                     \u001b[36m0.5815\u001b[0m        0.8216                     \u001b[35m0.6228\u001b[0m        \u001b[31m0.6706\u001b[0m  0.0098  0.8415\n",
      "      4                     \u001b[36m0.5929\u001b[0m        \u001b[32m0.7280\u001b[0m                     0.6070        0.6759  0.0096  0.8398\n",
      "      5                     \u001b[36m0.6080\u001b[0m        \u001b[32m0.7126\u001b[0m                     0.6032        0.6847  0.0093  0.8397\n",
      "      6                     \u001b[36m0.6246\u001b[0m        \u001b[32m0.6915\u001b[0m                     \u001b[35m0.6280\u001b[0m        \u001b[31m0.6470\u001b[0m  0.0090  0.8412\n",
      "      7                     0.6158        \u001b[32m0.6909\u001b[0m                     \u001b[35m0.6289\u001b[0m        0.6583  0.0085  0.8399\n",
      "      8                     \u001b[36m0.6331\u001b[0m        \u001b[32m0.6806\u001b[0m                     0.6021        0.7716  0.0080  0.8418\n",
      "      9                     0.6303        \u001b[32m0.6780\u001b[0m                     \u001b[35m0.6483\u001b[0m        \u001b[31m0.6295\u001b[0m  0.0075  0.8418\n",
      "     10                     \u001b[36m0.6395\u001b[0m        \u001b[32m0.6454\u001b[0m                     0.6350        0.6368  0.0069  0.8378\n",
      "     11                     \u001b[36m0.6549\u001b[0m        \u001b[32m0.6353\u001b[0m                     0.6425        0.6317  0.0063  0.8418\n",
      "     12                     0.6548        \u001b[32m0.6295\u001b[0m                     \u001b[35m0.6590\u001b[0m        \u001b[31m0.6190\u001b[0m  0.0057  0.8418\n",
      "     13                     \u001b[36m0.6631\u001b[0m        \u001b[32m0.6186\u001b[0m                     0.6466        0.6391  0.0050  0.8391\n",
      "     14                     0.6626        0.6235                     0.6541        0.6269  0.0043  0.8409\n",
      "     15                     \u001b[36m0.6638\u001b[0m        \u001b[32m0.6144\u001b[0m                     \u001b[35m0.6674\u001b[0m        \u001b[31m0.6166\u001b[0m  0.0037  0.8398\n",
      "     16                     \u001b[36m0.6768\u001b[0m        \u001b[32m0.6060\u001b[0m                     0.6577        0.6232  0.0031  0.8408\n",
      "     17                     0.6738        0.6063                     \u001b[35m0.6688\u001b[0m        \u001b[31m0.6137\u001b[0m  0.0025  0.8417\n",
      "     18                     \u001b[36m0.6897\u001b[0m        \u001b[32m0.5904\u001b[0m                     0.6641        \u001b[31m0.6118\u001b[0m  0.0020  0.8408\n",
      "     19                     \u001b[36m0.6905\u001b[0m        \u001b[32m0.5871\u001b[0m                     0.6674        \u001b[31m0.6111\u001b[0m  0.0015  0.8409\n",
      "     20                     0.6893        \u001b[32m0.5829\u001b[0m                     0.6665        \u001b[31m0.6104\u001b[0m  0.0010  0.8408\n",
      "     21                     \u001b[36m0.6931\u001b[0m        \u001b[32m0.5816\u001b[0m                     \u001b[35m0.6721\u001b[0m        \u001b[31m0.6090\u001b[0m  0.0007  0.8412\n",
      "     22                     \u001b[36m0.6992\u001b[0m        \u001b[32m0.5736\u001b[0m                     \u001b[35m0.6745\u001b[0m        \u001b[31m0.6062\u001b[0m  0.0004  0.8428\n",
      "     23                     \u001b[36m0.7008\u001b[0m        \u001b[32m0.5709\u001b[0m                     0.6718        0.6072  0.0002  0.8410\n",
      "     24                     \u001b[36m0.7011\u001b[0m        0.5718                     0.6684        0.6074  0.0000  0.8388\n",
      "     25                     0.7004        \u001b[32m0.5700\u001b[0m                     0.6693        0.6073  0.0000  0.8420\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5351\u001b[0m        \u001b[32m0.9336\u001b[0m                     \u001b[35m0.5219\u001b[0m        \u001b[31m0.9363\u001b[0m  0.0100  0.8339\n",
      "      2                     \u001b[36m0.5606\u001b[0m        \u001b[32m0.7735\u001b[0m                     \u001b[35m0.5614\u001b[0m        \u001b[31m0.7323\u001b[0m  0.0100  0.8399\n",
      "      3                     \u001b[36m0.5912\u001b[0m        \u001b[32m0.7458\u001b[0m                     \u001b[35m0.5935\u001b[0m        \u001b[31m0.6958\u001b[0m  0.0098  0.8409\n",
      "      4                     \u001b[36m0.5997\u001b[0m        \u001b[32m0.7287\u001b[0m                     \u001b[35m0.6138\u001b[0m        0.6990  0.0096  0.8388\n",
      "      5                     \u001b[36m0.6057\u001b[0m        \u001b[32m0.7215\u001b[0m                     \u001b[35m0.6270\u001b[0m        \u001b[31m0.6475\u001b[0m  0.0093  0.8408\n",
      "      6                     0.6038        \u001b[32m0.7062\u001b[0m                     \u001b[35m0.6387\u001b[0m        \u001b[31m0.6339\u001b[0m  0.0090  0.8395\n",
      "      7                     \u001b[36m0.6240\u001b[0m        \u001b[32m0.6855\u001b[0m                     \u001b[35m0.6491\u001b[0m        \u001b[31m0.6316\u001b[0m  0.0085  0.8388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8                     \u001b[36m0.6456\u001b[0m        \u001b[32m0.6589\u001b[0m                     0.6152        0.6579  0.0080  0.8430\n",
      "      9                     0.6447        \u001b[32m0.6485\u001b[0m                     0.6478        \u001b[31m0.6300\u001b[0m  0.0075  0.8398\n",
      "     10                     \u001b[36m0.6550\u001b[0m        \u001b[32m0.6384\u001b[0m                     \u001b[35m0.6556\u001b[0m        0.6448  0.0069  0.8428\n",
      "     11                     \u001b[36m0.6620\u001b[0m        \u001b[32m0.6361\u001b[0m                     \u001b[35m0.6621\u001b[0m        \u001b[31m0.6147\u001b[0m  0.0063  0.8408\n",
      "     12                     0.6549        \u001b[32m0.6277\u001b[0m                     0.6441        0.6283  0.0057  0.8385\n",
      "     13                     0.6565        \u001b[32m0.6264\u001b[0m                     \u001b[35m0.6675\u001b[0m        \u001b[31m0.6118\u001b[0m  0.0050  0.8418\n",
      "     14                     \u001b[36m0.6684\u001b[0m        \u001b[32m0.6168\u001b[0m                     0.6644        0.6171  0.0043  0.8418\n",
      "     15                     \u001b[36m0.6712\u001b[0m        \u001b[32m0.6046\u001b[0m                     \u001b[35m0.6687\u001b[0m        0.6197  0.0037  0.8408\n",
      "     16                     \u001b[36m0.6745\u001b[0m        0.6055                     0.6638        0.6169  0.0031  0.8435\n",
      "     17                     \u001b[36m0.6770\u001b[0m        \u001b[32m0.6017\u001b[0m                     0.6683        \u001b[31m0.6103\u001b[0m  0.0025  0.8377\n",
      "     18                     \u001b[36m0.6843\u001b[0m        \u001b[32m0.5901\u001b[0m                     \u001b[35m0.6727\u001b[0m        \u001b[31m0.6049\u001b[0m  0.0020  0.8431\n",
      "     19                     \u001b[36m0.6862\u001b[0m        \u001b[32m0.5899\u001b[0m                     0.6683        0.6066  0.0015  0.8448\n",
      "     20                     \u001b[36m0.6954\u001b[0m        \u001b[32m0.5832\u001b[0m                     \u001b[35m0.6747\u001b[0m        0.6073  0.0010  0.8405\n",
      "     21                     \u001b[36m0.6955\u001b[0m        \u001b[32m0.5811\u001b[0m                     0.6711        \u001b[31m0.6049\u001b[0m  0.0007  0.8418\n",
      "     22                     \u001b[36m0.7021\u001b[0m        \u001b[32m0.5731\u001b[0m                     0.6698        0.6060  0.0004  0.8400\n",
      "     23                     \u001b[36m0.7029\u001b[0m        \u001b[32m0.5721\u001b[0m                     0.6714        \u001b[31m0.6038\u001b[0m  0.0002  0.8398\n",
      "     24                     \u001b[36m0.7055\u001b[0m        \u001b[32m0.5712\u001b[0m                     \u001b[35m0.6755\u001b[0m        \u001b[31m0.6036\u001b[0m  0.0000  0.8418\n",
      "     25                     0.7003        0.5729                     0.6687        0.6037  0.0000  0.8403\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5337\u001b[0m        \u001b[32m0.9260\u001b[0m                     \u001b[35m0.5924\u001b[0m        \u001b[31m0.7068\u001b[0m  0.0100  0.8321\n",
      "      2                     \u001b[36m0.5693\u001b[0m        \u001b[32m0.7748\u001b[0m                     0.5484        0.8382  0.0100  0.8413\n",
      "      3                     \u001b[36m0.5805\u001b[0m        0.7756                     \u001b[35m0.6078\u001b[0m        \u001b[31m0.6580\u001b[0m  0.0098  0.8378\n",
      "      4                     \u001b[36m0.5977\u001b[0m        \u001b[32m0.7194\u001b[0m                     \u001b[35m0.6177\u001b[0m        0.6583  0.0096  0.8408\n",
      "      5                     \u001b[36m0.6173\u001b[0m        \u001b[32m0.6990\u001b[0m                     \u001b[35m0.6211\u001b[0m        0.6704  0.0093  0.8417\n",
      "      6                     0.6130        0.7065                     \u001b[35m0.6511\u001b[0m        \u001b[31m0.6453\u001b[0m  0.0090  0.8382\n",
      "      7                     \u001b[36m0.6309\u001b[0m        \u001b[32m0.6775\u001b[0m                     0.6349        \u001b[31m0.6438\u001b[0m  0.0085  0.8398\n",
      "      8                     0.6248        \u001b[32m0.6738\u001b[0m                     0.6389        0.6568  0.0080  0.8401\n",
      "      9                     \u001b[36m0.6435\u001b[0m        \u001b[32m0.6545\u001b[0m                     0.6365        0.6504  0.0075  0.8408\n",
      "     10                     \u001b[36m0.6447\u001b[0m        \u001b[32m0.6525\u001b[0m                     0.6417        \u001b[31m0.6373\u001b[0m  0.0069  0.8418\n",
      "     11                     \u001b[36m0.6538\u001b[0m        \u001b[32m0.6371\u001b[0m                     0.6306        0.6428  0.0063  0.8393\n",
      "     12                     \u001b[36m0.6572\u001b[0m        \u001b[32m0.6351\u001b[0m                     0.6503        \u001b[31m0.6253\u001b[0m  0.0057  0.8417\n",
      "     13                     0.6541        \u001b[32m0.6278\u001b[0m                     \u001b[35m0.6574\u001b[0m        \u001b[31m0.6178\u001b[0m  0.0050  0.8418\n",
      "     14                     \u001b[36m0.6702\u001b[0m        \u001b[32m0.6221\u001b[0m                     0.6364        0.6355  0.0043  0.8398\n",
      "     15                     \u001b[36m0.6727\u001b[0m        \u001b[32m0.6081\u001b[0m                     0.6548        0.6359  0.0037  0.8419\n",
      "     16                     \u001b[36m0.6757\u001b[0m        \u001b[32m0.6070\u001b[0m                     0.6524        0.6194  0.0031  0.8403\n",
      "     17                     \u001b[36m0.6897\u001b[0m        \u001b[32m0.5923\u001b[0m                     0.6474        0.6201  0.0025  0.8388\n",
      "     18                     \u001b[36m0.6930\u001b[0m        \u001b[32m0.5892\u001b[0m                     0.6551        0.6214  0.0020  0.8428\n",
      "     19                     \u001b[36m0.6955\u001b[0m        \u001b[32m0.5820\u001b[0m                     \u001b[35m0.6615\u001b[0m        \u001b[31m0.6140\u001b[0m  0.0015  0.8409\n",
      "     20                     \u001b[36m0.6992\u001b[0m        \u001b[32m0.5709\u001b[0m                     0.6563        0.6146  0.0010  0.8408\n",
      "     21                     \u001b[36m0.7069\u001b[0m        0.5740                     \u001b[35m0.6618\u001b[0m        \u001b[31m0.6135\u001b[0m  0.0007  0.8425\n",
      "     22                     0.7033        \u001b[32m0.5688\u001b[0m                     0.6585        \u001b[31m0.6128\u001b[0m  0.0004  0.8408\n",
      "     23                     \u001b[36m0.7080\u001b[0m        \u001b[32m0.5653\u001b[0m                     0.6590        \u001b[31m0.6127\u001b[0m  0.0002  0.8428\n",
      "     24                     \u001b[36m0.7095\u001b[0m        \u001b[32m0.5647\u001b[0m                     0.6589        0.6130  0.0000  0.8427\n",
      "     25                     0.7085        0.5650                     0.6611        0.6132  0.0000  0.8417\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5290\u001b[0m        \u001b[32m0.9829\u001b[0m                     \u001b[35m0.5716\u001b[0m        \u001b[31m0.6858\u001b[0m  0.0100  0.8358\n",
      "      2                     \u001b[36m0.5597\u001b[0m        \u001b[32m0.7527\u001b[0m                     \u001b[35m0.5830\u001b[0m        0.6871  0.0100  0.8408\n",
      "      3                     \u001b[36m0.5882\u001b[0m        \u001b[32m0.7486\u001b[0m                     \u001b[35m0.6275\u001b[0m        \u001b[31m0.6591\u001b[0m  0.0098  0.8397\n",
      "      4                     \u001b[36m0.6062\u001b[0m        \u001b[32m0.7189\u001b[0m                     0.6026        0.7056  0.0096  0.8433\n",
      "      5                     \u001b[36m0.6119\u001b[0m        \u001b[32m0.7053\u001b[0m                     0.5934        0.7395  0.0093  0.8393\n",
      "      6                     \u001b[36m0.6143\u001b[0m        \u001b[32m0.7025\u001b[0m                     0.6183        \u001b[31m0.6590\u001b[0m  0.0090  0.8405\n",
      "      7                     \u001b[36m0.6233\u001b[0m        \u001b[32m0.6989\u001b[0m                     0.5940        0.7351  0.0085  0.8423\n",
      "      8                     0.6217        \u001b[32m0.6893\u001b[0m                     \u001b[35m0.6444\u001b[0m        \u001b[31m0.6435\u001b[0m  0.0080  0.8413\n",
      "      9                     \u001b[36m0.6357\u001b[0m        \u001b[32m0.6627\u001b[0m                     0.6315        0.6450  0.0075  0.8409\n",
      "     10                     \u001b[36m0.6443\u001b[0m        \u001b[32m0.6526\u001b[0m                     0.6421        \u001b[31m0.6275\u001b[0m  0.0069  0.8418\n",
      "     11                     \u001b[36m0.6486\u001b[0m        \u001b[32m0.6459\u001b[0m                     \u001b[35m0.6453\u001b[0m        0.6455  0.0063  0.8388\n",
      "     12                     \u001b[36m0.6571\u001b[0m        \u001b[32m0.6295\u001b[0m                     0.6393        0.6516  0.0057  0.8419\n",
      "     13                     \u001b[36m0.6686\u001b[0m        \u001b[32m0.6229\u001b[0m                     \u001b[35m0.6580\u001b[0m        0.6334  0.0050  0.8398\n",
      "     14                     0.6633        \u001b[32m0.6226\u001b[0m                     0.6556        \u001b[31m0.6221\u001b[0m  0.0043  0.8397\n",
      "     15                     \u001b[36m0.6688\u001b[0m        \u001b[32m0.6173\u001b[0m                     0.6538        0.6252  0.0037  0.8417\n",
      "     16                     \u001b[36m0.6698\u001b[0m        \u001b[32m0.6087\u001b[0m                     \u001b[35m0.6646\u001b[0m        0.6230  0.0031  0.8398\n",
      "     17                     \u001b[36m0.6775\u001b[0m        \u001b[32m0.5988\u001b[0m                     0.6586        0.6233  0.0025  0.8410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18                     \u001b[36m0.6843\u001b[0m        \u001b[32m0.5936\u001b[0m                     \u001b[35m0.6678\u001b[0m        \u001b[31m0.6180\u001b[0m  0.0020  0.8426\n",
      "     19                     \u001b[36m0.6846\u001b[0m        0.5976                     0.6581        \u001b[31m0.6135\u001b[0m  0.0015  0.8418\n",
      "     20                     \u001b[36m0.6920\u001b[0m        \u001b[32m0.5836\u001b[0m                     0.6560        0.6221  0.0010  0.8438\n",
      "     21                     \u001b[36m0.6931\u001b[0m        \u001b[32m0.5816\u001b[0m                     0.6647        0.6144  0.0007  0.8408\n",
      "     22                     \u001b[36m0.6948\u001b[0m        \u001b[32m0.5811\u001b[0m                     \u001b[35m0.6709\u001b[0m        \u001b[31m0.6119\u001b[0m  0.0004  0.8418\n",
      "     23                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.5742\u001b[0m                     0.6654        \u001b[31m0.6115\u001b[0m  0.0002  0.8413\n",
      "     24                     \u001b[36m0.7003\u001b[0m        \u001b[32m0.5735\u001b[0m                     0.6673        \u001b[31m0.6113\u001b[0m  0.0000  0.8421\n",
      "     25                     \u001b[36m0.7033\u001b[0m        0.5744                     0.6659        0.6114  0.0000  0.8418\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5291\u001b[0m        \u001b[32m0.9002\u001b[0m                     \u001b[35m0.5764\u001b[0m        \u001b[31m0.6830\u001b[0m  0.0100  0.8358\n",
      "      2                     \u001b[36m0.5620\u001b[0m        \u001b[32m0.7783\u001b[0m                     \u001b[35m0.6248\u001b[0m        \u001b[31m0.6676\u001b[0m  0.0100  0.8408\n",
      "      3                     \u001b[36m0.5910\u001b[0m        \u001b[32m0.7205\u001b[0m                     0.6075        0.6868  0.0098  0.8409\n",
      "      4                     \u001b[36m0.5955\u001b[0m        0.7378                     \u001b[35m0.6262\u001b[0m        \u001b[31m0.6410\u001b[0m  0.0096  0.8428\n",
      "      5                     \u001b[36m0.6073\u001b[0m        \u001b[32m0.7166\u001b[0m                     \u001b[35m0.6528\u001b[0m        \u001b[31m0.6319\u001b[0m  0.0093  0.8408\n",
      "      6                     \u001b[36m0.6136\u001b[0m        \u001b[32m0.6927\u001b[0m                     0.6368        0.6478  0.0090  0.8408\n",
      "      7                     \u001b[36m0.6228\u001b[0m        \u001b[32m0.6715\u001b[0m                     0.6339        0.6427  0.0085  0.8399\n",
      "      8                     \u001b[36m0.6348\u001b[0m        \u001b[32m0.6619\u001b[0m                     \u001b[35m0.6742\u001b[0m        \u001b[31m0.6108\u001b[0m  0.0080  0.8399\n",
      "      9                     \u001b[36m0.6430\u001b[0m        0.6638                     0.6031        0.7048  0.0075  0.8405\n",
      "     10                     \u001b[36m0.6509\u001b[0m        \u001b[32m0.6484\u001b[0m                     0.6613        0.6205  0.0069  0.8408\n",
      "     11                     0.6429        \u001b[32m0.6440\u001b[0m                     0.6377        0.6482  0.0063  0.8414\n",
      "     12                     \u001b[36m0.6511\u001b[0m        \u001b[32m0.6391\u001b[0m                     0.6714        0.6137  0.0057  0.8418\n",
      "     13                     \u001b[36m0.6630\u001b[0m        \u001b[32m0.6226\u001b[0m                     0.6693        \u001b[31m0.6057\u001b[0m  0.0050  0.8428\n",
      "     14                     \u001b[36m0.6641\u001b[0m        0.6243                     0.6731        \u001b[31m0.6009\u001b[0m  0.0043  0.8448\n",
      "     15                     \u001b[36m0.6783\u001b[0m        \u001b[32m0.6100\u001b[0m                     \u001b[35m0.6766\u001b[0m        0.6045  0.0037  0.8385\n",
      "     16                     0.6762        0.6144                     0.6525        0.6221  0.0031  0.8427\n",
      "     17                     0.6751        \u001b[32m0.6021\u001b[0m                     0.6765        \u001b[31m0.5992\u001b[0m  0.0025  0.8431\n",
      "     18                     \u001b[36m0.6801\u001b[0m        \u001b[32m0.5989\u001b[0m                     \u001b[35m0.6847\u001b[0m        \u001b[31m0.5954\u001b[0m  0.0020  0.8387\n",
      "     19                     \u001b[36m0.6884\u001b[0m        \u001b[32m0.5866\u001b[0m                     0.6839        0.5990  0.0015  0.8406\n",
      "     20                     \u001b[36m0.6949\u001b[0m        \u001b[32m0.5815\u001b[0m                     \u001b[35m0.6855\u001b[0m        \u001b[31m0.5943\u001b[0m  0.0010  0.8421\n",
      "     21                     0.6936        \u001b[32m0.5813\u001b[0m                     0.6848        \u001b[31m0.5921\u001b[0m  0.0007  0.8398\n",
      "     22                     \u001b[36m0.7001\u001b[0m        \u001b[32m0.5771\u001b[0m                     \u001b[35m0.6917\u001b[0m        0.5921  0.0004  0.8428\n",
      "     23                     \u001b[36m0.7074\u001b[0m        \u001b[32m0.5735\u001b[0m                     0.6913        \u001b[31m0.5912\u001b[0m  0.0002  0.8428\n",
      "     24                     0.6981        0.5757                     0.6880        0.5913  0.0000  0.8418\n",
      "     25                     0.7064        \u001b[32m0.5694\u001b[0m                     0.6876        0.5913  0.0000  0.8438\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5317\u001b[0m        \u001b[32m0.9401\u001b[0m                     \u001b[35m0.5892\u001b[0m        \u001b[31m0.6896\u001b[0m  0.0100  0.8338\n",
      "      2                     \u001b[36m0.5663\u001b[0m        \u001b[32m0.7527\u001b[0m                     \u001b[35m0.6274\u001b[0m        \u001b[31m0.6552\u001b[0m  0.0100  0.8399\n",
      "      3                     \u001b[36m0.5809\u001b[0m        \u001b[32m0.7415\u001b[0m                     0.6036        0.7424  0.0098  0.8757\n",
      "      4                     \u001b[36m0.5970\u001b[0m        \u001b[32m0.7387\u001b[0m                     0.6077        0.6730  0.0096  0.8411\n",
      "      5                     \u001b[36m0.6084\u001b[0m        \u001b[32m0.7058\u001b[0m                     0.6150        0.7215  0.0093  0.8564\n",
      "      6                     \u001b[36m0.6155\u001b[0m        0.7075                     \u001b[35m0.6436\u001b[0m        \u001b[31m0.6359\u001b[0m  0.0090  0.8448\n",
      "      7                     \u001b[36m0.6192\u001b[0m        \u001b[32m0.6953\u001b[0m                     0.6109        0.7247  0.0085  0.8399\n",
      "      8                     \u001b[36m0.6334\u001b[0m        \u001b[32m0.6734\u001b[0m                     0.6270        0.6581  0.0080  0.8425\n",
      "      9                     0.6302        \u001b[32m0.6658\u001b[0m                     \u001b[35m0.6453\u001b[0m        \u001b[31m0.6308\u001b[0m  0.0075  0.8418\n",
      "     10                     \u001b[36m0.6387\u001b[0m        \u001b[32m0.6521\u001b[0m                     \u001b[35m0.6548\u001b[0m        \u001b[31m0.6297\u001b[0m  0.0069  0.8408\n",
      "     11                     \u001b[36m0.6535\u001b[0m        \u001b[32m0.6392\u001b[0m                     \u001b[35m0.6565\u001b[0m        0.6345  0.0063  0.8428\n",
      "     12                     0.6466        0.6419                     \u001b[35m0.6615\u001b[0m        \u001b[31m0.6218\u001b[0m  0.0057  0.8398\n",
      "     13                     \u001b[36m0.6572\u001b[0m        \u001b[32m0.6259\u001b[0m                     0.6530        0.6333  0.0050  0.8398\n",
      "     14                     \u001b[36m0.6667\u001b[0m        \u001b[32m0.6192\u001b[0m                     \u001b[35m0.6645\u001b[0m        \u001b[31m0.6141\u001b[0m  0.0043  0.8438\n",
      "     15                     \u001b[36m0.6667\u001b[0m        \u001b[32m0.6190\u001b[0m                     \u001b[35m0.6703\u001b[0m        0.6186  0.0037  0.8408\n",
      "     16                     \u001b[36m0.6743\u001b[0m        \u001b[32m0.6066\u001b[0m                     0.6601        0.6183  0.0031  0.8418\n",
      "     17                     \u001b[36m0.6818\u001b[0m        \u001b[32m0.6001\u001b[0m                     \u001b[35m0.6728\u001b[0m        \u001b[31m0.6106\u001b[0m  0.0025  0.8416\n",
      "     18                     \u001b[36m0.6833\u001b[0m        \u001b[32m0.5961\u001b[0m                     \u001b[35m0.6748\u001b[0m        0.6142  0.0020  0.8398\n",
      "     19                     0.6831        \u001b[32m0.5902\u001b[0m                     0.6656        0.6118  0.0015  0.8417\n",
      "     20                     \u001b[36m0.6938\u001b[0m        \u001b[32m0.5838\u001b[0m                     \u001b[35m0.6750\u001b[0m        0.6148  0.0010  0.8441\n",
      "     21                     \u001b[36m0.6945\u001b[0m        \u001b[32m0.5822\u001b[0m                     0.6662        \u001b[31m0.6081\u001b[0m  0.0007  0.8388\n",
      "     22                     0.6939        \u001b[32m0.5775\u001b[0m                     0.6741        \u001b[31m0.6044\u001b[0m  0.0004  0.8440\n",
      "     23                     \u001b[36m0.7032\u001b[0m        \u001b[32m0.5766\u001b[0m                     0.6733        0.6052  0.0002  0.8408\n",
      "     24                     \u001b[36m0.7038\u001b[0m        \u001b[32m0.5706\u001b[0m                     0.6703        0.6049  0.0000  0.8438\n",
      "     25                     \u001b[36m0.7044\u001b[0m        0.5728                     0.6732        0.6048  0.0000  0.8418\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5143\u001b[0m        \u001b[32m0.9129\u001b[0m                     \u001b[35m0.5561\u001b[0m        \u001b[31m0.8195\u001b[0m  0.0100  0.8338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                     \u001b[36m0.5723\u001b[0m        \u001b[32m0.7676\u001b[0m                     \u001b[35m0.6179\u001b[0m        \u001b[31m0.6544\u001b[0m  0.0100  0.8415\n",
      "      3                     \u001b[36m0.5844\u001b[0m        \u001b[32m0.7475\u001b[0m                     0.6100        0.6811  0.0098  0.8396\n",
      "      4                     \u001b[36m0.5908\u001b[0m        \u001b[32m0.7213\u001b[0m                     0.5924        0.6813  0.0096  0.8370\n",
      "      5                     \u001b[36m0.6017\u001b[0m        0.7240                     0.5797        0.7384  0.0093  0.8408\n",
      "      6                     \u001b[36m0.6084\u001b[0m        \u001b[32m0.7030\u001b[0m                     \u001b[35m0.6293\u001b[0m        \u001b[31m0.6542\u001b[0m  0.0090  0.8404\n",
      "      7                     \u001b[36m0.6261\u001b[0m        \u001b[32m0.6865\u001b[0m                     \u001b[35m0.6295\u001b[0m        0.6603  0.0085  0.8408\n",
      "      8                     \u001b[36m0.6315\u001b[0m        \u001b[32m0.6751\u001b[0m                     \u001b[35m0.6564\u001b[0m        \u001b[31m0.6216\u001b[0m  0.0080  0.8403\n",
      "      9                     0.6304        \u001b[32m0.6634\u001b[0m                     \u001b[35m0.6608\u001b[0m        0.6218  0.0075  0.8405\n",
      "     10                     \u001b[36m0.6382\u001b[0m        \u001b[32m0.6548\u001b[0m                     \u001b[35m0.6626\u001b[0m        \u001b[31m0.6150\u001b[0m  0.0069  0.8408\n",
      "     11                     \u001b[36m0.6439\u001b[0m        0.6562                     0.6287        0.6806  0.0063  0.8408\n",
      "     12                     \u001b[36m0.6445\u001b[0m        \u001b[32m0.6412\u001b[0m                     \u001b[35m0.6815\u001b[0m        \u001b[31m0.6086\u001b[0m  0.0057  0.8403\n",
      "     13                     \u001b[36m0.6573\u001b[0m        \u001b[32m0.6260\u001b[0m                     0.6669        0.6163  0.0050  0.8428\n",
      "     14                     \u001b[36m0.6649\u001b[0m        \u001b[32m0.6206\u001b[0m                     0.6769        0.6122  0.0043  0.8378\n",
      "     15                     \u001b[36m0.6773\u001b[0m        \u001b[32m0.6134\u001b[0m                     \u001b[35m0.6876\u001b[0m        \u001b[31m0.5951\u001b[0m  0.0037  0.8408\n",
      "     16                     0.6697        \u001b[32m0.6125\u001b[0m                     0.6729        0.6048  0.0031  0.8421\n",
      "     17                     0.6764        \u001b[32m0.6002\u001b[0m                     0.6843        0.5996  0.0025  0.8388\n",
      "     18                     \u001b[36m0.6847\u001b[0m        \u001b[32m0.5941\u001b[0m                     0.6815        0.5976  0.0020  0.8418\n",
      "     19                     \u001b[36m0.6853\u001b[0m        \u001b[32m0.5912\u001b[0m                     \u001b[35m0.6891\u001b[0m        0.5962  0.0015  0.8428\n",
      "     20                     \u001b[36m0.6892\u001b[0m        \u001b[32m0.5879\u001b[0m                     0.6866        0.5967  0.0010  0.8398\n",
      "     21                     \u001b[36m0.6922\u001b[0m        \u001b[32m0.5822\u001b[0m                     \u001b[35m0.6918\u001b[0m        \u001b[31m0.5915\u001b[0m  0.0007  0.8427\n",
      "     22                     0.6896        \u001b[32m0.5815\u001b[0m                     \u001b[35m0.6919\u001b[0m        \u001b[31m0.5907\u001b[0m  0.0004  0.8419\n",
      "     23                     \u001b[36m0.7041\u001b[0m        \u001b[32m0.5711\u001b[0m                     0.6915        \u001b[31m0.5903\u001b[0m  0.0002  0.8409\n",
      "     24                     \u001b[36m0.7047\u001b[0m        \u001b[32m0.5648\u001b[0m                     \u001b[35m0.6923\u001b[0m        0.5904  0.0000  0.8401\n",
      "     25                     0.7046        0.5712                     \u001b[35m0.6948\u001b[0m        \u001b[31m0.5899\u001b[0m  0.0000  0.8428\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5255\u001b[0m        \u001b[32m0.9106\u001b[0m                     \u001b[35m0.5663\u001b[0m        \u001b[31m0.6933\u001b[0m  0.0100  0.8338\n",
      "      2                     \u001b[36m0.5682\u001b[0m        \u001b[32m0.7648\u001b[0m                     0.5463        0.8908  0.0100  0.8413\n",
      "      3                     \u001b[36m0.5827\u001b[0m        \u001b[32m0.7390\u001b[0m                     \u001b[35m0.6108\u001b[0m        \u001b[31m0.6824\u001b[0m  0.0098  0.8389\n",
      "      4                     \u001b[36m0.5939\u001b[0m        \u001b[32m0.7237\u001b[0m                     \u001b[35m0.6126\u001b[0m        \u001b[31m0.6612\u001b[0m  0.0096  0.8408\n",
      "      5                     \u001b[36m0.6024\u001b[0m        0.7269                     \u001b[35m0.6225\u001b[0m        0.6801  0.0093  0.8409\n",
      "      6                     \u001b[36m0.6181\u001b[0m        \u001b[32m0.7040\u001b[0m                     \u001b[35m0.6486\u001b[0m        \u001b[31m0.6179\u001b[0m  0.0090  0.8415\n",
      "      7                     \u001b[36m0.6195\u001b[0m        \u001b[32m0.6934\u001b[0m                     0.6309        0.6496  0.0085  0.8418\n",
      "      8                     \u001b[36m0.6325\u001b[0m        \u001b[32m0.6755\u001b[0m                     0.6389        0.6405  0.0080  0.8418\n",
      "      9                     \u001b[36m0.6409\u001b[0m        \u001b[32m0.6584\u001b[0m                     0.6370        0.6373  0.0075  0.8408\n",
      "     10                     \u001b[36m0.6575\u001b[0m        \u001b[32m0.6458\u001b[0m                     \u001b[35m0.6568\u001b[0m        \u001b[31m0.6179\u001b[0m  0.0069  0.8434\n",
      "     11                     \u001b[36m0.6610\u001b[0m        \u001b[32m0.6387\u001b[0m                     0.6538        0.6270  0.0063  0.8393\n",
      "     12                     0.6575        \u001b[32m0.6277\u001b[0m                     0.6543        0.6195  0.0057  0.8425\n",
      "     13                     \u001b[36m0.6699\u001b[0m        \u001b[32m0.6176\u001b[0m                     \u001b[35m0.6657\u001b[0m        0.6272  0.0050  0.8418\n",
      "     14                     0.6680        0.6196                     0.6627        0.6179  0.0043  0.8398\n",
      "     15                     \u001b[36m0.6787\u001b[0m        \u001b[32m0.6065\u001b[0m                     0.6628        \u001b[31m0.6156\u001b[0m  0.0037  0.8424\n",
      "     16                     \u001b[36m0.6789\u001b[0m        \u001b[32m0.6020\u001b[0m                     \u001b[35m0.6722\u001b[0m        \u001b[31m0.6108\u001b[0m  0.0031  0.8398\n",
      "     17                     0.6788        \u001b[32m0.5985\u001b[0m                     \u001b[35m0.6733\u001b[0m        0.6118  0.0025  0.8398\n",
      "     18                     \u001b[36m0.6871\u001b[0m        \u001b[32m0.5966\u001b[0m                     0.6623        0.6115  0.0020  0.8434\n",
      "     19                     \u001b[36m0.6952\u001b[0m        \u001b[32m0.5809\u001b[0m                     0.6679        0.6143  0.0015  0.8398\n",
      "     20                     \u001b[36m0.6989\u001b[0m        0.5817                     0.6699        \u001b[31m0.6092\u001b[0m  0.0010  0.8433\n",
      "     21                     \u001b[36m0.7006\u001b[0m        \u001b[32m0.5789\u001b[0m                     0.6662        0.6097  0.0007  0.8438\n",
      "     22                     0.6998        \u001b[32m0.5715\u001b[0m                     0.6671        \u001b[31m0.6087\u001b[0m  0.0004  0.8418\n",
      "     23                     \u001b[36m0.7098\u001b[0m        \u001b[32m0.5681\u001b[0m                     0.6658        \u001b[31m0.6081\u001b[0m  0.0002  0.8434\n",
      "     24                     \u001b[36m0.7118\u001b[0m        \u001b[32m0.5666\u001b[0m                     0.6650        \u001b[31m0.6081\u001b[0m  0.0000  0.8423\n",
      "     25                     0.7108        0.5682                     0.6651        0.6081  0.0000  0.8406\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5356\u001b[0m        \u001b[32m0.9368\u001b[0m                     \u001b[35m0.5475\u001b[0m        \u001b[31m0.7757\u001b[0m  0.0100  0.8368\n",
      "      2                     \u001b[36m0.5685\u001b[0m        \u001b[32m0.7610\u001b[0m                     \u001b[35m0.5879\u001b[0m        \u001b[31m0.6841\u001b[0m  0.0100  0.8408\n",
      "      3                     \u001b[36m0.5866\u001b[0m        \u001b[32m0.7495\u001b[0m                     \u001b[35m0.6022\u001b[0m        \u001b[31m0.6755\u001b[0m  0.0098  0.8389\n",
      "      4                     \u001b[36m0.5910\u001b[0m        \u001b[32m0.7326\u001b[0m                     \u001b[35m0.6214\u001b[0m        \u001b[31m0.6593\u001b[0m  0.0096  0.8398\n",
      "      5                     \u001b[36m0.5996\u001b[0m        \u001b[32m0.7234\u001b[0m                     \u001b[35m0.6225\u001b[0m        0.6661  0.0093  0.8389\n",
      "      6                     \u001b[36m0.6138\u001b[0m        \u001b[32m0.7006\u001b[0m                     \u001b[35m0.6253\u001b[0m        0.6597  0.0090  0.8418\n",
      "      7                     \u001b[36m0.6225\u001b[0m        \u001b[32m0.6801\u001b[0m                     0.5869        0.7609  0.0085  0.8398\n",
      "      8                     \u001b[36m0.6268\u001b[0m        \u001b[32m0.6718\u001b[0m                     \u001b[35m0.6469\u001b[0m        \u001b[31m0.6381\u001b[0m  0.0080  0.8388\n",
      "      9                     \u001b[36m0.6440\u001b[0m        \u001b[32m0.6601\u001b[0m                     0.6446        0.6398  0.0075  0.8401\n",
      "     10                     0.6369        \u001b[32m0.6557\u001b[0m                     0.6213        0.6859  0.0069  0.8408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     \u001b[36m0.6476\u001b[0m        \u001b[32m0.6456\u001b[0m                     0.6410        \u001b[31m0.6331\u001b[0m  0.0063  0.8391\n",
      "     12                     \u001b[36m0.6597\u001b[0m        \u001b[32m0.6343\u001b[0m                     \u001b[35m0.6524\u001b[0m        \u001b[31m0.6278\u001b[0m  0.0057  0.8418\n",
      "     13                     0.6559        \u001b[32m0.6264\u001b[0m                     \u001b[35m0.6531\u001b[0m        \u001b[31m0.6227\u001b[0m  0.0050  0.8378\n",
      "     14                     \u001b[36m0.6638\u001b[0m        \u001b[32m0.6167\u001b[0m                     \u001b[35m0.6551\u001b[0m        0.6278  0.0043  0.8424\n",
      "     15                     \u001b[36m0.6734\u001b[0m        \u001b[32m0.6115\u001b[0m                     \u001b[35m0.6645\u001b[0m        \u001b[31m0.6206\u001b[0m  0.0037  0.8411\n",
      "     16                     \u001b[36m0.6766\u001b[0m        \u001b[32m0.6026\u001b[0m                     0.6626        0.6238  0.0031  0.8398\n",
      "     17                     \u001b[36m0.6786\u001b[0m        0.6068                     0.6585        \u001b[31m0.6149\u001b[0m  0.0025  0.8415\n",
      "     18                     \u001b[36m0.6793\u001b[0m        \u001b[32m0.5917\u001b[0m                     \u001b[35m0.6714\u001b[0m        \u001b[31m0.6122\u001b[0m  0.0020  0.8408\n",
      "     19                     \u001b[36m0.6856\u001b[0m        \u001b[32m0.5911\u001b[0m                     0.6590        0.6159  0.0015  0.8408\n",
      "     20                     \u001b[36m0.6920\u001b[0m        \u001b[32m0.5815\u001b[0m                     0.6628        \u001b[31m0.6118\u001b[0m  0.0010  0.8413\n",
      "     21                     \u001b[36m0.6932\u001b[0m        \u001b[32m0.5807\u001b[0m                     0.6670        \u001b[31m0.6075\u001b[0m  0.0007  0.8412\n",
      "     22                     \u001b[36m0.7084\u001b[0m        \u001b[32m0.5652\u001b[0m                     0.6636        0.6084  0.0004  0.8399\n",
      "     23                     0.7068        0.5682                     0.6641        0.6083  0.0002  0.8417\n",
      "     24                     0.7017        0.5714                     0.6632        0.6082  0.0000  0.8408\n",
      "     25                     \u001b[36m0.7103\u001b[0m        0.5691                     0.6679        0.6085  0.0000  0.8428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 0 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5094\u001b[0m        \u001b[32m0.9119\u001b[0m                     \u001b[35m0.5155\u001b[0m        \u001b[31m0.7650\u001b[0m  0.0100  3.3213\n",
      "      2                     \u001b[36m0.5207\u001b[0m        \u001b[32m0.7808\u001b[0m                     \u001b[35m0.5336\u001b[0m        \u001b[31m0.7075\u001b[0m  0.0100  3.0419\n",
      "      3                     \u001b[36m0.5250\u001b[0m        \u001b[32m0.7345\u001b[0m                     \u001b[35m0.5554\u001b[0m        \u001b[31m0.6898\u001b[0m  0.0098  3.0476\n",
      "      4                     \u001b[36m0.5426\u001b[0m        \u001b[32m0.7009\u001b[0m                     0.5433        \u001b[31m0.6872\u001b[0m  0.0096  3.0374\n",
      "      5                     \u001b[36m0.5472\u001b[0m        \u001b[32m0.6911\u001b[0m                     0.5475        0.6884  0.0093  3.0744\n",
      "      6                     \u001b[36m0.5501\u001b[0m        \u001b[32m0.6885\u001b[0m                     \u001b[35m0.5574\u001b[0m        \u001b[31m0.6844\u001b[0m  0.0090  3.0415\n",
      "      7                     \u001b[36m0.5609\u001b[0m        \u001b[32m0.6857\u001b[0m                     \u001b[35m0.5575\u001b[0m        0.6860  0.0085  3.0439\n",
      "      8                     0.5502        0.6876                     \u001b[35m0.5733\u001b[0m        \u001b[31m0.6804\u001b[0m  0.0080  3.0421\n",
      "      9                     0.5582        0.6861                     \u001b[35m0.5755\u001b[0m        \u001b[31m0.6792\u001b[0m  0.0075  3.0418\n",
      "     10                     0.5605        0.6865                     0.5557        0.6867  0.0069  3.0507\n",
      "     11                     0.5564        \u001b[32m0.6852\u001b[0m                     0.5702        0.6810  0.0063  3.0639\n",
      "     12                     \u001b[36m0.5652\u001b[0m        \u001b[32m0.6835\u001b[0m                     0.5708        0.6795  0.0057  3.0402\n",
      "     13                     0.5616        \u001b[32m0.6830\u001b[0m                     0.5472        0.6860  0.0050  3.0428\n",
      "     14                     \u001b[36m0.5668\u001b[0m        \u001b[32m0.6816\u001b[0m                     0.5638        0.6816  0.0043  3.0431\n",
      "     15                     \u001b[36m0.5732\u001b[0m        \u001b[32m0.6805\u001b[0m                     \u001b[35m0.5761\u001b[0m        \u001b[31m0.6779\u001b[0m  0.0037  3.0429\n",
      "     16                     0.5709        \u001b[32m0.6795\u001b[0m                     \u001b[35m0.5766\u001b[0m        0.6784  0.0031  3.0425\n",
      "     17                     \u001b[36m0.5755\u001b[0m        \u001b[32m0.6780\u001b[0m                     0.5721        0.6794  0.0025  3.0434\n",
      "     18                     \u001b[36m0.5768\u001b[0m        \u001b[32m0.6766\u001b[0m                     0.5754        0.6784  0.0020  3.0443\n",
      "     19                     \u001b[36m0.5804\u001b[0m        \u001b[32m0.6751\u001b[0m                     \u001b[35m0.5790\u001b[0m        \u001b[31m0.6777\u001b[0m  0.0015  3.0437\n",
      "     20                     \u001b[36m0.5824\u001b[0m        \u001b[32m0.6739\u001b[0m                     0.5775        0.6785  0.0010  3.0403\n",
      "     21                     \u001b[36m0.5868\u001b[0m        \u001b[32m0.6724\u001b[0m                     0.5789        0.6782  0.0007  3.0426\n",
      "     22                     \u001b[36m0.5902\u001b[0m        \u001b[32m0.6714\u001b[0m                     \u001b[35m0.5802\u001b[0m        0.6783  0.0004  3.0463\n",
      "     23                     0.5888        \u001b[32m0.6711\u001b[0m                     0.5773        0.6784  0.0002  3.0451\n",
      "     24                     0.5886        \u001b[32m0.6701\u001b[0m                     0.5780        0.6783  0.0000  3.0473\n",
      "     25                     \u001b[36m0.5909\u001b[0m        0.6703                     0.5771        0.6781  0.0000  3.0457\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5023\u001b[0m        \u001b[32m0.9027\u001b[0m                     \u001b[35m0.5173\u001b[0m        \u001b[31m0.7573\u001b[0m  0.0100  3.0359\n",
      "      2                     \u001b[36m0.5184\u001b[0m        \u001b[32m0.7675\u001b[0m                     \u001b[35m0.5462\u001b[0m        \u001b[31m0.6955\u001b[0m  0.0100  3.0408\n",
      "      3                     \u001b[36m0.5299\u001b[0m        \u001b[32m0.7239\u001b[0m                     \u001b[35m0.5567\u001b[0m        \u001b[31m0.6879\u001b[0m  0.0098  3.0412\n",
      "      4                     \u001b[36m0.5429\u001b[0m        \u001b[32m0.6976\u001b[0m                     \u001b[35m0.5582\u001b[0m        \u001b[31m0.6871\u001b[0m  0.0096  3.0412\n",
      "      5                     \u001b[36m0.5474\u001b[0m        \u001b[32m0.6900\u001b[0m                     \u001b[35m0.5736\u001b[0m        \u001b[31m0.6798\u001b[0m  0.0093  3.0425\n",
      "      6                     \u001b[36m0.5522\u001b[0m        \u001b[32m0.6881\u001b[0m                     \u001b[35m0.5749\u001b[0m        0.6803  0.0090  3.0364\n",
      "      7                     \u001b[36m0.5558\u001b[0m        \u001b[32m0.6866\u001b[0m                     0.5482        0.6876  0.0085  3.0446\n",
      "      8                     0.5527        0.6870                     \u001b[35m0.5780\u001b[0m        \u001b[31m0.6796\u001b[0m  0.0080  3.0423\n",
      "      9                     \u001b[36m0.5586\u001b[0m        \u001b[32m0.6857\u001b[0m                     0.5439        0.6909  0.0075  3.0421\n",
      "     10                     0.5553        0.6870                     0.5557        0.6870  0.0069  3.0440\n",
      "     11                     \u001b[36m0.5626\u001b[0m        0.6864                     \u001b[35m0.5798\u001b[0m        \u001b[31m0.6781\u001b[0m  0.0063  3.0453\n",
      "     12                     0.5553        0.6862                     0.5762        0.6792  0.0057  3.0449\n",
      "     13                     0.5587        \u001b[32m0.6832\u001b[0m                     0.5760        \u001b[31m0.6770\u001b[0m  0.0050  3.0439\n",
      "     14                     \u001b[36m0.5630\u001b[0m        0.6842                     0.5775        0.6794  0.0043  3.0432\n",
      "     15                     \u001b[36m0.5662\u001b[0m        \u001b[32m0.6818\u001b[0m                     \u001b[35m0.5807\u001b[0m        0.6791  0.0037  3.0481\n",
      "     16                     \u001b[36m0.5695\u001b[0m        \u001b[32m0.6801\u001b[0m                     \u001b[35m0.5859\u001b[0m        \u001b[31m0.6766\u001b[0m  0.0031  3.0441\n",
      "     17                     \u001b[36m0.5760\u001b[0m        \u001b[32m0.6780\u001b[0m                     0.5818        0.6773  0.0025  3.0431\n",
      "     18                     0.5743        0.6781                     0.5840        \u001b[31m0.6748\u001b[0m  0.0020  3.0448\n",
      "     19                     \u001b[36m0.5798\u001b[0m        \u001b[32m0.6767\u001b[0m                     \u001b[35m0.5867\u001b[0m        0.6751  0.0015  3.0476\n",
      "     20                     \u001b[36m0.5798\u001b[0m        \u001b[32m0.6756\u001b[0m                     \u001b[35m0.5928\u001b[0m        \u001b[31m0.6744\u001b[0m  0.0010  3.0515\n",
      "     21                     \u001b[36m0.5822\u001b[0m        \u001b[32m0.6738\u001b[0m                     0.5913        0.6746  0.0007  3.0481\n",
      "     22                     0.5810        \u001b[32m0.6737\u001b[0m                     0.5927        \u001b[31m0.6742\u001b[0m  0.0004  3.0464\n",
      "     23                     0.5815        \u001b[32m0.6727\u001b[0m                     \u001b[35m0.5930\u001b[0m        \u001b[31m0.6741\u001b[0m  0.0002  3.0468\n",
      "     24                     \u001b[36m0.5859\u001b[0m        \u001b[32m0.6722\u001b[0m                     0.5921        \u001b[31m0.6740\u001b[0m  0.0000  3.0748\n",
      "     25                     0.5857        0.6732                     \u001b[35m0.5932\u001b[0m        0.6741  0.0000  3.0708\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5067\u001b[0m        \u001b[32m0.8908\u001b[0m                     \u001b[35m0.4989\u001b[0m        \u001b[31m0.7557\u001b[0m  0.0100  3.0471\n",
      "      2                     \u001b[36m0.5170\u001b[0m        \u001b[32m0.7821\u001b[0m                     \u001b[35m0.5174\u001b[0m        0.8317  0.0100  3.0419\n",
      "      3                     \u001b[36m0.5269\u001b[0m        \u001b[32m0.7321\u001b[0m                     \u001b[35m0.5328\u001b[0m        \u001b[31m0.7089\u001b[0m  0.0098  3.0391\n",
      "      4                     \u001b[36m0.5365\u001b[0m        \u001b[32m0.7017\u001b[0m                     \u001b[35m0.5606\u001b[0m        \u001b[31m0.6801\u001b[0m  0.0096  3.0429\n",
      "      5                     \u001b[36m0.5522\u001b[0m        \u001b[32m0.6901\u001b[0m                     \u001b[35m0.5666\u001b[0m        \u001b[31m0.6790\u001b[0m  0.0093  3.0434\n",
      "      6                     0.5515        \u001b[32m0.6891\u001b[0m                     \u001b[35m0.5733\u001b[0m        \u001b[31m0.6768\u001b[0m  0.0090  3.0413\n",
      "      7                     \u001b[36m0.5531\u001b[0m        \u001b[32m0.6883\u001b[0m                     0.5685        0.6790  0.0085  3.0450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8                     0.5486        0.6887                     0.5672        0.6807  0.0080  3.0447\n",
      "      9                     \u001b[36m0.5568\u001b[0m        \u001b[32m0.6880\u001b[0m                     \u001b[35m0.5771\u001b[0m        \u001b[31m0.6753\u001b[0m  0.0075  3.0391\n",
      "     10                     \u001b[36m0.5576\u001b[0m        \u001b[32m0.6863\u001b[0m                     0.5754        0.6791  0.0069  3.0456\n",
      "     11                     \u001b[36m0.5595\u001b[0m        \u001b[32m0.6857\u001b[0m                     0.5669        0.6775  0.0063  3.0438\n",
      "     12                     0.5578        0.6861                     \u001b[35m0.5835\u001b[0m        \u001b[31m0.6747\u001b[0m  0.0057  3.0462\n",
      "     13                     0.5592        \u001b[32m0.6839\u001b[0m                     0.5807        0.6778  0.0050  3.0434\n",
      "     14                     \u001b[36m0.5658\u001b[0m        \u001b[32m0.6830\u001b[0m                     0.5700        0.6758  0.0043  3.0529\n",
      "     15                     \u001b[36m0.5662\u001b[0m        \u001b[32m0.6815\u001b[0m                     0.5376        0.6835  0.0037  3.0467\n",
      "     16                     \u001b[36m0.5685\u001b[0m        \u001b[32m0.6806\u001b[0m                     \u001b[35m0.5854\u001b[0m        \u001b[31m0.6736\u001b[0m  0.0031  3.0459\n",
      "     17                     \u001b[36m0.5721\u001b[0m        \u001b[32m0.6787\u001b[0m                     0.5661        0.6760  0.0025  3.0407\n",
      "     18                     \u001b[36m0.5752\u001b[0m        \u001b[32m0.6777\u001b[0m                     0.5825        \u001b[31m0.6729\u001b[0m  0.0020  3.0429\n",
      "     19                     \u001b[36m0.5812\u001b[0m        \u001b[32m0.6768\u001b[0m                     0.5734        0.6733  0.0015  3.0451\n",
      "     20                     \u001b[36m0.5830\u001b[0m        \u001b[32m0.6755\u001b[0m                     \u001b[35m0.5855\u001b[0m        \u001b[31m0.6715\u001b[0m  0.0010  3.0472\n",
      "     21                     0.5826        \u001b[32m0.6743\u001b[0m                     \u001b[35m0.5890\u001b[0m        0.6717  0.0007  3.0459\n",
      "     22                     0.5819        \u001b[32m0.6743\u001b[0m                     0.5887        \u001b[31m0.6711\u001b[0m  0.0004  3.0459\n",
      "     23                     \u001b[36m0.5871\u001b[0m        \u001b[32m0.6725\u001b[0m                     0.5872        \u001b[31m0.6708\u001b[0m  0.0002  3.0466\n",
      "     24                     0.5837        0.6727                     0.5882        0.6709  0.0000  3.0470\n",
      "     25                     0.5822        0.6727                     0.5875        0.6709  0.0000  3.0463\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4992\u001b[0m        \u001b[32m0.9021\u001b[0m                     \u001b[35m0.5167\u001b[0m        \u001b[31m0.7217\u001b[0m  0.0100  3.0341\n",
      "      2                     \u001b[36m0.5141\u001b[0m        \u001b[32m0.7789\u001b[0m                     \u001b[35m0.5232\u001b[0m        0.7348  0.0100  3.0426\n",
      "      3                     \u001b[36m0.5246\u001b[0m        \u001b[32m0.7369\u001b[0m                     \u001b[35m0.5374\u001b[0m        \u001b[31m0.6899\u001b[0m  0.0098  3.0409\n",
      "      4                     \u001b[36m0.5402\u001b[0m        \u001b[32m0.7044\u001b[0m                     \u001b[35m0.5539\u001b[0m        \u001b[31m0.6850\u001b[0m  0.0096  3.0390\n",
      "      5                     \u001b[36m0.5479\u001b[0m        \u001b[32m0.6916\u001b[0m                     \u001b[35m0.5583\u001b[0m        \u001b[31m0.6840\u001b[0m  0.0093  3.0445\n",
      "      6                     \u001b[36m0.5534\u001b[0m        \u001b[32m0.6883\u001b[0m                     \u001b[35m0.5692\u001b[0m        \u001b[31m0.6818\u001b[0m  0.0090  3.0434\n",
      "      7                     \u001b[36m0.5550\u001b[0m        \u001b[32m0.6863\u001b[0m                     \u001b[35m0.5695\u001b[0m        \u001b[31m0.6802\u001b[0m  0.0085  3.0436\n",
      "      8                     \u001b[36m0.5563\u001b[0m        0.6881                     \u001b[35m0.5747\u001b[0m        \u001b[31m0.6779\u001b[0m  0.0080  3.0488\n",
      "      9                     \u001b[36m0.5586\u001b[0m        0.6864                     0.5653        0.6806  0.0075  3.0414\n",
      "     10                     0.5573        \u001b[32m0.6860\u001b[0m                     0.5666        0.6811  0.0069  3.0426\n",
      "     11                     0.5577        \u001b[32m0.6846\u001b[0m                     0.5485        0.6927  0.0063  3.0474\n",
      "     12                     \u001b[36m0.5608\u001b[0m        0.6852                     0.5728        0.6795  0.0057  3.0781\n",
      "     13                     \u001b[36m0.5644\u001b[0m        \u001b[32m0.6826\u001b[0m                     0.5738        \u001b[31m0.6779\u001b[0m  0.0050  3.0668\n",
      "     14                     0.5635        0.6830                     \u001b[35m0.5756\u001b[0m        0.6789  0.0043  3.0464\n",
      "     15                     \u001b[36m0.5694\u001b[0m        \u001b[32m0.6814\u001b[0m                     0.5696        0.6795  0.0037  3.0474\n",
      "     16                     \u001b[36m0.5724\u001b[0m        \u001b[32m0.6787\u001b[0m                     0.5645        0.6825  0.0031  3.0451\n",
      "     17                     \u001b[36m0.5766\u001b[0m        \u001b[32m0.6779\u001b[0m                     0.5730        \u001b[31m0.6779\u001b[0m  0.0025  3.0468\n",
      "     18                     \u001b[36m0.5773\u001b[0m        \u001b[32m0.6763\u001b[0m                     0.5712        0.6783  0.0020  3.0458\n",
      "     19                     \u001b[36m0.5782\u001b[0m        \u001b[32m0.6754\u001b[0m                     0.5744        0.6784  0.0015  3.0469\n",
      "     20                     0.5780        0.6755                     \u001b[35m0.5815\u001b[0m        \u001b[31m0.6760\u001b[0m  0.0010  3.0475\n",
      "     21                     \u001b[36m0.5808\u001b[0m        \u001b[32m0.6745\u001b[0m                     \u001b[35m0.5820\u001b[0m        0.6763  0.0007  3.0488\n",
      "     22                     \u001b[36m0.5851\u001b[0m        \u001b[32m0.6721\u001b[0m                     0.5810        0.6762  0.0004  3.0460\n",
      "     23                     \u001b[36m0.5868\u001b[0m        0.6722                     0.5809        0.6761  0.0002  3.0513\n",
      "     24                     \u001b[36m0.5886\u001b[0m        \u001b[32m0.6711\u001b[0m                     0.5818        0.6760  0.0000  3.0464\n",
      "     25                     0.5864        0.6718                     0.5814        0.6760  0.0000  3.0505\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5053\u001b[0m        \u001b[32m0.8892\u001b[0m                     \u001b[35m0.5129\u001b[0m        \u001b[31m0.7066\u001b[0m  0.0100  3.0358\n",
      "      2                     \u001b[36m0.5127\u001b[0m        \u001b[32m0.7878\u001b[0m                     \u001b[35m0.5198\u001b[0m        \u001b[31m0.7054\u001b[0m  0.0100  3.0458\n",
      "      3                     \u001b[36m0.5192\u001b[0m        \u001b[32m0.7327\u001b[0m                     \u001b[35m0.5403\u001b[0m        \u001b[31m0.6893\u001b[0m  0.0098  3.0431\n",
      "      4                     \u001b[36m0.5244\u001b[0m        \u001b[32m0.7102\u001b[0m                     \u001b[35m0.5611\u001b[0m        \u001b[31m0.6853\u001b[0m  0.0096  3.0414\n",
      "      5                     \u001b[36m0.5408\u001b[0m        \u001b[32m0.6965\u001b[0m                     0.5602        \u001b[31m0.6799\u001b[0m  0.0093  3.0410\n",
      "      6                     \u001b[36m0.5437\u001b[0m        \u001b[32m0.6917\u001b[0m                     \u001b[35m0.5737\u001b[0m        \u001b[31m0.6777\u001b[0m  0.0090  3.0434\n",
      "      7                     \u001b[36m0.5476\u001b[0m        \u001b[32m0.6900\u001b[0m                     \u001b[35m0.5750\u001b[0m        0.6783  0.0085  3.0421\n",
      "      8                     0.5457        \u001b[32m0.6892\u001b[0m                     \u001b[35m0.5893\u001b[0m        \u001b[31m0.6760\u001b[0m  0.0080  4.1005\n",
      "      9                     \u001b[36m0.5510\u001b[0m        \u001b[32m0.6882\u001b[0m                     0.5813        \u001b[31m0.6752\u001b[0m  0.0075  3.1050\n",
      "     10                     \u001b[36m0.5561\u001b[0m        \u001b[32m0.6875\u001b[0m                     0.5495        0.6840  0.0069  3.0635\n",
      "     11                     0.5547        \u001b[32m0.6868\u001b[0m                     0.5733        0.6777  0.0063  3.0160\n",
      "     12                     0.5552        0.6873                     0.5553        0.6819  0.0057  3.0189\n",
      "     13                     0.5556        \u001b[32m0.6851\u001b[0m                     0.5649        0.6784  0.0050  3.0173\n",
      "     14                     \u001b[36m0.5594\u001b[0m        \u001b[32m0.6846\u001b[0m                     0.5837        \u001b[31m0.6744\u001b[0m  0.0043  3.0179\n",
      "     15                     \u001b[36m0.5647\u001b[0m        \u001b[32m0.6823\u001b[0m                     0.5850        \u001b[31m0.6741\u001b[0m  0.0037  3.0144\n",
      "     16                     \u001b[36m0.5659\u001b[0m        \u001b[32m0.6811\u001b[0m                     0.5888        \u001b[31m0.6729\u001b[0m  0.0031  3.0196\n",
      "     17                     \u001b[36m0.5695\u001b[0m        \u001b[32m0.6803\u001b[0m                     0.5887        0.6730  0.0025  3.0172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18                     \u001b[36m0.5722\u001b[0m        \u001b[32m0.6791\u001b[0m                     0.5883        0.6735  0.0020  3.0185\n",
      "     19                     \u001b[36m0.5728\u001b[0m        \u001b[32m0.6785\u001b[0m                     \u001b[35m0.5923\u001b[0m        \u001b[31m0.6715\u001b[0m  0.0015  3.0170\n",
      "     20                     \u001b[36m0.5767\u001b[0m        \u001b[32m0.6765\u001b[0m                     \u001b[35m0.5927\u001b[0m        \u001b[31m0.6706\u001b[0m  0.0010  3.0206\n",
      "     21                     \u001b[36m0.5796\u001b[0m        \u001b[32m0.6747\u001b[0m                     \u001b[35m0.5974\u001b[0m        \u001b[31m0.6696\u001b[0m  0.0007  3.0192\n",
      "     22                     \u001b[36m0.5843\u001b[0m        \u001b[32m0.6746\u001b[0m                     0.5943        0.6697  0.0004  3.0210\n",
      "     23                     0.5798        0.6748                     0.5964        \u001b[31m0.6694\u001b[0m  0.0002  3.0169\n",
      "     24                     0.5839        \u001b[32m0.6740\u001b[0m                     0.5964        0.6698  0.0000  3.0209\n",
      "     25                     \u001b[36m0.5862\u001b[0m        \u001b[32m0.6740\u001b[0m                     0.5966        0.6695  0.0000  3.0201\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5060\u001b[0m        \u001b[32m0.8849\u001b[0m                     \u001b[35m0.5032\u001b[0m        \u001b[31m0.7108\u001b[0m  0.0100  3.0086\n",
      "      2                     \u001b[36m0.5117\u001b[0m        \u001b[32m0.7932\u001b[0m                     \u001b[35m0.5155\u001b[0m        0.7382  0.0100  3.0104\n",
      "      3                     \u001b[36m0.5314\u001b[0m        \u001b[32m0.7291\u001b[0m                     \u001b[35m0.5426\u001b[0m        \u001b[31m0.6951\u001b[0m  0.0098  3.0121\n",
      "      4                     \u001b[36m0.5416\u001b[0m        \u001b[32m0.7017\u001b[0m                     \u001b[35m0.5550\u001b[0m        \u001b[31m0.6885\u001b[0m  0.0096  3.0120\n",
      "      5                     \u001b[36m0.5470\u001b[0m        \u001b[32m0.6923\u001b[0m                     0.5497        \u001b[31m0.6869\u001b[0m  0.0093  3.0151\n",
      "      6                     \u001b[36m0.5586\u001b[0m        \u001b[32m0.6867\u001b[0m                     \u001b[35m0.5609\u001b[0m        \u001b[31m0.6848\u001b[0m  0.0090  3.0130\n",
      "      7                     0.5516        0.6890                     \u001b[35m0.5636\u001b[0m        \u001b[31m0.6809\u001b[0m  0.0085  3.0152\n",
      "      8                     \u001b[36m0.5622\u001b[0m        \u001b[32m0.6863\u001b[0m                     \u001b[35m0.5678\u001b[0m        \u001b[31m0.6794\u001b[0m  0.0080  3.0112\n",
      "      9                     0.5576        0.6869                     \u001b[35m0.5687\u001b[0m        0.6808  0.0075  3.0164\n",
      "     10                     0.5569        \u001b[32m0.6859\u001b[0m                     0.5631        0.6818  0.0069  3.0138\n",
      "     11                     0.5606        \u001b[32m0.6855\u001b[0m                     0.5663        0.6796  0.0063  3.0156\n",
      "     12                     0.5592        0.6855                     \u001b[35m0.5755\u001b[0m        0.6796  0.0057  3.0132\n",
      "     13                     0.5597        \u001b[32m0.6844\u001b[0m                     0.5605        0.6850  0.0050  3.0167\n",
      "     14                     \u001b[36m0.5704\u001b[0m        \u001b[32m0.6809\u001b[0m                     \u001b[35m0.5797\u001b[0m        \u001b[31m0.6771\u001b[0m  0.0043  3.0446\n",
      "     15                     0.5691        0.6810                     0.5759        0.6790  0.0037  3.0166\n",
      "     16                     \u001b[36m0.5772\u001b[0m        \u001b[32m0.6780\u001b[0m                     \u001b[35m0.5833\u001b[0m        \u001b[31m0.6767\u001b[0m  0.0031  3.0177\n",
      "     17                     0.5760        \u001b[32m0.6771\u001b[0m                     0.5750        0.6789  0.0025  3.0168\n",
      "     18                     0.5751        0.6772                     0.5826        0.6777  0.0020  3.0158\n",
      "     19                     \u001b[36m0.5810\u001b[0m        \u001b[32m0.6750\u001b[0m                     0.5756        \u001b[31m0.6763\u001b[0m  0.0015  3.0172\n",
      "     20                     \u001b[36m0.5845\u001b[0m        \u001b[32m0.6730\u001b[0m                     0.5792        0.6766  0.0010  3.0163\n",
      "     21                     \u001b[36m0.5889\u001b[0m        \u001b[32m0.6724\u001b[0m                     0.5801        \u001b[31m0.6761\u001b[0m  0.0007  3.0183\n",
      "     22                     0.5880        \u001b[32m0.6717\u001b[0m                     0.5824        0.6763  0.0004  3.0180\n",
      "     23                     0.5876        \u001b[32m0.6706\u001b[0m                     0.5789        0.6763  0.0002  3.0189\n",
      "     24                     0.5868        \u001b[32m0.6705\u001b[0m                     0.5809        0.6763  0.0000  3.0196\n",
      "     25                     0.5878        \u001b[32m0.6704\u001b[0m                     0.5811        0.6762  0.0000  3.0210\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5083\u001b[0m        \u001b[32m0.9018\u001b[0m                     \u001b[35m0.5168\u001b[0m        \u001b[31m0.7474\u001b[0m  0.0100  3.0057\n",
      "      2                     \u001b[36m0.5153\u001b[0m        \u001b[32m0.7847\u001b[0m                     \u001b[35m0.5496\u001b[0m        \u001b[31m0.7074\u001b[0m  0.0100  3.0153\n",
      "      3                     \u001b[36m0.5343\u001b[0m        \u001b[32m0.7245\u001b[0m                     0.5407        \u001b[31m0.6905\u001b[0m  0.0098  3.0166\n",
      "      4                     \u001b[36m0.5460\u001b[0m        \u001b[32m0.6984\u001b[0m                     \u001b[35m0.5529\u001b[0m        \u001b[31m0.6865\u001b[0m  0.0096  3.0174\n",
      "      5                     \u001b[36m0.5490\u001b[0m        \u001b[32m0.6911\u001b[0m                     0.5519        0.6871  0.0093  3.0133\n",
      "      6                     \u001b[36m0.5568\u001b[0m        \u001b[32m0.6882\u001b[0m                     \u001b[35m0.5763\u001b[0m        \u001b[31m0.6802\u001b[0m  0.0090  3.0472\n",
      "      7                     0.5516        \u001b[32m0.6878\u001b[0m                     0.5626        0.6855  0.0085  3.0189\n",
      "      8                     \u001b[36m0.5576\u001b[0m        \u001b[32m0.6875\u001b[0m                     \u001b[35m0.5790\u001b[0m        \u001b[31m0.6785\u001b[0m  0.0080  3.0167\n",
      "      9                     0.5545        \u001b[32m0.6870\u001b[0m                     \u001b[35m0.5836\u001b[0m        0.6786  0.0075  3.0177\n",
      "     10                     \u001b[36m0.5593\u001b[0m        0.6871                     0.5791        0.6804  0.0069  3.0192\n",
      "     11                     0.5557        \u001b[32m0.6866\u001b[0m                     0.5823        \u001b[31m0.6777\u001b[0m  0.0063  3.0181\n",
      "     12                     \u001b[36m0.5617\u001b[0m        \u001b[32m0.6851\u001b[0m                     0.5749        0.6809  0.0057  3.0197\n",
      "     13                     0.5603        \u001b[32m0.6842\u001b[0m                     \u001b[35m0.5856\u001b[0m        \u001b[31m0.6757\u001b[0m  0.0050  3.0173\n",
      "     14                     \u001b[36m0.5627\u001b[0m        \u001b[32m0.6831\u001b[0m                     \u001b[35m0.5905\u001b[0m        \u001b[31m0.6748\u001b[0m  0.0043  3.0222\n",
      "     15                     \u001b[36m0.5686\u001b[0m        \u001b[32m0.6805\u001b[0m                     0.5866        0.6752  0.0037  3.0174\n",
      "     16                     \u001b[36m0.5720\u001b[0m        \u001b[32m0.6800\u001b[0m                     \u001b[35m0.5940\u001b[0m        0.6750  0.0031  3.0192\n",
      "     17                     \u001b[36m0.5786\u001b[0m        \u001b[32m0.6775\u001b[0m                     0.5810        \u001b[31m0.6744\u001b[0m  0.0025  3.0186\n",
      "     18                     \u001b[36m0.5795\u001b[0m        \u001b[32m0.6768\u001b[0m                     \u001b[35m0.5974\u001b[0m        \u001b[31m0.6744\u001b[0m  0.0020  3.0207\n",
      "     19                     \u001b[36m0.5832\u001b[0m        \u001b[32m0.6753\u001b[0m                     0.5808        \u001b[31m0.6737\u001b[0m  0.0015  3.0184\n",
      "     20                     0.5827        \u001b[32m0.6744\u001b[0m                     0.5933        \u001b[31m0.6733\u001b[0m  0.0010  3.0207\n",
      "     21                     0.5829        \u001b[32m0.6741\u001b[0m                     0.5933        \u001b[31m0.6731\u001b[0m  0.0007  3.0182\n",
      "     22                     \u001b[36m0.5892\u001b[0m        \u001b[32m0.6717\u001b[0m                     0.5906        \u001b[31m0.6725\u001b[0m  0.0004  3.0223\n",
      "     23                     \u001b[36m0.5913\u001b[0m        0.6729                     0.5934        0.6728  0.0002  3.0205\n",
      "     24                     0.5900        \u001b[32m0.6710\u001b[0m                     0.5939        0.6727  0.0000  3.0212\n",
      "     25                     0.5887        0.6710                     0.5906        0.6732  0.0000  3.0214\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5062\u001b[0m        \u001b[32m0.8917\u001b[0m                     \u001b[35m0.5116\u001b[0m        \u001b[31m0.7482\u001b[0m  0.0100  3.0066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                     \u001b[36m0.5184\u001b[0m        \u001b[32m0.7799\u001b[0m                     \u001b[35m0.5233\u001b[0m        \u001b[31m0.7191\u001b[0m  0.0100  3.0116\n",
      "      3                     \u001b[36m0.5347\u001b[0m        \u001b[32m0.7175\u001b[0m                     \u001b[35m0.5509\u001b[0m        \u001b[31m0.6858\u001b[0m  0.0098  3.0149\n",
      "      4                     \u001b[36m0.5410\u001b[0m        \u001b[32m0.6966\u001b[0m                     \u001b[35m0.5600\u001b[0m        \u001b[31m0.6826\u001b[0m  0.0096  3.0154\n",
      "      5                     \u001b[36m0.5506\u001b[0m        \u001b[32m0.6897\u001b[0m                     \u001b[35m0.5610\u001b[0m        \u001b[31m0.6826\u001b[0m  0.0093  3.0174\n",
      "      6                     \u001b[36m0.5557\u001b[0m        \u001b[32m0.6881\u001b[0m                     0.5603        \u001b[31m0.6823\u001b[0m  0.0090  3.0140\n",
      "      7                     0.5522        \u001b[32m0.6879\u001b[0m                     \u001b[35m0.5740\u001b[0m        \u001b[31m0.6799\u001b[0m  0.0085  3.0159\n",
      "      8                     \u001b[36m0.5588\u001b[0m        \u001b[32m0.6865\u001b[0m                     0.5663        0.6824  0.0080  3.0153\n",
      "      9                     0.5569        0.6875                     0.5600        0.6854  0.0075  3.0164\n",
      "     10                     0.5557        0.6870                     0.5663        0.6804  0.0069  3.0160\n",
      "     11                     0.5577        \u001b[32m0.6860\u001b[0m                     0.5632        0.6805  0.0063  3.0177\n",
      "     12                     \u001b[36m0.5647\u001b[0m        \u001b[32m0.6842\u001b[0m                     \u001b[35m0.5772\u001b[0m        \u001b[31m0.6790\u001b[0m  0.0057  3.0155\n",
      "     13                     0.5627        \u001b[32m0.6835\u001b[0m                     0.5622        0.6808  0.0050  3.0157\n",
      "     14                     \u001b[36m0.5671\u001b[0m        \u001b[32m0.6819\u001b[0m                     0.5528        0.6863  0.0043  3.0156\n",
      "     15                     0.5666        \u001b[32m0.6806\u001b[0m                     0.5755        \u001b[31m0.6785\u001b[0m  0.0037  3.0190\n",
      "     16                     \u001b[36m0.5720\u001b[0m        \u001b[32m0.6799\u001b[0m                     0.5762        0.6794  0.0031  3.0167\n",
      "     17                     0.5675        \u001b[32m0.6789\u001b[0m                     \u001b[35m0.5810\u001b[0m        \u001b[31m0.6773\u001b[0m  0.0025  3.0183\n",
      "     18                     \u001b[36m0.5743\u001b[0m        \u001b[32m0.6769\u001b[0m                     0.5808        0.6782  0.0020  3.0181\n",
      "     19                     \u001b[36m0.5794\u001b[0m        \u001b[32m0.6757\u001b[0m                     0.5737        0.6789  0.0015  3.0193\n",
      "     20                     \u001b[36m0.5821\u001b[0m        \u001b[32m0.6746\u001b[0m                     \u001b[35m0.5812\u001b[0m        0.6776  0.0010  3.0178\n",
      "     21                     \u001b[36m0.5867\u001b[0m        \u001b[32m0.6736\u001b[0m                     \u001b[35m0.5829\u001b[0m        \u001b[31m0.6768\u001b[0m  0.0007  3.0202\n",
      "     22                     0.5848        \u001b[32m0.6732\u001b[0m                     \u001b[35m0.5838\u001b[0m        \u001b[31m0.6767\u001b[0m  0.0004  3.0189\n",
      "     23                     \u001b[36m0.5912\u001b[0m        \u001b[32m0.6715\u001b[0m                     0.5819        \u001b[31m0.6766\u001b[0m  0.0002  3.0186\n",
      "     24                     0.5848        0.6724                     0.5834        0.6766  0.0000  3.0188\n",
      "     25                     0.5848        0.6724                     0.5825        0.6766  0.0000  3.0216\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5012\u001b[0m        \u001b[32m0.9059\u001b[0m                     \u001b[35m0.5202\u001b[0m        \u001b[31m0.7077\u001b[0m  0.0100  3.0103\n",
      "      2                     \u001b[36m0.5151\u001b[0m        \u001b[32m0.7750\u001b[0m                     \u001b[35m0.5276\u001b[0m        0.7101  0.0100  3.0129\n",
      "      3                     \u001b[36m0.5302\u001b[0m        \u001b[32m0.7199\u001b[0m                     \u001b[35m0.5606\u001b[0m        \u001b[31m0.6822\u001b[0m  0.0098  3.0179\n",
      "      4                     \u001b[36m0.5431\u001b[0m        \u001b[32m0.6996\u001b[0m                     0.5536        0.6829  0.0096  3.0152\n",
      "      5                     \u001b[36m0.5492\u001b[0m        \u001b[32m0.6910\u001b[0m                     0.5598        \u001b[31m0.6815\u001b[0m  0.0093  3.0137\n",
      "      6                     \u001b[36m0.5515\u001b[0m        \u001b[32m0.6893\u001b[0m                     0.5151        0.7041  0.0090  3.0159\n",
      "      7                     \u001b[36m0.5517\u001b[0m        \u001b[32m0.6886\u001b[0m                     \u001b[35m0.5803\u001b[0m        \u001b[31m0.6748\u001b[0m  0.0085  3.0157\n",
      "      8                     \u001b[36m0.5523\u001b[0m        \u001b[32m0.6877\u001b[0m                     0.5789        0.6790  0.0080  3.0147\n",
      "      9                     \u001b[36m0.5532\u001b[0m        \u001b[32m0.6869\u001b[0m                     0.5788        0.6751  0.0075  3.0147\n",
      "     10                     \u001b[36m0.5581\u001b[0m        \u001b[32m0.6865\u001b[0m                     0.5779        0.6761  0.0069  3.0161\n",
      "     11                     0.5517        0.6868                     0.5691        0.6791  0.0063  3.0168\n",
      "     12                     \u001b[36m0.5626\u001b[0m        \u001b[32m0.6851\u001b[0m                     0.5785        0.6761  0.0057  3.0160\n",
      "     13                     0.5624        \u001b[32m0.6841\u001b[0m                     0.5779        0.6758  0.0050  3.0145\n",
      "     14                     \u001b[36m0.5634\u001b[0m        \u001b[32m0.6822\u001b[0m                     0.5735        0.6765  0.0043  3.0185\n",
      "     15                     \u001b[36m0.5676\u001b[0m        \u001b[32m0.6811\u001b[0m                     \u001b[35m0.5883\u001b[0m        \u001b[31m0.6743\u001b[0m  0.0037  3.0161\n",
      "     16                     \u001b[36m0.5705\u001b[0m        \u001b[32m0.6796\u001b[0m                     0.5737        0.6780  0.0031  3.0171\n",
      "     17                     \u001b[36m0.5720\u001b[0m        \u001b[32m0.6789\u001b[0m                     0.5781        \u001b[31m0.6735\u001b[0m  0.0025  3.0164\n",
      "     18                     \u001b[36m0.5809\u001b[0m        \u001b[32m0.6761\u001b[0m                     \u001b[35m0.5885\u001b[0m        \u001b[31m0.6728\u001b[0m  0.0020  3.0196\n",
      "     19                     \u001b[36m0.5815\u001b[0m        \u001b[32m0.6753\u001b[0m                     0.5861        0.6733  0.0015  3.0176\n",
      "     20                     0.5798        \u001b[32m0.6749\u001b[0m                     \u001b[35m0.5915\u001b[0m        \u001b[31m0.6724\u001b[0m  0.0010  3.0181\n",
      "     21                     \u001b[36m0.5853\u001b[0m        \u001b[32m0.6727\u001b[0m                     \u001b[35m0.5919\u001b[0m        \u001b[31m0.6722\u001b[0m  0.0007  3.0167\n",
      "     22                     0.5821        0.6733                     0.5868        0.6724  0.0004  3.0195\n",
      "     23                     \u001b[36m0.5870\u001b[0m        \u001b[32m0.6718\u001b[0m                     0.5895        \u001b[31m0.6719\u001b[0m  0.0002  3.0177\n",
      "     24                     \u001b[36m0.5873\u001b[0m        0.6721                     0.5907        0.6719  0.0000  3.0199\n",
      "     25                     0.5873        0.6726                     0.5909        \u001b[31m0.6719\u001b[0m  0.0000  3.0183\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5006\u001b[0m        \u001b[32m0.8902\u001b[0m                     \u001b[35m0.5130\u001b[0m        \u001b[31m0.7851\u001b[0m  0.0100  3.0046\n",
      "      2                     \u001b[36m0.5101\u001b[0m        \u001b[32m0.7857\u001b[0m                     \u001b[35m0.5189\u001b[0m        \u001b[31m0.7447\u001b[0m  0.0100  3.0114\n",
      "      3                     \u001b[36m0.5276\u001b[0m        \u001b[32m0.7362\u001b[0m                     \u001b[35m0.5257\u001b[0m        \u001b[31m0.7122\u001b[0m  0.0098  3.0140\n",
      "      4                     \u001b[36m0.5397\u001b[0m        \u001b[32m0.7067\u001b[0m                     \u001b[35m0.5461\u001b[0m        \u001b[31m0.6879\u001b[0m  0.0096  3.0120\n",
      "      5                     \u001b[36m0.5508\u001b[0m        \u001b[32m0.6920\u001b[0m                     \u001b[35m0.5521\u001b[0m        0.6927  0.0093  3.0133\n",
      "      6                     \u001b[36m0.5537\u001b[0m        \u001b[32m0.6886\u001b[0m                     \u001b[35m0.5651\u001b[0m        \u001b[31m0.6828\u001b[0m  0.0090  3.0185\n",
      "      7                     \u001b[36m0.5545\u001b[0m        \u001b[32m0.6882\u001b[0m                     \u001b[35m0.5725\u001b[0m        \u001b[31m0.6800\u001b[0m  0.0085  3.0157\n",
      "      8                     0.5539        \u001b[32m0.6868\u001b[0m                     0.5702        0.6808  0.0080  3.0159\n",
      "      9                     \u001b[36m0.5552\u001b[0m        \u001b[32m0.6853\u001b[0m                     0.5581        0.6824  0.0075  3.0164\n",
      "     10                     \u001b[36m0.5570\u001b[0m        0.6876                     0.5647        0.6811  0.0069  3.0169\n",
      "     11                     \u001b[36m0.5570\u001b[0m        0.6858                     \u001b[35m0.5793\u001b[0m        \u001b[31m0.6774\u001b[0m  0.0063  3.0194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     12                     \u001b[36m0.5608\u001b[0m        \u001b[32m0.6845\u001b[0m                     0.5717        0.6802  0.0057  3.0157\n",
      "     13                     \u001b[36m0.5677\u001b[0m        \u001b[32m0.6830\u001b[0m                     0.5673        0.6814  0.0050  3.0198\n",
      "     14                     \u001b[36m0.5699\u001b[0m        \u001b[32m0.6828\u001b[0m                     0.5708        0.6814  0.0043  3.0162\n",
      "     15                     0.5681        \u001b[32m0.6814\u001b[0m                     0.5783        0.6778  0.0037  3.0179\n",
      "     16                     \u001b[36m0.5745\u001b[0m        \u001b[32m0.6794\u001b[0m                     0.5781        \u001b[31m0.6755\u001b[0m  0.0031  3.0186\n",
      "     17                     0.5711        \u001b[32m0.6790\u001b[0m                     0.5747        0.6770  0.0025  3.0177\n",
      "     18                     \u001b[36m0.5800\u001b[0m        \u001b[32m0.6765\u001b[0m                     \u001b[35m0.5836\u001b[0m        0.6778  0.0020  3.0176\n",
      "     19                     \u001b[36m0.5834\u001b[0m        \u001b[32m0.6756\u001b[0m                     0.5816        0.6764  0.0015  3.0195\n",
      "     20                     \u001b[36m0.5835\u001b[0m        \u001b[32m0.6744\u001b[0m                     0.5818        0.6769  0.0010  3.0187\n",
      "     21                     \u001b[36m0.5859\u001b[0m        \u001b[32m0.6730\u001b[0m                     \u001b[35m0.5842\u001b[0m        \u001b[31m0.6752\u001b[0m  0.0007  3.0193\n",
      "     22                     \u001b[36m0.5892\u001b[0m        \u001b[32m0.6720\u001b[0m                     \u001b[35m0.5849\u001b[0m        0.6754  0.0004  3.0169\n",
      "     23                     0.5864        \u001b[32m0.6715\u001b[0m                     \u001b[35m0.5866\u001b[0m        \u001b[31m0.6749\u001b[0m  0.0002  3.0204\n",
      "     24                     0.5856        \u001b[32m0.6714\u001b[0m                     0.5854        0.6750  0.0000  3.0495\n",
      "     25                     0.5839        \u001b[32m0.6711\u001b[0m                     0.5863        \u001b[31m0.6749\u001b[0m  0.0000  3.0230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[1 1 0 ... 0 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7224\u001b[0m        \u001b[32m0.7070\u001b[0m                     \u001b[35m0.8451\u001b[0m        \u001b[31m0.3814\u001b[0m  0.0100  1.4846\n",
      "      2                     \u001b[36m0.8022\u001b[0m        \u001b[32m0.5289\u001b[0m                     0.8334        0.4388  0.0100  1.2327\n",
      "      3                     \u001b[36m0.8276\u001b[0m        \u001b[32m0.4447\u001b[0m                     \u001b[35m0.8734\u001b[0m        \u001b[31m0.3144\u001b[0m  0.0098  1.2335\n",
      "      4                     \u001b[36m0.8444\u001b[0m        \u001b[32m0.3929\u001b[0m                     0.8713        \u001b[31m0.3073\u001b[0m  0.0096  1.2326\n",
      "      5                     \u001b[36m0.8544\u001b[0m        \u001b[32m0.3782\u001b[0m                     0.8710        0.3210  0.0093  1.2346\n",
      "      6                     \u001b[36m0.8587\u001b[0m        \u001b[32m0.3609\u001b[0m                     0.8637        0.3399  0.0090  1.2337\n",
      "      7                     \u001b[36m0.8649\u001b[0m        \u001b[32m0.3356\u001b[0m                     \u001b[35m0.8774\u001b[0m        \u001b[31m0.3025\u001b[0m  0.0085  1.2337\n",
      "      8                     0.8635        0.3368                     \u001b[35m0.8857\u001b[0m        0.3054  0.0080  1.2328\n",
      "      9                     \u001b[36m0.8721\u001b[0m        \u001b[32m0.3205\u001b[0m                     \u001b[35m0.9024\u001b[0m        \u001b[31m0.2687\u001b[0m  0.0075  1.2334\n",
      "     10                     \u001b[36m0.8819\u001b[0m        \u001b[32m0.3103\u001b[0m                     0.8815        0.2850  0.0069  1.2322\n",
      "     11                     0.8753        \u001b[32m0.3004\u001b[0m                     0.8899        0.2821  0.0063  1.2327\n",
      "     12                     \u001b[36m0.8846\u001b[0m        \u001b[32m0.2961\u001b[0m                     0.8811        0.2790  0.0057  1.2348\n",
      "     13                     \u001b[36m0.8929\u001b[0m        \u001b[32m0.2765\u001b[0m                     0.8805        0.3005  0.0050  1.2328\n",
      "     14                     0.8891        \u001b[32m0.2728\u001b[0m                     0.8998        \u001b[31m0.2579\u001b[0m  0.0043  1.2331\n",
      "     15                     0.8888        \u001b[32m0.2703\u001b[0m                     \u001b[35m0.9068\u001b[0m        0.2652  0.0037  1.2360\n",
      "     16                     \u001b[36m0.8985\u001b[0m        \u001b[32m0.2532\u001b[0m                     0.8996        0.2603  0.0031  1.2341\n",
      "     17                     \u001b[36m0.9049\u001b[0m        \u001b[32m0.2512\u001b[0m                     0.8956        0.2754  0.0025  1.2347\n",
      "     18                     0.9036        \u001b[32m0.2377\u001b[0m                     \u001b[35m0.9124\u001b[0m        \u001b[31m0.2507\u001b[0m  0.0020  1.2338\n",
      "     19                     0.9036        0.2406                     0.9087        0.2543  0.0015  1.2360\n",
      "     20                     \u001b[36m0.9152\u001b[0m        \u001b[32m0.2320\u001b[0m                     0.9039        0.2512  0.0010  1.2347\n",
      "     21                     \u001b[36m0.9164\u001b[0m        \u001b[32m0.2207\u001b[0m                     \u001b[35m0.9150\u001b[0m        \u001b[31m0.2465\u001b[0m  0.0007  1.2358\n",
      "     22                     0.9148        0.2218                     0.9099        0.2490  0.0004  1.2348\n",
      "     23                     \u001b[36m0.9180\u001b[0m        \u001b[32m0.2161\u001b[0m                     0.9091        0.2471  0.0002  1.2358\n",
      "     24                     \u001b[36m0.9223\u001b[0m        \u001b[32m0.2113\u001b[0m                     0.9072        0.2503  0.0000  1.2347\n",
      "     25                     0.9217        \u001b[32m0.2081\u001b[0m                     0.9083        0.2472  0.0000  1.2357\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7134\u001b[0m        \u001b[32m0.7209\u001b[0m                     \u001b[35m0.8365\u001b[0m        \u001b[31m0.3860\u001b[0m  0.0100  1.2268\n",
      "      2                     \u001b[36m0.8098\u001b[0m        \u001b[32m0.5041\u001b[0m                     \u001b[35m0.8617\u001b[0m        \u001b[31m0.3723\u001b[0m  0.0100  1.2337\n",
      "      3                     \u001b[36m0.8370\u001b[0m        \u001b[32m0.4314\u001b[0m                     \u001b[35m0.8740\u001b[0m        \u001b[31m0.3384\u001b[0m  0.0098  1.2342\n",
      "      4                     0.8319        \u001b[32m0.4217\u001b[0m                     0.8590        0.3516  0.0096  1.2328\n",
      "      5                     \u001b[36m0.8504\u001b[0m        \u001b[32m0.3890\u001b[0m                     0.8700        \u001b[31m0.3221\u001b[0m  0.0093  1.2328\n",
      "      6                     \u001b[36m0.8559\u001b[0m        \u001b[32m0.3676\u001b[0m                     \u001b[35m0.8833\u001b[0m        \u001b[31m0.3165\u001b[0m  0.0090  1.2337\n",
      "      7                     \u001b[36m0.8645\u001b[0m        \u001b[32m0.3371\u001b[0m                     0.8831        0.3185  0.0085  1.2340\n",
      "      8                     \u001b[36m0.8673\u001b[0m        \u001b[32m0.3241\u001b[0m                     \u001b[35m0.9011\u001b[0m        \u001b[31m0.2929\u001b[0m  0.0080  1.2340\n",
      "      9                     \u001b[36m0.8773\u001b[0m        \u001b[32m0.3161\u001b[0m                     0.8635        0.3208  0.0075  1.2335\n",
      "     10                     \u001b[36m0.8795\u001b[0m        \u001b[32m0.3043\u001b[0m                     \u001b[35m0.9020\u001b[0m        \u001b[31m0.2749\u001b[0m  0.0069  1.2337\n",
      "     11                     0.8778        \u001b[32m0.2977\u001b[0m                     0.8933        0.2956  0.0063  1.2337\n",
      "     12                     \u001b[36m0.8852\u001b[0m        0.2987                     0.8946        0.2797  0.0057  1.2336\n",
      "     13                     \u001b[36m0.8900\u001b[0m        \u001b[32m0.2771\u001b[0m                     0.8973        0.2760  0.0050  1.2339\n",
      "     14                     \u001b[36m0.8950\u001b[0m        \u001b[32m0.2624\u001b[0m                     0.8988        \u001b[31m0.2685\u001b[0m  0.0043  1.2348\n",
      "     15                     \u001b[36m0.9034\u001b[0m        \u001b[32m0.2509\u001b[0m                     0.9003        0.2735  0.0037  1.2340\n",
      "     16                     0.9019        \u001b[32m0.2473\u001b[0m                     0.8987        \u001b[31m0.2618\u001b[0m  0.0031  1.2358\n",
      "     17                     \u001b[36m0.9065\u001b[0m        \u001b[32m0.2375\u001b[0m                     \u001b[35m0.9043\u001b[0m        \u001b[31m0.2549\u001b[0m  0.0025  1.2338\n",
      "     18                     \u001b[36m0.9089\u001b[0m        \u001b[32m0.2339\u001b[0m                     \u001b[35m0.9053\u001b[0m        \u001b[31m0.2544\u001b[0m  0.0020  1.2357\n",
      "     19                     \u001b[36m0.9116\u001b[0m        \u001b[32m0.2281\u001b[0m                     \u001b[35m0.9104\u001b[0m        0.2563  0.0015  1.2340\n",
      "     20                     0.9095        0.2321                     0.9064        \u001b[31m0.2508\u001b[0m  0.0010  1.2352\n",
      "     21                     \u001b[36m0.9161\u001b[0m        \u001b[32m0.2148\u001b[0m                     0.9072        0.2514  0.0007  1.2347\n",
      "     22                     0.9159        \u001b[32m0.2138\u001b[0m                     0.9078        0.2513  0.0004  1.2357\n",
      "     23                     \u001b[36m0.9232\u001b[0m        \u001b[32m0.2061\u001b[0m                     0.9058        \u001b[31m0.2496\u001b[0m  0.0002  1.2352\n",
      "     24                     0.9206        0.2094                     0.9082        \u001b[31m0.2493\u001b[0m  0.0000  1.2357\n",
      "     25                     0.9210        0.2106                     0.9071        0.2506  0.0000  1.2367\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7037\u001b[0m        \u001b[32m0.8083\u001b[0m                     \u001b[35m0.7889\u001b[0m        \u001b[31m0.4688\u001b[0m  0.0100  1.2268\n",
      "      2                     \u001b[36m0.8117\u001b[0m        \u001b[32m0.4743\u001b[0m                     \u001b[35m0.8595\u001b[0m        \u001b[31m0.3494\u001b[0m  0.0100  1.2337\n",
      "      3                     \u001b[36m0.8265\u001b[0m        \u001b[32m0.4580\u001b[0m                     0.8292        0.4314  0.0098  1.2338\n",
      "      4                     \u001b[36m0.8379\u001b[0m        \u001b[32m0.4436\u001b[0m                     0.8566        0.3653  0.0096  1.2332\n",
      "      5                     0.8315        \u001b[32m0.4357\u001b[0m                     \u001b[35m0.8629\u001b[0m        0.3527  0.0093  1.2339\n",
      "      6                     \u001b[36m0.8539\u001b[0m        \u001b[32m0.3848\u001b[0m                     \u001b[35m0.8802\u001b[0m        \u001b[31m0.3204\u001b[0m  0.0090  1.2328\n",
      "      7                     \u001b[36m0.8574\u001b[0m        \u001b[32m0.3616\u001b[0m                     0.8646        0.3477  0.0085  1.2327\n",
      "      8                     \u001b[36m0.8655\u001b[0m        \u001b[32m0.3440\u001b[0m                     \u001b[35m0.8816\u001b[0m        \u001b[31m0.3112\u001b[0m  0.0080  1.2327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.8737\u001b[0m        \u001b[32m0.3208\u001b[0m                     0.8670        0.3218  0.0075  1.2347\n",
      "     10                     \u001b[36m0.8748\u001b[0m        \u001b[32m0.3126\u001b[0m                     0.8518        0.3779  0.0069  1.2319\n",
      "     11                     0.8735        0.3242                     0.8726        0.3204  0.0063  1.2339\n",
      "     12                     \u001b[36m0.8766\u001b[0m        \u001b[32m0.3064\u001b[0m                     \u001b[35m0.8861\u001b[0m        \u001b[31m0.2696\u001b[0m  0.0057  1.2355\n",
      "     13                     \u001b[36m0.8843\u001b[0m        \u001b[32m0.2835\u001b[0m                     0.8786        0.2892  0.0050  1.2338\n",
      "     14                     \u001b[36m0.8897\u001b[0m        \u001b[32m0.2832\u001b[0m                     \u001b[35m0.8986\u001b[0m        0.2697  0.0043  1.2347\n",
      "     15                     \u001b[36m0.8920\u001b[0m        \u001b[32m0.2757\u001b[0m                     0.8977        \u001b[31m0.2673\u001b[0m  0.0037  1.2343\n",
      "     16                     \u001b[36m0.8954\u001b[0m        \u001b[32m0.2634\u001b[0m                     0.8912        \u001b[31m0.2665\u001b[0m  0.0031  1.2338\n",
      "     17                     \u001b[36m0.9004\u001b[0m        \u001b[32m0.2542\u001b[0m                     0.8956        \u001b[31m0.2583\u001b[0m  0.0025  1.2325\n",
      "     18                     \u001b[36m0.9064\u001b[0m        \u001b[32m0.2529\u001b[0m                     0.8933        0.2640  0.0020  1.2348\n",
      "     19                     \u001b[36m0.9074\u001b[0m        \u001b[32m0.2437\u001b[0m                     0.8931        0.2605  0.0015  1.2349\n",
      "     20                     \u001b[36m0.9107\u001b[0m        \u001b[32m0.2370\u001b[0m                     0.8985        \u001b[31m0.2559\u001b[0m  0.0010  1.2337\n",
      "     21                     \u001b[36m0.9162\u001b[0m        \u001b[32m0.2260\u001b[0m                     \u001b[35m0.9002\u001b[0m        \u001b[31m0.2528\u001b[0m  0.0007  1.2333\n",
      "     22                     0.9132        0.2278                     0.8995        0.2537  0.0004  1.2358\n",
      "     23                     0.9137        0.2283                     0.8975        \u001b[31m0.2528\u001b[0m  0.0002  1.2351\n",
      "     24                     \u001b[36m0.9170\u001b[0m        \u001b[32m0.2237\u001b[0m                     0.8967        0.2535  0.0000  1.2347\n",
      "     25                     \u001b[36m0.9181\u001b[0m        \u001b[32m0.2169\u001b[0m                     0.8960        0.2540  0.0000  1.2352\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7087\u001b[0m        \u001b[32m0.7694\u001b[0m                     \u001b[35m0.8219\u001b[0m        \u001b[31m0.4390\u001b[0m  0.0100  1.2269\n",
      "      2                     \u001b[36m0.8017\u001b[0m        \u001b[32m0.5340\u001b[0m                     \u001b[35m0.8475\u001b[0m        \u001b[31m0.3975\u001b[0m  0.0100  1.2327\n",
      "      3                     \u001b[36m0.8200\u001b[0m        \u001b[32m0.4683\u001b[0m                     \u001b[35m0.8612\u001b[0m        \u001b[31m0.3358\u001b[0m  0.0098  1.2327\n",
      "      4                     \u001b[36m0.8382\u001b[0m        \u001b[32m0.4359\u001b[0m                     \u001b[35m0.8715\u001b[0m        \u001b[31m0.3148\u001b[0m  0.0096  1.2318\n",
      "      5                     \u001b[36m0.8466\u001b[0m        \u001b[32m0.3880\u001b[0m                     \u001b[35m0.8729\u001b[0m        0.3178  0.0093  1.2333\n",
      "      6                     \u001b[36m0.8554\u001b[0m        \u001b[32m0.3699\u001b[0m                     \u001b[35m0.8814\u001b[0m        \u001b[31m0.2775\u001b[0m  0.0090  1.2349\n",
      "      7                     \u001b[36m0.8640\u001b[0m        \u001b[32m0.3372\u001b[0m                     \u001b[35m0.8858\u001b[0m        0.2899  0.0085  1.2339\n",
      "      8                     0.8633        0.3385                     \u001b[35m0.8870\u001b[0m        0.2928  0.0080  1.2346\n",
      "      9                     \u001b[36m0.8720\u001b[0m        \u001b[32m0.3219\u001b[0m                     0.8806        0.2960  0.0075  1.2337\n",
      "     10                     0.8690        \u001b[32m0.3169\u001b[0m                     \u001b[35m0.8877\u001b[0m        0.2833  0.0069  1.2337\n",
      "     11                     \u001b[36m0.8803\u001b[0m        \u001b[32m0.2974\u001b[0m                     \u001b[35m0.8929\u001b[0m        0.2854  0.0063  1.2329\n",
      "     12                     \u001b[36m0.8873\u001b[0m        \u001b[32m0.2858\u001b[0m                     0.8868        \u001b[31m0.2773\u001b[0m  0.0057  1.2334\n",
      "     13                     \u001b[36m0.8883\u001b[0m        \u001b[32m0.2852\u001b[0m                     \u001b[35m0.8933\u001b[0m        \u001b[31m0.2726\u001b[0m  0.0050  1.2343\n",
      "     14                     \u001b[36m0.8954\u001b[0m        \u001b[32m0.2697\u001b[0m                     \u001b[35m0.8996\u001b[0m        0.2790  0.0043  1.2337\n",
      "     15                     \u001b[36m0.8988\u001b[0m        \u001b[32m0.2649\u001b[0m                     \u001b[35m0.9071\u001b[0m        \u001b[31m0.2577\u001b[0m  0.0037  1.2342\n",
      "     16                     0.8978        \u001b[32m0.2606\u001b[0m                     0.9021        \u001b[31m0.2570\u001b[0m  0.0031  1.2346\n",
      "     17                     \u001b[36m0.9070\u001b[0m        \u001b[32m0.2440\u001b[0m                     0.9059        \u001b[31m0.2507\u001b[0m  0.0025  1.2337\n",
      "     18                     0.9044        \u001b[32m0.2392\u001b[0m                     \u001b[35m0.9072\u001b[0m        \u001b[31m0.2487\u001b[0m  0.0020  1.2374\n",
      "     19                     0.9059        \u001b[32m0.2372\u001b[0m                     0.9019        \u001b[31m0.2447\u001b[0m  0.0015  1.2348\n",
      "     20                     \u001b[36m0.9135\u001b[0m        \u001b[32m0.2324\u001b[0m                     \u001b[35m0.9085\u001b[0m        0.2478  0.0010  1.2357\n",
      "     21                     0.9107        0.2336                     0.9053        0.2472  0.0007  1.2357\n",
      "     22                     \u001b[36m0.9179\u001b[0m        \u001b[32m0.2175\u001b[0m                     \u001b[35m0.9095\u001b[0m        0.2477  0.0004  1.2358\n",
      "     23                     \u001b[36m0.9193\u001b[0m        \u001b[32m0.2147\u001b[0m                     0.9091        0.2459  0.0002  1.2358\n",
      "     24                     \u001b[36m0.9212\u001b[0m        0.2155                     \u001b[35m0.9115\u001b[0m        0.2476  0.0000  1.2341\n",
      "     25                     0.9177        0.2189                     0.9096        0.2473  0.0000  1.2657\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7000\u001b[0m        \u001b[32m0.7493\u001b[0m                     \u001b[35m0.8346\u001b[0m        \u001b[31m0.4137\u001b[0m  0.0100  1.2258\n",
      "      2                     \u001b[36m0.8083\u001b[0m        \u001b[32m0.4969\u001b[0m                     0.8297        0.4221  0.0100  1.2328\n",
      "      3                     \u001b[36m0.8200\u001b[0m        \u001b[32m0.4727\u001b[0m                     \u001b[35m0.8577\u001b[0m        \u001b[31m0.3398\u001b[0m  0.0098  1.2316\n",
      "      4                     \u001b[36m0.8314\u001b[0m        \u001b[32m0.4251\u001b[0m                     \u001b[35m0.8598\u001b[0m        0.3476  0.0096  1.2329\n",
      "      5                     \u001b[36m0.8392\u001b[0m        \u001b[32m0.4037\u001b[0m                     \u001b[35m0.8628\u001b[0m        \u001b[31m0.3349\u001b[0m  0.0093  1.2330\n",
      "      6                     \u001b[36m0.8591\u001b[0m        \u001b[32m0.3486\u001b[0m                     \u001b[35m0.8764\u001b[0m        \u001b[31m0.3326\u001b[0m  0.0090  1.2327\n",
      "      7                     \u001b[36m0.8619\u001b[0m        0.3499                     0.8686        \u001b[31m0.3228\u001b[0m  0.0085  1.2328\n",
      "      8                     \u001b[36m0.8631\u001b[0m        \u001b[32m0.3346\u001b[0m                     0.8758        \u001b[31m0.3186\u001b[0m  0.0080  1.2328\n",
      "      9                     \u001b[36m0.8719\u001b[0m        \u001b[32m0.3159\u001b[0m                     \u001b[35m0.8824\u001b[0m        0.3278  0.0075  1.2344\n",
      "     10                     \u001b[36m0.8810\u001b[0m        \u001b[32m0.3017\u001b[0m                     0.8670        0.3202  0.0069  1.2351\n",
      "     11                     \u001b[36m0.8811\u001b[0m        \u001b[32m0.3005\u001b[0m                     \u001b[35m0.8860\u001b[0m        \u001b[31m0.2995\u001b[0m  0.0063  1.2328\n",
      "     12                     \u001b[36m0.8880\u001b[0m        \u001b[32m0.2938\u001b[0m                     0.8807        0.3042  0.0057  1.2357\n",
      "     13                     \u001b[36m0.8890\u001b[0m        \u001b[32m0.2753\u001b[0m                     0.8857        \u001b[31m0.2976\u001b[0m  0.0050  1.2347\n",
      "     14                     \u001b[36m0.8955\u001b[0m        \u001b[32m0.2684\u001b[0m                     0.8746        0.3107  0.0043  1.2328\n",
      "     15                     \u001b[36m0.9001\u001b[0m        \u001b[32m0.2521\u001b[0m                     \u001b[35m0.8949\u001b[0m        \u001b[31m0.2840\u001b[0m  0.0037  1.2340\n",
      "     16                     \u001b[36m0.9036\u001b[0m        \u001b[32m0.2494\u001b[0m                     0.8917        0.2858  0.0031  1.2350\n",
      "     17                     0.9025        \u001b[32m0.2366\u001b[0m                     0.8942        0.2909  0.0025  1.2349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18                     \u001b[36m0.9071\u001b[0m        \u001b[32m0.2323\u001b[0m                     0.8929        \u001b[31m0.2787\u001b[0m  0.0020  1.2353\n",
      "     19                     \u001b[36m0.9105\u001b[0m        \u001b[32m0.2320\u001b[0m                     0.8855        0.2871  0.0015  1.2348\n",
      "     20                     \u001b[36m0.9214\u001b[0m        \u001b[32m0.2227\u001b[0m                     0.8908        0.2838  0.0010  1.2357\n",
      "     21                     0.9154        0.2233                     0.8898        0.2828  0.0007  1.2338\n",
      "     22                     0.9170        \u001b[32m0.2162\u001b[0m                     0.8867        0.2859  0.0004  1.2348\n",
      "     23                     0.9174        \u001b[32m0.2155\u001b[0m                     0.8920        0.2834  0.0002  1.2357\n",
      "     24                     0.9192        \u001b[32m0.2137\u001b[0m                     0.8925        0.2830  0.0000  1.2347\n",
      "     25                     \u001b[36m0.9231\u001b[0m        \u001b[32m0.2047\u001b[0m                     0.8923        0.2846  0.0000  1.2361\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7001\u001b[0m        \u001b[32m0.7726\u001b[0m                     \u001b[35m0.7920\u001b[0m        \u001b[31m0.4910\u001b[0m  0.0100  1.2278\n",
      "      2                     \u001b[36m0.8086\u001b[0m        \u001b[32m0.4899\u001b[0m                     \u001b[35m0.8446\u001b[0m        \u001b[31m0.4181\u001b[0m  0.0100  1.2327\n",
      "      3                     \u001b[36m0.8265\u001b[0m        \u001b[32m0.4506\u001b[0m                     0.8429        \u001b[31m0.4023\u001b[0m  0.0098  1.2320\n",
      "      4                     \u001b[36m0.8368\u001b[0m        \u001b[32m0.4228\u001b[0m                     \u001b[35m0.8596\u001b[0m        \u001b[31m0.3243\u001b[0m  0.0096  1.2339\n",
      "      5                     \u001b[36m0.8518\u001b[0m        \u001b[32m0.3941\u001b[0m                     \u001b[35m0.8682\u001b[0m        0.3408  0.0093  1.2328\n",
      "      6                     0.8464        \u001b[32m0.3929\u001b[0m                     0.8602        \u001b[31m0.3238\u001b[0m  0.0090  1.2338\n",
      "      7                     \u001b[36m0.8665\u001b[0m        \u001b[32m0.3622\u001b[0m                     \u001b[35m0.8765\u001b[0m        \u001b[31m0.3025\u001b[0m  0.0085  1.2336\n",
      "      8                     0.8650        \u001b[32m0.3461\u001b[0m                     \u001b[35m0.8785\u001b[0m        \u001b[31m0.2807\u001b[0m  0.0080  1.2337\n",
      "      9                     \u001b[36m0.8765\u001b[0m        \u001b[32m0.3239\u001b[0m                     0.8652        0.3181  0.0075  1.2328\n",
      "     10                     0.8748        \u001b[32m0.3191\u001b[0m                     \u001b[35m0.8859\u001b[0m        \u001b[31m0.2728\u001b[0m  0.0069  1.2335\n",
      "     11                     \u001b[36m0.8797\u001b[0m        \u001b[32m0.3112\u001b[0m                     0.8739        0.2754  0.0063  1.2340\n",
      "     12                     \u001b[36m0.8875\u001b[0m        \u001b[32m0.2872\u001b[0m                     \u001b[35m0.8912\u001b[0m        \u001b[31m0.2714\u001b[0m  0.0057  1.2347\n",
      "     13                     \u001b[36m0.8898\u001b[0m        \u001b[32m0.2835\u001b[0m                     0.8781        0.2882  0.0050  1.2334\n",
      "     14                     0.8876        \u001b[32m0.2784\u001b[0m                     0.8833        0.2714  0.0043  1.2339\n",
      "     15                     \u001b[36m0.8977\u001b[0m        \u001b[32m0.2574\u001b[0m                     0.8838        0.2764  0.0037  1.2347\n",
      "     16                     \u001b[36m0.9014\u001b[0m        \u001b[32m0.2543\u001b[0m                     0.8909        \u001b[31m0.2626\u001b[0m  0.0031  1.2347\n",
      "     17                     \u001b[36m0.9041\u001b[0m        \u001b[32m0.2483\u001b[0m                     0.8765        0.2646  0.0025  1.2337\n",
      "     18                     \u001b[36m0.9105\u001b[0m        \u001b[32m0.2388\u001b[0m                     0.8837        0.2679  0.0020  1.2361\n",
      "     19                     0.9084        \u001b[32m0.2353\u001b[0m                     0.8896        \u001b[31m0.2557\u001b[0m  0.0015  1.2350\n",
      "     20                     \u001b[36m0.9131\u001b[0m        \u001b[32m0.2263\u001b[0m                     0.8883        0.2565  0.0010  1.2340\n",
      "     21                     \u001b[36m0.9191\u001b[0m        \u001b[32m0.2172\u001b[0m                     \u001b[35m0.8926\u001b[0m        \u001b[31m0.2498\u001b[0m  0.0007  1.2354\n",
      "     22                     0.9148        0.2220                     0.8900        \u001b[31m0.2478\u001b[0m  0.0004  1.2357\n",
      "     23                     0.9171        0.2195                     0.8882        0.2508  0.0002  1.2343\n",
      "     24                     \u001b[36m0.9239\u001b[0m        \u001b[32m0.2158\u001b[0m                     0.8896        0.2517  0.0000  1.2348\n",
      "     25                     0.9201        \u001b[32m0.2133\u001b[0m                     0.8883        0.2529  0.0000  1.2353\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7194\u001b[0m        \u001b[32m0.7097\u001b[0m                     \u001b[35m0.7798\u001b[0m        \u001b[31m0.5830\u001b[0m  0.0100  1.2274\n",
      "      2                     \u001b[36m0.8111\u001b[0m        \u001b[32m0.4859\u001b[0m                     \u001b[35m0.8276\u001b[0m        \u001b[31m0.4120\u001b[0m  0.0100  1.2335\n",
      "      3                     \u001b[36m0.8191\u001b[0m        \u001b[32m0.4711\u001b[0m                     \u001b[35m0.8327\u001b[0m        \u001b[31m0.4081\u001b[0m  0.0098  1.2328\n",
      "      4                     \u001b[36m0.8403\u001b[0m        \u001b[32m0.4149\u001b[0m                     \u001b[35m0.8815\u001b[0m        \u001b[31m0.3014\u001b[0m  0.0096  1.2330\n",
      "      5                     \u001b[36m0.8487\u001b[0m        \u001b[32m0.3971\u001b[0m                     \u001b[35m0.8823\u001b[0m        \u001b[31m0.2913\u001b[0m  0.0093  1.2322\n",
      "      6                     \u001b[36m0.8598\u001b[0m        \u001b[32m0.3511\u001b[0m                     0.8742        0.3065  0.0090  1.2329\n",
      "      7                     0.8585        \u001b[32m0.3471\u001b[0m                     \u001b[35m0.8884\u001b[0m        \u001b[31m0.2873\u001b[0m  0.0085  1.2333\n",
      "      8                     \u001b[36m0.8626\u001b[0m        \u001b[32m0.3323\u001b[0m                     0.8507        0.3609  0.0080  1.2318\n",
      "      9                     \u001b[36m0.8645\u001b[0m        0.3398                     \u001b[35m0.8885\u001b[0m        \u001b[31m0.2825\u001b[0m  0.0075  1.2360\n",
      "     10                     \u001b[36m0.8762\u001b[0m        \u001b[32m0.3151\u001b[0m                     \u001b[35m0.8958\u001b[0m        \u001b[31m0.2753\u001b[0m  0.0069  1.2326\n",
      "     11                     \u001b[36m0.8768\u001b[0m        \u001b[32m0.3009\u001b[0m                     0.8821        0.2789  0.0063  1.2325\n",
      "     12                     \u001b[36m0.8912\u001b[0m        \u001b[32m0.2865\u001b[0m                     0.8914        \u001b[31m0.2559\u001b[0m  0.0057  1.2321\n",
      "     13                     0.8910        \u001b[32m0.2775\u001b[0m                     \u001b[35m0.9037\u001b[0m        \u001b[31m0.2458\u001b[0m  0.0050  1.2325\n",
      "     14                     \u001b[36m0.8968\u001b[0m        \u001b[32m0.2673\u001b[0m                     0.8980        \u001b[31m0.2359\u001b[0m  0.0043  1.2329\n",
      "     15                     0.8939        \u001b[32m0.2665\u001b[0m                     0.8856        0.2610  0.0037  1.2320\n",
      "     16                     \u001b[36m0.8998\u001b[0m        \u001b[32m0.2589\u001b[0m                     \u001b[35m0.9103\u001b[0m        0.2360  0.0031  1.2317\n",
      "     17                     \u001b[36m0.9043\u001b[0m        \u001b[32m0.2531\u001b[0m                     0.9085        0.2401  0.0025  1.2637\n",
      "     18                     0.9030        0.2534                     \u001b[35m0.9159\u001b[0m        \u001b[31m0.2312\u001b[0m  0.0020  1.2351\n",
      "     19                     \u001b[36m0.9079\u001b[0m        \u001b[32m0.2331\u001b[0m                     0.9071        \u001b[31m0.2303\u001b[0m  0.0015  1.2335\n",
      "     20                     \u001b[36m0.9101\u001b[0m        \u001b[32m0.2329\u001b[0m                     0.9131        \u001b[31m0.2280\u001b[0m  0.0010  1.2337\n",
      "     21                     \u001b[36m0.9154\u001b[0m        \u001b[32m0.2216\u001b[0m                     0.9104        0.2316  0.0007  1.2339\n",
      "     22                     0.9107        0.2277                     0.9151        0.2281  0.0004  1.2358\n",
      "     23                     \u001b[36m0.9179\u001b[0m        \u001b[32m0.2198\u001b[0m                     0.9094        0.2295  0.0002  1.2337\n",
      "     24                     0.9138        \u001b[32m0.2190\u001b[0m                     0.9104        0.2291  0.0000  1.2347\n",
      "     25                     0.9129        0.2194                     0.9098        0.2298  0.0000  1.2351\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7220\u001b[0m        \u001b[32m0.7283\u001b[0m                     \u001b[35m0.8469\u001b[0m        \u001b[31m0.3848\u001b[0m  0.0100  1.2267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                     \u001b[36m0.8115\u001b[0m        \u001b[32m0.4860\u001b[0m                     \u001b[35m0.8630\u001b[0m        \u001b[31m0.3589\u001b[0m  0.0100  1.2327\n",
      "      3                     \u001b[36m0.8354\u001b[0m        \u001b[32m0.4271\u001b[0m                     0.8312        0.4070  0.0098  1.2323\n",
      "      4                     \u001b[36m0.8396\u001b[0m        \u001b[32m0.4100\u001b[0m                     \u001b[35m0.8710\u001b[0m        \u001b[31m0.3202\u001b[0m  0.0096  1.2328\n",
      "      5                     \u001b[36m0.8535\u001b[0m        \u001b[32m0.3739\u001b[0m                     \u001b[35m0.8781\u001b[0m        \u001b[31m0.2929\u001b[0m  0.0093  1.2338\n",
      "      6                     \u001b[36m0.8591\u001b[0m        \u001b[32m0.3532\u001b[0m                     \u001b[35m0.8799\u001b[0m        0.3228  0.0090  1.2321\n",
      "      7                     \u001b[36m0.8667\u001b[0m        \u001b[32m0.3458\u001b[0m                     \u001b[35m0.8888\u001b[0m        \u001b[31m0.2925\u001b[0m  0.0085  1.2337\n",
      "      8                     0.8624        \u001b[32m0.3284\u001b[0m                     0.8828        0.2998  0.0080  1.2336\n",
      "      9                     \u001b[36m0.8720\u001b[0m        \u001b[32m0.3174\u001b[0m                     \u001b[35m0.8925\u001b[0m        0.3001  0.0075  1.2325\n",
      "     10                     0.8688        0.3196                     0.8824        0.3080  0.0069  1.2329\n",
      "     11                     \u001b[36m0.8838\u001b[0m        \u001b[32m0.2918\u001b[0m                     0.8904        \u001b[31m0.2755\u001b[0m  0.0063  1.2348\n",
      "     12                     0.8833        0.2922                     0.8873        0.2796  0.0057  1.2364\n",
      "     13                     \u001b[36m0.8899\u001b[0m        \u001b[32m0.2778\u001b[0m                     0.8762        0.2958  0.0050  1.2362\n",
      "     14                     0.8870        0.2845                     0.8800        0.3060  0.0043  1.2348\n",
      "     15                     \u001b[36m0.8989\u001b[0m        \u001b[32m0.2574\u001b[0m                     \u001b[35m0.8963\u001b[0m        0.2755  0.0037  1.2345\n",
      "     16                     \u001b[36m0.9046\u001b[0m        \u001b[32m0.2461\u001b[0m                     0.8953        \u001b[31m0.2726\u001b[0m  0.0031  1.2346\n",
      "     17                     \u001b[36m0.9061\u001b[0m        \u001b[32m0.2399\u001b[0m                     \u001b[35m0.8997\u001b[0m        \u001b[31m0.2706\u001b[0m  0.0025  1.2339\n",
      "     18                     0.9022        0.2403                     0.8958        0.2769  0.0020  1.2338\n",
      "     19                     \u001b[36m0.9062\u001b[0m        \u001b[32m0.2338\u001b[0m                     \u001b[35m0.9012\u001b[0m        \u001b[31m0.2612\u001b[0m  0.0015  1.2347\n",
      "     20                     \u001b[36m0.9098\u001b[0m        \u001b[32m0.2194\u001b[0m                     \u001b[35m0.9030\u001b[0m        \u001b[31m0.2610\u001b[0m  0.0010  1.2347\n",
      "     21                     \u001b[36m0.9162\u001b[0m        \u001b[32m0.2147\u001b[0m                     \u001b[35m0.9059\u001b[0m        \u001b[31m0.2591\u001b[0m  0.0007  1.2357\n",
      "     22                     \u001b[36m0.9163\u001b[0m        0.2194                     \u001b[35m0.9072\u001b[0m        \u001b[31m0.2575\u001b[0m  0.0004  1.2345\n",
      "     23                     \u001b[36m0.9204\u001b[0m        \u001b[32m0.2123\u001b[0m                     \u001b[35m0.9089\u001b[0m        \u001b[31m0.2574\u001b[0m  0.0002  1.2351\n",
      "     24                     0.9196        \u001b[32m0.2108\u001b[0m                     \u001b[35m0.9094\u001b[0m        0.2574  0.0000  1.2336\n",
      "     25                     0.9160        0.2112                     0.9087        \u001b[31m0.2573\u001b[0m  0.0000  1.2347\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7173\u001b[0m        \u001b[32m0.7238\u001b[0m                     \u001b[35m0.8356\u001b[0m        \u001b[31m0.3887\u001b[0m  0.0100  1.2267\n",
      "      2                     \u001b[36m0.7984\u001b[0m        \u001b[32m0.5293\u001b[0m                     0.8187        0.4277  0.0100  1.2320\n",
      "      3                     \u001b[36m0.8199\u001b[0m        \u001b[32m0.4697\u001b[0m                     \u001b[35m0.8671\u001b[0m        \u001b[31m0.3096\u001b[0m  0.0098  1.2326\n",
      "      4                     \u001b[36m0.8420\u001b[0m        \u001b[32m0.4183\u001b[0m                     \u001b[35m0.8675\u001b[0m        \u001b[31m0.3084\u001b[0m  0.0096  1.2336\n",
      "      5                     0.8409        \u001b[32m0.4086\u001b[0m                     \u001b[35m0.8739\u001b[0m        0.3162  0.0093  1.2332\n",
      "      6                     \u001b[36m0.8514\u001b[0m        \u001b[32m0.3683\u001b[0m                     \u001b[35m0.8889\u001b[0m        \u001b[31m0.2909\u001b[0m  0.0090  1.2318\n",
      "      7                     \u001b[36m0.8615\u001b[0m        \u001b[32m0.3396\u001b[0m                     \u001b[35m0.8992\u001b[0m        \u001b[31m0.2640\u001b[0m  0.0085  1.2329\n",
      "      8                     \u001b[36m0.8665\u001b[0m        \u001b[32m0.3349\u001b[0m                     0.8895        0.2708  0.0080  1.2333\n",
      "      9                     \u001b[36m0.8734\u001b[0m        \u001b[32m0.3119\u001b[0m                     0.8853        0.2713  0.0075  1.2327\n",
      "     10                     0.8718        0.3203                     0.8900        0.2886  0.0069  1.2335\n",
      "     11                     0.8727        \u001b[32m0.3069\u001b[0m                     0.8925        \u001b[31m0.2577\u001b[0m  0.0063  1.2327\n",
      "     12                     \u001b[36m0.8905\u001b[0m        \u001b[32m0.2892\u001b[0m                     0.8876        0.2848  0.0057  1.2339\n",
      "     13                     0.8868        \u001b[32m0.2785\u001b[0m                     0.8925        0.2688  0.0050  1.2326\n",
      "     14                     0.8855        \u001b[32m0.2730\u001b[0m                     \u001b[35m0.9041\u001b[0m        \u001b[31m0.2504\u001b[0m  0.0043  1.2337\n",
      "     15                     \u001b[36m0.8960\u001b[0m        \u001b[32m0.2583\u001b[0m                     0.8832        0.2700  0.0037  1.2342\n",
      "     16                     \u001b[36m0.8985\u001b[0m        \u001b[32m0.2558\u001b[0m                     0.9012        0.2571  0.0031  1.2329\n",
      "     17                     \u001b[36m0.9007\u001b[0m        \u001b[32m0.2452\u001b[0m                     \u001b[35m0.9105\u001b[0m        \u001b[31m0.2478\u001b[0m  0.0025  1.2337\n",
      "     18                     \u001b[36m0.9063\u001b[0m        \u001b[32m0.2373\u001b[0m                     0.8915        0.2654  0.0020  1.2340\n",
      "     19                     \u001b[36m0.9115\u001b[0m        \u001b[32m0.2260\u001b[0m                     0.9096        \u001b[31m0.2419\u001b[0m  0.0015  1.2341\n",
      "     20                     \u001b[36m0.9154\u001b[0m        \u001b[32m0.2244\u001b[0m                     0.9073        \u001b[31m0.2418\u001b[0m  0.0010  1.2341\n",
      "     21                     0.9150        \u001b[32m0.2205\u001b[0m                     0.9091        \u001b[31m0.2377\u001b[0m  0.0007  1.2347\n",
      "     22                     0.9081        0.2225                     \u001b[35m0.9112\u001b[0m        \u001b[31m0.2377\u001b[0m  0.0004  1.2341\n",
      "     23                     \u001b[36m0.9227\u001b[0m        \u001b[32m0.2159\u001b[0m                     \u001b[35m0.9115\u001b[0m        \u001b[31m0.2350\u001b[0m  0.0002  1.2337\n",
      "     24                     0.9182        \u001b[32m0.2126\u001b[0m                     \u001b[35m0.9136\u001b[0m        0.2355  0.0000  1.2342\n",
      "     25                     0.9165        \u001b[32m0.2102\u001b[0m                     0.9133        0.2351  0.0000  1.2365\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7239\u001b[0m        \u001b[32m0.7538\u001b[0m                     \u001b[35m0.8180\u001b[0m        \u001b[31m0.4546\u001b[0m  0.0100  1.2277\n",
      "      2                     \u001b[36m0.8073\u001b[0m        \u001b[32m0.5010\u001b[0m                     \u001b[35m0.8315\u001b[0m        \u001b[31m0.4423\u001b[0m  0.0100  1.2338\n",
      "      3                     \u001b[36m0.8305\u001b[0m        \u001b[32m0.4417\u001b[0m                     \u001b[35m0.8653\u001b[0m        \u001b[31m0.3211\u001b[0m  0.0098  1.2337\n",
      "      4                     \u001b[36m0.8385\u001b[0m        \u001b[32m0.4104\u001b[0m                     0.8653        0.3415  0.0096  1.2341\n",
      "      5                     \u001b[36m0.8486\u001b[0m        \u001b[32m0.3820\u001b[0m                     0.8609        \u001b[31m0.3131\u001b[0m  0.0093  1.2347\n",
      "      6                     \u001b[36m0.8634\u001b[0m        \u001b[32m0.3485\u001b[0m                     \u001b[35m0.8770\u001b[0m        \u001b[31m0.2921\u001b[0m  0.0090  1.2333\n",
      "      7                     \u001b[36m0.8642\u001b[0m        \u001b[32m0.3404\u001b[0m                     \u001b[35m0.8889\u001b[0m        \u001b[31m0.2815\u001b[0m  0.0085  1.2347\n",
      "      8                     \u001b[36m0.8654\u001b[0m        \u001b[32m0.3267\u001b[0m                     \u001b[35m0.8890\u001b[0m        0.3023  0.0080  1.2335\n",
      "      9                     \u001b[36m0.8728\u001b[0m        \u001b[32m0.3155\u001b[0m                     0.8820        0.2877  0.0075  1.2337\n",
      "     10                     \u001b[36m0.8778\u001b[0m        \u001b[32m0.3033\u001b[0m                     0.8846        0.3025  0.0069  1.2338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     \u001b[36m0.8814\u001b[0m        0.3035                     0.8862        \u001b[31m0.2809\u001b[0m  0.0063  1.2345\n",
      "     12                     \u001b[36m0.8872\u001b[0m        \u001b[32m0.2858\u001b[0m                     0.8883        0.2966  0.0057  1.2347\n",
      "     13                     \u001b[36m0.8898\u001b[0m        \u001b[32m0.2786\u001b[0m                     \u001b[35m0.8893\u001b[0m        \u001b[31m0.2727\u001b[0m  0.0050  1.2347\n",
      "     14                     \u001b[36m0.8923\u001b[0m        \u001b[32m0.2685\u001b[0m                     \u001b[35m0.8904\u001b[0m        0.2854  0.0043  1.2351\n",
      "     15                     \u001b[36m0.8962\u001b[0m        \u001b[32m0.2642\u001b[0m                     \u001b[35m0.8918\u001b[0m        0.2788  0.0037  1.2357\n",
      "     16                     \u001b[36m0.9024\u001b[0m        \u001b[32m0.2515\u001b[0m                     \u001b[35m0.8977\u001b[0m        \u001b[31m0.2615\u001b[0m  0.0031  1.2367\n",
      "     17                     \u001b[36m0.9058\u001b[0m        \u001b[32m0.2448\u001b[0m                     \u001b[35m0.8989\u001b[0m        0.2658  0.0025  1.2367\n",
      "     18                     \u001b[36m0.9098\u001b[0m        \u001b[32m0.2359\u001b[0m                     \u001b[35m0.9026\u001b[0m        \u001b[31m0.2613\u001b[0m  0.0020  1.2373\n",
      "     19                     0.9055        \u001b[32m0.2338\u001b[0m                     0.9019        0.2617  0.0015  1.2358\n",
      "     20                     0.9084        \u001b[32m0.2284\u001b[0m                     \u001b[35m0.9039\u001b[0m        \u001b[31m0.2566\u001b[0m  0.0010  1.2357\n",
      "     21                     \u001b[36m0.9109\u001b[0m        \u001b[32m0.2206\u001b[0m                     0.9017        0.2578  0.0007  1.2318\n",
      "     22                     \u001b[36m0.9135\u001b[0m        \u001b[32m0.2171\u001b[0m                     0.9021        0.2611  0.0004  1.2354\n",
      "     23                     \u001b[36m0.9159\u001b[0m        \u001b[32m0.2165\u001b[0m                     0.8983        0.2621  0.0002  1.2358\n",
      "     24                     \u001b[36m0.9177\u001b[0m        0.2171                     \u001b[35m0.9042\u001b[0m        0.2573  0.0000  1.2359\n",
      "     25                     \u001b[36m0.9194\u001b[0m        \u001b[32m0.2133\u001b[0m                     \u001b[35m0.9044\u001b[0m        0.2593  0.0000  1.2366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[1 1 0 ... 0 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5470\u001b[0m        \u001b[32m0.8812\u001b[0m                     \u001b[35m0.6645\u001b[0m        \u001b[31m0.6158\u001b[0m  0.0100  1.2275\n",
      "      2                     \u001b[36m0.6503\u001b[0m        \u001b[32m0.6714\u001b[0m                     \u001b[35m0.6955\u001b[0m        \u001b[31m0.5781\u001b[0m  0.0100  1.2325\n",
      "      3                     \u001b[36m0.6841\u001b[0m        \u001b[32m0.6200\u001b[0m                     \u001b[35m0.7069\u001b[0m        \u001b[31m0.5641\u001b[0m  0.0098  1.2328\n",
      "      4                     \u001b[36m0.6971\u001b[0m        \u001b[32m0.6065\u001b[0m                     0.7038        0.5988  0.0096  1.2317\n",
      "      5                     \u001b[36m0.7038\u001b[0m        \u001b[32m0.5902\u001b[0m                     \u001b[35m0.7257\u001b[0m        \u001b[31m0.5473\u001b[0m  0.0093  1.2327\n",
      "      6                     \u001b[36m0.7143\u001b[0m        \u001b[32m0.5797\u001b[0m                     \u001b[35m0.7327\u001b[0m        \u001b[31m0.5319\u001b[0m  0.0090  1.2341\n",
      "      7                     \u001b[36m0.7246\u001b[0m        \u001b[32m0.5560\u001b[0m                     \u001b[35m0.7384\u001b[0m        \u001b[31m0.5222\u001b[0m  0.0085  1.2327\n",
      "      8                     \u001b[36m0.7332\u001b[0m        \u001b[32m0.5436\u001b[0m                     \u001b[35m0.7395\u001b[0m        0.5234  0.0080  1.2339\n",
      "      9                     \u001b[36m0.7370\u001b[0m        \u001b[32m0.5361\u001b[0m                     \u001b[35m0.7531\u001b[0m        \u001b[31m0.5030\u001b[0m  0.0075  1.2337\n",
      "     10                     \u001b[36m0.7463\u001b[0m        \u001b[32m0.5285\u001b[0m                     0.7481        0.5103  0.0069  1.2340\n",
      "     11                     \u001b[36m0.7510\u001b[0m        \u001b[32m0.5189\u001b[0m                     \u001b[35m0.7584\u001b[0m        \u001b[31m0.5004\u001b[0m  0.0063  1.2327\n",
      "     12                     0.7504        \u001b[32m0.5157\u001b[0m                     \u001b[35m0.7599\u001b[0m        \u001b[31m0.4960\u001b[0m  0.0057  1.2334\n",
      "     13                     \u001b[36m0.7519\u001b[0m        \u001b[32m0.5101\u001b[0m                     0.7568        0.4977  0.0050  1.2340\n",
      "     14                     \u001b[36m0.7557\u001b[0m        \u001b[32m0.5042\u001b[0m                     \u001b[35m0.7611\u001b[0m        \u001b[31m0.4891\u001b[0m  0.0043  1.2353\n",
      "     15                     \u001b[36m0.7582\u001b[0m        \u001b[32m0.5002\u001b[0m                     \u001b[35m0.7619\u001b[0m        \u001b[31m0.4868\u001b[0m  0.0037  1.2344\n",
      "     16                     \u001b[36m0.7719\u001b[0m        \u001b[32m0.4871\u001b[0m                     \u001b[35m0.7672\u001b[0m        \u001b[31m0.4816\u001b[0m  0.0031  1.2339\n",
      "     17                     \u001b[36m0.7743\u001b[0m        \u001b[32m0.4843\u001b[0m                     \u001b[35m0.7693\u001b[0m        \u001b[31m0.4800\u001b[0m  0.0025  1.2341\n",
      "     18                     0.7718        \u001b[32m0.4799\u001b[0m                     \u001b[35m0.7731\u001b[0m        \u001b[31m0.4777\u001b[0m  0.0020  1.2348\n",
      "     19                     0.7713        \u001b[32m0.4792\u001b[0m                     0.7711        \u001b[31m0.4776\u001b[0m  0.0015  1.2355\n",
      "     20                     \u001b[36m0.7775\u001b[0m        \u001b[32m0.4705\u001b[0m                     \u001b[35m0.7741\u001b[0m        0.4789  0.0010  1.2342\n",
      "     21                     \u001b[36m0.7783\u001b[0m        0.4721                     0.7706        \u001b[31m0.4754\u001b[0m  0.0007  1.2353\n",
      "     22                     \u001b[36m0.7816\u001b[0m        \u001b[32m0.4661\u001b[0m                     0.7728        \u001b[31m0.4713\u001b[0m  0.0004  1.2348\n",
      "     23                     \u001b[36m0.7837\u001b[0m        \u001b[32m0.4595\u001b[0m                     \u001b[35m0.7743\u001b[0m        \u001b[31m0.4707\u001b[0m  0.0002  1.2357\n",
      "     24                     0.7832        0.4624                     \u001b[35m0.7767\u001b[0m        \u001b[31m0.4703\u001b[0m  0.0000  1.2368\n",
      "     25                     0.7819        0.4625                     0.7764        0.4706  0.0000  1.2350\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5483\u001b[0m        \u001b[32m0.8427\u001b[0m                     \u001b[35m0.6481\u001b[0m        \u001b[31m0.6368\u001b[0m  0.0100  1.2257\n",
      "      2                     \u001b[36m0.6397\u001b[0m        \u001b[32m0.6902\u001b[0m                     \u001b[35m0.6999\u001b[0m        \u001b[31m0.5715\u001b[0m  0.0100  1.2325\n",
      "      3                     \u001b[36m0.6789\u001b[0m        \u001b[32m0.6404\u001b[0m                     \u001b[35m0.7301\u001b[0m        \u001b[31m0.5366\u001b[0m  0.0098  1.2328\n",
      "      4                     \u001b[36m0.6957\u001b[0m        \u001b[32m0.6056\u001b[0m                     \u001b[35m0.7449\u001b[0m        \u001b[31m0.5176\u001b[0m  0.0096  1.2328\n",
      "      5                     \u001b[36m0.7029\u001b[0m        \u001b[32m0.5938\u001b[0m                     \u001b[35m0.7588\u001b[0m        \u001b[31m0.5120\u001b[0m  0.0093  1.2347\n",
      "      6                     \u001b[36m0.7165\u001b[0m        \u001b[32m0.5650\u001b[0m                     0.7535        \u001b[31m0.5083\u001b[0m  0.0090  1.2357\n",
      "      7                     \u001b[36m0.7273\u001b[0m        \u001b[32m0.5539\u001b[0m                     \u001b[35m0.7600\u001b[0m        \u001b[31m0.5023\u001b[0m  0.0085  1.2341\n",
      "      8                     \u001b[36m0.7359\u001b[0m        \u001b[32m0.5418\u001b[0m                     0.7586        \u001b[31m0.4986\u001b[0m  0.0080  1.2337\n",
      "      9                     \u001b[36m0.7396\u001b[0m        \u001b[32m0.5281\u001b[0m                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.4899\u001b[0m  0.0075  1.2348\n",
      "     10                     \u001b[36m0.7466\u001b[0m        \u001b[32m0.5235\u001b[0m                     0.7663        \u001b[31m0.4793\u001b[0m  0.0069  1.2327\n",
      "     11                     \u001b[36m0.7476\u001b[0m        \u001b[32m0.5179\u001b[0m                     0.7660        0.4892  0.0063  1.2327\n",
      "     12                     \u001b[36m0.7552\u001b[0m        \u001b[32m0.5126\u001b[0m                     \u001b[35m0.7754\u001b[0m        \u001b[31m0.4734\u001b[0m  0.0057  1.2352\n",
      "     13                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.5074\u001b[0m                     0.7746        0.4765  0.0050  1.2368\n",
      "     14                     \u001b[36m0.7602\u001b[0m        \u001b[32m0.5045\u001b[0m                     \u001b[35m0.7795\u001b[0m        \u001b[31m0.4721\u001b[0m  0.0043  1.2346\n",
      "     15                     0.7580        \u001b[32m0.5008\u001b[0m                     0.7687        0.4794  0.0037  1.2349\n",
      "     16                     \u001b[36m0.7637\u001b[0m        \u001b[32m0.4924\u001b[0m                     0.7776        \u001b[31m0.4664\u001b[0m  0.0031  1.2359\n",
      "     17                     \u001b[36m0.7711\u001b[0m        \u001b[32m0.4823\u001b[0m                     \u001b[35m0.7859\u001b[0m        \u001b[31m0.4609\u001b[0m  0.0025  1.2360\n",
      "     18                     \u001b[36m0.7754\u001b[0m        \u001b[32m0.4796\u001b[0m                     0.7815        \u001b[31m0.4591\u001b[0m  0.0020  1.2339\n",
      "     19                     \u001b[36m0.7755\u001b[0m        \u001b[32m0.4737\u001b[0m                     0.7850        0.4595  0.0015  1.2354\n",
      "     20                     \u001b[36m0.7782\u001b[0m        \u001b[32m0.4685\u001b[0m                     \u001b[35m0.7871\u001b[0m        \u001b[31m0.4532\u001b[0m  0.0010  1.2347\n",
      "     21                     \u001b[36m0.7790\u001b[0m        0.4697                     \u001b[35m0.7904\u001b[0m        0.4535  0.0007  1.2347\n",
      "     22                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4645\u001b[0m                     \u001b[35m0.7927\u001b[0m        \u001b[31m0.4509\u001b[0m  0.0004  1.2367\n",
      "     23                     0.7808        \u001b[32m0.4611\u001b[0m                     0.7899        0.4510  0.0002  1.2367\n",
      "     24                     \u001b[36m0.7849\u001b[0m        \u001b[32m0.4589\u001b[0m                     0.7899        \u001b[31m0.4504\u001b[0m  0.0000  1.2367\n",
      "     25                     \u001b[36m0.7873\u001b[0m        \u001b[32m0.4578\u001b[0m                     0.7909        \u001b[31m0.4500\u001b[0m  0.0000  1.2360\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5634\u001b[0m        \u001b[32m0.8806\u001b[0m                     \u001b[35m0.6703\u001b[0m        \u001b[31m0.6103\u001b[0m  0.0100  1.2274\n",
      "      2                     \u001b[36m0.6466\u001b[0m        \u001b[32m0.6796\u001b[0m                     \u001b[35m0.7187\u001b[0m        \u001b[31m0.5561\u001b[0m  0.0100  1.2338\n",
      "      3                     \u001b[36m0.6753\u001b[0m        \u001b[32m0.6363\u001b[0m                     0.7098        0.5639  0.0098  1.2327\n",
      "      4                     \u001b[36m0.6908\u001b[0m        \u001b[32m0.6104\u001b[0m                     0.7114        0.5672  0.0096  1.2337\n",
      "      5                     \u001b[36m0.6995\u001b[0m        \u001b[32m0.5920\u001b[0m                     \u001b[35m0.7349\u001b[0m        \u001b[31m0.5286\u001b[0m  0.0093  1.2317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6                     \u001b[36m0.7095\u001b[0m        \u001b[32m0.5757\u001b[0m                     \u001b[35m0.7363\u001b[0m        \u001b[31m0.5198\u001b[0m  0.0090  1.2347\n",
      "      7                     \u001b[36m0.7229\u001b[0m        \u001b[32m0.5576\u001b[0m                     \u001b[35m0.7549\u001b[0m        \u001b[31m0.5032\u001b[0m  0.0085  1.2337\n",
      "      8                     \u001b[36m0.7273\u001b[0m        \u001b[32m0.5470\u001b[0m                     \u001b[35m0.7660\u001b[0m        0.5035  0.0080  1.2337\n",
      "      9                     \u001b[36m0.7364\u001b[0m        \u001b[32m0.5362\u001b[0m                     0.7500        0.5083  0.0075  1.2348\n",
      "     10                     \u001b[36m0.7470\u001b[0m        \u001b[32m0.5231\u001b[0m                     0.7586        0.5055  0.0069  1.2347\n",
      "     11                     0.7406        0.5261                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.4859\u001b[0m  0.0063  1.2327\n",
      "     12                     \u001b[36m0.7494\u001b[0m        \u001b[32m0.5118\u001b[0m                     0.7637        0.4914  0.0057  1.2574\n",
      "     13                     \u001b[36m0.7531\u001b[0m        \u001b[32m0.5118\u001b[0m                     0.7677        0.4938  0.0050  1.2378\n",
      "     14                     \u001b[36m0.7548\u001b[0m        \u001b[32m0.5046\u001b[0m                     \u001b[35m0.7790\u001b[0m        \u001b[31m0.4782\u001b[0m  0.0043  1.2337\n",
      "     15                     \u001b[36m0.7643\u001b[0m        \u001b[32m0.5013\u001b[0m                     0.7752        \u001b[31m0.4749\u001b[0m  0.0037  1.2347\n",
      "     16                     0.7623        \u001b[32m0.4977\u001b[0m                     0.7748        \u001b[31m0.4722\u001b[0m  0.0031  1.2348\n",
      "     17                     \u001b[36m0.7653\u001b[0m        \u001b[32m0.4916\u001b[0m                     \u001b[35m0.7825\u001b[0m        \u001b[31m0.4667\u001b[0m  0.0025  1.2348\n",
      "     18                     \u001b[36m0.7660\u001b[0m        \u001b[32m0.4888\u001b[0m                     \u001b[35m0.7854\u001b[0m        \u001b[31m0.4663\u001b[0m  0.0020  1.2354\n",
      "     19                     \u001b[36m0.7728\u001b[0m        \u001b[32m0.4792\u001b[0m                     \u001b[35m0.7872\u001b[0m        \u001b[31m0.4618\u001b[0m  0.0015  1.2356\n",
      "     20                     0.7679        0.4828                     0.7830        0.4648  0.0010  1.2352\n",
      "     21                     \u001b[36m0.7789\u001b[0m        \u001b[32m0.4707\u001b[0m                     \u001b[35m0.7882\u001b[0m        \u001b[31m0.4607\u001b[0m  0.0007  1.2357\n",
      "     22                     \u001b[36m0.7796\u001b[0m        \u001b[32m0.4686\u001b[0m                     \u001b[35m0.7902\u001b[0m        \u001b[31m0.4590\u001b[0m  0.0004  1.2342\n",
      "     23                     \u001b[36m0.7800\u001b[0m        \u001b[32m0.4683\u001b[0m                     \u001b[35m0.7907\u001b[0m        \u001b[31m0.4577\u001b[0m  0.0002  1.2381\n",
      "     24                     0.7786        \u001b[32m0.4669\u001b[0m                     \u001b[35m0.7916\u001b[0m        0.4578  0.0000  1.2364\n",
      "     25                     \u001b[36m0.7853\u001b[0m        \u001b[32m0.4663\u001b[0m                     \u001b[35m0.7925\u001b[0m        \u001b[31m0.4576\u001b[0m  0.0000  1.2362\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5556\u001b[0m        \u001b[32m0.8856\u001b[0m                     \u001b[35m0.6584\u001b[0m        \u001b[31m0.6188\u001b[0m  0.0100  1.2270\n",
      "      2                     \u001b[36m0.6355\u001b[0m        \u001b[32m0.6948\u001b[0m                     \u001b[35m0.6700\u001b[0m        \u001b[31m0.6124\u001b[0m  0.0100  1.2338\n",
      "      3                     \u001b[36m0.6757\u001b[0m        \u001b[32m0.6371\u001b[0m                     \u001b[35m0.7292\u001b[0m        \u001b[31m0.5341\u001b[0m  0.0098  1.2337\n",
      "      4                     \u001b[36m0.6939\u001b[0m        \u001b[32m0.6070\u001b[0m                     \u001b[35m0.7423\u001b[0m        \u001b[31m0.5289\u001b[0m  0.0096  1.2323\n",
      "      5                     \u001b[36m0.6957\u001b[0m        \u001b[32m0.5947\u001b[0m                     \u001b[35m0.7523\u001b[0m        \u001b[31m0.5095\u001b[0m  0.0093  1.2343\n",
      "      6                     \u001b[36m0.7200\u001b[0m        \u001b[32m0.5659\u001b[0m                     \u001b[35m0.7625\u001b[0m        \u001b[31m0.5015\u001b[0m  0.0090  1.2337\n",
      "      7                     \u001b[36m0.7301\u001b[0m        \u001b[32m0.5464\u001b[0m                     \u001b[35m0.7626\u001b[0m        \u001b[31m0.4997\u001b[0m  0.0085  1.2338\n",
      "      8                     \u001b[36m0.7307\u001b[0m        \u001b[32m0.5451\u001b[0m                     0.7461        0.5206  0.0080  1.2338\n",
      "      9                     \u001b[36m0.7379\u001b[0m        \u001b[32m0.5313\u001b[0m                     \u001b[35m0.7772\u001b[0m        \u001b[31m0.4843\u001b[0m  0.0075  1.2327\n",
      "     10                     \u001b[36m0.7410\u001b[0m        \u001b[32m0.5289\u001b[0m                     0.7686        0.4980  0.0069  1.2349\n",
      "     11                     \u001b[36m0.7458\u001b[0m        \u001b[32m0.5206\u001b[0m                     \u001b[35m0.7782\u001b[0m        \u001b[31m0.4754\u001b[0m  0.0063  1.2347\n",
      "     12                     \u001b[36m0.7527\u001b[0m        \u001b[32m0.5142\u001b[0m                     0.7727        0.4818  0.0057  1.2345\n",
      "     13                     \u001b[36m0.7564\u001b[0m        \u001b[32m0.5061\u001b[0m                     0.7694        0.4851  0.0050  1.2424\n",
      "     14                     0.7555        0.5072                     \u001b[35m0.7827\u001b[0m        \u001b[31m0.4740\u001b[0m  0.0043  1.2342\n",
      "     15                     \u001b[36m0.7585\u001b[0m        \u001b[32m0.4953\u001b[0m                     \u001b[35m0.7881\u001b[0m        \u001b[31m0.4628\u001b[0m  0.0037  1.2338\n",
      "     16                     \u001b[36m0.7645\u001b[0m        \u001b[32m0.4937\u001b[0m                     0.7820        \u001b[31m0.4607\u001b[0m  0.0031  1.2348\n",
      "     17                     \u001b[36m0.7653\u001b[0m        \u001b[32m0.4853\u001b[0m                     0.7845        \u001b[31m0.4572\u001b[0m  0.0025  1.2344\n",
      "     18                     \u001b[36m0.7701\u001b[0m        \u001b[32m0.4838\u001b[0m                     \u001b[35m0.7891\u001b[0m        0.4591  0.0020  1.2357\n",
      "     19                     \u001b[36m0.7728\u001b[0m        \u001b[32m0.4769\u001b[0m                     \u001b[35m0.7987\u001b[0m        \u001b[31m0.4500\u001b[0m  0.0015  1.2363\n",
      "     20                     \u001b[36m0.7729\u001b[0m        \u001b[32m0.4732\u001b[0m                     0.7926        0.4513  0.0010  1.2367\n",
      "     21                     \u001b[36m0.7784\u001b[0m        \u001b[32m0.4709\u001b[0m                     0.7978        0.4509  0.0007  1.2367\n",
      "     22                     0.7783        \u001b[32m0.4666\u001b[0m                     0.7937        \u001b[31m0.4482\u001b[0m  0.0004  1.2358\n",
      "     23                     \u001b[36m0.7807\u001b[0m        0.4671                     0.7976        \u001b[31m0.4478\u001b[0m  0.0002  1.2359\n",
      "     24                     \u001b[36m0.7839\u001b[0m        \u001b[32m0.4625\u001b[0m                     0.7972        \u001b[31m0.4475\u001b[0m  0.0000  1.2361\n",
      "     25                     \u001b[36m0.7840\u001b[0m        \u001b[32m0.4611\u001b[0m                     0.7946        \u001b[31m0.4468\u001b[0m  0.0000  1.2357\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5653\u001b[0m        \u001b[32m0.8413\u001b[0m                     \u001b[35m0.6604\u001b[0m        \u001b[31m0.6132\u001b[0m  0.0100  1.2267\n",
      "      2                     \u001b[36m0.6483\u001b[0m        \u001b[32m0.6756\u001b[0m                     0.6175        0.7491  0.0100  1.2341\n",
      "      3                     \u001b[36m0.6624\u001b[0m        \u001b[32m0.6710\u001b[0m                     \u001b[35m0.7156\u001b[0m        \u001b[31m0.5556\u001b[0m  0.0098  1.2377\n",
      "      4                     \u001b[36m0.6856\u001b[0m        \u001b[32m0.6212\u001b[0m                     \u001b[35m0.7246\u001b[0m        \u001b[31m0.5434\u001b[0m  0.0096  1.2376\n",
      "      5                     \u001b[36m0.7046\u001b[0m        \u001b[32m0.5901\u001b[0m                     \u001b[35m0.7277\u001b[0m        \u001b[31m0.5426\u001b[0m  0.0093  1.2345\n",
      "      6                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.5664\u001b[0m                     \u001b[35m0.7505\u001b[0m        \u001b[31m0.5269\u001b[0m  0.0090  1.2341\n",
      "      7                     \u001b[36m0.7230\u001b[0m        \u001b[32m0.5583\u001b[0m                     0.7434        \u001b[31m0.5155\u001b[0m  0.0085  1.2336\n",
      "      8                     \u001b[36m0.7257\u001b[0m        \u001b[32m0.5470\u001b[0m                     \u001b[35m0.7540\u001b[0m        \u001b[31m0.5068\u001b[0m  0.0080  1.2342\n",
      "      9                     \u001b[36m0.7374\u001b[0m        \u001b[32m0.5367\u001b[0m                     \u001b[35m0.7584\u001b[0m        \u001b[31m0.4985\u001b[0m  0.0075  1.2328\n",
      "     10                     \u001b[36m0.7452\u001b[0m        \u001b[32m0.5253\u001b[0m                     \u001b[35m0.7594\u001b[0m        0.5024  0.0069  1.2358\n",
      "     11                     0.7433        \u001b[32m0.5233\u001b[0m                     \u001b[35m0.7658\u001b[0m        \u001b[31m0.4857\u001b[0m  0.0063  1.2378\n",
      "     12                     \u001b[36m0.7519\u001b[0m        \u001b[32m0.5148\u001b[0m                     \u001b[35m0.7685\u001b[0m        0.4883  0.0057  1.2363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13                     0.7509        \u001b[32m0.5139\u001b[0m                     0.7635        0.4919  0.0050  1.2348\n",
      "     14                     \u001b[36m0.7574\u001b[0m        \u001b[32m0.5041\u001b[0m                     \u001b[35m0.7715\u001b[0m        \u001b[31m0.4845\u001b[0m  0.0043  1.2356\n",
      "     15                     \u001b[36m0.7626\u001b[0m        \u001b[32m0.4990\u001b[0m                     \u001b[35m0.7771\u001b[0m        \u001b[31m0.4774\u001b[0m  0.0037  1.2337\n",
      "     16                     \u001b[36m0.7639\u001b[0m        \u001b[32m0.4881\u001b[0m                     \u001b[35m0.7788\u001b[0m        \u001b[31m0.4689\u001b[0m  0.0031  1.2353\n",
      "     17                     \u001b[36m0.7672\u001b[0m        \u001b[32m0.4876\u001b[0m                     0.7786        0.4714  0.0025  1.2357\n",
      "     18                     \u001b[36m0.7728\u001b[0m        \u001b[32m0.4825\u001b[0m                     \u001b[35m0.7806\u001b[0m        \u001b[31m0.4689\u001b[0m  0.0020  1.2343\n",
      "     19                     \u001b[36m0.7755\u001b[0m        \u001b[32m0.4771\u001b[0m                     \u001b[35m0.7839\u001b[0m        \u001b[31m0.4670\u001b[0m  0.0015  1.2353\n",
      "     20                     0.7735        0.4804                     \u001b[35m0.7920\u001b[0m        \u001b[31m0.4605\u001b[0m  0.0010  1.2353\n",
      "     21                     \u001b[36m0.7781\u001b[0m        \u001b[32m0.4696\u001b[0m                     \u001b[35m0.7930\u001b[0m        \u001b[31m0.4572\u001b[0m  0.0007  1.2348\n",
      "     22                     \u001b[36m0.7800\u001b[0m        0.4701                     0.7922        \u001b[31m0.4570\u001b[0m  0.0004  1.2356\n",
      "     23                     0.7779        \u001b[32m0.4688\u001b[0m                     \u001b[35m0.7931\u001b[0m        \u001b[31m0.4564\u001b[0m  0.0002  1.2357\n",
      "     24                     0.7797        \u001b[32m0.4637\u001b[0m                     \u001b[35m0.7932\u001b[0m        \u001b[31m0.4562\u001b[0m  0.0000  1.2348\n",
      "     25                     \u001b[36m0.7802\u001b[0m        \u001b[32m0.4635\u001b[0m                     0.7928        0.4566  0.0000  1.2397\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5467\u001b[0m        \u001b[32m0.8631\u001b[0m                     \u001b[35m0.6501\u001b[0m        \u001b[31m0.6220\u001b[0m  0.0100  1.2277\n",
      "      2                     \u001b[36m0.6311\u001b[0m        \u001b[32m0.7035\u001b[0m                     \u001b[35m0.7159\u001b[0m        \u001b[31m0.5618\u001b[0m  0.0100  1.2329\n",
      "      3                     \u001b[36m0.6810\u001b[0m        \u001b[32m0.6356\u001b[0m                     0.7130        \u001b[31m0.5544\u001b[0m  0.0098  1.2320\n",
      "      4                     \u001b[36m0.6889\u001b[0m        \u001b[32m0.6093\u001b[0m                     0.7132        0.5578  0.0096  1.2330\n",
      "      5                     \u001b[36m0.7038\u001b[0m        \u001b[32m0.5897\u001b[0m                     \u001b[35m0.7559\u001b[0m        \u001b[31m0.5135\u001b[0m  0.0093  1.2334\n",
      "      6                     \u001b[36m0.7137\u001b[0m        \u001b[32m0.5732\u001b[0m                     0.7559        0.5172  0.0090  1.2346\n",
      "      7                     \u001b[36m0.7215\u001b[0m        \u001b[32m0.5632\u001b[0m                     \u001b[35m0.7601\u001b[0m        \u001b[31m0.5099\u001b[0m  0.0085  1.2328\n",
      "      8                     \u001b[36m0.7279\u001b[0m        \u001b[32m0.5498\u001b[0m                     \u001b[35m0.7607\u001b[0m        \u001b[31m0.5050\u001b[0m  0.0080  1.2350\n",
      "      9                     \u001b[36m0.7366\u001b[0m        \u001b[32m0.5312\u001b[0m                     \u001b[35m0.7621\u001b[0m        \u001b[31m0.4954\u001b[0m  0.0075  1.2339\n",
      "     10                     \u001b[36m0.7459\u001b[0m        \u001b[32m0.5288\u001b[0m                     \u001b[35m0.7622\u001b[0m        0.5026  0.0069  1.2345\n",
      "     11                     0.7433        \u001b[32m0.5228\u001b[0m                     \u001b[35m0.7829\u001b[0m        \u001b[31m0.4746\u001b[0m  0.0063  1.2341\n",
      "     12                     \u001b[36m0.7461\u001b[0m        \u001b[32m0.5185\u001b[0m                     0.7800        0.4747  0.0057  1.2338\n",
      "     13                     \u001b[36m0.7566\u001b[0m        \u001b[32m0.5053\u001b[0m                     0.7762        0.4769  0.0050  1.2341\n",
      "     14                     \u001b[36m0.7588\u001b[0m        \u001b[32m0.5041\u001b[0m                     \u001b[35m0.7839\u001b[0m        \u001b[31m0.4721\u001b[0m  0.0043  1.2333\n",
      "     15                     \u001b[36m0.7595\u001b[0m        \u001b[32m0.4966\u001b[0m                     \u001b[35m0.7872\u001b[0m        \u001b[31m0.4633\u001b[0m  0.0037  1.2347\n",
      "     16                     \u001b[36m0.7679\u001b[0m        \u001b[32m0.4871\u001b[0m                     \u001b[35m0.7874\u001b[0m        0.4637  0.0031  1.2357\n",
      "     17                     \u001b[36m0.7698\u001b[0m        \u001b[32m0.4834\u001b[0m                     \u001b[35m0.7927\u001b[0m        \u001b[31m0.4565\u001b[0m  0.0025  1.2412\n",
      "     18                     \u001b[36m0.7723\u001b[0m        \u001b[32m0.4834\u001b[0m                     0.7897        \u001b[31m0.4556\u001b[0m  0.0020  1.2345\n",
      "     19                     \u001b[36m0.7742\u001b[0m        \u001b[32m0.4765\u001b[0m                     0.7879        \u001b[31m0.4515\u001b[0m  0.0015  1.2357\n",
      "     20                     \u001b[36m0.7823\u001b[0m        \u001b[32m0.4668\u001b[0m                     0.7888        0.4544  0.0010  1.2351\n",
      "     21                     0.7821        \u001b[32m0.4656\u001b[0m                     0.7920        \u001b[31m0.4497\u001b[0m  0.0007  1.2348\n",
      "     22                     \u001b[36m0.7831\u001b[0m        \u001b[32m0.4626\u001b[0m                     \u001b[35m0.7956\u001b[0m        \u001b[31m0.4464\u001b[0m  0.0004  1.2352\n",
      "     23                     0.7824        \u001b[32m0.4587\u001b[0m                     \u001b[35m0.7964\u001b[0m        0.4466  0.0002  1.2357\n",
      "     24                     \u001b[36m0.7855\u001b[0m        \u001b[32m0.4566\u001b[0m                     0.7943        \u001b[31m0.4459\u001b[0m  0.0000  1.2356\n",
      "     25                     0.7846        \u001b[32m0.4556\u001b[0m                     0.7937        0.4460  0.0000  1.2380\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5596\u001b[0m        \u001b[32m0.8378\u001b[0m                     \u001b[35m0.6479\u001b[0m        \u001b[31m0.6230\u001b[0m  0.0100  1.2277\n",
      "      2                     \u001b[36m0.6358\u001b[0m        \u001b[32m0.6927\u001b[0m                     0.6381        0.7172  0.0100  1.2333\n",
      "      3                     \u001b[36m0.6724\u001b[0m        \u001b[32m0.6537\u001b[0m                     \u001b[35m0.6708\u001b[0m        0.6411  0.0098  1.2315\n",
      "      4                     \u001b[36m0.6960\u001b[0m        \u001b[32m0.6061\u001b[0m                     \u001b[35m0.7274\u001b[0m        \u001b[31m0.5560\u001b[0m  0.0096  1.2326\n",
      "      5                     \u001b[36m0.7042\u001b[0m        \u001b[32m0.5927\u001b[0m                     \u001b[35m0.7419\u001b[0m        \u001b[31m0.5270\u001b[0m  0.0093  1.2319\n",
      "      6                     \u001b[36m0.7105\u001b[0m        \u001b[32m0.5794\u001b[0m                     0.7114        0.5674  0.0090  1.2322\n",
      "      7                     \u001b[36m0.7229\u001b[0m        \u001b[32m0.5578\u001b[0m                     0.7344        0.5327  0.0085  1.2322\n",
      "      8                     \u001b[36m0.7285\u001b[0m        \u001b[32m0.5480\u001b[0m                     \u001b[35m0.7488\u001b[0m        \u001b[31m0.5129\u001b[0m  0.0080  1.2326\n",
      "      9                     \u001b[36m0.7361\u001b[0m        \u001b[32m0.5368\u001b[0m                     \u001b[35m0.7513\u001b[0m        \u001b[31m0.5076\u001b[0m  0.0075  1.2328\n",
      "     10                     \u001b[36m0.7432\u001b[0m        \u001b[32m0.5250\u001b[0m                     \u001b[35m0.7528\u001b[0m        \u001b[31m0.5043\u001b[0m  0.0069  1.2332\n",
      "     11                     \u001b[36m0.7489\u001b[0m        \u001b[32m0.5174\u001b[0m                     \u001b[35m0.7691\u001b[0m        \u001b[31m0.4919\u001b[0m  0.0063  1.2324\n",
      "     12                     \u001b[36m0.7513\u001b[0m        \u001b[32m0.5164\u001b[0m                     0.7551        0.4977  0.0057  1.2338\n",
      "     13                     \u001b[36m0.7517\u001b[0m        \u001b[32m0.5084\u001b[0m                     \u001b[35m0.7720\u001b[0m        0.4935  0.0050  1.2339\n",
      "     14                     \u001b[36m0.7605\u001b[0m        \u001b[32m0.5028\u001b[0m                     0.7628        0.4934  0.0043  1.2340\n",
      "     15                     \u001b[36m0.7659\u001b[0m        \u001b[32m0.4946\u001b[0m                     \u001b[35m0.7746\u001b[0m        \u001b[31m0.4815\u001b[0m  0.0037  1.2343\n",
      "     16                     0.7626        \u001b[32m0.4909\u001b[0m                     \u001b[35m0.7790\u001b[0m        0.4819  0.0031  1.2344\n",
      "     17                     \u001b[36m0.7683\u001b[0m        0.4919                     0.7774        \u001b[31m0.4792\u001b[0m  0.0025  1.2357\n",
      "     18                     \u001b[36m0.7716\u001b[0m        \u001b[32m0.4822\u001b[0m                     0.7781        \u001b[31m0.4750\u001b[0m  0.0020  1.2343\n",
      "     19                     \u001b[36m0.7772\u001b[0m        \u001b[32m0.4786\u001b[0m                     0.7775        \u001b[31m0.4739\u001b[0m  0.0015  1.2340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                     0.7745        \u001b[32m0.4753\u001b[0m                     \u001b[35m0.7829\u001b[0m        \u001b[31m0.4702\u001b[0m  0.0010  1.2348\n",
      "     21                     \u001b[36m0.7795\u001b[0m        \u001b[32m0.4684\u001b[0m                     \u001b[35m0.7850\u001b[0m        \u001b[31m0.4671\u001b[0m  0.0007  1.2341\n",
      "     22                     \u001b[36m0.7807\u001b[0m        \u001b[32m0.4664\u001b[0m                     \u001b[35m0.7889\u001b[0m        \u001b[31m0.4668\u001b[0m  0.0004  1.2349\n",
      "     23                     0.7783        \u001b[32m0.4631\u001b[0m                     0.7874        0.4669  0.0002  1.2342\n",
      "     24                     0.7807        \u001b[32m0.4596\u001b[0m                     0.7889        \u001b[31m0.4665\u001b[0m  0.0000  1.2339\n",
      "     25                     0.7783        0.4673                     \u001b[35m0.7898\u001b[0m        \u001b[31m0.4663\u001b[0m  0.0000  1.2352\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5523\u001b[0m        \u001b[32m0.8668\u001b[0m                     \u001b[35m0.6263\u001b[0m        \u001b[31m0.6475\u001b[0m  0.0100  1.2290\n",
      "      2                     \u001b[36m0.6461\u001b[0m        \u001b[32m0.6756\u001b[0m                     \u001b[35m0.6923\u001b[0m        \u001b[31m0.5868\u001b[0m  0.0100  1.2428\n",
      "      3                     \u001b[36m0.6845\u001b[0m        \u001b[32m0.6245\u001b[0m                     \u001b[35m0.7271\u001b[0m        \u001b[31m0.5462\u001b[0m  0.0098  1.2405\n",
      "      4                     \u001b[36m0.7025\u001b[0m        \u001b[32m0.5986\u001b[0m                     \u001b[35m0.7331\u001b[0m        \u001b[31m0.5377\u001b[0m  0.0096  1.2328\n",
      "      5                     \u001b[36m0.7090\u001b[0m        \u001b[32m0.5800\u001b[0m                     0.7235        0.5716  0.0093  1.2337\n",
      "      6                     \u001b[36m0.7194\u001b[0m        \u001b[32m0.5643\u001b[0m                     \u001b[35m0.7487\u001b[0m        \u001b[31m0.5225\u001b[0m  0.0090  1.2338\n",
      "      7                     \u001b[36m0.7351\u001b[0m        \u001b[32m0.5421\u001b[0m                     \u001b[35m0.7499\u001b[0m        0.5278  0.0085  1.2330\n",
      "      8                     \u001b[36m0.7378\u001b[0m        \u001b[32m0.5315\u001b[0m                     \u001b[35m0.7530\u001b[0m        \u001b[31m0.5148\u001b[0m  0.0080  1.2332\n",
      "      9                     \u001b[36m0.7444\u001b[0m        \u001b[32m0.5269\u001b[0m                     \u001b[35m0.7538\u001b[0m        \u001b[31m0.5146\u001b[0m  0.0075  1.2348\n",
      "     10                     \u001b[36m0.7459\u001b[0m        \u001b[32m0.5235\u001b[0m                     \u001b[35m0.7580\u001b[0m        \u001b[31m0.5124\u001b[0m  0.0069  1.2342\n",
      "     11                     \u001b[36m0.7523\u001b[0m        \u001b[32m0.5160\u001b[0m                     0.7559        \u001b[31m0.5083\u001b[0m  0.0063  1.2339\n",
      "     12                     0.7514        \u001b[32m0.5133\u001b[0m                     \u001b[35m0.7626\u001b[0m        \u001b[31m0.5016\u001b[0m  0.0057  1.2342\n",
      "     13                     \u001b[36m0.7568\u001b[0m        \u001b[32m0.5067\u001b[0m                     0.7571        0.5123  0.0050  1.2347\n",
      "     14                     \u001b[36m0.7590\u001b[0m        \u001b[32m0.5024\u001b[0m                     \u001b[35m0.7677\u001b[0m        \u001b[31m0.4883\u001b[0m  0.0043  1.2347\n",
      "     15                     \u001b[36m0.7633\u001b[0m        \u001b[32m0.4979\u001b[0m                     \u001b[35m0.7722\u001b[0m        0.4908  0.0037  1.2367\n",
      "     16                     \u001b[36m0.7662\u001b[0m        \u001b[32m0.4859\u001b[0m                     \u001b[35m0.7734\u001b[0m        \u001b[31m0.4864\u001b[0m  0.0031  1.2357\n",
      "     17                     0.7661        0.4909                     \u001b[35m0.7759\u001b[0m        \u001b[31m0.4792\u001b[0m  0.0025  1.2340\n",
      "     18                     \u001b[36m0.7736\u001b[0m        \u001b[32m0.4812\u001b[0m                     \u001b[35m0.7775\u001b[0m        \u001b[31m0.4786\u001b[0m  0.0020  1.2347\n",
      "     19                     0.7727        \u001b[32m0.4795\u001b[0m                     0.7759        \u001b[31m0.4779\u001b[0m  0.0015  1.2349\n",
      "     20                     \u001b[36m0.7781\u001b[0m        \u001b[32m0.4738\u001b[0m                     \u001b[35m0.7776\u001b[0m        \u001b[31m0.4771\u001b[0m  0.0010  1.2348\n",
      "     21                     \u001b[36m0.7788\u001b[0m        \u001b[32m0.4670\u001b[0m                     \u001b[35m0.7801\u001b[0m        \u001b[31m0.4728\u001b[0m  0.0007  1.2344\n",
      "     22                     \u001b[36m0.7813\u001b[0m        \u001b[32m0.4660\u001b[0m                     0.7759        0.4744  0.0004  1.2347\n",
      "     23                     0.7799        \u001b[32m0.4639\u001b[0m                     0.7785        \u001b[31m0.4726\u001b[0m  0.0002  1.2345\n",
      "     24                     0.7808        0.4653                     0.7788        0.4728  0.0000  1.2348\n",
      "     25                     \u001b[36m0.7815\u001b[0m        \u001b[32m0.4606\u001b[0m                     0.7785        \u001b[31m0.4726\u001b[0m  0.0000  1.2344\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5548\u001b[0m        \u001b[32m0.8759\u001b[0m                     \u001b[35m0.6505\u001b[0m        \u001b[31m0.6366\u001b[0m  0.0100  1.2272\n",
      "      2                     \u001b[36m0.6291\u001b[0m        \u001b[32m0.7103\u001b[0m                     \u001b[35m0.6593\u001b[0m        0.6714  0.0100  1.2329\n",
      "      3                     \u001b[36m0.6677\u001b[0m        \u001b[32m0.6514\u001b[0m                     \u001b[35m0.7152\u001b[0m        \u001b[31m0.5635\u001b[0m  0.0098  1.2324\n",
      "      4                     \u001b[36m0.6774\u001b[0m        \u001b[32m0.6336\u001b[0m                     \u001b[35m0.7370\u001b[0m        \u001b[31m0.5313\u001b[0m  0.0096  1.2327\n",
      "      5                     \u001b[36m0.6955\u001b[0m        \u001b[32m0.6053\u001b[0m                     \u001b[35m0.7498\u001b[0m        \u001b[31m0.5194\u001b[0m  0.0093  1.2327\n",
      "      6                     \u001b[36m0.7115\u001b[0m        \u001b[32m0.5730\u001b[0m                     \u001b[35m0.7532\u001b[0m        \u001b[31m0.5078\u001b[0m  0.0090  1.2326\n",
      "      7                     \u001b[36m0.7196\u001b[0m        \u001b[32m0.5606\u001b[0m                     0.7429        0.5159  0.0085  1.2327\n",
      "      8                     \u001b[36m0.7283\u001b[0m        \u001b[32m0.5452\u001b[0m                     \u001b[35m0.7643\u001b[0m        \u001b[31m0.5040\u001b[0m  0.0080  1.2322\n",
      "      9                     \u001b[36m0.7327\u001b[0m        0.5461                     0.7624        \u001b[31m0.4947\u001b[0m  0.0075  1.2337\n",
      "     10                     \u001b[36m0.7402\u001b[0m        \u001b[32m0.5280\u001b[0m                     0.7622        0.4985  0.0069  1.2327\n",
      "     11                     \u001b[36m0.7410\u001b[0m        0.5282                     \u001b[35m0.7732\u001b[0m        \u001b[31m0.4900\u001b[0m  0.0063  1.2333\n",
      "     12                     \u001b[36m0.7487\u001b[0m        \u001b[32m0.5199\u001b[0m                     0.7684        \u001b[31m0.4856\u001b[0m  0.0057  1.2327\n",
      "     13                     0.7480        \u001b[32m0.5173\u001b[0m                     \u001b[35m0.7732\u001b[0m        \u001b[31m0.4850\u001b[0m  0.0050  1.2336\n",
      "     14                     \u001b[36m0.7517\u001b[0m        \u001b[32m0.5100\u001b[0m                     \u001b[35m0.7739\u001b[0m        \u001b[31m0.4808\u001b[0m  0.0043  1.2363\n",
      "     15                     \u001b[36m0.7612\u001b[0m        \u001b[32m0.4987\u001b[0m                     \u001b[35m0.7757\u001b[0m        \u001b[31m0.4711\u001b[0m  0.0037  1.2342\n",
      "     16                     0.7580        0.4993                     \u001b[35m0.7770\u001b[0m        0.4767  0.0031  1.2340\n",
      "     17                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4903\u001b[0m                     \u001b[35m0.7873\u001b[0m        \u001b[31m0.4669\u001b[0m  0.0025  1.2358\n",
      "     18                     \u001b[36m0.7696\u001b[0m        \u001b[32m0.4822\u001b[0m                     0.7826        0.4680  0.0020  1.2342\n",
      "     19                     \u001b[36m0.7726\u001b[0m        0.4831                     \u001b[35m0.7897\u001b[0m        \u001b[31m0.4633\u001b[0m  0.0015  1.2379\n",
      "     20                     \u001b[36m0.7751\u001b[0m        \u001b[32m0.4751\u001b[0m                     0.7825        \u001b[31m0.4603\u001b[0m  0.0010  1.2371\n",
      "     21                     0.7737        0.4753                     \u001b[35m0.7901\u001b[0m        \u001b[31m0.4577\u001b[0m  0.0007  1.2662\n",
      "     22                     \u001b[36m0.7839\u001b[0m        \u001b[32m0.4654\u001b[0m                     \u001b[35m0.7910\u001b[0m        \u001b[31m0.4543\u001b[0m  0.0004  1.2347\n",
      "     23                     0.7803        \u001b[32m0.4642\u001b[0m                     \u001b[35m0.7946\u001b[0m        0.4544  0.0002  1.2348\n",
      "     24                     \u001b[36m0.7844\u001b[0m        0.4652                     0.7921        \u001b[31m0.4540\u001b[0m  0.0000  1.2351\n",
      "     25                     0.7772        0.4698                     0.7915        \u001b[31m0.4536\u001b[0m  0.0000  1.2367\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5458\u001b[0m        \u001b[32m0.8493\u001b[0m                     \u001b[35m0.6158\u001b[0m        \u001b[31m0.6656\u001b[0m  0.0100  1.2257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                     \u001b[36m0.6323\u001b[0m        \u001b[32m0.7085\u001b[0m                     \u001b[35m0.7131\u001b[0m        \u001b[31m0.5577\u001b[0m  0.0100  1.2337\n",
      "      3                     \u001b[36m0.6725\u001b[0m        \u001b[32m0.6554\u001b[0m                     \u001b[35m0.7199\u001b[0m        \u001b[31m0.5508\u001b[0m  0.0098  1.2331\n",
      "      4                     \u001b[36m0.6946\u001b[0m        \u001b[32m0.6046\u001b[0m                     \u001b[35m0.7318\u001b[0m        \u001b[31m0.5443\u001b[0m  0.0096  1.2357\n",
      "      5                     \u001b[36m0.7032\u001b[0m        \u001b[32m0.5924\u001b[0m                     0.7240        0.5496  0.0093  1.2350\n",
      "      6                     \u001b[36m0.7224\u001b[0m        \u001b[32m0.5631\u001b[0m                     \u001b[35m0.7359\u001b[0m        \u001b[31m0.5266\u001b[0m  0.0090  1.2339\n",
      "      7                     \u001b[36m0.7257\u001b[0m        \u001b[32m0.5528\u001b[0m                     \u001b[35m0.7502\u001b[0m        \u001b[31m0.5126\u001b[0m  0.0085  1.2345\n",
      "      8                     \u001b[36m0.7298\u001b[0m        \u001b[32m0.5463\u001b[0m                     0.7331        0.5302  0.0080  1.2341\n",
      "      9                     \u001b[36m0.7364\u001b[0m        \u001b[32m0.5358\u001b[0m                     \u001b[35m0.7507\u001b[0m        \u001b[31m0.5118\u001b[0m  0.0075  1.2339\n",
      "     10                     \u001b[36m0.7413\u001b[0m        \u001b[32m0.5284\u001b[0m                     \u001b[35m0.7540\u001b[0m        \u001b[31m0.5074\u001b[0m  0.0069  1.2340\n",
      "     11                     \u001b[36m0.7482\u001b[0m        \u001b[32m0.5206\u001b[0m                     \u001b[35m0.7665\u001b[0m        \u001b[31m0.4919\u001b[0m  0.0063  1.2357\n",
      "     12                     \u001b[36m0.7543\u001b[0m        \u001b[32m0.5160\u001b[0m                     \u001b[35m0.7670\u001b[0m        \u001b[31m0.4916\u001b[0m  0.0057  1.2362\n",
      "     13                     \u001b[36m0.7627\u001b[0m        \u001b[32m0.5031\u001b[0m                     \u001b[35m0.7680\u001b[0m        \u001b[31m0.4874\u001b[0m  0.0050  1.2345\n",
      "     14                     0.7588        0.5044                     \u001b[35m0.7722\u001b[0m        \u001b[31m0.4842\u001b[0m  0.0043  1.2349\n",
      "     15                     0.7617        \u001b[32m0.4974\u001b[0m                     \u001b[35m0.7737\u001b[0m        \u001b[31m0.4818\u001b[0m  0.0037  1.2346\n",
      "     16                     \u001b[36m0.7662\u001b[0m        \u001b[32m0.4949\u001b[0m                     0.7712        \u001b[31m0.4807\u001b[0m  0.0031  1.2347\n",
      "     17                     0.7660        \u001b[32m0.4915\u001b[0m                     \u001b[35m0.7774\u001b[0m        \u001b[31m0.4768\u001b[0m  0.0025  1.2343\n",
      "     18                     0.7646        \u001b[32m0.4869\u001b[0m                     0.7718        0.4770  0.0020  1.2348\n",
      "     19                     \u001b[36m0.7768\u001b[0m        \u001b[32m0.4777\u001b[0m                     0.7770        \u001b[31m0.4715\u001b[0m  0.0015  1.2354\n",
      "     20                     0.7750        0.4789                     \u001b[35m0.7794\u001b[0m        \u001b[31m0.4691\u001b[0m  0.0010  1.2352\n",
      "     21                     \u001b[36m0.7784\u001b[0m        \u001b[32m0.4708\u001b[0m                     0.7784        \u001b[31m0.4670\u001b[0m  0.0007  1.2366\n",
      "     22                     \u001b[36m0.7790\u001b[0m        0.4737                     \u001b[35m0.7830\u001b[0m        0.4686  0.0004  1.2362\n",
      "     23                     0.7752        \u001b[32m0.4699\u001b[0m                     0.7828        \u001b[31m0.4661\u001b[0m  0.0002  1.2367\n",
      "     24                     \u001b[36m0.7794\u001b[0m        \u001b[32m0.4688\u001b[0m                     0.7829        \u001b[31m0.4655\u001b[0m  0.0000  1.2348\n",
      "     25                     \u001b[36m0.7846\u001b[0m        \u001b[32m0.4619\u001b[0m                     \u001b[35m0.7846\u001b[0m        0.4657  0.0000  1.2358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5436\u001b[0m        \u001b[32m1.2061\u001b[0m                     \u001b[35m0.6368\u001b[0m        \u001b[31m0.6535\u001b[0m  0.0100  1.0552\n",
      "      2                     \u001b[36m0.6119\u001b[0m        \u001b[32m0.7817\u001b[0m                     \u001b[35m0.6832\u001b[0m        0.6538  0.0100  0.4513\n",
      "      3                     \u001b[36m0.6625\u001b[0m        \u001b[32m0.6564\u001b[0m                     \u001b[35m0.7055\u001b[0m        \u001b[31m0.5877\u001b[0m  0.0098  0.4498\n",
      "      4                     \u001b[36m0.6848\u001b[0m        \u001b[32m0.6155\u001b[0m                     0.6338        0.6573  0.0096  0.4523\n",
      "      5                     \u001b[36m0.6896\u001b[0m        \u001b[32m0.6131\u001b[0m                     \u001b[35m0.7265\u001b[0m        \u001b[31m0.5480\u001b[0m  0.0093  0.4499\n",
      "      6                     \u001b[36m0.7190\u001b[0m        \u001b[32m0.5774\u001b[0m                     \u001b[35m0.7475\u001b[0m        \u001b[31m0.5310\u001b[0m  0.0090  0.4498\n",
      "      7                     \u001b[36m0.7297\u001b[0m        \u001b[32m0.5476\u001b[0m                     0.7322        \u001b[31m0.5302\u001b[0m  0.0085  0.4496\n",
      "      8                     \u001b[36m0.7405\u001b[0m        \u001b[32m0.5258\u001b[0m                     \u001b[35m0.7638\u001b[0m        \u001b[31m0.5120\u001b[0m  0.0080  0.4498\n",
      "      9                     \u001b[36m0.7416\u001b[0m        0.5270                     \u001b[35m0.7665\u001b[0m        \u001b[31m0.4986\u001b[0m  0.0075  0.4504\n",
      "     10                     \u001b[36m0.7526\u001b[0m        \u001b[32m0.5113\u001b[0m                     \u001b[35m0.7715\u001b[0m        \u001b[31m0.4928\u001b[0m  0.0069  0.4498\n",
      "     11                     \u001b[36m0.7559\u001b[0m        \u001b[32m0.4920\u001b[0m                     0.7544        0.5081  0.0063  0.4498\n",
      "     12                     \u001b[36m0.7792\u001b[0m        \u001b[32m0.4610\u001b[0m                     0.7715        \u001b[31m0.4912\u001b[0m  0.0057  0.4498\n",
      "     13                     \u001b[36m0.7875\u001b[0m        0.4616                     \u001b[35m0.7985\u001b[0m        \u001b[31m0.4483\u001b[0m  0.0050  0.4498\n",
      "     14                     \u001b[36m0.7909\u001b[0m        \u001b[32m0.4530\u001b[0m                     0.7884        0.4581  0.0043  0.4507\n",
      "     15                     \u001b[36m0.7967\u001b[0m        \u001b[32m0.4347\u001b[0m                     0.7959        0.4695  0.0037  0.4503\n",
      "     16                     \u001b[36m0.8017\u001b[0m        \u001b[32m0.4301\u001b[0m                     0.7692        0.5012  0.0031  0.4503\n",
      "     17                     \u001b[36m0.8030\u001b[0m        \u001b[32m0.4230\u001b[0m                     0.7870        0.4647  0.0025  0.4518\n",
      "     18                     \u001b[36m0.8103\u001b[0m        \u001b[32m0.4045\u001b[0m                     \u001b[35m0.8083\u001b[0m        \u001b[31m0.4388\u001b[0m  0.0020  0.4501\n",
      "     19                     \u001b[36m0.8135\u001b[0m        \u001b[32m0.4031\u001b[0m                     0.8081        0.4562  0.0015  0.4508\n",
      "     20                     \u001b[36m0.8192\u001b[0m        \u001b[32m0.3946\u001b[0m                     0.7997        0.4603  0.0010  0.4507\n",
      "     21                     0.8145        0.3968                     \u001b[35m0.8085\u001b[0m        0.4408  0.0007  0.4498\n",
      "     22                     \u001b[36m0.8259\u001b[0m        \u001b[32m0.3816\u001b[0m                     \u001b[35m0.8179\u001b[0m        0.4391  0.0004  0.4508\n",
      "     23                     \u001b[36m0.8337\u001b[0m        \u001b[32m0.3779\u001b[0m                     0.8006        0.4617  0.0002  0.4518\n",
      "     24                     0.8275        0.3831                     0.7955        0.4666  0.0000  0.4500\n",
      "     25                     0.8303        0.3794                     \u001b[35m0.8188\u001b[0m        \u001b[31m0.4367\u001b[0m  0.0000  0.4518\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5483\u001b[0m        \u001b[32m1.2399\u001b[0m                     \u001b[35m0.5872\u001b[0m        \u001b[31m0.8002\u001b[0m  0.0100  0.4498\n",
      "      2                     \u001b[36m0.6039\u001b[0m        \u001b[32m0.7796\u001b[0m                     \u001b[35m0.6331\u001b[0m        \u001b[31m0.6955\u001b[0m  0.0100  0.4518\n",
      "      3                     \u001b[36m0.6680\u001b[0m        \u001b[32m0.6458\u001b[0m                     \u001b[35m0.6596\u001b[0m        \u001b[31m0.6093\u001b[0m  0.0098  0.4514\n",
      "      4                     \u001b[36m0.6851\u001b[0m        \u001b[32m0.6159\u001b[0m                     \u001b[35m0.7310\u001b[0m        \u001b[31m0.5348\u001b[0m  0.0096  0.4518\n",
      "      5                     \u001b[36m0.7006\u001b[0m        \u001b[32m0.5904\u001b[0m                     \u001b[35m0.7402\u001b[0m        \u001b[31m0.5225\u001b[0m  0.0093  0.4498\n",
      "      6                     \u001b[36m0.7133\u001b[0m        \u001b[32m0.5729\u001b[0m                     \u001b[35m0.7459\u001b[0m        \u001b[31m0.5130\u001b[0m  0.0090  0.4494\n",
      "      7                     \u001b[36m0.7238\u001b[0m        \u001b[32m0.5494\u001b[0m                     \u001b[35m0.7487\u001b[0m        \u001b[31m0.5036\u001b[0m  0.0085  0.4504\n",
      "      8                     \u001b[36m0.7429\u001b[0m        \u001b[32m0.5295\u001b[0m                     \u001b[35m0.7554\u001b[0m        \u001b[31m0.4919\u001b[0m  0.0080  0.4498\n",
      "      9                     \u001b[36m0.7476\u001b[0m        \u001b[32m0.5260\u001b[0m                     \u001b[35m0.7636\u001b[0m        \u001b[31m0.4662\u001b[0m  0.0075  0.4498\n",
      "     10                     \u001b[36m0.7539\u001b[0m        \u001b[32m0.5055\u001b[0m                     \u001b[35m0.7846\u001b[0m        \u001b[31m0.4531\u001b[0m  0.0069  0.4498\n",
      "     11                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4830\u001b[0m                     0.7733        0.4616  0.0063  0.4499\n",
      "     12                     \u001b[36m0.7768\u001b[0m        \u001b[32m0.4735\u001b[0m                     0.7603        0.4745  0.0057  0.4518\n",
      "     13                     \u001b[36m0.7782\u001b[0m        \u001b[32m0.4608\u001b[0m                     0.7686        0.4676  0.0050  0.4508\n",
      "     14                     \u001b[36m0.7849\u001b[0m        \u001b[32m0.4544\u001b[0m                     0.7674        0.4790  0.0043  0.4498\n",
      "     15                     \u001b[36m0.7932\u001b[0m        \u001b[32m0.4421\u001b[0m                     0.7718        0.4571  0.0037  0.4538\n",
      "     16                     0.7924        \u001b[32m0.4387\u001b[0m                     0.7755        \u001b[31m0.4528\u001b[0m  0.0031  0.4512\n",
      "     17                     \u001b[36m0.8012\u001b[0m        \u001b[32m0.4271\u001b[0m                     0.7843        \u001b[31m0.4461\u001b[0m  0.0025  0.4508\n",
      "     18                     \u001b[36m0.8054\u001b[0m        \u001b[32m0.4168\u001b[0m                     0.7708        0.4651  0.0020  0.4508\n",
      "     19                     \u001b[36m0.8086\u001b[0m        \u001b[32m0.4117\u001b[0m                     \u001b[35m0.7902\u001b[0m        \u001b[31m0.4338\u001b[0m  0.0015  0.4507\n",
      "     20                     \u001b[36m0.8230\u001b[0m        \u001b[32m0.3959\u001b[0m                     \u001b[35m0.8009\u001b[0m        \u001b[31m0.4290\u001b[0m  0.0010  0.4508\n",
      "     21                     0.8226        \u001b[32m0.3900\u001b[0m                     0.7875        0.4436  0.0007  0.4518\n",
      "     22                     \u001b[36m0.8289\u001b[0m        \u001b[32m0.3864\u001b[0m                     0.7931        0.4300  0.0004  0.4509\n",
      "     23                     0.8267        \u001b[32m0.3832\u001b[0m                     0.7945        \u001b[31m0.4280\u001b[0m  0.0002  0.4498\n",
      "     24                     \u001b[36m0.8329\u001b[0m        \u001b[32m0.3791\u001b[0m                     0.7903        0.4522  0.0000  0.4508\n",
      "     25                     \u001b[36m0.8344\u001b[0m        \u001b[32m0.3691\u001b[0m                     0.7971        \u001b[31m0.4280\u001b[0m  0.0000  0.4508\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5633\u001b[0m        \u001b[32m1.1363\u001b[0m                     \u001b[35m0.6225\u001b[0m        \u001b[31m0.7507\u001b[0m  0.0100  0.4478\n",
      "      2                     \u001b[36m0.6213\u001b[0m        \u001b[32m0.7142\u001b[0m                     0.6146        \u001b[31m0.7158\u001b[0m  0.0100  0.4498\n",
      "      3                     \u001b[36m0.6750\u001b[0m        \u001b[32m0.6355\u001b[0m                     \u001b[35m0.6887\u001b[0m        \u001b[31m0.6038\u001b[0m  0.0098  0.4508\n",
      "      4                     \u001b[36m0.6943\u001b[0m        \u001b[32m0.6058\u001b[0m                     0.6753        0.6753  0.0096  0.4498\n",
      "      5                     \u001b[36m0.7084\u001b[0m        \u001b[32m0.5915\u001b[0m                     \u001b[35m0.7318\u001b[0m        \u001b[31m0.5514\u001b[0m  0.0093  0.4498\n",
      "      6                     \u001b[36m0.7139\u001b[0m        \u001b[32m0.5687\u001b[0m                     0.7149        0.5593  0.0090  0.4503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7                     \u001b[36m0.7329\u001b[0m        \u001b[32m0.5419\u001b[0m                     \u001b[35m0.7486\u001b[0m        \u001b[31m0.5201\u001b[0m  0.0085  0.4501\n",
      "      8                     \u001b[36m0.7514\u001b[0m        \u001b[32m0.5092\u001b[0m                     \u001b[35m0.7719\u001b[0m        \u001b[31m0.4988\u001b[0m  0.0080  0.4505\n",
      "      9                     \u001b[36m0.7662\u001b[0m        \u001b[32m0.4872\u001b[0m                     0.7654        0.5114  0.0075  0.4503\n",
      "     10                     \u001b[36m0.7712\u001b[0m        \u001b[32m0.4854\u001b[0m                     0.7576        \u001b[31m0.4935\u001b[0m  0.0069  0.4504\n",
      "     11                     \u001b[36m0.7725\u001b[0m        \u001b[32m0.4749\u001b[0m                     0.7662        0.5023  0.0063  0.4503\n",
      "     12                     \u001b[36m0.7815\u001b[0m        \u001b[32m0.4671\u001b[0m                     0.7696        0.4938  0.0057  0.4516\n",
      "     13                     \u001b[36m0.7920\u001b[0m        \u001b[32m0.4447\u001b[0m                     0.7705        \u001b[31m0.4925\u001b[0m  0.0050  0.4498\n",
      "     14                     0.7915        \u001b[32m0.4393\u001b[0m                     0.7480        0.5005  0.0043  0.4499\n",
      "     15                     \u001b[36m0.8008\u001b[0m        \u001b[32m0.4378\u001b[0m                     \u001b[35m0.7894\u001b[0m        \u001b[31m0.4679\u001b[0m  0.0037  0.4519\n",
      "     16                     \u001b[36m0.8032\u001b[0m        \u001b[32m0.4234\u001b[0m                     0.7869        0.4715  0.0031  0.4505\n",
      "     17                     \u001b[36m0.8139\u001b[0m        \u001b[32m0.4060\u001b[0m                     0.7721        0.4835  0.0025  0.4502\n",
      "     18                     \u001b[36m0.8211\u001b[0m        \u001b[32m0.3926\u001b[0m                     0.7838        \u001b[31m0.4652\u001b[0m  0.0020  0.4498\n",
      "     19                     \u001b[36m0.8231\u001b[0m        \u001b[32m0.3871\u001b[0m                     \u001b[35m0.7941\u001b[0m        0.4735  0.0015  0.4508\n",
      "     20                     \u001b[36m0.8238\u001b[0m        0.3873                     0.7749        0.4918  0.0010  0.4517\n",
      "     21                     \u001b[36m0.8263\u001b[0m        \u001b[32m0.3796\u001b[0m                     0.7774        0.4775  0.0007  0.4508\n",
      "     22                     \u001b[36m0.8373\u001b[0m        \u001b[32m0.3704\u001b[0m                     0.7895        \u001b[31m0.4623\u001b[0m  0.0004  0.4498\n",
      "     23                     0.8334        0.3713                     0.7749        0.4856  0.0002  0.4508\n",
      "     24                     0.8349        0.3741                     0.7856        0.4692  0.0000  0.4528\n",
      "     25                     0.8352        \u001b[32m0.3703\u001b[0m                     0.7870        \u001b[31m0.4612\u001b[0m  0.0000  0.4508\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5707\u001b[0m        \u001b[32m1.1354\u001b[0m                     \u001b[35m0.5949\u001b[0m        \u001b[31m0.8730\u001b[0m  0.0100  0.4498\n",
      "      2                     \u001b[36m0.6231\u001b[0m        \u001b[32m0.7803\u001b[0m                     0.5756        0.8775  0.0100  0.4498\n",
      "      3                     \u001b[36m0.6589\u001b[0m        \u001b[32m0.6583\u001b[0m                     \u001b[35m0.7122\u001b[0m        \u001b[31m0.5731\u001b[0m  0.0098  0.4498\n",
      "      4                     \u001b[36m0.6741\u001b[0m        \u001b[32m0.6467\u001b[0m                     0.6619        0.6378  0.0096  0.4488\n",
      "      5                     \u001b[36m0.6928\u001b[0m        \u001b[32m0.5936\u001b[0m                     0.6980        0.5793  0.0093  0.4496\n",
      "      6                     \u001b[36m0.7156\u001b[0m        \u001b[32m0.5629\u001b[0m                     \u001b[35m0.7442\u001b[0m        \u001b[31m0.5316\u001b[0m  0.0090  0.4503\n",
      "      7                     \u001b[36m0.7238\u001b[0m        \u001b[32m0.5518\u001b[0m                     \u001b[35m0.7487\u001b[0m        0.5348  0.0085  0.4518\n",
      "      8                     \u001b[36m0.7467\u001b[0m        \u001b[32m0.5189\u001b[0m                     0.6703        0.6937  0.0080  0.4498\n",
      "      9                     \u001b[36m0.7500\u001b[0m        0.5211                     \u001b[35m0.7525\u001b[0m        \u001b[31m0.5057\u001b[0m  0.0075  0.4499\n",
      "     10                     \u001b[36m0.7649\u001b[0m        \u001b[32m0.4911\u001b[0m                     \u001b[35m0.7672\u001b[0m        \u001b[31m0.4737\u001b[0m  0.0069  0.4488\n",
      "     11                     \u001b[36m0.7713\u001b[0m        \u001b[32m0.4691\u001b[0m                     0.7664        0.4845  0.0063  0.4506\n",
      "     12                     \u001b[36m0.7762\u001b[0m        0.4703                     \u001b[35m0.7788\u001b[0m        0.4758  0.0057  0.4516\n",
      "     13                     \u001b[36m0.7886\u001b[0m        \u001b[32m0.4542\u001b[0m                     \u001b[35m0.7799\u001b[0m        \u001b[31m0.4677\u001b[0m  0.0050  0.4508\n",
      "     14                     \u001b[36m0.7941\u001b[0m        0.4561                     \u001b[35m0.7898\u001b[0m        \u001b[31m0.4621\u001b[0m  0.0043  0.4498\n",
      "     15                     0.7932        \u001b[32m0.4422\u001b[0m                     \u001b[35m0.8063\u001b[0m        \u001b[31m0.4474\u001b[0m  0.0037  0.4509\n",
      "     16                     \u001b[36m0.8044\u001b[0m        \u001b[32m0.4255\u001b[0m                     0.7910        \u001b[31m0.4426\u001b[0m  0.0031  0.4508\n",
      "     17                     \u001b[36m0.8070\u001b[0m        \u001b[32m0.4205\u001b[0m                     0.7884        0.4577  0.0025  0.4498\n",
      "     18                     \u001b[36m0.8072\u001b[0m        0.4212                     \u001b[35m0.8065\u001b[0m        \u001b[31m0.4301\u001b[0m  0.0020  0.4498\n",
      "     19                     \u001b[36m0.8106\u001b[0m        \u001b[32m0.4129\u001b[0m                     \u001b[35m0.8068\u001b[0m        0.4353  0.0015  0.4499\n",
      "     20                     \u001b[36m0.8204\u001b[0m        \u001b[32m0.3916\u001b[0m                     0.7983        0.4338  0.0010  0.4511\n",
      "     21                     \u001b[36m0.8250\u001b[0m        \u001b[32m0.3897\u001b[0m                     \u001b[35m0.8109\u001b[0m        0.4311  0.0007  0.4508\n",
      "     22                     \u001b[36m0.8282\u001b[0m        \u001b[32m0.3805\u001b[0m                     0.8093        0.4310  0.0004  0.4508\n",
      "     23                     \u001b[36m0.8316\u001b[0m        \u001b[32m0.3803\u001b[0m                     0.8050        0.4318  0.0002  0.4498\n",
      "     24                     \u001b[36m0.8332\u001b[0m        \u001b[32m0.3777\u001b[0m                     0.8042        0.4326  0.0000  0.4508\n",
      "     25                     0.8261        0.3838                     0.8060        0.4319  0.0000  0.4508\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5386\u001b[0m        \u001b[32m1.1177\u001b[0m                     \u001b[35m0.5855\u001b[0m        \u001b[31m0.6980\u001b[0m  0.0100  0.4478\n",
      "      2                     \u001b[36m0.6188\u001b[0m        \u001b[32m0.7305\u001b[0m                     \u001b[35m0.6294\u001b[0m        \u001b[31m0.6689\u001b[0m  0.0100  0.4499\n",
      "      3                     \u001b[36m0.6545\u001b[0m        \u001b[32m0.6569\u001b[0m                     \u001b[35m0.6668\u001b[0m        \u001b[31m0.6071\u001b[0m  0.0098  0.4518\n",
      "      4                     \u001b[36m0.6786\u001b[0m        \u001b[32m0.6201\u001b[0m                     \u001b[35m0.6795\u001b[0m        0.6075  0.0096  0.4498\n",
      "      5                     \u001b[36m0.7013\u001b[0m        \u001b[32m0.5871\u001b[0m                     \u001b[35m0.7447\u001b[0m        \u001b[31m0.5293\u001b[0m  0.0093  0.4495\n",
      "      6                     \u001b[36m0.7209\u001b[0m        \u001b[32m0.5563\u001b[0m                     \u001b[35m0.7625\u001b[0m        \u001b[31m0.5066\u001b[0m  0.0090  0.4492\n",
      "      7                     \u001b[36m0.7388\u001b[0m        \u001b[32m0.5382\u001b[0m                     0.7148        0.5525  0.0085  0.4508\n",
      "      8                     \u001b[36m0.7407\u001b[0m        \u001b[32m0.5257\u001b[0m                     0.7573        \u001b[31m0.4879\u001b[0m  0.0080  0.4498\n",
      "      9                     \u001b[36m0.7502\u001b[0m        \u001b[32m0.5124\u001b[0m                     \u001b[35m0.7865\u001b[0m        \u001b[31m0.4643\u001b[0m  0.0075  0.4499\n",
      "     10                     \u001b[36m0.7718\u001b[0m        \u001b[32m0.4866\u001b[0m                     0.7767        0.4754  0.0069  0.4499\n",
      "     11                     \u001b[36m0.7738\u001b[0m        \u001b[32m0.4774\u001b[0m                     0.7836        \u001b[31m0.4557\u001b[0m  0.0063  0.4498\n",
      "     12                     0.7713        \u001b[32m0.4714\u001b[0m                     \u001b[35m0.7928\u001b[0m        \u001b[31m0.4487\u001b[0m  0.0057  0.4518\n",
      "     13                     \u001b[36m0.7824\u001b[0m        \u001b[32m0.4568\u001b[0m                     0.7910        \u001b[31m0.4398\u001b[0m  0.0050  0.4498\n",
      "     14                     0.7792        0.4578                     \u001b[35m0.8006\u001b[0m        0.4463  0.0043  0.4498\n",
      "     15                     \u001b[36m0.8019\u001b[0m        \u001b[32m0.4334\u001b[0m                     0.7929        0.4422  0.0037  0.4508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16                     0.7936        0.4389                     \u001b[35m0.8059\u001b[0m        \u001b[31m0.4301\u001b[0m  0.0031  0.4502\n",
      "     17                     \u001b[36m0.8115\u001b[0m        \u001b[32m0.4175\u001b[0m                     0.7920        0.4474  0.0025  0.4499\n",
      "     18                     \u001b[36m0.8191\u001b[0m        \u001b[32m0.4080\u001b[0m                     0.7521        0.4857  0.0020  0.4508\n",
      "     19                     0.8132        \u001b[32m0.4044\u001b[0m                     0.7903        0.4447  0.0015  0.4509\n",
      "     20                     \u001b[36m0.8234\u001b[0m        \u001b[32m0.3903\u001b[0m                     0.8038        0.4328  0.0010  0.4519\n",
      "     21                     \u001b[36m0.8280\u001b[0m        \u001b[32m0.3831\u001b[0m                     0.7871        0.4540  0.0007  0.4505\n",
      "     22                     0.8277        \u001b[32m0.3819\u001b[0m                     0.7928        0.4404  0.0004  0.4506\n",
      "     23                     \u001b[36m0.8315\u001b[0m        \u001b[32m0.3781\u001b[0m                     0.7968        \u001b[31m0.4281\u001b[0m  0.0002  0.4508\n",
      "     24                     0.8266        0.3781                     0.7959        \u001b[31m0.4276\u001b[0m  0.0000  0.4508\n",
      "     25                     \u001b[36m0.8327\u001b[0m        0.3787                     0.7929        0.4401  0.0000  0.4508\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5521\u001b[0m        \u001b[32m1.2014\u001b[0m                     \u001b[35m0.6121\u001b[0m        \u001b[31m0.6891\u001b[0m  0.0100  0.4468\n",
      "      2                     \u001b[36m0.5987\u001b[0m        \u001b[32m0.8518\u001b[0m                     0.6062        0.9110  0.0100  0.4492\n",
      "      3                     \u001b[36m0.6372\u001b[0m        \u001b[32m0.7310\u001b[0m                     \u001b[35m0.6946\u001b[0m        \u001b[31m0.5824\u001b[0m  0.0098  0.4509\n",
      "      4                     \u001b[36m0.6742\u001b[0m        \u001b[32m0.6349\u001b[0m                     \u001b[35m0.7050\u001b[0m        \u001b[31m0.5666\u001b[0m  0.0096  0.4513\n",
      "      5                     \u001b[36m0.6946\u001b[0m        \u001b[32m0.5847\u001b[0m                     0.7039        0.5950  0.0093  0.4503\n",
      "      6                     \u001b[36m0.7099\u001b[0m        \u001b[32m0.5824\u001b[0m                     \u001b[35m0.7322\u001b[0m        \u001b[31m0.5350\u001b[0m  0.0090  0.4508\n",
      "      7                     \u001b[36m0.7290\u001b[0m        \u001b[32m0.5412\u001b[0m                     \u001b[35m0.7333\u001b[0m        0.5399  0.0085  0.4508\n",
      "      8                     \u001b[36m0.7362\u001b[0m        \u001b[32m0.5290\u001b[0m                     \u001b[35m0.7620\u001b[0m        \u001b[31m0.5003\u001b[0m  0.0080  0.4501\n",
      "      9                     \u001b[36m0.7494\u001b[0m        \u001b[32m0.5112\u001b[0m                     \u001b[35m0.7732\u001b[0m        \u001b[31m0.4871\u001b[0m  0.0075  0.4504\n",
      "     10                     \u001b[36m0.7603\u001b[0m        \u001b[32m0.4910\u001b[0m                     0.7671        0.4936  0.0069  0.4501\n",
      "     11                     \u001b[36m0.7666\u001b[0m        \u001b[32m0.4751\u001b[0m                     \u001b[35m0.7750\u001b[0m        \u001b[31m0.4771\u001b[0m  0.0063  0.4518\n",
      "     12                     \u001b[36m0.7744\u001b[0m        \u001b[32m0.4700\u001b[0m                     \u001b[35m0.7929\u001b[0m        \u001b[31m0.4627\u001b[0m  0.0057  0.4498\n",
      "     13                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.4545\u001b[0m                     0.7731        \u001b[31m0.4608\u001b[0m  0.0050  0.4501\n",
      "     14                     \u001b[36m0.7887\u001b[0m        0.4547                     0.7897        0.4609  0.0043  0.4498\n",
      "     15                     \u001b[36m0.7943\u001b[0m        \u001b[32m0.4364\u001b[0m                     0.7898        \u001b[31m0.4601\u001b[0m  0.0037  0.4494\n",
      "     16                     \u001b[36m0.8031\u001b[0m        \u001b[32m0.4279\u001b[0m                     0.7737        0.4719  0.0031  0.4507\n",
      "     17                     \u001b[36m0.8055\u001b[0m        \u001b[32m0.4186\u001b[0m                     0.7885        0.4662  0.0025  0.4498\n",
      "     18                     \u001b[36m0.8105\u001b[0m        \u001b[32m0.4100\u001b[0m                     \u001b[35m0.7990\u001b[0m        \u001b[31m0.4444\u001b[0m  0.0020  0.4498\n",
      "     19                     \u001b[36m0.8219\u001b[0m        \u001b[32m0.3991\u001b[0m                     0.7396        0.5260  0.0015  0.4508\n",
      "     20                     0.8177        \u001b[32m0.3929\u001b[0m                     0.7922        0.4529  0.0010  0.4508\n",
      "     21                     \u001b[36m0.8248\u001b[0m        \u001b[32m0.3923\u001b[0m                     0.7659        0.4999  0.0007  0.4498\n",
      "     22                     \u001b[36m0.8321\u001b[0m        \u001b[32m0.3802\u001b[0m                     0.7876        0.4496  0.0004  0.4508\n",
      "     23                     0.8296        \u001b[32m0.3776\u001b[0m                     0.7785        0.4687  0.0002  0.4509\n",
      "     24                     0.8305        \u001b[32m0.3736\u001b[0m                     0.7941        0.4467  0.0000  0.4498\n",
      "     25                     0.8283        0.3739                     0.7901        0.4539  0.0000  0.4514\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5757\u001b[0m        \u001b[32m1.0787\u001b[0m                     \u001b[35m0.6067\u001b[0m        \u001b[31m0.7101\u001b[0m  0.0100  0.4458\n",
      "      2                     \u001b[36m0.6200\u001b[0m        \u001b[32m0.8230\u001b[0m                     \u001b[35m0.6557\u001b[0m        \u001b[31m0.6359\u001b[0m  0.0100  0.4508\n",
      "      3                     \u001b[36m0.6740\u001b[0m        \u001b[32m0.6471\u001b[0m                     \u001b[35m0.7224\u001b[0m        \u001b[31m0.5678\u001b[0m  0.0098  0.4508\n",
      "      4                     \u001b[36m0.6963\u001b[0m        \u001b[32m0.5995\u001b[0m                     0.7203        \u001b[31m0.5514\u001b[0m  0.0096  0.4498\n",
      "      5                     \u001b[36m0.7273\u001b[0m        \u001b[32m0.5485\u001b[0m                     \u001b[35m0.7605\u001b[0m        \u001b[31m0.5031\u001b[0m  0.0093  0.4498\n",
      "      6                     0.7214        0.5540                     0.7463        0.5051  0.0090  0.4498\n",
      "      7                     \u001b[36m0.7377\u001b[0m        \u001b[32m0.5170\u001b[0m                     \u001b[35m0.7691\u001b[0m        \u001b[31m0.4896\u001b[0m  0.0085  0.4528\n",
      "      8                     \u001b[36m0.7654\u001b[0m        \u001b[32m0.5026\u001b[0m                     0.7470        0.5148  0.0080  0.4498\n",
      "      9                     0.7640        \u001b[32m0.4905\u001b[0m                     0.7575        0.5035  0.0075  0.4498\n",
      "     10                     \u001b[36m0.7690\u001b[0m        \u001b[32m0.4892\u001b[0m                     0.7332        0.5655  0.0069  0.4498\n",
      "     11                     \u001b[36m0.7757\u001b[0m        \u001b[32m0.4786\u001b[0m                     0.7658        0.4998  0.0063  0.4498\n",
      "     12                     \u001b[36m0.7860\u001b[0m        \u001b[32m0.4608\u001b[0m                     0.7659        \u001b[31m0.4860\u001b[0m  0.0057  0.4517\n",
      "     13                     0.7848        \u001b[32m0.4500\u001b[0m                     \u001b[35m0.7881\u001b[0m        \u001b[31m0.4685\u001b[0m  0.0050  0.4518\n",
      "     14                     \u001b[36m0.7976\u001b[0m        \u001b[32m0.4317\u001b[0m                     \u001b[35m0.8038\u001b[0m        \u001b[31m0.4515\u001b[0m  0.0043  0.4499\n",
      "     15                     \u001b[36m0.8016\u001b[0m        \u001b[32m0.4259\u001b[0m                     0.7914        0.4610  0.0037  0.4503\n",
      "     16                     \u001b[36m0.8140\u001b[0m        \u001b[32m0.4063\u001b[0m                     0.7885        0.4516  0.0031  0.4508\n",
      "     17                     0.8131        \u001b[32m0.4031\u001b[0m                     0.7706        0.4780  0.0025  0.4498\n",
      "     18                     \u001b[36m0.8179\u001b[0m        \u001b[32m0.3914\u001b[0m                     0.7887        0.4678  0.0020  0.4499\n",
      "     19                     \u001b[36m0.8338\u001b[0m        \u001b[32m0.3822\u001b[0m                     0.7853        0.4757  0.0015  0.4498\n",
      "     20                     0.8317        \u001b[32m0.3737\u001b[0m                     0.7870        0.4581  0.0010  0.4512\n",
      "     21                     \u001b[36m0.8358\u001b[0m        \u001b[32m0.3676\u001b[0m                     0.7874        0.4613  0.0007  0.4501\n",
      "     22                     \u001b[36m0.8371\u001b[0m        \u001b[32m0.3668\u001b[0m                     0.7839        0.4629  0.0004  0.4508\n",
      "     23                     \u001b[36m0.8397\u001b[0m        \u001b[32m0.3658\u001b[0m                     0.7822        0.4634  0.0002  0.4528\n",
      "     24                     0.8313        0.3699                     0.7714        0.4808  0.0000  0.4508\n",
      "     25                     \u001b[36m0.8402\u001b[0m        \u001b[32m0.3610\u001b[0m                     0.7956        0.4567  0.0000  0.4519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5786\u001b[0m        \u001b[32m1.0920\u001b[0m                     \u001b[35m0.6550\u001b[0m        \u001b[31m0.6540\u001b[0m  0.0100  0.4488\n",
      "      2                     \u001b[36m0.6481\u001b[0m        \u001b[32m0.6786\u001b[0m                     0.6271        0.7580  0.0100  0.4508\n",
      "      3                     \u001b[36m0.6683\u001b[0m        \u001b[32m0.6477\u001b[0m                     \u001b[35m0.7215\u001b[0m        \u001b[31m0.5866\u001b[0m  0.0098  0.4501\n",
      "      4                     \u001b[36m0.6940\u001b[0m        \u001b[32m0.6010\u001b[0m                     0.7155        \u001b[31m0.5675\u001b[0m  0.0096  0.4494\n",
      "      5                     \u001b[36m0.7158\u001b[0m        \u001b[32m0.5691\u001b[0m                     0.6968        0.6175  0.0093  0.4527\n",
      "      6                     0.7137        0.5718                     \u001b[35m0.7462\u001b[0m        \u001b[31m0.5293\u001b[0m  0.0090  0.4511\n",
      "      7                     \u001b[36m0.7233\u001b[0m        0.5748                     \u001b[35m0.7587\u001b[0m        \u001b[31m0.5072\u001b[0m  0.0085  0.4506\n",
      "      8                     \u001b[36m0.7384\u001b[0m        \u001b[32m0.5212\u001b[0m                     0.7304        0.5610  0.0080  0.4508\n",
      "      9                     \u001b[36m0.7536\u001b[0m        0.5246                     \u001b[35m0.7647\u001b[0m        0.5162  0.0075  0.4499\n",
      "     10                     \u001b[36m0.7659\u001b[0m        \u001b[32m0.4827\u001b[0m                     0.7504        0.5085  0.0069  0.4506\n",
      "     11                     \u001b[36m0.7746\u001b[0m        \u001b[32m0.4771\u001b[0m                     0.7616        \u001b[31m0.4887\u001b[0m  0.0063  0.4518\n",
      "     12                     \u001b[36m0.7833\u001b[0m        \u001b[32m0.4635\u001b[0m                     \u001b[35m0.7703\u001b[0m        \u001b[31m0.4842\u001b[0m  0.0057  0.4500\n",
      "     13                     0.7797        \u001b[32m0.4514\u001b[0m                     0.7676        0.5062  0.0050  0.4511\n",
      "     14                     \u001b[36m0.7922\u001b[0m        \u001b[32m0.4490\u001b[0m                     \u001b[35m0.7842\u001b[0m        \u001b[31m0.4593\u001b[0m  0.0043  0.4498\n",
      "     15                     0.7917        \u001b[32m0.4374\u001b[0m                     0.7827        \u001b[31m0.4591\u001b[0m  0.0037  0.4508\n",
      "     16                     \u001b[36m0.8000\u001b[0m        \u001b[32m0.4242\u001b[0m                     0.7837        \u001b[31m0.4541\u001b[0m  0.0031  0.4503\n",
      "     17                     \u001b[36m0.8053\u001b[0m        \u001b[32m0.4187\u001b[0m                     \u001b[35m0.7875\u001b[0m        0.4634  0.0025  0.4499\n",
      "     18                     \u001b[36m0.8131\u001b[0m        \u001b[32m0.4058\u001b[0m                     0.7813        0.4624  0.0020  0.4498\n",
      "     19                     \u001b[36m0.8272\u001b[0m        \u001b[32m0.3919\u001b[0m                     \u001b[35m0.7887\u001b[0m        0.4544  0.0015  0.4528\n",
      "     20                     0.8209        0.3949                     0.7885        \u001b[31m0.4478\u001b[0m  0.0010  0.4508\n",
      "     21                     \u001b[36m0.8304\u001b[0m        \u001b[32m0.3747\u001b[0m                     0.7883        \u001b[31m0.4474\u001b[0m  0.0007  0.4508\n",
      "     22                     \u001b[36m0.8333\u001b[0m        \u001b[32m0.3744\u001b[0m                     0.7884        \u001b[31m0.4470\u001b[0m  0.0004  0.4508\n",
      "     23                     0.8254        0.3798                     \u001b[35m0.7888\u001b[0m        0.4476  0.0002  0.4524\n",
      "     24                     0.8317        \u001b[32m0.3742\u001b[0m                     0.7870        \u001b[31m0.4464\u001b[0m  0.0000  0.4528\n",
      "     25                     \u001b[36m0.8388\u001b[0m        \u001b[32m0.3661\u001b[0m                     \u001b[35m0.7893\u001b[0m        \u001b[31m0.4454\u001b[0m  0.0000  0.4508\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5464\u001b[0m        \u001b[32m1.1720\u001b[0m                     \u001b[35m0.6388\u001b[0m        \u001b[31m0.6672\u001b[0m  0.0100  0.4485\n",
      "      2                     \u001b[36m0.6160\u001b[0m        \u001b[32m0.7576\u001b[0m                     0.6261        \u001b[31m0.6666\u001b[0m  0.0100  0.4498\n",
      "      3                     \u001b[36m0.6444\u001b[0m        \u001b[32m0.7004\u001b[0m                     \u001b[35m0.6967\u001b[0m        \u001b[31m0.6162\u001b[0m  0.0098  0.4494\n",
      "      4                     \u001b[36m0.6808\u001b[0m        \u001b[32m0.6218\u001b[0m                     \u001b[35m0.7095\u001b[0m        \u001b[31m0.5789\u001b[0m  0.0096  0.4498\n",
      "      5                     \u001b[36m0.6894\u001b[0m        \u001b[32m0.6216\u001b[0m                     \u001b[35m0.7340\u001b[0m        \u001b[31m0.5764\u001b[0m  0.0093  0.4498\n",
      "      6                     \u001b[36m0.7082\u001b[0m        \u001b[32m0.5714\u001b[0m                     0.7263        \u001b[31m0.5501\u001b[0m  0.0090  0.4496\n",
      "      7                     \u001b[36m0.7375\u001b[0m        \u001b[32m0.5382\u001b[0m                     0.7267        \u001b[31m0.5172\u001b[0m  0.0085  0.4498\n",
      "      8                     \u001b[36m0.7459\u001b[0m        \u001b[32m0.5303\u001b[0m                     0.7339        0.5336  0.0080  0.4508\n",
      "      9                     \u001b[36m0.7525\u001b[0m        \u001b[32m0.5072\u001b[0m                     \u001b[35m0.7485\u001b[0m        \u001b[31m0.5031\u001b[0m  0.0075  0.4518\n",
      "     10                     \u001b[36m0.7656\u001b[0m        \u001b[32m0.4949\u001b[0m                     \u001b[35m0.7497\u001b[0m        0.5293  0.0069  0.4498\n",
      "     11                     \u001b[36m0.7733\u001b[0m        \u001b[32m0.4674\u001b[0m                     \u001b[35m0.7720\u001b[0m        \u001b[31m0.4928\u001b[0m  0.0063  0.4518\n",
      "     12                     \u001b[36m0.7861\u001b[0m        \u001b[32m0.4564\u001b[0m                     0.7449        0.5388  0.0057  0.4499\n",
      "     13                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4543\u001b[0m                     \u001b[35m0.7736\u001b[0m        \u001b[31m0.4705\u001b[0m  0.0050  0.4498\n",
      "     14                     \u001b[36m0.7926\u001b[0m        \u001b[32m0.4407\u001b[0m                     0.7689        0.4754  0.0043  0.4508\n",
      "     15                     \u001b[36m0.7978\u001b[0m        \u001b[32m0.4331\u001b[0m                     \u001b[35m0.7853\u001b[0m        \u001b[31m0.4684\u001b[0m  0.0037  0.4498\n",
      "     16                     \u001b[36m0.8074\u001b[0m        \u001b[32m0.4181\u001b[0m                     0.7786        \u001b[31m0.4557\u001b[0m  0.0031  0.4503\n",
      "     17                     \u001b[36m0.8101\u001b[0m        \u001b[32m0.4103\u001b[0m                     0.7722        0.4706  0.0025  0.4518\n",
      "     18                     \u001b[36m0.8146\u001b[0m        \u001b[32m0.4081\u001b[0m                     0.7825        0.4679  0.0020  0.4501\n",
      "     19                     \u001b[36m0.8206\u001b[0m        \u001b[32m0.3927\u001b[0m                     0.7774        0.4569  0.0015  0.4508\n",
      "     20                     \u001b[36m0.8276\u001b[0m        \u001b[32m0.3824\u001b[0m                     0.7755        \u001b[31m0.4557\u001b[0m  0.0010  0.4528\n",
      "     21                     0.8240        \u001b[32m0.3810\u001b[0m                     0.7786        \u001b[31m0.4518\u001b[0m  0.0007  0.4498\n",
      "     22                     \u001b[36m0.8278\u001b[0m        \u001b[32m0.3794\u001b[0m                     0.7782        \u001b[31m0.4497\u001b[0m  0.0004  0.4509\n",
      "     23                     \u001b[36m0.8354\u001b[0m        \u001b[32m0.3673\u001b[0m                     0.7807        0.4510  0.0002  0.4498\n",
      "     24                     0.8351        \u001b[32m0.3668\u001b[0m                     0.7808        0.4517  0.0000  0.4498\n",
      "     25                     0.8336        0.3697                     0.7790        0.4514  0.0000  0.4499\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5638\u001b[0m        \u001b[32m1.1736\u001b[0m                     \u001b[35m0.6042\u001b[0m        \u001b[31m0.6738\u001b[0m  0.0100  0.4488\n",
      "      2                     \u001b[36m0.6504\u001b[0m        \u001b[32m0.6659\u001b[0m                     \u001b[35m0.6496\u001b[0m        \u001b[31m0.6729\u001b[0m  0.0100  0.4500\n",
      "      3                     \u001b[36m0.6650\u001b[0m        \u001b[32m0.6571\u001b[0m                     0.6380        0.7565  0.0098  0.4502\n",
      "      4                     \u001b[36m0.6879\u001b[0m        \u001b[32m0.6285\u001b[0m                     \u001b[35m0.7028\u001b[0m        \u001b[31m0.5750\u001b[0m  0.0096  0.4498\n",
      "      5                     \u001b[36m0.7229\u001b[0m        \u001b[32m0.5620\u001b[0m                     0.6354        0.7952  0.0093  0.4499\n",
      "      6                     0.7081        0.5957                     0.6952        0.6000  0.0090  0.4528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7                     \u001b[36m0.7312\u001b[0m        \u001b[32m0.5418\u001b[0m                     0.6916        0.5903  0.0085  0.4508\n",
      "      8                     \u001b[36m0.7392\u001b[0m        \u001b[32m0.5400\u001b[0m                     \u001b[35m0.7113\u001b[0m        \u001b[31m0.5577\u001b[0m  0.0080  0.4508\n",
      "      9                     \u001b[36m0.7500\u001b[0m        \u001b[32m0.5185\u001b[0m                     \u001b[35m0.7391\u001b[0m        \u001b[31m0.5306\u001b[0m  0.0075  0.4499\n",
      "     10                     \u001b[36m0.7673\u001b[0m        \u001b[32m0.4942\u001b[0m                     0.7377        \u001b[31m0.5261\u001b[0m  0.0069  0.4516\n",
      "     11                     \u001b[36m0.7797\u001b[0m        \u001b[32m0.4723\u001b[0m                     \u001b[35m0.7545\u001b[0m        \u001b[31m0.4989\u001b[0m  0.0063  0.4501\n",
      "     12                     \u001b[36m0.7803\u001b[0m        0.4728                     0.7479        0.5057  0.0057  0.4508\n",
      "     13                     0.7801        \u001b[32m0.4665\u001b[0m                     \u001b[35m0.7662\u001b[0m        \u001b[31m0.4850\u001b[0m  0.0050  0.4499\n",
      "     14                     \u001b[36m0.7898\u001b[0m        \u001b[32m0.4531\u001b[0m                     0.7656        \u001b[31m0.4745\u001b[0m  0.0043  0.4518\n",
      "     15                     \u001b[36m0.7931\u001b[0m        \u001b[32m0.4444\u001b[0m                     \u001b[35m0.7765\u001b[0m        \u001b[31m0.4681\u001b[0m  0.0037  0.4498\n",
      "     16                     \u001b[36m0.7973\u001b[0m        \u001b[32m0.4347\u001b[0m                     0.7700        0.4821  0.0031  0.4500\n",
      "     17                     \u001b[36m0.8036\u001b[0m        \u001b[32m0.4266\u001b[0m                     0.7732        \u001b[31m0.4661\u001b[0m  0.0025  0.4507\n",
      "     18                     \u001b[36m0.8095\u001b[0m        \u001b[32m0.4163\u001b[0m                     \u001b[35m0.7796\u001b[0m        \u001b[31m0.4594\u001b[0m  0.0020  0.4508\n",
      "     19                     \u001b[36m0.8160\u001b[0m        \u001b[32m0.3989\u001b[0m                     \u001b[35m0.7816\u001b[0m        0.4642  0.0015  0.4509\n",
      "     20                     \u001b[36m0.8209\u001b[0m        \u001b[32m0.3951\u001b[0m                     \u001b[35m0.7862\u001b[0m        0.4636  0.0010  0.4523\n",
      "     21                     \u001b[36m0.8239\u001b[0m        \u001b[32m0.3895\u001b[0m                     0.7812        0.4641  0.0007  0.4508\n",
      "     22                     \u001b[36m0.8265\u001b[0m        \u001b[32m0.3762\u001b[0m                     0.7768        0.4656  0.0004  0.4518\n",
      "     23                     \u001b[36m0.8298\u001b[0m        0.3783                     0.7792        0.4615  0.0002  0.4518\n",
      "     24                     \u001b[36m0.8347\u001b[0m        \u001b[32m0.3720\u001b[0m                     0.7787        0.4679  0.0000  0.4508\n",
      "     25                     0.8317        0.3803                     0.7811        0.4632  0.0000  0.4523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 1 1 ... 1 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6235\u001b[0m        \u001b[32m1.1815\u001b[0m                     \u001b[35m0.5542\u001b[0m        \u001b[31m1.3016\u001b[0m  0.0100  0.4868\n",
      "      2                     \u001b[36m0.6565\u001b[0m        \u001b[32m0.8987\u001b[0m                     \u001b[35m0.5654\u001b[0m        \u001b[31m0.9418\u001b[0m  0.0100  0.2737\n",
      "      3                     \u001b[36m0.7143\u001b[0m        \u001b[32m0.5895\u001b[0m                     \u001b[35m0.7396\u001b[0m        \u001b[31m0.5421\u001b[0m  0.0098  0.2693\n",
      "      4                     0.6920        0.6405                     0.6352        0.7108  0.0096  0.2693\n",
      "      5                     0.6779        0.7105                     0.5116        0.8751  0.0093  0.2693\n",
      "      6                     \u001b[36m0.7266\u001b[0m        \u001b[32m0.5653\u001b[0m                     0.6665        0.7681  0.0090  0.2693\n",
      "      7                     0.6680        0.7790                     0.7187        0.5501  0.0085  0.2689\n",
      "      8                     0.7161        \u001b[32m0.5428\u001b[0m                     0.7204        0.5740  0.0080  0.2693\n",
      "      9                     0.7260        0.5553                     0.7279        \u001b[31m0.5319\u001b[0m  0.0075  0.2693\n",
      "     10                     \u001b[36m0.7322\u001b[0m        \u001b[32m0.5423\u001b[0m                     0.7168        0.5436  0.0069  0.2694\n",
      "     11                     \u001b[36m0.7407\u001b[0m        \u001b[32m0.5247\u001b[0m                     0.6732        0.6070  0.0063  0.2693\n",
      "     12                     0.7267        0.5446                     0.7196        0.5451  0.0057  0.2693\n",
      "     13                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5116\u001b[0m                     0.7248        \u001b[31m0.5309\u001b[0m  0.0050  0.2687\n",
      "     14                     \u001b[36m0.7634\u001b[0m        \u001b[32m0.4956\u001b[0m                     \u001b[35m0.7516\u001b[0m        0.5395  0.0043  0.2693\n",
      "     15                     0.7600        \u001b[32m0.4941\u001b[0m                     0.7372        \u001b[31m0.5249\u001b[0m  0.0037  0.2693\n",
      "     16                     0.7598        \u001b[32m0.4917\u001b[0m                     0.7443        \u001b[31m0.5097\u001b[0m  0.0031  0.2693\n",
      "     17                     0.7609        \u001b[32m0.4867\u001b[0m                     0.7351        0.5164  0.0025  0.2694\n",
      "     18                     \u001b[36m0.7713\u001b[0m        \u001b[32m0.4765\u001b[0m                     \u001b[35m0.7552\u001b[0m        \u001b[31m0.4962\u001b[0m  0.0020  0.2693\n",
      "     19                     0.7701        \u001b[32m0.4734\u001b[0m                     0.7364        0.5089  0.0015  0.2697\n",
      "     20                     0.7668        \u001b[32m0.4684\u001b[0m                     0.7451        0.5032  0.0010  0.2692\n",
      "     21                     \u001b[36m0.7722\u001b[0m        \u001b[32m0.4682\u001b[0m                     0.7440        0.4987  0.0007  0.2703\n",
      "     22                     \u001b[36m0.7734\u001b[0m        \u001b[32m0.4610\u001b[0m                     0.7531        0.4965  0.0004  0.2693\n",
      "     23                     \u001b[36m0.7753\u001b[0m        0.4657                     \u001b[35m0.7568\u001b[0m        0.4971  0.0002  0.2693\n",
      "     24                     0.7714        0.4659                     \u001b[35m0.7603\u001b[0m        0.4965  0.0000  0.2703\n",
      "     25                     \u001b[36m0.7755\u001b[0m        \u001b[32m0.4606\u001b[0m                     0.7521        0.4977  0.0000  0.2703\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6009\u001b[0m        \u001b[32m1.2285\u001b[0m                     \u001b[35m0.6738\u001b[0m        \u001b[31m0.6312\u001b[0m  0.0100  0.2673\n",
      "      2                     \u001b[36m0.6690\u001b[0m        \u001b[32m0.6782\u001b[0m                     0.6043        0.8570  0.0100  0.2700\n",
      "      3                     0.6661        0.7745                     0.6004        1.0518  0.0098  0.2693\n",
      "      4                     \u001b[36m0.6870\u001b[0m        \u001b[32m0.6635\u001b[0m                     \u001b[35m0.6879\u001b[0m        0.6520  0.0096  0.2693\n",
      "      5                     \u001b[36m0.7170\u001b[0m        \u001b[32m0.5890\u001b[0m                     \u001b[35m0.7064\u001b[0m        \u001b[31m0.6194\u001b[0m  0.0093  0.2703\n",
      "      6                     0.6537        0.7205                     0.6610        0.8391  0.0090  0.2703\n",
      "      7                     0.6825        0.6630                     0.6174        0.8580  0.0085  0.2693\n",
      "      8                     \u001b[36m0.7231\u001b[0m        \u001b[32m0.5762\u001b[0m                     0.6702        0.6823  0.0080  0.2693\n",
      "      9                     \u001b[36m0.7239\u001b[0m        \u001b[32m0.5648\u001b[0m                     0.6603        0.6841  0.0075  0.2703\n",
      "     10                     0.6744        0.7307                     0.7025        \u001b[31m0.5732\u001b[0m  0.0069  0.2703\n",
      "     11                     0.7095        \u001b[32m0.5623\u001b[0m                     \u001b[35m0.7219\u001b[0m        \u001b[31m0.5688\u001b[0m  0.0063  0.2693\n",
      "     12                     \u001b[36m0.7294\u001b[0m        \u001b[32m0.5433\u001b[0m                     0.6968        0.5759  0.0057  0.2703\n",
      "     13                     \u001b[36m0.7428\u001b[0m        \u001b[32m0.5205\u001b[0m                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5465\u001b[0m  0.0050  0.2693\n",
      "     14                     \u001b[36m0.7489\u001b[0m        \u001b[32m0.5134\u001b[0m                     \u001b[35m0.7335\u001b[0m        \u001b[31m0.5380\u001b[0m  0.0043  0.2703\n",
      "     15                     0.7378        \u001b[32m0.5120\u001b[0m                     \u001b[35m0.7371\u001b[0m        \u001b[31m0.5368\u001b[0m  0.0037  0.2693\n",
      "     16                     0.7467        \u001b[32m0.4998\u001b[0m                     \u001b[35m0.7396\u001b[0m        \u001b[31m0.5310\u001b[0m  0.0031  0.2693\n",
      "     17                     \u001b[36m0.7532\u001b[0m        \u001b[32m0.4902\u001b[0m                     \u001b[35m0.7401\u001b[0m        \u001b[31m0.5189\u001b[0m  0.0025  0.2693\n",
      "     18                     \u001b[36m0.7541\u001b[0m        0.4954                     0.6944        0.5675  0.0020  0.2703\n",
      "     19                     \u001b[36m0.7547\u001b[0m        0.4953                     0.7380        0.5236  0.0015  0.2693\n",
      "     20                     \u001b[36m0.7592\u001b[0m        \u001b[32m0.4871\u001b[0m                     \u001b[35m0.7457\u001b[0m        0.5308  0.0010  0.2693\n",
      "     21                     \u001b[36m0.7674\u001b[0m        \u001b[32m0.4799\u001b[0m                     0.7407        0.5303  0.0007  0.2694\n",
      "     22                     0.7667        0.4802                     0.7427        0.5240  0.0004  0.2703\n",
      "     23                     0.7620        0.4829                     0.7448        0.5198  0.0002  0.2693\n",
      "     24                     0.7663        \u001b[32m0.4772\u001b[0m                     0.7423        0.5215  0.0000  0.2703\n",
      "     25                     0.7666        \u001b[32m0.4752\u001b[0m                     0.7402        0.5213  0.0000  0.2703\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6211\u001b[0m        \u001b[32m1.3946\u001b[0m                     \u001b[35m0.5754\u001b[0m        \u001b[31m1.5473\u001b[0m  0.0100  0.2663\n",
      "      2                     \u001b[36m0.6583\u001b[0m        \u001b[32m0.8469\u001b[0m                     \u001b[35m0.7099\u001b[0m        \u001b[31m0.8189\u001b[0m  0.0100  0.2703\n",
      "      3                     \u001b[36m0.6915\u001b[0m        \u001b[32m0.6495\u001b[0m                     \u001b[35m0.7137\u001b[0m        \u001b[31m0.6132\u001b[0m  0.0098  0.2697\n",
      "      4                     \u001b[36m0.7287\u001b[0m        \u001b[32m0.5687\u001b[0m                     0.6915        \u001b[31m0.6101\u001b[0m  0.0096  0.2703\n",
      "      5                     0.7014        0.6231                     0.7066        0.6433  0.0093  0.2693\n",
      "      6                     0.7184        \u001b[32m0.5642\u001b[0m                     \u001b[35m0.7237\u001b[0m        0.6499  0.0090  0.2693\n",
      "      7                     \u001b[36m0.7297\u001b[0m        0.5717                     0.7060        \u001b[31m0.5897\u001b[0m  0.0085  0.2686\n",
      "      8                     \u001b[36m0.7434\u001b[0m        \u001b[32m0.5306\u001b[0m                     0.6526        0.6330  0.0080  0.2693\n",
      "      9                     0.7240        0.5773                     \u001b[35m0.7320\u001b[0m        \u001b[31m0.5475\u001b[0m  0.0075  0.2692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     0.7354        0.5331                     \u001b[35m0.7382\u001b[0m        \u001b[31m0.5357\u001b[0m  0.0069  0.2699\n",
      "     11                     0.7369        \u001b[32m0.5240\u001b[0m                     0.6973        0.6414  0.0063  0.2703\n",
      "     12                     \u001b[36m0.7515\u001b[0m        \u001b[32m0.5200\u001b[0m                     0.7225        0.5392  0.0057  0.2723\n",
      "     13                     \u001b[36m0.7571\u001b[0m        \u001b[32m0.4977\u001b[0m                     0.7349        \u001b[31m0.5150\u001b[0m  0.0050  0.2693\n",
      "     14                     \u001b[36m0.7625\u001b[0m        \u001b[32m0.4916\u001b[0m                     0.7099        0.5649  0.0043  0.2697\n",
      "     15                     0.7534        0.5285                     0.7348        0.5489  0.0037  0.2693\n",
      "     16                     \u001b[36m0.7674\u001b[0m        \u001b[32m0.4800\u001b[0m                     \u001b[35m0.7523\u001b[0m        0.5284  0.0031  0.2703\n",
      "     17                     \u001b[36m0.7759\u001b[0m        \u001b[32m0.4713\u001b[0m                     0.7085        0.5454  0.0025  0.2693\n",
      "     18                     0.7603        0.4980                     0.7357        0.5273  0.0020  0.2693\n",
      "     19                     \u001b[36m0.7771\u001b[0m        \u001b[32m0.4659\u001b[0m                     0.7487        0.5180  0.0015  0.2693\n",
      "     20                     \u001b[36m0.7845\u001b[0m        \u001b[32m0.4635\u001b[0m                     \u001b[35m0.7565\u001b[0m        \u001b[31m0.5090\u001b[0m  0.0010  0.2693\n",
      "     21                     \u001b[36m0.7896\u001b[0m        \u001b[32m0.4540\u001b[0m                     0.7500        \u001b[31m0.5078\u001b[0m  0.0007  0.2693\n",
      "     22                     0.7862        \u001b[32m0.4503\u001b[0m                     0.7474        0.5105  0.0004  0.2698\n",
      "     23                     \u001b[36m0.7915\u001b[0m        0.4506                     0.7503        0.5119  0.0002  0.2703\n",
      "     24                     \u001b[36m0.7959\u001b[0m        \u001b[32m0.4438\u001b[0m                     0.7495        \u001b[31m0.5070\u001b[0m  0.0000  0.2698\n",
      "     25                     0.7926        \u001b[32m0.4418\u001b[0m                     0.7522        0.5112  0.0000  0.2703\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6007\u001b[0m        \u001b[32m1.3150\u001b[0m                     \u001b[35m0.6286\u001b[0m        \u001b[31m0.9110\u001b[0m  0.0100  0.2663\n",
      "      2                     \u001b[36m0.6676\u001b[0m        \u001b[32m0.7593\u001b[0m                     0.6283        \u001b[31m0.7863\u001b[0m  0.0100  0.2703\n",
      "      3                     \u001b[36m0.6831\u001b[0m        \u001b[32m0.7260\u001b[0m                     0.5367        0.8620  0.0098  0.2693\n",
      "      4                     0.6407        0.8561                     \u001b[35m0.6978\u001b[0m        \u001b[31m0.6010\u001b[0m  0.0096  0.2693\n",
      "      5                     0.6552        0.7276                     \u001b[35m0.7452\u001b[0m        \u001b[31m0.5248\u001b[0m  0.0093  0.2693\n",
      "      6                     \u001b[36m0.7206\u001b[0m        \u001b[32m0.5847\u001b[0m                     0.6581        0.6253  0.0090  0.2693\n",
      "      7                     0.7029        0.6183                     0.6837        0.5742  0.0085  0.2693\n",
      "      8                     0.7009        0.5876                     \u001b[35m0.7464\u001b[0m        \u001b[31m0.5100\u001b[0m  0.0080  0.2693\n",
      "      9                     0.7084        \u001b[32m0.5620\u001b[0m                     \u001b[35m0.7735\u001b[0m        \u001b[31m0.4869\u001b[0m  0.0075  0.2693\n",
      "     10                     \u001b[36m0.7320\u001b[0m        \u001b[32m0.5253\u001b[0m                     0.7242        0.5211  0.0069  0.2693\n",
      "     11                     \u001b[36m0.7449\u001b[0m        \u001b[32m0.5070\u001b[0m                     0.7324        0.5184  0.0063  0.2693\n",
      "     12                     \u001b[36m0.7460\u001b[0m        0.5130                     0.7487        0.5221  0.0057  0.2693\n",
      "     13                     0.7317        0.5475                     0.7342        0.5418  0.0050  0.2703\n",
      "     14                     \u001b[36m0.7520\u001b[0m        0.5091                     \u001b[35m0.7854\u001b[0m        0.4894  0.0043  0.2703\n",
      "     15                     \u001b[36m0.7626\u001b[0m        \u001b[32m0.4909\u001b[0m                     0.7703        \u001b[31m0.4842\u001b[0m  0.0037  0.2693\n",
      "     16                     \u001b[36m0.7651\u001b[0m        0.4942                     0.7675        0.4903  0.0031  0.2693\n",
      "     17                     0.7550        0.5036                     0.7610        0.4910  0.0025  0.2694\n",
      "     18                     0.7528        0.5012                     0.7657        0.4846  0.0020  0.2703\n",
      "     19                     0.7643        \u001b[32m0.4897\u001b[0m                     0.7591        0.4875  0.0015  0.2693\n",
      "     20                     0.7594        \u001b[32m0.4859\u001b[0m                     0.7785        \u001b[31m0.4788\u001b[0m  0.0010  0.2703\n",
      "     21                     0.7608        \u001b[32m0.4775\u001b[0m                     0.7776        \u001b[31m0.4751\u001b[0m  0.0007  0.2703\n",
      "     22                     0.7519        0.4833                     0.7676        0.4814  0.0004  0.2703\n",
      "     23                     0.7644        \u001b[32m0.4772\u001b[0m                     0.7679        0.4807  0.0002  0.2703\n",
      "     24                     \u001b[36m0.7651\u001b[0m        0.4790                     0.7679        0.4794  0.0000  0.2703\n",
      "     25                     \u001b[36m0.7690\u001b[0m        \u001b[32m0.4706\u001b[0m                     0.7704        0.4836  0.0000  0.2703\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6165\u001b[0m        \u001b[32m1.3406\u001b[0m                     \u001b[35m0.5907\u001b[0m        \u001b[31m1.2060\u001b[0m  0.0100  0.2663\n",
      "      2                     \u001b[36m0.6187\u001b[0m        \u001b[32m0.8799\u001b[0m                     \u001b[35m0.6079\u001b[0m        \u001b[31m1.0306\u001b[0m  0.0100  0.2713\n",
      "      3                     \u001b[36m0.6893\u001b[0m        \u001b[32m0.6758\u001b[0m                     \u001b[35m0.6124\u001b[0m        \u001b[31m0.9961\u001b[0m  0.0098  0.2693\n",
      "      4                     0.6832        0.7031                     \u001b[35m0.7215\u001b[0m        \u001b[31m0.5427\u001b[0m  0.0096  0.2703\n",
      "      5                     \u001b[36m0.7232\u001b[0m        \u001b[32m0.5676\u001b[0m                     0.6613        0.6321  0.0093  0.2693\n",
      "      6                     0.6761        0.7171                     0.6716        0.6140  0.0090  0.2684\n",
      "      7                     0.7149        0.5993                     \u001b[35m0.7436\u001b[0m        \u001b[31m0.5304\u001b[0m  0.0085  0.2723\n",
      "      8                     \u001b[36m0.7387\u001b[0m        \u001b[32m0.5357\u001b[0m                     0.7355        \u001b[31m0.5220\u001b[0m  0.0080  0.2693\n",
      "      9                     0.7309        0.5423                     0.6757        0.6336  0.0075  0.2693\n",
      "     10                     0.7382        0.5461                     0.7338        0.5310  0.0069  0.2693\n",
      "     11                     0.7259        0.5441                     0.7226        \u001b[31m0.5201\u001b[0m  0.0063  0.2693\n",
      "     12                     \u001b[36m0.7394\u001b[0m        0.5387                     0.7367        \u001b[31m0.5033\u001b[0m  0.0057  0.2693\n",
      "     13                     \u001b[36m0.7426\u001b[0m        \u001b[32m0.5194\u001b[0m                     0.7185        0.5103  0.0050  0.2693\n",
      "     14                     \u001b[36m0.7604\u001b[0m        \u001b[32m0.5001\u001b[0m                     0.7369        0.5146  0.0043  0.2693\n",
      "     15                     0.7451        0.5083                     0.7204        0.5394  0.0037  0.2697\n",
      "     16                     0.7554        0.5014                     0.7219        0.5312  0.0031  0.2693\n",
      "     17                     0.7514        0.5149                     0.7347        \u001b[31m0.5008\u001b[0m  0.0025  0.2693\n",
      "     18                     0.7595        \u001b[32m0.4901\u001b[0m                     0.7279        0.5077  0.0020  0.2697\n",
      "     19                     0.7451        0.5017                     0.7336        \u001b[31m0.4957\u001b[0m  0.0015  0.2693\n",
      "     20                     0.7595        \u001b[32m0.4854\u001b[0m                     0.7357        \u001b[31m0.4946\u001b[0m  0.0010  0.2694\n",
      "     21                     \u001b[36m0.7663\u001b[0m        \u001b[32m0.4751\u001b[0m                     0.7345        0.4984  0.0007  0.2696\n",
      "     22                     \u001b[36m0.7672\u001b[0m        0.4798                     \u001b[35m0.7483\u001b[0m        \u001b[31m0.4887\u001b[0m  0.0004  0.2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23                     \u001b[36m0.7719\u001b[0m        \u001b[32m0.4731\u001b[0m                     0.7333        0.4937  0.0002  0.2703\n",
      "     24                     \u001b[36m0.7737\u001b[0m        \u001b[32m0.4695\u001b[0m                     0.7372        0.4978  0.0000  0.2703\n",
      "     25                     0.7726        0.4728                     0.7449        0.4888  0.0000  0.2698\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6267\u001b[0m        \u001b[32m1.3151\u001b[0m                     \u001b[35m0.6389\u001b[0m        \u001b[31m0.7821\u001b[0m  0.0100  0.2663\n",
      "      2                     \u001b[36m0.6950\u001b[0m        \u001b[32m0.7124\u001b[0m                     \u001b[35m0.7074\u001b[0m        \u001b[31m0.6395\u001b[0m  0.0100  0.2723\n",
      "      3                     \u001b[36m0.7113\u001b[0m        \u001b[32m0.6049\u001b[0m                     0.6476        0.7325  0.0098  0.2693\n",
      "      4                     0.6961        0.6555                     0.6918        0.8725  0.0096  0.2703\n",
      "      5                     \u001b[36m0.7284\u001b[0m        0.6050                     \u001b[35m0.7343\u001b[0m        \u001b[31m0.5776\u001b[0m  0.0093  0.2704\n",
      "      6                     \u001b[36m0.7385\u001b[0m        \u001b[32m0.5393\u001b[0m                     0.6600        0.7483  0.0090  0.2693\n",
      "      7                     0.7160        0.6344                     0.6865        0.6815  0.0085  0.2693\n",
      "      8                     0.7317        \u001b[32m0.5209\u001b[0m                     \u001b[35m0.7404\u001b[0m        \u001b[31m0.5409\u001b[0m  0.0080  0.2693\n",
      "      9                     0.7346        0.5557                     0.6819        0.6450  0.0075  0.2693\n",
      "     10                     0.7295        0.5864                     0.7206        0.5694  0.0069  0.2693\n",
      "     11                     \u001b[36m0.7507\u001b[0m        \u001b[32m0.5019\u001b[0m                     0.7282        0.5550  0.0063  0.2693\n",
      "     12                     \u001b[36m0.7547\u001b[0m        \u001b[32m0.4963\u001b[0m                     0.7206        \u001b[31m0.5300\u001b[0m  0.0057  0.2703\n",
      "     13                     \u001b[36m0.7578\u001b[0m        \u001b[32m0.4935\u001b[0m                     0.7235        0.5347  0.0050  0.2693\n",
      "     14                     0.7441        0.5234                     0.7093        \u001b[31m0.5256\u001b[0m  0.0043  0.2699\n",
      "     15                     \u001b[36m0.7676\u001b[0m        \u001b[32m0.4796\u001b[0m                     0.7132        0.5491  0.0037  0.2703\n",
      "     16                     0.7331        0.5311                     \u001b[35m0.7574\u001b[0m        \u001b[31m0.5088\u001b[0m  0.0031  0.2688\n",
      "     17                     \u001b[36m0.7762\u001b[0m        \u001b[32m0.4741\u001b[0m                     0.7376        0.5152  0.0025  0.2693\n",
      "     18                     0.7750        \u001b[32m0.4605\u001b[0m                     0.7351        0.5197  0.0020  0.2693\n",
      "     19                     \u001b[36m0.7799\u001b[0m        \u001b[32m0.4520\u001b[0m                     0.7442        \u001b[31m0.5052\u001b[0m  0.0015  0.2693\n",
      "     20                     \u001b[36m0.7862\u001b[0m        \u001b[32m0.4492\u001b[0m                     0.7500        0.5091  0.0010  0.2693\n",
      "     21                     \u001b[36m0.7949\u001b[0m        \u001b[32m0.4386\u001b[0m                     0.7424        0.5129  0.0007  0.2704\n",
      "     22                     0.7905        0.4397                     0.7431        0.5111  0.0004  0.2703\n",
      "     23                     0.7917        \u001b[32m0.4352\u001b[0m                     0.7436        0.5063  0.0002  0.2703\n",
      "     24                     0.7883        0.4383                     0.7461        0.5152  0.0000  0.2693\n",
      "     25                     0.7907        \u001b[32m0.4327\u001b[0m                     0.7514        0.5160  0.0000  0.2702\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6236\u001b[0m        \u001b[32m1.1035\u001b[0m                     \u001b[35m0.6366\u001b[0m        \u001b[31m0.8049\u001b[0m  0.0100  0.2683\n",
      "      2                     0.5945        \u001b[32m0.9949\u001b[0m                     \u001b[35m0.6594\u001b[0m        \u001b[31m0.6856\u001b[0m  0.0100  0.2733\n",
      "      3                     \u001b[36m0.6560\u001b[0m        \u001b[32m0.7475\u001b[0m                     0.5579        1.0306  0.0098  0.2693\n",
      "      4                     0.6514        0.7886                     \u001b[35m0.7169\u001b[0m        \u001b[31m0.5685\u001b[0m  0.0096  0.2693\n",
      "      5                     \u001b[36m0.7042\u001b[0m        \u001b[32m0.5821\u001b[0m                     0.6531        0.7441  0.0093  0.2693\n",
      "      6                     0.6966        0.6694                     0.7060        0.6122  0.0090  0.2693\n",
      "      7                     0.7008        0.5957                     \u001b[35m0.7662\u001b[0m        \u001b[31m0.4918\u001b[0m  0.0085  0.2703\n",
      "      8                     \u001b[36m0.7133\u001b[0m        0.5938                     0.5957        0.6631  0.0080  0.2700\n",
      "      9                     0.6606        0.6936                     0.7203        0.5717  0.0075  0.2693\n",
      "     10                     \u001b[36m0.7157\u001b[0m        \u001b[32m0.5549\u001b[0m                     0.7632        \u001b[31m0.4890\u001b[0m  0.0069  0.2693\n",
      "     11                     \u001b[36m0.7332\u001b[0m        \u001b[32m0.5345\u001b[0m                     0.7513        0.5036  0.0063  0.2703\n",
      "     12                     0.7307        \u001b[32m0.5286\u001b[0m                     0.7282        0.4978  0.0057  0.2693\n",
      "     13                     \u001b[36m0.7455\u001b[0m        \u001b[32m0.5126\u001b[0m                     0.7584        \u001b[31m0.4816\u001b[0m  0.0050  0.2703\n",
      "     14                     0.7425        \u001b[32m0.5103\u001b[0m                     0.7571        0.4863  0.0043  0.2693\n",
      "     15                     \u001b[36m0.7578\u001b[0m        \u001b[32m0.4956\u001b[0m                     0.7594        0.4843  0.0037  0.2693\n",
      "     16                     0.7519        0.5015                     \u001b[35m0.7676\u001b[0m        0.4824  0.0031  0.2704\n",
      "     17                     \u001b[36m0.7584\u001b[0m        \u001b[32m0.4927\u001b[0m                     0.7558        \u001b[31m0.4749\u001b[0m  0.0025  0.2693\n",
      "     18                     0.7489        0.5108                     0.7644        0.4935  0.0020  0.2703\n",
      "     19                     0.7561        \u001b[32m0.4899\u001b[0m                     \u001b[35m0.7701\u001b[0m        \u001b[31m0.4744\u001b[0m  0.0015  0.2703\n",
      "     20                     0.7490        0.4925                     0.7540        0.4823  0.0010  0.2693\n",
      "     21                     0.7524        0.4939                     0.7585        0.4787  0.0007  0.2704\n",
      "     22                     0.7551        \u001b[32m0.4878\u001b[0m                     0.7578        0.4768  0.0004  0.2693\n",
      "     23                     0.7581        \u001b[32m0.4837\u001b[0m                     0.7599        0.4768  0.0002  0.2693\n",
      "     24                     \u001b[36m0.7593\u001b[0m        \u001b[32m0.4816\u001b[0m                     0.7616        0.4760  0.0000  0.2703\n",
      "     25                     \u001b[36m0.7639\u001b[0m        \u001b[32m0.4789\u001b[0m                     0.7632        0.4762  0.0000  0.2693\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6232\u001b[0m        \u001b[32m1.3685\u001b[0m                     \u001b[35m0.6895\u001b[0m        \u001b[31m1.1504\u001b[0m  0.0100  0.2683\n",
      "      2                     \u001b[36m0.6711\u001b[0m        \u001b[32m0.7966\u001b[0m                     0.5956        \u001b[31m0.8330\u001b[0m  0.0100  0.2713\n",
      "      3                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.6476\u001b[0m                     0.5769        0.8809  0.0098  0.2693\n",
      "      4                     0.6887        0.6828                     \u001b[35m0.6933\u001b[0m        \u001b[31m0.6391\u001b[0m  0.0096  0.2703\n",
      "      5                     \u001b[36m0.7229\u001b[0m        \u001b[32m0.5488\u001b[0m                     \u001b[35m0.6970\u001b[0m        \u001b[31m0.5667\u001b[0m  0.0093  0.2713\n",
      "      6                     \u001b[36m0.7372\u001b[0m        \u001b[32m0.5464\u001b[0m                     \u001b[35m0.6971\u001b[0m        0.5807  0.0090  0.2693\n",
      "      7                     0.7259        0.5904                     0.6724        0.6346  0.0085  0.2699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8                     0.7313        0.5619                     \u001b[35m0.7332\u001b[0m        0.5804  0.0080  0.2703\n",
      "      9                     \u001b[36m0.7537\u001b[0m        \u001b[32m0.5160\u001b[0m                     0.6941        0.5736  0.0075  0.2693\n",
      "     10                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4880\u001b[0m                     0.7326        \u001b[31m0.5217\u001b[0m  0.0069  0.2693\n",
      "     11                     0.7498        0.5264                     \u001b[35m0.7367\u001b[0m        0.5362  0.0063  0.2691\n",
      "     12                     0.7609        \u001b[32m0.4862\u001b[0m                     \u001b[35m0.7451\u001b[0m        \u001b[31m0.5075\u001b[0m  0.0057  0.2693\n",
      "     13                     0.7475        0.5184                     0.7222        0.5381  0.0050  0.2693\n",
      "     14                     0.7278        0.5503                     0.7313        0.5293  0.0043  0.2693\n",
      "     15                     \u001b[36m0.7678\u001b[0m        \u001b[32m0.4820\u001b[0m                     \u001b[35m0.7568\u001b[0m        \u001b[31m0.4973\u001b[0m  0.0037  0.2693\n",
      "     16                     0.7482        0.5267                     0.7270        0.5266  0.0031  0.2703\n",
      "     17                     \u001b[36m0.7768\u001b[0m        \u001b[32m0.4634\u001b[0m                     0.7317        0.5336  0.0025  0.2693\n",
      "     18                     \u001b[36m0.7773\u001b[0m        0.4709                     0.7502        \u001b[31m0.4954\u001b[0m  0.0020  0.2693\n",
      "     19                     0.7739        \u001b[32m0.4608\u001b[0m                     0.7495        0.5190  0.0015  0.2693\n",
      "     20                     \u001b[36m0.7832\u001b[0m        0.4665                     0.7426        0.5136  0.0010  0.2703\n",
      "     21                     \u001b[36m0.7854\u001b[0m        \u001b[32m0.4500\u001b[0m                     0.7282        0.5263  0.0007  0.2693\n",
      "     22                     0.7843        \u001b[32m0.4487\u001b[0m                     0.7285        0.5321  0.0004  0.2693\n",
      "     23                     0.7826        \u001b[32m0.4398\u001b[0m                     0.7226        0.5382  0.0002  0.2693\n",
      "     24                     \u001b[36m0.7924\u001b[0m        \u001b[32m0.4389\u001b[0m                     0.7523        0.5118  0.0000  0.2693\n",
      "     25                     \u001b[36m0.7966\u001b[0m        0.4453                     0.7524        0.5070  0.0000  0.2693\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6220\u001b[0m        \u001b[32m1.1769\u001b[0m                     \u001b[35m0.6257\u001b[0m        \u001b[31m1.1089\u001b[0m  0.0100  0.2673\n",
      "      2                     \u001b[36m0.6373\u001b[0m        \u001b[32m0.8022\u001b[0m                     \u001b[35m0.7323\u001b[0m        \u001b[31m0.5399\u001b[0m  0.0100  0.2723\n",
      "      3                     \u001b[36m0.6857\u001b[0m        \u001b[32m0.6559\u001b[0m                     0.6024        0.8837  0.0098  0.2693\n",
      "      4                     \u001b[36m0.7155\u001b[0m        \u001b[32m0.6286\u001b[0m                     0.6035        0.8986  0.0096  0.2703\n",
      "      5                     0.6802        0.7161                     0.6218        0.8178  0.0093  0.2693\n",
      "      6                     0.6931        0.6387                     0.6681        0.6501  0.0090  0.2693\n",
      "      7                     \u001b[36m0.7243\u001b[0m        \u001b[32m0.5504\u001b[0m                     0.7157        0.6203  0.0085  0.2693\n",
      "      8                     \u001b[36m0.7348\u001b[0m        0.5620                     0.6781        0.6837  0.0080  0.2694\n",
      "      9                     \u001b[36m0.7538\u001b[0m        \u001b[32m0.5115\u001b[0m                     0.7244        0.5764  0.0075  0.2693\n",
      "     10                     0.6840        0.6359                     0.7225        0.5557  0.0069  0.2693\n",
      "     11                     0.7023        0.5832                     \u001b[35m0.7355\u001b[0m        \u001b[31m0.5377\u001b[0m  0.0063  0.2698\n",
      "     12                     0.7359        0.5341                     0.7298        0.5440  0.0057  0.2683\n",
      "     13                     0.7426        \u001b[32m0.5083\u001b[0m                     \u001b[35m0.7391\u001b[0m        0.5467  0.0050  0.2693\n",
      "     14                     0.7475        \u001b[32m0.5063\u001b[0m                     \u001b[35m0.7483\u001b[0m        0.5427  0.0043  0.2703\n",
      "     15                     0.7399        0.5209                     0.7261        0.5657  0.0037  0.2693\n",
      "     16                     0.7517        0.5071                     \u001b[35m0.7484\u001b[0m        0.5406  0.0031  0.2693\n",
      "     17                     0.7426        0.5090                     0.7254        0.5549  0.0025  0.2693\n",
      "     18                     0.7524        \u001b[32m0.4973\u001b[0m                     0.7361        0.5582  0.0020  0.2693\n",
      "     19                     \u001b[36m0.7625\u001b[0m        \u001b[32m0.4811\u001b[0m                     0.7376        0.5547  0.0015  0.2693\n",
      "     20                     0.7567        0.4959                     0.7275        0.5500  0.0010  0.2693\n",
      "     21                     \u001b[36m0.7678\u001b[0m        0.4846                     0.7377        0.5448  0.0007  0.2693\n",
      "     22                     \u001b[36m0.7698\u001b[0m        \u001b[32m0.4741\u001b[0m                     0.7345        0.5431  0.0004  0.2697\n",
      "     23                     0.7667        \u001b[32m0.4732\u001b[0m                     0.7392        0.5453  0.0002  0.2694\n",
      "     24                     0.7632        0.4733                     0.7385        0.5411  0.0000  0.2693\n",
      "     25                     0.7646        0.4763                     0.7301        0.5421  0.0000  0.2694\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6203\u001b[0m        \u001b[32m1.2694\u001b[0m                     \u001b[35m0.6895\u001b[0m        \u001b[31m1.0769\u001b[0m  0.0100  0.2663\n",
      "      2                     \u001b[36m0.6794\u001b[0m        \u001b[32m0.7147\u001b[0m                     0.6475        1.1424  0.0100  0.2733\n",
      "      3                     \u001b[36m0.6971\u001b[0m        \u001b[32m0.6703\u001b[0m                     0.5980        \u001b[31m0.7095\u001b[0m  0.0098  0.2693\n",
      "      4                     \u001b[36m0.7044\u001b[0m        \u001b[32m0.6227\u001b[0m                     0.6680        0.7419  0.0096  0.2693\n",
      "      5                     0.6648        0.7839                     0.6859        \u001b[31m0.6595\u001b[0m  0.0093  0.2693\n",
      "      6                     \u001b[36m0.7289\u001b[0m        \u001b[32m0.5715\u001b[0m                     \u001b[35m0.7104\u001b[0m        \u001b[31m0.5879\u001b[0m  0.0090  0.2713\n",
      "      7                     0.7178        0.5878                     0.7017        \u001b[31m0.5771\u001b[0m  0.0085  0.2706\n",
      "      8                     \u001b[36m0.7338\u001b[0m        \u001b[32m0.5338\u001b[0m                     0.7103        0.5838  0.0080  0.2694\n",
      "      9                     0.7336        0.5480                     0.6826        0.6636  0.0075  0.2693\n",
      "     10                     0.7296        0.5487                     \u001b[35m0.7187\u001b[0m        \u001b[31m0.5757\u001b[0m  0.0069  0.2693\n",
      "     11                     \u001b[36m0.7493\u001b[0m        \u001b[32m0.5160\u001b[0m                     0.7111        0.6029  0.0063  0.2693\n",
      "     12                     0.7490        \u001b[32m0.5140\u001b[0m                     \u001b[35m0.7348\u001b[0m        \u001b[31m0.5331\u001b[0m  0.0057  0.2693\n",
      "     13                     0.7178        0.5486                     \u001b[35m0.7354\u001b[0m        \u001b[31m0.5216\u001b[0m  0.0050  0.2703\n",
      "     14                     \u001b[36m0.7641\u001b[0m        \u001b[32m0.4861\u001b[0m                     \u001b[35m0.7465\u001b[0m        \u001b[31m0.5167\u001b[0m  0.0043  0.2693\n",
      "     15                     0.7527        0.4900                     \u001b[35m0.7468\u001b[0m        \u001b[31m0.5106\u001b[0m  0.0037  0.2695\n",
      "     16                     0.7634        \u001b[32m0.4756\u001b[0m                     0.7456        \u001b[31m0.5015\u001b[0m  0.0031  0.2703\n",
      "     17                     \u001b[36m0.7661\u001b[0m        \u001b[32m0.4755\u001b[0m                     0.7461        0.5054  0.0025  0.2703\n",
      "     18                     0.7642        \u001b[32m0.4700\u001b[0m                     \u001b[35m0.7543\u001b[0m        0.5062  0.0020  0.2693\n",
      "     19                     0.7619        0.4734                     0.7517        0.5156  0.0015  0.2692\n",
      "     20                     \u001b[36m0.7729\u001b[0m        0.4774                     \u001b[35m0.7552\u001b[0m        \u001b[31m0.4994\u001b[0m  0.0010  0.2703\n",
      "     21                     0.7703        \u001b[32m0.4633\u001b[0m                     \u001b[35m0.7565\u001b[0m        \u001b[31m0.4961\u001b[0m  0.0007  0.2703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22                     \u001b[36m0.7856\u001b[0m        \u001b[32m0.4443\u001b[0m                     0.7524        \u001b[31m0.4943\u001b[0m  0.0004  0.2703\n",
      "     23                     0.7702        0.4544                     \u001b[35m0.7597\u001b[0m        0.4982  0.0002  0.2702\n",
      "     24                     0.7851        0.4516                     0.7553        0.5024  0.0000  0.2703\n",
      "     25                     0.7835        0.4537                     \u001b[35m0.7613\u001b[0m        0.4973  0.0000  0.2703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[1 0 0 ... 0 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5823\u001b[0m        \u001b[32m1.2670\u001b[0m                     \u001b[35m0.5674\u001b[0m        \u001b[31m1.0113\u001b[0m  0.0100  0.8031\n",
      "      2                     \u001b[36m0.6154\u001b[0m        \u001b[32m0.7432\u001b[0m                     \u001b[35m0.6173\u001b[0m        \u001b[31m0.6493\u001b[0m  0.0100  0.5874\n",
      "      3                     \u001b[36m0.6619\u001b[0m        \u001b[32m0.6424\u001b[0m                     \u001b[35m0.7143\u001b[0m        \u001b[31m0.5735\u001b[0m  0.0098  0.5839\n",
      "      4                     \u001b[36m0.6723\u001b[0m        0.6427                     0.6916        0.5826  0.0096  0.5848\n",
      "      5                     \u001b[36m0.6921\u001b[0m        \u001b[32m0.5929\u001b[0m                     0.7099        \u001b[31m0.5616\u001b[0m  0.0093  0.5842\n",
      "      6                     \u001b[36m0.6951\u001b[0m        \u001b[32m0.5838\u001b[0m                     0.7026        0.5624  0.0090  0.5843\n",
      "      7                     \u001b[36m0.7154\u001b[0m        \u001b[32m0.5672\u001b[0m                     \u001b[35m0.7191\u001b[0m        \u001b[31m0.5492\u001b[0m  0.0085  0.5846\n",
      "      8                     \u001b[36m0.7243\u001b[0m        \u001b[32m0.5535\u001b[0m                     0.6705        0.5880  0.0080  0.5854\n",
      "      9                     \u001b[36m0.7310\u001b[0m        \u001b[32m0.5481\u001b[0m                     0.7077        0.5555  0.0075  0.5843\n",
      "     10                     0.7298        \u001b[32m0.5357\u001b[0m                     0.7031        0.5563  0.0069  0.5851\n",
      "     11                     \u001b[36m0.7389\u001b[0m        \u001b[32m0.5241\u001b[0m                     \u001b[35m0.7601\u001b[0m        \u001b[31m0.5001\u001b[0m  0.0063  0.5845\n",
      "     12                     \u001b[36m0.7412\u001b[0m        \u001b[32m0.5195\u001b[0m                     0.7251        0.5392  0.0057  0.5858\n",
      "     13                     \u001b[36m0.7440\u001b[0m        \u001b[32m0.5153\u001b[0m                     0.7439        0.5085  0.0050  0.5852\n",
      "     14                     \u001b[36m0.7528\u001b[0m        \u001b[32m0.5052\u001b[0m                     0.7540        \u001b[31m0.4988\u001b[0m  0.0043  0.5864\n",
      "     15                     \u001b[36m0.7598\u001b[0m        \u001b[32m0.5011\u001b[0m                     0.7223        0.5344  0.0037  0.5865\n",
      "     16                     \u001b[36m0.7648\u001b[0m        \u001b[32m0.4889\u001b[0m                     0.7498        0.5023  0.0031  0.5875\n",
      "     17                     0.7604        0.4911                     0.7562        \u001b[31m0.4921\u001b[0m  0.0025  0.5857\n",
      "     18                     \u001b[36m0.7694\u001b[0m        \u001b[32m0.4734\u001b[0m                     0.7526        0.4990  0.0020  0.5842\n",
      "     19                     \u001b[36m0.7774\u001b[0m        \u001b[32m0.4684\u001b[0m                     0.7523        \u001b[31m0.4888\u001b[0m  0.0015  0.5871\n",
      "     20                     \u001b[36m0.7860\u001b[0m        \u001b[32m0.4597\u001b[0m                     0.7589        \u001b[31m0.4791\u001b[0m  0.0010  0.5861\n",
      "     21                     0.7828        0.4615                     0.7545        0.4803  0.0007  0.5904\n",
      "     22                     \u001b[36m0.7866\u001b[0m        \u001b[32m0.4555\u001b[0m                     \u001b[35m0.7664\u001b[0m        \u001b[31m0.4783\u001b[0m  0.0004  0.5852\n",
      "     23                     \u001b[36m0.7957\u001b[0m        \u001b[32m0.4474\u001b[0m                     0.7521        0.4857  0.0002  0.5865\n",
      "     24                     0.7893        \u001b[32m0.4469\u001b[0m                     0.7631        \u001b[31m0.4778\u001b[0m  0.0000  0.5854\n",
      "     25                     \u001b[36m0.7965\u001b[0m        0.4479                     0.7606        0.4806  0.0000  0.5850\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6089\u001b[0m        \u001b[32m1.1749\u001b[0m                     \u001b[35m0.5701\u001b[0m        \u001b[31m0.7818\u001b[0m  0.0100  0.5832\n",
      "      2                     \u001b[36m0.6337\u001b[0m        \u001b[32m0.7273\u001b[0m                     \u001b[35m0.6369\u001b[0m        \u001b[31m0.6782\u001b[0m  0.0100  0.5865\n",
      "      3                     \u001b[36m0.6667\u001b[0m        \u001b[32m0.6770\u001b[0m                     \u001b[35m0.6680\u001b[0m        \u001b[31m0.6119\u001b[0m  0.0098  0.5844\n",
      "      4                     \u001b[36m0.6814\u001b[0m        \u001b[32m0.6171\u001b[0m                     0.6660        \u001b[31m0.6053\u001b[0m  0.0096  0.5864\n",
      "      5                     \u001b[36m0.6834\u001b[0m        0.6235                     0.6164        0.7366  0.0093  0.5845\n",
      "      6                     \u001b[36m0.7061\u001b[0m        0.6249                     \u001b[35m0.6827\u001b[0m        \u001b[31m0.5670\u001b[0m  0.0090  0.5854\n",
      "      7                     0.6967        \u001b[32m0.6067\u001b[0m                     0.6693        0.5973  0.0085  0.5844\n",
      "      8                     \u001b[36m0.7205\u001b[0m        \u001b[32m0.5586\u001b[0m                     0.6688        0.5861  0.0080  0.5855\n",
      "      9                     \u001b[36m0.7225\u001b[0m        \u001b[32m0.5496\u001b[0m                     0.6711        0.5757  0.0075  0.5854\n",
      "     10                     \u001b[36m0.7333\u001b[0m        \u001b[32m0.5336\u001b[0m                     \u001b[35m0.6953\u001b[0m        0.5798  0.0069  0.5855\n",
      "     11                     \u001b[36m0.7334\u001b[0m        \u001b[32m0.5324\u001b[0m                     0.6815        0.5857  0.0063  0.5854\n",
      "     12                     \u001b[36m0.7390\u001b[0m        \u001b[32m0.5232\u001b[0m                     \u001b[35m0.6996\u001b[0m        \u001b[31m0.5666\u001b[0m  0.0057  0.5832\n",
      "     13                     \u001b[36m0.7547\u001b[0m        \u001b[32m0.5141\u001b[0m                     0.6931        0.5760  0.0050  0.5857\n",
      "     14                     \u001b[36m0.7564\u001b[0m        \u001b[32m0.5112\u001b[0m                     \u001b[35m0.7243\u001b[0m        \u001b[31m0.5521\u001b[0m  0.0043  0.5865\n",
      "     15                     \u001b[36m0.7619\u001b[0m        \u001b[32m0.5044\u001b[0m                     \u001b[35m0.7387\u001b[0m        \u001b[31m0.5389\u001b[0m  0.0037  0.5855\n",
      "     16                     \u001b[36m0.7633\u001b[0m        0.5070                     0.7131        0.5498  0.0031  0.5865\n",
      "     17                     \u001b[36m0.7688\u001b[0m        \u001b[32m0.4899\u001b[0m                     0.7101        0.5477  0.0025  0.5864\n",
      "     18                     \u001b[36m0.7711\u001b[0m        \u001b[32m0.4851\u001b[0m                     0.7251        \u001b[31m0.5368\u001b[0m  0.0020  0.5871\n",
      "     19                     0.7687        \u001b[32m0.4822\u001b[0m                     0.7209        0.5522  0.0015  0.5870\n",
      "     20                     0.7700        \u001b[32m0.4760\u001b[0m                     \u001b[35m0.7470\u001b[0m        \u001b[31m0.5302\u001b[0m  0.0010  0.5855\n",
      "     21                     \u001b[36m0.7780\u001b[0m        \u001b[32m0.4711\u001b[0m                     0.7401        0.5343  0.0007  0.5854\n",
      "     22                     0.7695        0.4711                     0.7411        0.5357  0.0004  0.5875\n",
      "     23                     \u001b[36m0.7783\u001b[0m        \u001b[32m0.4588\u001b[0m                     0.7260        0.5410  0.0002  0.5859\n",
      "     24                     \u001b[36m0.7794\u001b[0m        0.4661                     0.7394        0.5351  0.0000  0.5845\n",
      "     25                     \u001b[36m0.7818\u001b[0m        0.4590                     0.7310        0.5318  0.0000  0.5843\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6152\u001b[0m        \u001b[32m1.0097\u001b[0m                     \u001b[35m0.6206\u001b[0m        \u001b[31m0.8508\u001b[0m  0.0100  0.5825\n",
      "      2                     \u001b[36m0.6530\u001b[0m        \u001b[32m0.7180\u001b[0m                     0.5526        0.9433  0.0100  0.5845\n",
      "      3                     0.6485        0.7308                     0.6120        \u001b[31m0.7057\u001b[0m  0.0098  0.5845\n",
      "      4                     \u001b[36m0.6849\u001b[0m        \u001b[32m0.6463\u001b[0m                     \u001b[35m0.6898\u001b[0m        \u001b[31m0.6432\u001b[0m  0.0096  0.5864\n",
      "      5                     \u001b[36m0.7069\u001b[0m        \u001b[32m0.5893\u001b[0m                     \u001b[35m0.6947\u001b[0m        \u001b[31m0.5637\u001b[0m  0.0093  0.5855\n",
      "      6                     0.6997        0.6022                     \u001b[35m0.6963\u001b[0m        0.5773  0.0090  0.5875\n",
      "      7                     \u001b[36m0.7188\u001b[0m        \u001b[32m0.5687\u001b[0m                     \u001b[35m0.7058\u001b[0m        \u001b[31m0.5478\u001b[0m  0.0085  0.5874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8                     \u001b[36m0.7275\u001b[0m        \u001b[32m0.5458\u001b[0m                     0.6901        0.6032  0.0080  0.5875\n",
      "      9                     0.7272        0.5514                     0.7043        0.5579  0.0075  0.5855\n",
      "     10                     \u001b[36m0.7367\u001b[0m        \u001b[32m0.5381\u001b[0m                     \u001b[35m0.7181\u001b[0m        \u001b[31m0.5473\u001b[0m  0.0069  0.5860\n",
      "     11                     \u001b[36m0.7515\u001b[0m        \u001b[32m0.5236\u001b[0m                     0.7012        0.5668  0.0063  0.5855\n",
      "     12                     0.7490        \u001b[32m0.5225\u001b[0m                     \u001b[35m0.7263\u001b[0m        0.5512  0.0057  0.5867\n",
      "     13                     \u001b[36m0.7525\u001b[0m        \u001b[32m0.5135\u001b[0m                     0.7099        \u001b[31m0.5411\u001b[0m  0.0050  0.5865\n",
      "     14                     \u001b[36m0.7630\u001b[0m        \u001b[32m0.4942\u001b[0m                     \u001b[35m0.7328\u001b[0m        \u001b[31m0.5253\u001b[0m  0.0043  0.5861\n",
      "     15                     \u001b[36m0.7655\u001b[0m        0.4967                     0.7233        0.5409  0.0037  0.5856\n",
      "     16                     \u001b[36m0.7755\u001b[0m        \u001b[32m0.4806\u001b[0m                     0.7050        0.5670  0.0031  0.5865\n",
      "     17                     0.7675        0.4829                     0.7280        0.5402  0.0025  0.5860\n",
      "     18                     0.7732        0.4858                     \u001b[35m0.7419\u001b[0m        0.5291  0.0020  0.5854\n",
      "     19                     \u001b[36m0.7828\u001b[0m        \u001b[32m0.4697\u001b[0m                     0.7251        0.5479  0.0015  0.5864\n",
      "     20                     \u001b[36m0.7852\u001b[0m        \u001b[32m0.4611\u001b[0m                     0.7373        0.5315  0.0010  0.5840\n",
      "     21                     \u001b[36m0.7897\u001b[0m        \u001b[32m0.4602\u001b[0m                     \u001b[35m0.7442\u001b[0m        0.5267  0.0007  0.5855\n",
      "     22                     0.7874        \u001b[32m0.4575\u001b[0m                     0.7439        0.5281  0.0004  0.5845\n",
      "     23                     \u001b[36m0.7938\u001b[0m        \u001b[32m0.4486\u001b[0m                     \u001b[35m0.7476\u001b[0m        0.5309  0.0002  0.5845\n",
      "     24                     0.7874        0.4501                     \u001b[35m0.7529\u001b[0m        0.5292  0.0000  0.5855\n",
      "     25                     0.7880        \u001b[32m0.4472\u001b[0m                     0.7521        \u001b[31m0.5252\u001b[0m  0.0000  0.5854\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6044\u001b[0m        \u001b[32m1.2975\u001b[0m                     \u001b[35m0.5933\u001b[0m        \u001b[31m0.7422\u001b[0m  0.0100  0.5843\n",
      "      2                     \u001b[36m0.6253\u001b[0m        \u001b[32m0.8144\u001b[0m                     0.5658        0.7701  0.0100  0.5865\n",
      "      3                     \u001b[36m0.6457\u001b[0m        \u001b[32m0.7220\u001b[0m                     \u001b[35m0.6639\u001b[0m        \u001b[31m0.6213\u001b[0m  0.0098  0.5845\n",
      "      4                     \u001b[36m0.6882\u001b[0m        \u001b[32m0.6054\u001b[0m                     \u001b[35m0.7185\u001b[0m        \u001b[31m0.5781\u001b[0m  0.0096  0.5856\n",
      "      5                     \u001b[36m0.6886\u001b[0m        0.6094                     0.6685        0.6264  0.0093  0.5854\n",
      "      6                     0.6723        0.6277                     0.6836        0.5902  0.0090  0.5865\n",
      "      7                     \u001b[36m0.7077\u001b[0m        \u001b[32m0.5785\u001b[0m                     \u001b[35m0.7259\u001b[0m        \u001b[31m0.5674\u001b[0m  0.0085  0.5854\n",
      "      8                     0.7060        \u001b[32m0.5642\u001b[0m                     0.6896        0.5839  0.0080  0.5844\n",
      "      9                     \u001b[36m0.7195\u001b[0m        \u001b[32m0.5537\u001b[0m                     \u001b[35m0.7446\u001b[0m        \u001b[31m0.5366\u001b[0m  0.0075  0.5836\n",
      "     10                     \u001b[36m0.7334\u001b[0m        \u001b[32m0.5433\u001b[0m                     \u001b[35m0.7458\u001b[0m        \u001b[31m0.5357\u001b[0m  0.0069  0.5879\n",
      "     11                     0.7285        \u001b[32m0.5433\u001b[0m                     0.7374        \u001b[31m0.5206\u001b[0m  0.0063  0.5865\n",
      "     12                     \u001b[36m0.7372\u001b[0m        \u001b[32m0.5216\u001b[0m                     0.7410        0.5302  0.0057  0.5848\n",
      "     13                     \u001b[36m0.7440\u001b[0m        \u001b[32m0.5151\u001b[0m                     \u001b[35m0.7638\u001b[0m        \u001b[31m0.5127\u001b[0m  0.0050  0.5857\n",
      "     14                     \u001b[36m0.7443\u001b[0m        \u001b[32m0.5116\u001b[0m                     0.7516        0.5197  0.0043  0.5874\n",
      "     15                     \u001b[36m0.7539\u001b[0m        \u001b[32m0.4950\u001b[0m                     \u001b[35m0.7735\u001b[0m        \u001b[31m0.5088\u001b[0m  0.0037  0.5869\n",
      "     16                     \u001b[36m0.7618\u001b[0m        \u001b[32m0.4918\u001b[0m                     0.7455        0.5111  0.0031  0.5869\n",
      "     17                     \u001b[36m0.7631\u001b[0m        \u001b[32m0.4911\u001b[0m                     0.7670        \u001b[31m0.5022\u001b[0m  0.0025  0.5864\n",
      "     18                     \u001b[36m0.7724\u001b[0m        \u001b[32m0.4857\u001b[0m                     0.7466        0.5189  0.0020  0.5883\n",
      "     19                     \u001b[36m0.7804\u001b[0m        \u001b[32m0.4752\u001b[0m                     0.7571        0.5054  0.0015  0.5864\n",
      "     20                     0.7689        \u001b[32m0.4701\u001b[0m                     \u001b[35m0.7887\u001b[0m        \u001b[31m0.4929\u001b[0m  0.0010  0.5849\n",
      "     21                     0.7744        \u001b[32m0.4677\u001b[0m                     0.7685        0.4952  0.0007  0.5845\n",
      "     22                     0.7753        \u001b[32m0.4626\u001b[0m                     0.7668        0.5045  0.0004  0.5859\n",
      "     23                     \u001b[36m0.7824\u001b[0m        \u001b[32m0.4553\u001b[0m                     0.7698        0.4938  0.0002  0.5842\n",
      "     24                     0.7821        0.4606                     0.7665        0.4958  0.0000  0.5854\n",
      "     25                     0.7805        0.4616                     0.7693        0.4946  0.0000  0.5849\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5973\u001b[0m        \u001b[32m1.3205\u001b[0m                     \u001b[35m0.6114\u001b[0m        \u001b[31m0.6496\u001b[0m  0.0100  0.5814\n",
      "      2                     \u001b[36m0.6298\u001b[0m        \u001b[32m0.7244\u001b[0m                     0.5926        0.6849  0.0100  0.5848\n",
      "      3                     \u001b[36m0.6685\u001b[0m        \u001b[32m0.6817\u001b[0m                     \u001b[35m0.7120\u001b[0m        \u001b[31m0.5610\u001b[0m  0.0098  0.5862\n",
      "      4                     \u001b[36m0.6773\u001b[0m        \u001b[32m0.6218\u001b[0m                     0.6914        0.5975  0.0096  0.5846\n",
      "      5                     \u001b[36m0.7013\u001b[0m        \u001b[32m0.6000\u001b[0m                     0.6848        0.5730  0.0093  0.5851\n",
      "      6                     0.6897        0.6121                     \u001b[35m0.7147\u001b[0m        0.5620  0.0090  0.5844\n",
      "      7                     \u001b[36m0.7114\u001b[0m        \u001b[32m0.5658\u001b[0m                     0.7005        0.5652  0.0085  0.5846\n",
      "      8                     \u001b[36m0.7218\u001b[0m        \u001b[32m0.5521\u001b[0m                     \u001b[35m0.7284\u001b[0m        \u001b[31m0.5513\u001b[0m  0.0080  0.5852\n",
      "      9                     \u001b[36m0.7300\u001b[0m        \u001b[32m0.5354\u001b[0m                     0.7262        \u001b[31m0.5340\u001b[0m  0.0075  0.5852\n",
      "     10                     \u001b[36m0.7331\u001b[0m        0.5387                     0.7166        0.5547  0.0069  0.5883\n",
      "     11                     \u001b[36m0.7415\u001b[0m        \u001b[32m0.5223\u001b[0m                     \u001b[35m0.7598\u001b[0m        \u001b[31m0.5179\u001b[0m  0.0063  0.5854\n",
      "     12                     \u001b[36m0.7518\u001b[0m        \u001b[32m0.5128\u001b[0m                     0.7297        0.5389  0.0057  0.5860\n",
      "     13                     \u001b[36m0.7561\u001b[0m        \u001b[32m0.5090\u001b[0m                     \u001b[35m0.7606\u001b[0m        0.5250  0.0050  0.5860\n",
      "     14                     \u001b[36m0.7653\u001b[0m        \u001b[32m0.4910\u001b[0m                     0.7594        \u001b[31m0.5071\u001b[0m  0.0043  0.5845\n",
      "     15                     0.7622        \u001b[32m0.4875\u001b[0m                     0.7533        0.5233  0.0037  0.5854\n",
      "     16                     \u001b[36m0.7678\u001b[0m        \u001b[32m0.4846\u001b[0m                     0.7582        0.5097  0.0031  0.5842\n",
      "     17                     \u001b[36m0.7693\u001b[0m        \u001b[32m0.4767\u001b[0m                     0.7512        0.5229  0.0025  0.5859\n",
      "     18                     \u001b[36m0.7717\u001b[0m        \u001b[32m0.4726\u001b[0m                     0.7592        0.5111  0.0020  0.5855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19                     \u001b[36m0.7768\u001b[0m        \u001b[32m0.4602\u001b[0m                     \u001b[35m0.7705\u001b[0m        \u001b[31m0.5017\u001b[0m  0.0015  0.5855\n",
      "     20                     \u001b[36m0.7882\u001b[0m        \u001b[32m0.4507\u001b[0m                     0.7641        0.5052  0.0010  0.5864\n",
      "     21                     0.7860        \u001b[32m0.4503\u001b[0m                     \u001b[35m0.7713\u001b[0m        \u001b[31m0.4978\u001b[0m  0.0007  0.5862\n",
      "     22                     \u001b[36m0.7949\u001b[0m        \u001b[32m0.4446\u001b[0m                     0.7688        0.5020  0.0004  0.5865\n",
      "     23                     \u001b[36m0.7977\u001b[0m        \u001b[32m0.4443\u001b[0m                     0.7662        0.5036  0.0002  0.5857\n",
      "     24                     0.7893        \u001b[32m0.4443\u001b[0m                     0.7681        0.5047  0.0000  0.5854\n",
      "     25                     0.7956        \u001b[32m0.4417\u001b[0m                     0.7656        0.5010  0.0000  0.5844\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5848\u001b[0m        \u001b[32m1.1949\u001b[0m                     \u001b[35m0.6608\u001b[0m        \u001b[31m0.9030\u001b[0m  0.0100  0.5833\n",
      "      2                     \u001b[36m0.6204\u001b[0m        \u001b[32m0.7435\u001b[0m                     0.6497        \u001b[31m0.7971\u001b[0m  0.0100  0.5855\n",
      "      3                     \u001b[36m0.6648\u001b[0m        \u001b[32m0.6532\u001b[0m                     \u001b[35m0.6866\u001b[0m        \u001b[31m0.5861\u001b[0m  0.0098  0.5854\n",
      "      4                     \u001b[36m0.6816\u001b[0m        \u001b[32m0.6256\u001b[0m                     0.5337        0.8428  0.0096  0.5839\n",
      "      5                     \u001b[36m0.6855\u001b[0m        0.6257                     \u001b[35m0.7123\u001b[0m        \u001b[31m0.5682\u001b[0m  0.0093  0.5856\n",
      "      6                     \u001b[36m0.6899\u001b[0m        \u001b[32m0.5989\u001b[0m                     0.7082        \u001b[31m0.5520\u001b[0m  0.0090  0.5835\n",
      "      7                     \u001b[36m0.7093\u001b[0m        \u001b[32m0.5631\u001b[0m                     0.6992        0.5690  0.0085  0.5835\n",
      "      8                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.5619\u001b[0m                     \u001b[35m0.7390\u001b[0m        \u001b[31m0.5346\u001b[0m  0.0080  0.5849\n",
      "      9                     \u001b[36m0.7305\u001b[0m        \u001b[32m0.5410\u001b[0m                     0.6713        0.6121  0.0075  0.5855\n",
      "     10                     0.7263        0.5452                     0.7252        0.5445  0.0069  0.5854\n",
      "     11                     0.7295        0.5415                     0.7254        0.5443  0.0063  0.5840\n",
      "     12                     \u001b[36m0.7417\u001b[0m        \u001b[32m0.5263\u001b[0m                     0.7255        0.5447  0.0057  0.5854\n",
      "     13                     \u001b[36m0.7501\u001b[0m        \u001b[32m0.5158\u001b[0m                     \u001b[35m0.7493\u001b[0m        \u001b[31m0.5204\u001b[0m  0.0050  0.5845\n",
      "     14                     0.7448        \u001b[32m0.5121\u001b[0m                     \u001b[35m0.7520\u001b[0m        \u001b[31m0.5195\u001b[0m  0.0043  0.5866\n",
      "     15                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.5038\u001b[0m                     0.7386        0.5201  0.0037  0.5845\n",
      "     16                     \u001b[36m0.7669\u001b[0m        \u001b[32m0.4902\u001b[0m                     \u001b[35m0.7638\u001b[0m        \u001b[31m0.5025\u001b[0m  0.0031  0.5850\n",
      "     17                     \u001b[36m0.7674\u001b[0m        0.4915                     \u001b[35m0.7643\u001b[0m        0.5069  0.0025  0.5849\n",
      "     18                     \u001b[36m0.7755\u001b[0m        \u001b[32m0.4753\u001b[0m                     0.7629        0.5062  0.0020  0.5864\n",
      "     19                     \u001b[36m0.7869\u001b[0m        \u001b[32m0.4690\u001b[0m                     \u001b[35m0.7703\u001b[0m        0.5068  0.0015  0.5845\n",
      "     20                     0.7782        0.4720                     0.7653        \u001b[31m0.5021\u001b[0m  0.0010  0.5845\n",
      "     21                     0.7800        \u001b[32m0.4640\u001b[0m                     0.7569        0.5033  0.0007  0.5854\n",
      "     22                     0.7802        \u001b[32m0.4562\u001b[0m                     0.7547        0.5031  0.0004  0.5855\n",
      "     23                     0.7793        \u001b[32m0.4552\u001b[0m                     0.7595        \u001b[31m0.4993\u001b[0m  0.0002  0.5864\n",
      "     24                     \u001b[36m0.7959\u001b[0m        \u001b[32m0.4423\u001b[0m                     0.7599        0.5093  0.0000  0.5844\n",
      "     25                     0.7864        0.4548                     0.7604        0.5052  0.0000  0.5875\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5916\u001b[0m        \u001b[32m1.1440\u001b[0m                     \u001b[35m0.6401\u001b[0m        \u001b[31m0.7509\u001b[0m  0.0100  0.5845\n",
      "      2                     \u001b[36m0.6541\u001b[0m        \u001b[32m0.6772\u001b[0m                     0.6317        0.8129  0.0100  0.5855\n",
      "      3                     0.6438        0.7093                     0.6265        \u001b[31m0.7146\u001b[0m  0.0098  0.5859\n",
      "      4                     \u001b[36m0.6774\u001b[0m        \u001b[32m0.6126\u001b[0m                     0.5591        0.7669  0.0096  0.5838\n",
      "      5                     \u001b[36m0.6920\u001b[0m        \u001b[32m0.5844\u001b[0m                     \u001b[35m0.6985\u001b[0m        \u001b[31m0.5839\u001b[0m  0.0093  0.5851\n",
      "      6                     \u001b[36m0.7111\u001b[0m        \u001b[32m0.5668\u001b[0m                     0.6909        0.5882  0.0090  0.5849\n",
      "      7                     0.7027        0.5701                     \u001b[35m0.7005\u001b[0m        \u001b[31m0.5734\u001b[0m  0.0085  0.5849\n",
      "      8                     \u001b[36m0.7152\u001b[0m        \u001b[32m0.5454\u001b[0m                     \u001b[35m0.7041\u001b[0m        \u001b[31m0.5612\u001b[0m  0.0080  0.5854\n",
      "      9                     \u001b[36m0.7317\u001b[0m        \u001b[32m0.5368\u001b[0m                     \u001b[35m0.7086\u001b[0m        0.5639  0.0075  0.5843\n",
      "     10                     \u001b[36m0.7373\u001b[0m        \u001b[32m0.5292\u001b[0m                     \u001b[35m0.7240\u001b[0m        \u001b[31m0.5544\u001b[0m  0.0069  0.5859\n",
      "     11                     \u001b[36m0.7518\u001b[0m        \u001b[32m0.5143\u001b[0m                     0.7236        \u001b[31m0.5418\u001b[0m  0.0063  0.5855\n",
      "     12                     0.7485        \u001b[32m0.5131\u001b[0m                     \u001b[35m0.7326\u001b[0m        \u001b[31m0.5322\u001b[0m  0.0057  0.5870\n",
      "     13                     0.7508        \u001b[32m0.5074\u001b[0m                     \u001b[35m0.7396\u001b[0m        0.5388  0.0050  0.5844\n",
      "     14                     \u001b[36m0.7688\u001b[0m        \u001b[32m0.4943\u001b[0m                     0.7350        0.5607  0.0043  0.5846\n",
      "     15                     0.7568        \u001b[32m0.4937\u001b[0m                     \u001b[35m0.7578\u001b[0m        \u001b[31m0.5125\u001b[0m  0.0037  0.5865\n",
      "     16                     0.7660        \u001b[32m0.4841\u001b[0m                     0.7489        0.5304  0.0031  0.5860\n",
      "     17                     \u001b[36m0.7763\u001b[0m        \u001b[32m0.4808\u001b[0m                     0.7470        0.5214  0.0025  0.5875\n",
      "     18                     \u001b[36m0.7793\u001b[0m        \u001b[32m0.4640\u001b[0m                     0.7424        0.5365  0.0020  0.5864\n",
      "     19                     \u001b[36m0.7849\u001b[0m        \u001b[32m0.4599\u001b[0m                     \u001b[35m0.7597\u001b[0m        0.5137  0.0015  0.5864\n",
      "     20                     \u001b[36m0.7878\u001b[0m        \u001b[32m0.4507\u001b[0m                     0.7597        0.5160  0.0010  0.5844\n",
      "     21                     0.7850        \u001b[32m0.4422\u001b[0m                     \u001b[35m0.7682\u001b[0m        \u001b[31m0.5077\u001b[0m  0.0007  0.5870\n",
      "     22                     \u001b[36m0.7946\u001b[0m        0.4513                     0.7632        0.5116  0.0004  0.5848\n",
      "     23                     \u001b[36m0.7953\u001b[0m        0.4440                     0.7629        0.5137  0.0002  0.5858\n",
      "     24                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4385\u001b[0m                     \u001b[35m0.7686\u001b[0m        0.5079  0.0000  0.5871\n",
      "     25                     0.7935        0.4461                     \u001b[35m0.7715\u001b[0m        \u001b[31m0.5076\u001b[0m  0.0000  0.5864\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5872\u001b[0m        \u001b[32m1.1370\u001b[0m                     \u001b[35m0.5579\u001b[0m        \u001b[31m1.1752\u001b[0m  0.0100  0.5845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                     \u001b[36m0.6333\u001b[0m        \u001b[32m0.7704\u001b[0m                     \u001b[35m0.6428\u001b[0m        \u001b[31m0.7352\u001b[0m  0.0100  0.5850\n",
      "      3                     \u001b[36m0.6505\u001b[0m        \u001b[32m0.6917\u001b[0m                     0.6320        \u001b[31m0.6965\u001b[0m  0.0098  0.5864\n",
      "      4                     \u001b[36m0.6533\u001b[0m        \u001b[32m0.6841\u001b[0m                     \u001b[35m0.6913\u001b[0m        \u001b[31m0.5770\u001b[0m  0.0096  0.5835\n",
      "      5                     \u001b[36m0.6832\u001b[0m        \u001b[32m0.6095\u001b[0m                     0.6913        \u001b[31m0.5747\u001b[0m  0.0093  0.5838\n",
      "      6                     \u001b[36m0.6968\u001b[0m        \u001b[32m0.5818\u001b[0m                     \u001b[35m0.7267\u001b[0m        \u001b[31m0.5540\u001b[0m  0.0090  0.5832\n",
      "      7                     \u001b[36m0.7086\u001b[0m        \u001b[32m0.5659\u001b[0m                     \u001b[35m0.7364\u001b[0m        0.5550  0.0085  0.5855\n",
      "      8                     \u001b[36m0.7156\u001b[0m        \u001b[32m0.5601\u001b[0m                     0.7219        0.5544  0.0080  0.5855\n",
      "      9                     \u001b[36m0.7250\u001b[0m        \u001b[32m0.5503\u001b[0m                     0.7208        \u001b[31m0.5520\u001b[0m  0.0075  0.5865\n",
      "     10                     \u001b[36m0.7332\u001b[0m        \u001b[32m0.5410\u001b[0m                     \u001b[35m0.7435\u001b[0m        \u001b[31m0.5365\u001b[0m  0.0069  0.5845\n",
      "     11                     \u001b[36m0.7406\u001b[0m        \u001b[32m0.5239\u001b[0m                     0.7313        0.5477  0.0063  0.5838\n",
      "     12                     \u001b[36m0.7534\u001b[0m        \u001b[32m0.5122\u001b[0m                     0.7303        0.5409  0.0057  0.5860\n",
      "     13                     \u001b[36m0.7570\u001b[0m        0.5124                     0.7376        \u001b[31m0.5265\u001b[0m  0.0050  0.5865\n",
      "     14                     \u001b[36m0.7638\u001b[0m        \u001b[32m0.4938\u001b[0m                     0.7360        0.5286  0.0043  0.5854\n",
      "     15                     0.7626        0.4944                     0.7371        0.5272  0.0037  0.5839\n",
      "     16                     \u001b[36m0.7743\u001b[0m        \u001b[32m0.4815\u001b[0m                     0.7230        0.5385  0.0031  0.5854\n",
      "     17                     0.7688        0.4815                     0.7327        0.5379  0.0025  0.5865\n",
      "     18                     \u001b[36m0.7745\u001b[0m        \u001b[32m0.4695\u001b[0m                     \u001b[35m0.7681\u001b[0m        \u001b[31m0.5050\u001b[0m  0.0020  0.5865\n",
      "     19                     \u001b[36m0.7785\u001b[0m        \u001b[32m0.4694\u001b[0m                     0.7429        0.5151  0.0015  0.5865\n",
      "     20                     \u001b[36m0.7849\u001b[0m        \u001b[32m0.4654\u001b[0m                     0.7569        0.5128  0.0010  0.5865\n",
      "     21                     \u001b[36m0.7854\u001b[0m        \u001b[32m0.4487\u001b[0m                     0.7677        0.5100  0.0007  0.5865\n",
      "     22                     \u001b[36m0.7926\u001b[0m        0.4543                     0.7508        0.5111  0.0004  0.5855\n",
      "     23                     0.7881        0.4504                     0.7546        0.5118  0.0002  0.5836\n",
      "     24                     \u001b[36m0.7969\u001b[0m        \u001b[32m0.4420\u001b[0m                     0.7647        0.5059  0.0000  0.5865\n",
      "     25                     \u001b[36m0.7994\u001b[0m        0.4440                     \u001b[35m0.7692\u001b[0m        \u001b[31m0.5043\u001b[0m  0.0000  0.5865\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5844\u001b[0m        \u001b[32m1.1507\u001b[0m                     \u001b[35m0.5748\u001b[0m        \u001b[31m0.8914\u001b[0m  0.0100  0.5855\n",
      "      2                     \u001b[36m0.6412\u001b[0m        \u001b[32m0.7663\u001b[0m                     \u001b[35m0.6793\u001b[0m        \u001b[31m0.6384\u001b[0m  0.0100  0.5861\n",
      "      3                     \u001b[36m0.6585\u001b[0m        \u001b[32m0.6748\u001b[0m                     0.6438        0.6908  0.0098  0.5845\n",
      "      4                     \u001b[36m0.6775\u001b[0m        \u001b[32m0.6428\u001b[0m                     \u001b[35m0.6989\u001b[0m        \u001b[31m0.5637\u001b[0m  0.0096  0.5845\n",
      "      5                     \u001b[36m0.6902\u001b[0m        \u001b[32m0.5917\u001b[0m                     0.6933        0.5927  0.0093  0.5845\n",
      "      6                     \u001b[36m0.7039\u001b[0m        \u001b[32m0.5811\u001b[0m                     0.6947        0.5956  0.0090  0.5845\n",
      "      7                     0.7004        0.5897                     \u001b[35m0.7434\u001b[0m        \u001b[31m0.5383\u001b[0m  0.0085  0.5855\n",
      "      8                     \u001b[36m0.7209\u001b[0m        \u001b[32m0.5595\u001b[0m                     0.6979        0.5524  0.0080  0.5844\n",
      "      9                     \u001b[36m0.7220\u001b[0m        \u001b[32m0.5541\u001b[0m                     0.7270        \u001b[31m0.5335\u001b[0m  0.0075  0.5854\n",
      "     10                     \u001b[36m0.7249\u001b[0m        \u001b[32m0.5443\u001b[0m                     0.7072        0.5624  0.0069  0.5844\n",
      "     11                     \u001b[36m0.7430\u001b[0m        \u001b[32m0.5329\u001b[0m                     0.7411        \u001b[31m0.5167\u001b[0m  0.0063  0.5865\n",
      "     12                     \u001b[36m0.7472\u001b[0m        \u001b[32m0.5233\u001b[0m                     0.7207        0.5523  0.0057  0.5854\n",
      "     13                     \u001b[36m0.7537\u001b[0m        \u001b[32m0.5121\u001b[0m                     0.7434        0.5345  0.0050  0.5855\n",
      "     14                     \u001b[36m0.7646\u001b[0m        \u001b[32m0.5032\u001b[0m                     0.7429        0.5323  0.0043  0.5845\n",
      "     15                     \u001b[36m0.7671\u001b[0m        \u001b[32m0.4930\u001b[0m                     \u001b[35m0.7443\u001b[0m        \u001b[31m0.5067\u001b[0m  0.0037  0.5854\n",
      "     16                     \u001b[36m0.7705\u001b[0m        \u001b[32m0.4870\u001b[0m                     \u001b[35m0.7568\u001b[0m        \u001b[31m0.5053\u001b[0m  0.0031  0.5847\n",
      "     17                     0.7660        \u001b[32m0.4774\u001b[0m                     \u001b[35m0.7615\u001b[0m        0.5140  0.0025  0.5855\n",
      "     18                     \u001b[36m0.7740\u001b[0m        \u001b[32m0.4739\u001b[0m                     0.7506        0.5108  0.0020  0.5848\n",
      "     19                     \u001b[36m0.7814\u001b[0m        \u001b[32m0.4640\u001b[0m                     0.7554        \u001b[31m0.5044\u001b[0m  0.0015  0.5843\n",
      "     20                     \u001b[36m0.7830\u001b[0m        0.4655                     0.7574        0.5059  0.0010  0.5883\n",
      "     21                     \u001b[36m0.7882\u001b[0m        0.4655                     0.7494        0.5096  0.0007  0.5855\n",
      "     22                     \u001b[36m0.7887\u001b[0m        \u001b[32m0.4544\u001b[0m                     0.7575        \u001b[31m0.4989\u001b[0m  0.0004  0.5860\n",
      "     23                     \u001b[36m0.7934\u001b[0m        \u001b[32m0.4496\u001b[0m                     0.7529        0.5071  0.0002  0.5864\n",
      "     24                     \u001b[36m0.7981\u001b[0m        \u001b[32m0.4481\u001b[0m                     0.7556        \u001b[31m0.4965\u001b[0m  0.0000  0.5884\n",
      "     25                     0.7908        \u001b[32m0.4472\u001b[0m                     0.7546        0.5068  0.0000  0.5875\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5910\u001b[0m        \u001b[32m1.1853\u001b[0m                     \u001b[35m0.6627\u001b[0m        \u001b[31m0.8745\u001b[0m  0.0100  0.5845\n",
      "      2                     \u001b[36m0.6374\u001b[0m        \u001b[32m0.7602\u001b[0m                     \u001b[35m0.6656\u001b[0m        \u001b[31m0.6438\u001b[0m  0.0100  0.5854\n",
      "      3                     \u001b[36m0.6531\u001b[0m        \u001b[32m0.6960\u001b[0m                     \u001b[35m0.6730\u001b[0m        0.6590  0.0098  0.5841\n",
      "      4                     \u001b[36m0.6890\u001b[0m        \u001b[32m0.6106\u001b[0m                     \u001b[35m0.6940\u001b[0m        \u001b[31m0.6138\u001b[0m  0.0096  0.5835\n",
      "      5                     \u001b[36m0.6936\u001b[0m        \u001b[32m0.6014\u001b[0m                     \u001b[35m0.7034\u001b[0m        \u001b[31m0.5808\u001b[0m  0.0093  0.5837\n",
      "      6                     \u001b[36m0.7095\u001b[0m        \u001b[32m0.5674\u001b[0m                     \u001b[35m0.7053\u001b[0m        0.6083  0.0090  0.5849\n",
      "      7                     \u001b[36m0.7265\u001b[0m        \u001b[32m0.5503\u001b[0m                     \u001b[35m0.7126\u001b[0m        0.5928  0.0085  0.5832\n",
      "      8                     0.7227        0.5548                     \u001b[35m0.7260\u001b[0m        \u001b[31m0.5730\u001b[0m  0.0080  0.5844\n",
      "      9                     \u001b[36m0.7338\u001b[0m        \u001b[32m0.5435\u001b[0m                     0.7033        0.5961  0.0075  0.5849\n",
      "     10                     \u001b[36m0.7394\u001b[0m        \u001b[32m0.5274\u001b[0m                     0.7133        \u001b[31m0.5682\u001b[0m  0.0069  0.5845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     \u001b[36m0.7492\u001b[0m        \u001b[32m0.5145\u001b[0m                     0.7112        0.5745  0.0063  0.5831\n",
      "     12                     0.7482        \u001b[32m0.5060\u001b[0m                     \u001b[35m0.7349\u001b[0m        \u001b[31m0.5423\u001b[0m  0.0057  0.5844\n",
      "     13                     0.7451        0.5138                     0.7265        0.5565  0.0050  0.5854\n",
      "     14                     \u001b[36m0.7625\u001b[0m        \u001b[32m0.4930\u001b[0m                     0.7230        0.5518  0.0043  0.5844\n",
      "     15                     \u001b[36m0.7682\u001b[0m        \u001b[32m0.4857\u001b[0m                     \u001b[35m0.7413\u001b[0m        0.5502  0.0037  0.5844\n",
      "     16                     0.7644        0.4867                     0.7354        0.5444  0.0031  0.5845\n",
      "     17                     \u001b[36m0.7719\u001b[0m        \u001b[32m0.4714\u001b[0m                     \u001b[35m0.7488\u001b[0m        \u001b[31m0.5341\u001b[0m  0.0025  0.5835\n",
      "     18                     \u001b[36m0.7870\u001b[0m        \u001b[32m0.4534\u001b[0m                     0.7177        0.5640  0.0020  0.5844\n",
      "     19                     0.7775        0.4566                     0.7410        0.5356  0.0015  0.5845\n",
      "     20                     0.7853        0.4572                     0.7463        0.5423  0.0010  0.5844\n",
      "     21                     \u001b[36m0.7936\u001b[0m        \u001b[32m0.4408\u001b[0m                     0.7341        0.5443  0.0007  0.5844\n",
      "     22                     \u001b[36m0.7953\u001b[0m        \u001b[32m0.4407\u001b[0m                     \u001b[35m0.7554\u001b[0m        0.5427  0.0004  0.5844\n",
      "     23                     0.7877        0.4451                     0.7485        \u001b[31m0.5339\u001b[0m  0.0002  0.5847\n",
      "     24                     0.7917        0.4430                     0.7550        0.5392  0.0000  0.5853\n",
      "     25                     \u001b[36m0.7978\u001b[0m        \u001b[32m0.4370\u001b[0m                     0.7471        0.5347  0.0000  0.5863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 0 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5307\u001b[0m        \u001b[32m1.0295\u001b[0m                     \u001b[35m0.5879\u001b[0m        \u001b[31m0.7533\u001b[0m  0.0100  1.0985\n",
      "      2                     \u001b[36m0.5659\u001b[0m        \u001b[32m0.8688\u001b[0m                     \u001b[35m0.6493\u001b[0m        \u001b[31m0.6749\u001b[0m  0.0100  0.8414\n",
      "      3                     \u001b[36m0.6253\u001b[0m        \u001b[32m0.6777\u001b[0m                     0.6361        0.6861  0.0098  0.8369\n",
      "      4                     \u001b[36m0.6331\u001b[0m        \u001b[32m0.6664\u001b[0m                     \u001b[35m0.6611\u001b[0m        \u001b[31m0.6690\u001b[0m  0.0096  0.8378\n",
      "      5                     \u001b[36m0.6544\u001b[0m        \u001b[32m0.6252\u001b[0m                     \u001b[35m0.6683\u001b[0m        \u001b[31m0.6328\u001b[0m  0.0093  0.8368\n",
      "      6                     \u001b[36m0.6577\u001b[0m        0.6254                     0.6605        \u001b[31m0.6191\u001b[0m  0.0090  0.8388\n",
      "      7                     \u001b[36m0.6644\u001b[0m        \u001b[32m0.6158\u001b[0m                     \u001b[35m0.6726\u001b[0m        \u001b[31m0.6097\u001b[0m  0.0085  0.8361\n",
      "      8                     \u001b[36m0.6683\u001b[0m        \u001b[32m0.6096\u001b[0m                     0.6442        0.6290  0.0080  0.8391\n",
      "      9                     \u001b[36m0.6685\u001b[0m        0.6097                     0.6674        \u001b[31m0.6053\u001b[0m  0.0075  0.8378\n",
      "     10                     \u001b[36m0.6702\u001b[0m        \u001b[32m0.6040\u001b[0m                     0.6679        0.6128  0.0069  0.8392\n",
      "     11                     \u001b[36m0.6784\u001b[0m        \u001b[32m0.5982\u001b[0m                     0.6642        0.6110  0.0063  0.8393\n",
      "     12                     0.6721        0.5989                     0.6719        \u001b[31m0.5966\u001b[0m  0.0057  0.8368\n",
      "     13                     \u001b[36m0.6846\u001b[0m        \u001b[32m0.5930\u001b[0m                     0.6694        0.6050  0.0050  0.8383\n",
      "     14                     \u001b[36m0.6855\u001b[0m        \u001b[32m0.5911\u001b[0m                     \u001b[35m0.6893\u001b[0m        \u001b[31m0.5911\u001b[0m  0.0043  0.8378\n",
      "     15                     \u001b[36m0.6899\u001b[0m        \u001b[32m0.5862\u001b[0m                     \u001b[35m0.6898\u001b[0m        0.5937  0.0037  0.8381\n",
      "     16                     \u001b[36m0.6902\u001b[0m        \u001b[32m0.5850\u001b[0m                     0.6810        0.6020  0.0031  0.8394\n",
      "     17                     \u001b[36m0.6975\u001b[0m        \u001b[32m0.5804\u001b[0m                     \u001b[35m0.6919\u001b[0m        0.5940  0.0025  0.8369\n",
      "     18                     \u001b[36m0.7007\u001b[0m        \u001b[32m0.5778\u001b[0m                     0.6873        0.5958  0.0020  0.8398\n",
      "     19                     0.6964        \u001b[32m0.5743\u001b[0m                     \u001b[35m0.6978\u001b[0m        \u001b[31m0.5909\u001b[0m  0.0015  0.8387\n",
      "     20                     \u001b[36m0.7036\u001b[0m        \u001b[32m0.5706\u001b[0m                     0.6896        \u001b[31m0.5862\u001b[0m  0.0010  0.8388\n",
      "     21                     \u001b[36m0.7068\u001b[0m        \u001b[32m0.5658\u001b[0m                     0.6969        0.5884  0.0007  0.8398\n",
      "     22                     \u001b[36m0.7113\u001b[0m        \u001b[32m0.5629\u001b[0m                     0.6970        0.5879  0.0004  0.8398\n",
      "     23                     \u001b[36m0.7140\u001b[0m        \u001b[32m0.5604\u001b[0m                     0.6952        0.5876  0.0002  0.8378\n",
      "     24                     0.7073        0.5638                     0.6966        0.5875  0.0000  0.8388\n",
      "     25                     0.7092        0.5633                     \u001b[35m0.7002\u001b[0m        0.5867  0.0000  0.8388\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5366\u001b[0m        \u001b[32m1.1454\u001b[0m                     \u001b[35m0.6166\u001b[0m        \u001b[31m0.6826\u001b[0m  0.0100  0.8378\n",
      "      2                     \u001b[36m0.6219\u001b[0m        \u001b[32m0.7003\u001b[0m                     \u001b[35m0.6358\u001b[0m        0.6926  0.0100  0.8375\n",
      "      3                     \u001b[36m0.6275\u001b[0m        \u001b[32m0.6886\u001b[0m                     \u001b[35m0.6663\u001b[0m        \u001b[31m0.6243\u001b[0m  0.0098  0.8387\n",
      "      4                     \u001b[36m0.6450\u001b[0m        \u001b[32m0.6408\u001b[0m                     0.6517        0.6373  0.0096  0.8384\n",
      "      5                     \u001b[36m0.6590\u001b[0m        \u001b[32m0.6287\u001b[0m                     0.6565        \u001b[31m0.6202\u001b[0m  0.0093  0.8381\n",
      "      6                     \u001b[36m0.6602\u001b[0m        \u001b[32m0.6240\u001b[0m                     \u001b[35m0.6664\u001b[0m        \u001b[31m0.6195\u001b[0m  0.0090  0.8372\n",
      "      7                     \u001b[36m0.6696\u001b[0m        \u001b[32m0.6132\u001b[0m                     0.6584        0.6199  0.0085  0.8387\n",
      "      8                     0.6684        \u001b[32m0.6092\u001b[0m                     0.6445        0.6379  0.0080  0.8377\n",
      "      9                     \u001b[36m0.6776\u001b[0m        \u001b[32m0.6066\u001b[0m                     0.6636        \u001b[31m0.6115\u001b[0m  0.0075  0.8388\n",
      "     10                     0.6731        \u001b[32m0.6015\u001b[0m                     0.6641        \u001b[31m0.6084\u001b[0m  0.0069  0.8378\n",
      "     11                     \u001b[36m0.6860\u001b[0m        \u001b[32m0.5966\u001b[0m                     \u001b[35m0.6705\u001b[0m        \u001b[31m0.6058\u001b[0m  0.0063  0.8378\n",
      "     12                     0.6820        0.5971                     0.6671        \u001b[31m0.6042\u001b[0m  0.0057  0.8378\n",
      "     13                     0.6805        \u001b[32m0.5932\u001b[0m                     \u001b[35m0.6809\u001b[0m        \u001b[31m0.5990\u001b[0m  0.0050  0.8368\n",
      "     14                     \u001b[36m0.6863\u001b[0m        \u001b[32m0.5888\u001b[0m                     0.6706        0.5997  0.0043  0.8378\n",
      "     15                     \u001b[36m0.6929\u001b[0m        \u001b[32m0.5828\u001b[0m                     0.6743        \u001b[31m0.5947\u001b[0m  0.0037  0.8398\n",
      "     16                     \u001b[36m0.6929\u001b[0m        \u001b[32m0.5823\u001b[0m                     0.6776        0.5950  0.0031  0.8364\n",
      "     17                     \u001b[36m0.6971\u001b[0m        \u001b[32m0.5776\u001b[0m                     0.6774        0.5958  0.0025  0.8388\n",
      "     18                     0.6946        0.5784                     \u001b[35m0.6832\u001b[0m        \u001b[31m0.5936\u001b[0m  0.0020  0.8378\n",
      "     19                     \u001b[36m0.6972\u001b[0m        \u001b[32m0.5705\u001b[0m                     \u001b[35m0.6845\u001b[0m        0.5964  0.0015  0.8398\n",
      "     20                     \u001b[36m0.7013\u001b[0m        \u001b[32m0.5683\u001b[0m                     0.6828        \u001b[31m0.5916\u001b[0m  0.0010  0.8388\n",
      "     21                     \u001b[36m0.7058\u001b[0m        \u001b[32m0.5677\u001b[0m                     0.6828        0.5917  0.0007  0.8413\n",
      "     22                     \u001b[36m0.7065\u001b[0m        \u001b[32m0.5638\u001b[0m                     0.6828        0.5921  0.0004  0.8401\n",
      "     23                     \u001b[36m0.7094\u001b[0m        0.5648                     \u001b[35m0.6868\u001b[0m        \u001b[31m0.5910\u001b[0m  0.0002  0.8386\n",
      "     24                     \u001b[36m0.7095\u001b[0m        \u001b[32m0.5628\u001b[0m                     0.6860        \u001b[31m0.5905\u001b[0m  0.0000  0.8398\n",
      "     25                     \u001b[36m0.7104\u001b[0m        \u001b[32m0.5598\u001b[0m                     0.6867        \u001b[31m0.5900\u001b[0m  0.0000  0.8388\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5300\u001b[0m        \u001b[32m1.0143\u001b[0m                     \u001b[35m0.5717\u001b[0m        \u001b[31m0.7112\u001b[0m  0.0100  0.8338\n",
      "      2                     \u001b[36m0.5709\u001b[0m        \u001b[32m0.8225\u001b[0m                     \u001b[35m0.6424\u001b[0m        \u001b[31m0.6550\u001b[0m  0.0100  0.8358\n",
      "      3                     \u001b[36m0.6329\u001b[0m        \u001b[32m0.6756\u001b[0m                     0.6072        0.6678  0.0098  0.8378\n",
      "      4                     \u001b[36m0.6421\u001b[0m        \u001b[32m0.6575\u001b[0m                     \u001b[35m0.6740\u001b[0m        \u001b[31m0.6114\u001b[0m  0.0096  0.8369\n",
      "      5                     \u001b[36m0.6574\u001b[0m        \u001b[32m0.6275\u001b[0m                     0.6568        0.6252  0.0093  0.8367\n",
      "      6                     0.6552        \u001b[32m0.6244\u001b[0m                     0.6722        0.6149  0.0090  0.8374\n",
      "      7                     \u001b[36m0.6662\u001b[0m        \u001b[32m0.6131\u001b[0m                     \u001b[35m0.6748\u001b[0m        \u001b[31m0.6011\u001b[0m  0.0085  0.8378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8                     \u001b[36m0.6700\u001b[0m        \u001b[32m0.6088\u001b[0m                     0.6651        0.6084  0.0080  0.8392\n",
      "      9                     0.6686        \u001b[32m0.6070\u001b[0m                     \u001b[35m0.6886\u001b[0m        \u001b[31m0.5952\u001b[0m  0.0075  0.8381\n",
      "     10                     \u001b[36m0.6773\u001b[0m        \u001b[32m0.5987\u001b[0m                     0.6702        0.6002  0.0069  0.8378\n",
      "     11                     0.6738        \u001b[32m0.5973\u001b[0m                     0.6688        0.6096  0.0063  0.8372\n",
      "     12                     0.6773        \u001b[32m0.5951\u001b[0m                     0.6640        0.6029  0.0057  0.8378\n",
      "     13                     \u001b[36m0.6878\u001b[0m        \u001b[32m0.5882\u001b[0m                     0.6758        0.5979  0.0050  0.8388\n",
      "     14                     0.6867        \u001b[32m0.5852\u001b[0m                     0.6688        0.6151  0.0043  0.8368\n",
      "     15                     \u001b[36m0.6882\u001b[0m        0.5864                     0.6783        0.6044  0.0037  0.8390\n",
      "     16                     \u001b[36m0.6923\u001b[0m        \u001b[32m0.5830\u001b[0m                     0.6802        \u001b[31m0.5876\u001b[0m  0.0031  0.8388\n",
      "     17                     \u001b[36m0.6982\u001b[0m        \u001b[32m0.5769\u001b[0m                     0.6837        0.5968  0.0025  0.8360\n",
      "     18                     \u001b[36m0.6993\u001b[0m        \u001b[32m0.5721\u001b[0m                     0.6736        0.5919  0.0020  0.8374\n",
      "     19                     \u001b[36m0.6998\u001b[0m        \u001b[32m0.5692\u001b[0m                     0.6838        0.5936  0.0015  0.8381\n",
      "     20                     \u001b[36m0.7055\u001b[0m        \u001b[32m0.5638\u001b[0m                     0.6838        0.5884  0.0010  0.8388\n",
      "     21                     0.7016        0.5665                     0.6758        0.5951  0.0007  0.8403\n",
      "     22                     \u001b[36m0.7071\u001b[0m        \u001b[32m0.5618\u001b[0m                     0.6860        0.5880  0.0004  0.8388\n",
      "     23                     0.7048        \u001b[32m0.5590\u001b[0m                     0.6806        0.5894  0.0002  0.8364\n",
      "     24                     \u001b[36m0.7083\u001b[0m        0.5599                     0.6825        0.5901  0.0000  0.8402\n",
      "     25                     \u001b[36m0.7104\u001b[0m        \u001b[32m0.5589\u001b[0m                     0.6784        0.5903  0.0000  0.8384\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5349\u001b[0m        \u001b[32m1.0906\u001b[0m                     \u001b[35m0.5827\u001b[0m        \u001b[31m0.7476\u001b[0m  0.0100  0.8372\n",
      "      2                     \u001b[36m0.5978\u001b[0m        \u001b[32m0.7416\u001b[0m                     \u001b[35m0.6340\u001b[0m        \u001b[31m0.6630\u001b[0m  0.0100  0.8381\n",
      "      3                     \u001b[36m0.6294\u001b[0m        \u001b[32m0.6639\u001b[0m                     \u001b[35m0.6693\u001b[0m        \u001b[31m0.6142\u001b[0m  0.0098  0.8379\n",
      "      4                     \u001b[36m0.6437\u001b[0m        \u001b[32m0.6459\u001b[0m                     0.6648        \u001b[31m0.6129\u001b[0m  0.0096  0.8368\n",
      "      5                     \u001b[36m0.6558\u001b[0m        \u001b[32m0.6318\u001b[0m                     0.6552        0.6192  0.0093  0.8371\n",
      "      6                     \u001b[36m0.6582\u001b[0m        \u001b[32m0.6213\u001b[0m                     \u001b[35m0.6769\u001b[0m        0.6146  0.0090  0.8368\n",
      "      7                     \u001b[36m0.6650\u001b[0m        \u001b[32m0.6129\u001b[0m                     0.6743        \u001b[31m0.6053\u001b[0m  0.0085  0.8381\n",
      "      8                     \u001b[36m0.6745\u001b[0m        \u001b[32m0.6089\u001b[0m                     \u001b[35m0.6823\u001b[0m        \u001b[31m0.5925\u001b[0m  0.0080  0.8391\n",
      "      9                     \u001b[36m0.6774\u001b[0m        \u001b[32m0.6017\u001b[0m                     \u001b[35m0.6852\u001b[0m        0.5971  0.0075  0.8379\n",
      "     10                     \u001b[36m0.6799\u001b[0m        \u001b[32m0.6010\u001b[0m                     0.6735        0.6063  0.0069  0.8384\n",
      "     11                     \u001b[36m0.6810\u001b[0m        \u001b[32m0.5983\u001b[0m                     0.6769        0.5989  0.0063  0.8391\n",
      "     12                     0.6791        \u001b[32m0.5959\u001b[0m                     0.6838        0.5948  0.0057  0.8388\n",
      "     13                     \u001b[36m0.6918\u001b[0m        \u001b[32m0.5883\u001b[0m                     0.6765        0.5950  0.0050  0.8398\n",
      "     14                     0.6886        0.5892                     0.6825        0.5954  0.0043  0.8377\n",
      "     15                     0.6917        \u001b[32m0.5837\u001b[0m                     \u001b[35m0.6888\u001b[0m        \u001b[31m0.5900\u001b[0m  0.0037  0.8371\n",
      "     16                     0.6888        \u001b[32m0.5835\u001b[0m                     \u001b[35m0.6917\u001b[0m        0.5976  0.0031  0.8378\n",
      "     17                     \u001b[36m0.6934\u001b[0m        \u001b[32m0.5783\u001b[0m                     \u001b[35m0.7030\u001b[0m        \u001b[31m0.5832\u001b[0m  0.0025  0.8394\n",
      "     18                     \u001b[36m0.7047\u001b[0m        \u001b[32m0.5722\u001b[0m                     0.6919        0.5851  0.0020  0.8368\n",
      "     19                     0.7026        \u001b[32m0.5657\u001b[0m                     0.6909        0.5854  0.0015  0.8393\n",
      "     20                     \u001b[36m0.7126\u001b[0m        \u001b[32m0.5628\u001b[0m                     0.6980        0.5845  0.0010  0.8387\n",
      "     21                     0.7122        \u001b[32m0.5604\u001b[0m                     0.6910        0.5867  0.0007  0.8389\n",
      "     22                     \u001b[36m0.7132\u001b[0m        \u001b[32m0.5581\u001b[0m                     0.6944        0.5871  0.0004  0.8388\n",
      "     23                     0.7108        0.5590                     0.6957        0.5863  0.0002  0.8378\n",
      "     24                     \u001b[36m0.7174\u001b[0m        \u001b[32m0.5502\u001b[0m                     0.6975        0.5850  0.0000  0.8388\n",
      "     25                     0.7134        0.5557                     0.6970        0.5855  0.0000  0.8379\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5302\u001b[0m        \u001b[32m1.0531\u001b[0m                     \u001b[35m0.5334\u001b[0m        \u001b[31m0.8936\u001b[0m  0.0100  0.8348\n",
      "      2                     \u001b[36m0.5610\u001b[0m        \u001b[32m0.8377\u001b[0m                     \u001b[35m0.6013\u001b[0m        \u001b[31m0.6833\u001b[0m  0.0100  0.8378\n",
      "      3                     \u001b[36m0.6106\u001b[0m        \u001b[32m0.6759\u001b[0m                     \u001b[35m0.6618\u001b[0m        \u001b[31m0.6253\u001b[0m  0.0098  0.8358\n",
      "      4                     \u001b[36m0.6387\u001b[0m        \u001b[32m0.6617\u001b[0m                     0.6387        0.6544  0.0096  0.8373\n",
      "      5                     \u001b[36m0.6559\u001b[0m        \u001b[32m0.6301\u001b[0m                     0.6534        0.6267  0.0093  0.8375\n",
      "      6                     \u001b[36m0.6575\u001b[0m        \u001b[32m0.6236\u001b[0m                     0.6466        0.6318  0.0090  0.8360\n",
      "      7                     \u001b[36m0.6708\u001b[0m        \u001b[32m0.6136\u001b[0m                     0.6581        \u001b[31m0.6124\u001b[0m  0.0085  0.8378\n",
      "      8                     0.6669        \u001b[32m0.6108\u001b[0m                     \u001b[35m0.6673\u001b[0m        \u001b[31m0.6048\u001b[0m  0.0080  0.8388\n",
      "      9                     \u001b[36m0.6768\u001b[0m        \u001b[32m0.6053\u001b[0m                     0.6494        0.6174  0.0075  0.8361\n",
      "     10                     0.6737        \u001b[32m0.6044\u001b[0m                     0.6646        0.6173  0.0069  0.8408\n",
      "     11                     0.6762        \u001b[32m0.6015\u001b[0m                     0.6647        0.6104  0.0063  0.8391\n",
      "     12                     \u001b[36m0.6811\u001b[0m        \u001b[32m0.5960\u001b[0m                     0.6664        0.6100  0.0057  0.8379\n",
      "     13                     \u001b[36m0.6832\u001b[0m        \u001b[32m0.5943\u001b[0m                     \u001b[35m0.6780\u001b[0m        \u001b[31m0.6048\u001b[0m  0.0050  0.8368\n",
      "     14                     \u001b[36m0.6844\u001b[0m        \u001b[32m0.5930\u001b[0m                     0.6757        0.6058  0.0043  0.8388\n",
      "     15                     \u001b[36m0.6847\u001b[0m        \u001b[32m0.5891\u001b[0m                     0.6777        \u001b[31m0.6037\u001b[0m  0.0037  0.8388\n",
      "     16                     \u001b[36m0.6909\u001b[0m        \u001b[32m0.5820\u001b[0m                     0.6675        0.6040  0.0031  0.8376\n",
      "     17                     \u001b[36m0.6960\u001b[0m        \u001b[32m0.5785\u001b[0m                     \u001b[35m0.6812\u001b[0m        \u001b[31m0.5982\u001b[0m  0.0025  0.8407\n",
      "     18                     \u001b[36m0.6988\u001b[0m        \u001b[32m0.5750\u001b[0m                     0.6737        0.6008  0.0020  0.8399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19                     \u001b[36m0.7006\u001b[0m        \u001b[32m0.5740\u001b[0m                     0.6797        0.6005  0.0015  0.8382\n",
      "     20                     \u001b[36m0.7035\u001b[0m        0.5746                     \u001b[35m0.6852\u001b[0m        0.6007  0.0010  0.8417\n",
      "     21                     0.7015        \u001b[32m0.5700\u001b[0m                     0.6812        0.5990  0.0007  0.8370\n",
      "     22                     \u001b[36m0.7058\u001b[0m        \u001b[32m0.5674\u001b[0m                     0.6835        0.5982  0.0004  0.8373\n",
      "     23                     \u001b[36m0.7092\u001b[0m        \u001b[32m0.5649\u001b[0m                     \u001b[35m0.6858\u001b[0m        \u001b[31m0.5981\u001b[0m  0.0002  0.8388\n",
      "     24                     \u001b[36m0.7141\u001b[0m        \u001b[32m0.5624\u001b[0m                     \u001b[35m0.6863\u001b[0m        0.5986  0.0000  0.8379\n",
      "     25                     0.7130        \u001b[32m0.5589\u001b[0m                     0.6839        \u001b[31m0.5978\u001b[0m  0.0000  0.8398\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5210\u001b[0m        \u001b[32m1.1038\u001b[0m                     \u001b[35m0.5923\u001b[0m        \u001b[31m0.6993\u001b[0m  0.0100  0.8338\n",
      "      2                     \u001b[36m0.6090\u001b[0m        \u001b[32m0.7208\u001b[0m                     0.5560        0.7210  0.0100  0.8378\n",
      "      3                     \u001b[36m0.6294\u001b[0m        \u001b[32m0.6813\u001b[0m                     \u001b[35m0.6294\u001b[0m        \u001b[31m0.6451\u001b[0m  0.0098  0.8375\n",
      "      4                     \u001b[36m0.6338\u001b[0m        \u001b[32m0.6603\u001b[0m                     \u001b[35m0.6506\u001b[0m        \u001b[31m0.6256\u001b[0m  0.0096  0.8373\n",
      "      5                     \u001b[36m0.6536\u001b[0m        \u001b[32m0.6375\u001b[0m                     \u001b[35m0.6557\u001b[0m        \u001b[31m0.6186\u001b[0m  0.0093  0.8378\n",
      "      6                     \u001b[36m0.6590\u001b[0m        \u001b[32m0.6234\u001b[0m                     \u001b[35m0.6637\u001b[0m        0.6229  0.0090  0.8378\n",
      "      7                     \u001b[36m0.6661\u001b[0m        \u001b[32m0.6201\u001b[0m                     \u001b[35m0.6658\u001b[0m        \u001b[31m0.6082\u001b[0m  0.0085  0.8377\n",
      "      8                     \u001b[36m0.6662\u001b[0m        \u001b[32m0.6136\u001b[0m                     \u001b[35m0.6673\u001b[0m        \u001b[31m0.6043\u001b[0m  0.0080  0.8367\n",
      "      9                     \u001b[36m0.6697\u001b[0m        \u001b[32m0.6062\u001b[0m                     0.6636        0.6101  0.0075  0.8378\n",
      "     10                     \u001b[36m0.6729\u001b[0m        \u001b[32m0.6023\u001b[0m                     \u001b[35m0.6768\u001b[0m        \u001b[31m0.5938\u001b[0m  0.0069  0.8380\n",
      "     11                     \u001b[36m0.6824\u001b[0m        \u001b[32m0.5976\u001b[0m                     0.6743        0.6019  0.0063  0.8386\n",
      "     12                     0.6822        \u001b[32m0.5971\u001b[0m                     0.6757        0.5984  0.0057  0.8373\n",
      "     13                     \u001b[36m0.6838\u001b[0m        \u001b[32m0.5926\u001b[0m                     \u001b[35m0.6799\u001b[0m        0.5939  0.0050  0.8376\n",
      "     14                     \u001b[36m0.6865\u001b[0m        \u001b[32m0.5864\u001b[0m                     \u001b[35m0.6843\u001b[0m        \u001b[31m0.5934\u001b[0m  0.0043  0.8372\n",
      "     15                     0.6854        0.5868                     0.6807        \u001b[31m0.5915\u001b[0m  0.0037  0.8383\n",
      "     16                     \u001b[36m0.6951\u001b[0m        \u001b[32m0.5850\u001b[0m                     \u001b[35m0.6871\u001b[0m        \u001b[31m0.5882\u001b[0m  0.0031  0.8397\n",
      "     17                     0.6924        \u001b[32m0.5803\u001b[0m                     \u001b[35m0.6975\u001b[0m        \u001b[31m0.5836\u001b[0m  0.0025  0.8397\n",
      "     18                     \u001b[36m0.6996\u001b[0m        \u001b[32m0.5773\u001b[0m                     0.6836        0.5878  0.0020  0.8378\n",
      "     19                     \u001b[36m0.7018\u001b[0m        \u001b[32m0.5718\u001b[0m                     0.6875        0.5862  0.0015  0.8395\n",
      "     20                     \u001b[36m0.7059\u001b[0m        \u001b[32m0.5673\u001b[0m                     0.6901        \u001b[31m0.5829\u001b[0m  0.0010  0.8369\n",
      "     21                     0.7046        \u001b[32m0.5636\u001b[0m                     0.6924        0.5837  0.0007  0.8387\n",
      "     22                     \u001b[36m0.7088\u001b[0m        \u001b[32m0.5608\u001b[0m                     0.6891        \u001b[31m0.5821\u001b[0m  0.0004  0.8379\n",
      "     23                     \u001b[36m0.7143\u001b[0m        \u001b[32m0.5566\u001b[0m                     0.6905        0.5830  0.0002  0.8379\n",
      "     24                     0.7110        0.5583                     0.6901        0.5827  0.0000  0.8369\n",
      "     25                     0.7118        0.5590                     0.6900        0.5827  0.0000  0.8391\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5426\u001b[0m        \u001b[32m1.0686\u001b[0m                     \u001b[35m0.5444\u001b[0m        \u001b[31m0.8035\u001b[0m  0.0100  0.8368\n",
      "      2                     \u001b[36m0.5802\u001b[0m        \u001b[32m0.7804\u001b[0m                     \u001b[35m0.6414\u001b[0m        \u001b[31m0.6653\u001b[0m  0.0100  0.8388\n",
      "      3                     \u001b[36m0.6215\u001b[0m        \u001b[32m0.6887\u001b[0m                     \u001b[35m0.6604\u001b[0m        \u001b[31m0.6321\u001b[0m  0.0098  0.8358\n",
      "      4                     \u001b[36m0.6487\u001b[0m        \u001b[32m0.6422\u001b[0m                     0.6374        \u001b[31m0.6272\u001b[0m  0.0096  0.8375\n",
      "      5                     \u001b[36m0.6621\u001b[0m        \u001b[32m0.6242\u001b[0m                     0.6512        0.6314  0.0093  0.8378\n",
      "      6                     0.6588        \u001b[32m0.6220\u001b[0m                     0.6510        \u001b[31m0.6192\u001b[0m  0.0090  0.8375\n",
      "      7                     \u001b[36m0.6700\u001b[0m        \u001b[32m0.6110\u001b[0m                     \u001b[35m0.6620\u001b[0m        \u001b[31m0.6017\u001b[0m  0.0085  0.8378\n",
      "      8                     \u001b[36m0.6729\u001b[0m        \u001b[32m0.6059\u001b[0m                     \u001b[35m0.6713\u001b[0m        0.6117  0.0080  0.8388\n",
      "      9                     0.6706        \u001b[32m0.6041\u001b[0m                     \u001b[35m0.6757\u001b[0m        \u001b[31m0.5968\u001b[0m  0.0075  0.8378\n",
      "     10                     \u001b[36m0.6849\u001b[0m        \u001b[32m0.6003\u001b[0m                     0.6705        0.6065  0.0069  0.8378\n",
      "     11                     0.6823        \u001b[32m0.5971\u001b[0m                     0.6695        0.6031  0.0063  0.8376\n",
      "     12                     0.6806        \u001b[32m0.5921\u001b[0m                     0.6691        0.6069  0.0057  0.8380\n",
      "     13                     0.6829        \u001b[32m0.5921\u001b[0m                     0.6687        0.6011  0.0050  0.8368\n",
      "     14                     0.6842        \u001b[32m0.5891\u001b[0m                     \u001b[35m0.6812\u001b[0m        \u001b[31m0.5948\u001b[0m  0.0043  0.8388\n",
      "     15                     \u001b[36m0.6902\u001b[0m        \u001b[32m0.5863\u001b[0m                     0.6722        \u001b[31m0.5922\u001b[0m  0.0037  0.8388\n",
      "     16                     \u001b[36m0.6976\u001b[0m        \u001b[32m0.5792\u001b[0m                     \u001b[35m0.6830\u001b[0m        0.5941  0.0031  0.8390\n",
      "     17                     \u001b[36m0.6977\u001b[0m        \u001b[32m0.5756\u001b[0m                     \u001b[35m0.6833\u001b[0m        0.5939  0.0025  0.8370\n",
      "     18                     \u001b[36m0.7006\u001b[0m        0.5763                     \u001b[35m0.6861\u001b[0m        0.5929  0.0020  0.8388\n",
      "     19                     \u001b[36m0.7024\u001b[0m        \u001b[32m0.5710\u001b[0m                     0.6789        0.5928  0.0015  0.8388\n",
      "     20                     0.6988        \u001b[32m0.5708\u001b[0m                     0.6805        \u001b[31m0.5872\u001b[0m  0.0010  0.8358\n",
      "     21                     \u001b[36m0.7054\u001b[0m        \u001b[32m0.5669\u001b[0m                     \u001b[35m0.6883\u001b[0m        \u001b[31m0.5865\u001b[0m  0.0007  0.8373\n",
      "     22                     \u001b[36m0.7064\u001b[0m        \u001b[32m0.5642\u001b[0m                     0.6820        \u001b[31m0.5864\u001b[0m  0.0004  0.8388\n",
      "     23                     \u001b[36m0.7075\u001b[0m        \u001b[32m0.5632\u001b[0m                     0.6840        0.5876  0.0002  0.8389\n",
      "     24                     \u001b[36m0.7132\u001b[0m        \u001b[32m0.5608\u001b[0m                     0.6801        0.5869  0.0000  0.8399\n",
      "     25                     0.7075        0.5610                     0.6862        0.5870  0.0000  0.8378\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5444\u001b[0m        \u001b[32m1.0166\u001b[0m                     \u001b[35m0.5373\u001b[0m        \u001b[31m0.7751\u001b[0m  0.0100  0.8358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                     \u001b[36m0.5956\u001b[0m        \u001b[32m0.7351\u001b[0m                     \u001b[35m0.6033\u001b[0m        \u001b[31m0.6685\u001b[0m  0.0100  0.8388\n",
      "      3                     \u001b[36m0.6306\u001b[0m        \u001b[32m0.6790\u001b[0m                     \u001b[35m0.6583\u001b[0m        \u001b[31m0.6335\u001b[0m  0.0098  0.8364\n",
      "      4                     \u001b[36m0.6462\u001b[0m        \u001b[32m0.6425\u001b[0m                     0.6432        \u001b[31m0.6326\u001b[0m  0.0096  0.8364\n",
      "      5                     \u001b[36m0.6479\u001b[0m        \u001b[32m0.6349\u001b[0m                     \u001b[35m0.6586\u001b[0m        \u001b[31m0.6167\u001b[0m  0.0093  0.8373\n",
      "      6                     \u001b[36m0.6492\u001b[0m        \u001b[32m0.6309\u001b[0m                     \u001b[35m0.6818\u001b[0m        \u001b[31m0.6014\u001b[0m  0.0090  0.8377\n",
      "      7                     \u001b[36m0.6664\u001b[0m        \u001b[32m0.6186\u001b[0m                     0.6767        0.6092  0.0085  0.8379\n",
      "      8                     0.6640        \u001b[32m0.6135\u001b[0m                     0.6777        0.6060  0.0080  0.8381\n",
      "      9                     \u001b[36m0.6723\u001b[0m        \u001b[32m0.6068\u001b[0m                     0.6760        0.6045  0.0075  0.8378\n",
      "     10                     0.6688        \u001b[32m0.6044\u001b[0m                     0.6674        0.6053  0.0069  0.8389\n",
      "     11                     \u001b[36m0.6727\u001b[0m        \u001b[32m0.6038\u001b[0m                     0.6740        \u001b[31m0.5951\u001b[0m  0.0063  0.8371\n",
      "     12                     \u001b[36m0.6765\u001b[0m        \u001b[32m0.5980\u001b[0m                     0.6769        0.5977  0.0057  0.8393\n",
      "     13                     \u001b[36m0.6788\u001b[0m        \u001b[32m0.5970\u001b[0m                     \u001b[35m0.6833\u001b[0m        \u001b[31m0.5891\u001b[0m  0.0050  0.8391\n",
      "     14                     \u001b[36m0.6792\u001b[0m        \u001b[32m0.5888\u001b[0m                     \u001b[35m0.6855\u001b[0m        \u001b[31m0.5871\u001b[0m  0.0043  0.8378\n",
      "     15                     \u001b[36m0.6900\u001b[0m        \u001b[32m0.5865\u001b[0m                     0.6813        0.5924  0.0037  0.8392\n",
      "     16                     0.6845        0.5866                     0.6854        0.5893  0.0031  0.8368\n",
      "     17                     \u001b[36m0.6984\u001b[0m        \u001b[32m0.5813\u001b[0m                     0.6783        \u001b[31m0.5857\u001b[0m  0.0025  0.8389\n",
      "     18                     0.6889        \u001b[32m0.5809\u001b[0m                     \u001b[35m0.6885\u001b[0m        \u001b[31m0.5852\u001b[0m  0.0020  0.8377\n",
      "     19                     0.6922        \u001b[32m0.5788\u001b[0m                     \u001b[35m0.6946\u001b[0m        \u001b[31m0.5828\u001b[0m  0.0015  0.8378\n",
      "     20                     0.6950        \u001b[32m0.5722\u001b[0m                     0.6915        \u001b[31m0.5812\u001b[0m  0.0010  0.8372\n",
      "     21                     \u001b[36m0.7036\u001b[0m        \u001b[32m0.5667\u001b[0m                     0.6937        \u001b[31m0.5806\u001b[0m  0.0007  0.8679\n",
      "     22                     0.7010        \u001b[32m0.5648\u001b[0m                     0.6934        0.5818  0.0004  0.8388\n",
      "     23                     0.7018        0.5654                     0.6933        \u001b[31m0.5803\u001b[0m  0.0002  0.8398\n",
      "     24                     \u001b[36m0.7049\u001b[0m        \u001b[32m0.5627\u001b[0m                     0.6924        \u001b[31m0.5795\u001b[0m  0.0000  0.8388\n",
      "     25                     0.7044        0.5652                     0.6924        0.5801  0.0000  0.8371\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5209\u001b[0m        \u001b[32m0.9700\u001b[0m                     \u001b[35m0.5566\u001b[0m        \u001b[31m0.6902\u001b[0m  0.0100  0.8352\n",
      "      2                     \u001b[36m0.5885\u001b[0m        \u001b[32m0.7318\u001b[0m                     \u001b[35m0.6385\u001b[0m        \u001b[31m0.6597\u001b[0m  0.0100  0.8386\n",
      "      3                     \u001b[36m0.6289\u001b[0m        \u001b[32m0.6648\u001b[0m                     \u001b[35m0.6486\u001b[0m        \u001b[31m0.6243\u001b[0m  0.0098  0.8388\n",
      "      4                     \u001b[36m0.6447\u001b[0m        \u001b[32m0.6411\u001b[0m                     \u001b[35m0.6552\u001b[0m        \u001b[31m0.6243\u001b[0m  0.0096  0.8398\n",
      "      5                     \u001b[36m0.6547\u001b[0m        \u001b[32m0.6302\u001b[0m                     0.6543        0.6275  0.0093  0.8376\n",
      "      6                     \u001b[36m0.6630\u001b[0m        \u001b[32m0.6216\u001b[0m                     0.6356        0.6340  0.0090  0.8379\n",
      "      7                     \u001b[36m0.6655\u001b[0m        \u001b[32m0.6215\u001b[0m                     0.6335        0.6414  0.0085  0.8393\n",
      "      8                     \u001b[36m0.6705\u001b[0m        \u001b[32m0.6056\u001b[0m                     \u001b[35m0.6577\u001b[0m        \u001b[31m0.6181\u001b[0m  0.0080  0.8379\n",
      "      9                     \u001b[36m0.6737\u001b[0m        \u001b[32m0.6022\u001b[0m                     0.6527        \u001b[31m0.6093\u001b[0m  0.0075  0.8391\n",
      "     10                     \u001b[36m0.6743\u001b[0m        0.6027                     0.6535        0.6164  0.0069  0.8378\n",
      "     11                     \u001b[36m0.6817\u001b[0m        \u001b[32m0.5967\u001b[0m                     \u001b[35m0.6621\u001b[0m        0.6116  0.0063  0.8398\n",
      "     12                     0.6785        \u001b[32m0.5933\u001b[0m                     \u001b[35m0.6674\u001b[0m        \u001b[31m0.6039\u001b[0m  0.0057  0.8398\n",
      "     13                     \u001b[36m0.6879\u001b[0m        \u001b[32m0.5887\u001b[0m                     0.6670        0.6042  0.0050  0.8381\n",
      "     14                     0.6867        \u001b[32m0.5850\u001b[0m                     0.6664        \u001b[31m0.6015\u001b[0m  0.0043  0.8395\n",
      "     15                     \u001b[36m0.6934\u001b[0m        \u001b[32m0.5824\u001b[0m                     \u001b[35m0.6730\u001b[0m        \u001b[31m0.6006\u001b[0m  0.0037  0.8360\n",
      "     16                     0.6902        \u001b[32m0.5761\u001b[0m                     \u001b[35m0.6774\u001b[0m        \u001b[31m0.5996\u001b[0m  0.0031  0.8388\n",
      "     17                     \u001b[36m0.6973\u001b[0m        \u001b[32m0.5754\u001b[0m                     0.6716        \u001b[31m0.5964\u001b[0m  0.0025  0.8393\n",
      "     18                     \u001b[36m0.7026\u001b[0m        \u001b[32m0.5722\u001b[0m                     0.6628        0.6008  0.0020  0.8386\n",
      "     19                     0.7008        \u001b[32m0.5722\u001b[0m                     0.6753        \u001b[31m0.5958\u001b[0m  0.0015  0.8388\n",
      "     20                     \u001b[36m0.7058\u001b[0m        \u001b[32m0.5664\u001b[0m                     \u001b[35m0.6793\u001b[0m        \u001b[31m0.5958\u001b[0m  0.0010  0.8389\n",
      "     21                     \u001b[36m0.7062\u001b[0m        \u001b[32m0.5628\u001b[0m                     \u001b[35m0.6807\u001b[0m        \u001b[31m0.5940\u001b[0m  0.0007  0.8391\n",
      "     22                     \u001b[36m0.7091\u001b[0m        \u001b[32m0.5623\u001b[0m                     0.6789        \u001b[31m0.5938\u001b[0m  0.0004  0.8407\n",
      "     23                     \u001b[36m0.7110\u001b[0m        \u001b[32m0.5599\u001b[0m                     0.6789        0.5940  0.0002  0.8388\n",
      "     24                     0.7065        \u001b[32m0.5588\u001b[0m                     0.6803        \u001b[31m0.5934\u001b[0m  0.0000  0.8408\n",
      "     25                     0.7099        \u001b[32m0.5570\u001b[0m                     0.6796        0.5942  0.0000  0.8402\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5243\u001b[0m        \u001b[32m1.0566\u001b[0m                     \u001b[35m0.5184\u001b[0m        \u001b[31m0.8107\u001b[0m  0.0100  0.8348\n",
      "      2                     \u001b[36m0.5758\u001b[0m        \u001b[32m0.7835\u001b[0m                     \u001b[35m0.5868\u001b[0m        \u001b[31m0.7225\u001b[0m  0.0100  0.8378\n",
      "      3                     \u001b[36m0.6002\u001b[0m        \u001b[32m0.7102\u001b[0m                     \u001b[35m0.6560\u001b[0m        \u001b[31m0.6418\u001b[0m  0.0098  0.8379\n",
      "      4                     \u001b[36m0.6391\u001b[0m        \u001b[32m0.6570\u001b[0m                     0.6507        \u001b[31m0.6255\u001b[0m  0.0096  0.8368\n",
      "      5                     \u001b[36m0.6466\u001b[0m        \u001b[32m0.6424\u001b[0m                     \u001b[35m0.6603\u001b[0m        \u001b[31m0.6235\u001b[0m  0.0093  0.8379\n",
      "      6                     \u001b[36m0.6575\u001b[0m        \u001b[32m0.6213\u001b[0m                     \u001b[35m0.6652\u001b[0m        \u001b[31m0.6192\u001b[0m  0.0090  0.8378\n",
      "      7                     \u001b[36m0.6661\u001b[0m        \u001b[32m0.6170\u001b[0m                     0.6550        \u001b[31m0.6190\u001b[0m  0.0085  0.8378\n",
      "      8                     \u001b[36m0.6670\u001b[0m        \u001b[32m0.6145\u001b[0m                     \u001b[35m0.6776\u001b[0m        \u001b[31m0.6111\u001b[0m  0.0080  0.8388\n",
      "      9                     \u001b[36m0.6723\u001b[0m        \u001b[32m0.6066\u001b[0m                     0.6667        \u001b[31m0.6076\u001b[0m  0.0075  0.8384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     \u001b[36m0.6752\u001b[0m        \u001b[32m0.6030\u001b[0m                     0.6730        0.6092  0.0069  0.8417\n",
      "     11                     \u001b[36m0.6811\u001b[0m        \u001b[32m0.5972\u001b[0m                     \u001b[35m0.6824\u001b[0m        \u001b[31m0.6015\u001b[0m  0.0063  0.8383\n",
      "     12                     0.6810        \u001b[32m0.5949\u001b[0m                     0.6773        \u001b[31m0.6008\u001b[0m  0.0057  0.8378\n",
      "     13                     0.6791        \u001b[32m0.5946\u001b[0m                     0.6769        \u001b[31m0.5989\u001b[0m  0.0050  0.8378\n",
      "     14                     \u001b[36m0.6876\u001b[0m        \u001b[32m0.5866\u001b[0m                     0.6680        0.5998  0.0043  0.8388\n",
      "     15                     \u001b[36m0.6876\u001b[0m        \u001b[32m0.5858\u001b[0m                     \u001b[35m0.6868\u001b[0m        \u001b[31m0.5973\u001b[0m  0.0037  0.8377\n",
      "     16                     \u001b[36m0.6900\u001b[0m        \u001b[32m0.5826\u001b[0m                     \u001b[35m0.6917\u001b[0m        \u001b[31m0.5934\u001b[0m  0.0031  0.8379\n",
      "     17                     \u001b[36m0.6975\u001b[0m        \u001b[32m0.5776\u001b[0m                     0.6818        0.5969  0.0025  0.8378\n",
      "     18                     0.6974        \u001b[32m0.5737\u001b[0m                     0.6843        0.5960  0.0020  0.8388\n",
      "     19                     \u001b[36m0.7029\u001b[0m        \u001b[32m0.5706\u001b[0m                     \u001b[35m0.6940\u001b[0m        \u001b[31m0.5893\u001b[0m  0.0015  0.8374\n",
      "     20                     \u001b[36m0.7055\u001b[0m        \u001b[32m0.5705\u001b[0m                     0.6909        0.5904  0.0010  0.8388\n",
      "     21                     0.7006        \u001b[32m0.5685\u001b[0m                     0.6882        \u001b[31m0.5888\u001b[0m  0.0007  0.8388\n",
      "     22                     \u001b[36m0.7063\u001b[0m        \u001b[32m0.5643\u001b[0m                     0.6918        \u001b[31m0.5875\u001b[0m  0.0004  0.8388\n",
      "     23                     \u001b[36m0.7069\u001b[0m        \u001b[32m0.5613\u001b[0m                     0.6864        0.5879  0.0002  0.8388\n",
      "     24                     \u001b[36m0.7075\u001b[0m        \u001b[32m0.5607\u001b[0m                     0.6909        0.5881  0.0000  0.8378\n",
      "     25                     \u001b[36m0.7114\u001b[0m        \u001b[32m0.5602\u001b[0m                     0.6931        0.5883  0.0000  0.8397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 0 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5357\u001b[0m        \u001b[32m0.8717\u001b[0m                     \u001b[35m0.5750\u001b[0m        \u001b[31m0.6844\u001b[0m  0.0100  3.3772\n",
      "      2                     \u001b[36m0.5663\u001b[0m        \u001b[32m0.6976\u001b[0m                     0.5728        \u001b[31m0.6768\u001b[0m  0.0100  3.0227\n",
      "      3                     \u001b[36m0.5820\u001b[0m        \u001b[32m0.6770\u001b[0m                     0.5658        0.6774  0.0098  3.0226\n",
      "      4                     0.5803        \u001b[32m0.6762\u001b[0m                     \u001b[35m0.5849\u001b[0m        \u001b[31m0.6690\u001b[0m  0.0096  3.0226\n",
      "      5                     \u001b[36m0.5828\u001b[0m        \u001b[32m0.6750\u001b[0m                     0.5829        0.6763  0.0093  3.0207\n",
      "      6                     \u001b[36m0.5834\u001b[0m        \u001b[32m0.6737\u001b[0m                     \u001b[35m0.5874\u001b[0m        0.6699  0.0090  3.0221\n",
      "      7                     0.5822        0.6744                     0.5762        0.6731  0.0085  3.0246\n",
      "      8                     \u001b[36m0.5840\u001b[0m        0.6738                     0.5791        0.6739  0.0080  3.0246\n",
      "      9                     \u001b[36m0.5853\u001b[0m        \u001b[32m0.6729\u001b[0m                     \u001b[35m0.5915\u001b[0m        \u001b[31m0.6681\u001b[0m  0.0075  3.0224\n",
      "     10                     \u001b[36m0.5872\u001b[0m        \u001b[32m0.6704\u001b[0m                     0.5877        \u001b[31m0.6680\u001b[0m  0.0069  3.0223\n",
      "     11                     \u001b[36m0.5948\u001b[0m        \u001b[32m0.6682\u001b[0m                     0.5832        0.6708  0.0063  3.0204\n",
      "     12                     0.5943        \u001b[32m0.6676\u001b[0m                     \u001b[35m0.5962\u001b[0m        \u001b[31m0.6672\u001b[0m  0.0057  3.0240\n",
      "     13                     \u001b[36m0.5958\u001b[0m        \u001b[32m0.6664\u001b[0m                     0.5929        \u001b[31m0.6671\u001b[0m  0.0050  3.0248\n",
      "     14                     0.5957        \u001b[32m0.6653\u001b[0m                     0.5847        0.6680  0.0043  3.0242\n",
      "     15                     \u001b[36m0.6023\u001b[0m        \u001b[32m0.6618\u001b[0m                     \u001b[35m0.6017\u001b[0m        \u001b[31m0.6657\u001b[0m  0.0037  3.0247\n",
      "     16                     \u001b[36m0.6070\u001b[0m        \u001b[32m0.6598\u001b[0m                     0.5814        0.6701  0.0031  3.0262\n",
      "     17                     0.6061        \u001b[32m0.6579\u001b[0m                     0.5947        \u001b[31m0.6650\u001b[0m  0.0025  3.0411\n",
      "     18                     \u001b[36m0.6097\u001b[0m        \u001b[32m0.6568\u001b[0m                     0.5966        \u001b[31m0.6648\u001b[0m  0.0020  3.0256\n",
      "     19                     \u001b[36m0.6126\u001b[0m        \u001b[32m0.6539\u001b[0m                     0.5925        0.6649  0.0015  3.0272\n",
      "     20                     \u001b[36m0.6159\u001b[0m        \u001b[32m0.6517\u001b[0m                     0.5986        0.6649  0.0010  3.0259\n",
      "     21                     0.6145        0.6532                     0.5966        0.6651  0.0007  3.0277\n",
      "     22                     \u001b[36m0.6217\u001b[0m        \u001b[32m0.6492\u001b[0m                     0.5986        \u001b[31m0.6647\u001b[0m  0.0004  3.0271\n",
      "     23                     0.6213        0.6493                     0.5969        0.6647  0.0002  3.0276\n",
      "     24                     0.6177        0.6505                     0.5977        0.6654  0.0000  3.0289\n",
      "     25                     0.6192        0.6497                     0.5975        \u001b[31m0.6645\u001b[0m  0.0000  3.0286\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5363\u001b[0m        \u001b[32m0.8517\u001b[0m                     \u001b[35m0.5502\u001b[0m        \u001b[31m0.6876\u001b[0m  0.0100  3.0195\n",
      "      2                     \u001b[36m0.5674\u001b[0m        \u001b[32m0.6866\u001b[0m                     \u001b[35m0.5871\u001b[0m        \u001b[31m0.6733\u001b[0m  0.0100  3.0214\n",
      "      3                     \u001b[36m0.5763\u001b[0m        \u001b[32m0.6771\u001b[0m                     \u001b[35m0.5880\u001b[0m        0.6736  0.0098  3.0235\n",
      "      4                     \u001b[36m0.5763\u001b[0m        0.6778                     0.5851        0.6734  0.0096  3.0211\n",
      "      5                     \u001b[36m0.5798\u001b[0m        0.6771                     0.5774        0.6771  0.0093  3.0228\n",
      "      6                     \u001b[36m0.5799\u001b[0m        \u001b[32m0.6750\u001b[0m                     0.5487        0.6952  0.0090  3.0254\n",
      "      7                     0.5794        \u001b[32m0.6749\u001b[0m                     \u001b[35m0.5896\u001b[0m        \u001b[31m0.6711\u001b[0m  0.0085  3.0304\n",
      "      8                     \u001b[36m0.5844\u001b[0m        \u001b[32m0.6732\u001b[0m                     0.5774        0.6733  0.0080  3.0284\n",
      "      9                     0.5811        0.6734                     0.5834        0.6731  0.0075  3.0312\n",
      "     10                     \u001b[36m0.5889\u001b[0m        \u001b[32m0.6719\u001b[0m                     0.5684        0.6788  0.0069  3.0303\n",
      "     11                     \u001b[36m0.5900\u001b[0m        \u001b[32m0.6714\u001b[0m                     0.5867        \u001b[31m0.6691\u001b[0m  0.0063  3.0309\n",
      "     12                     0.5886        \u001b[32m0.6686\u001b[0m                     0.5821        0.6788  0.0057  3.0318\n",
      "     13                     \u001b[36m0.5909\u001b[0m        \u001b[32m0.6673\u001b[0m                     0.5879        0.6768  0.0050  3.0311\n",
      "     14                     \u001b[36m0.5950\u001b[0m        \u001b[32m0.6653\u001b[0m                     \u001b[35m0.5922\u001b[0m        \u001b[31m0.6687\u001b[0m  0.0043  3.0352\n",
      "     15                     \u001b[36m0.5986\u001b[0m        \u001b[32m0.6634\u001b[0m                     \u001b[35m0.5972\u001b[0m        \u001b[31m0.6656\u001b[0m  0.0037  3.0311\n",
      "     16                     \u001b[36m0.6013\u001b[0m        \u001b[32m0.6623\u001b[0m                     0.5861        0.6684  0.0031  3.0307\n",
      "     17                     \u001b[36m0.6055\u001b[0m        \u001b[32m0.6603\u001b[0m                     0.5833        0.6695  0.0025  3.0337\n",
      "     18                     0.6042        \u001b[32m0.6582\u001b[0m                     0.5903        0.6675  0.0020  3.0319\n",
      "     19                     \u001b[36m0.6110\u001b[0m        \u001b[32m0.6562\u001b[0m                     0.5927        0.6660  0.0015  3.0345\n",
      "     20                     \u001b[36m0.6129\u001b[0m        \u001b[32m0.6558\u001b[0m                     \u001b[35m0.5997\u001b[0m        \u001b[31m0.6647\u001b[0m  0.0010  3.0344\n",
      "     21                     0.6117        \u001b[32m0.6533\u001b[0m                     0.5921        0.6675  0.0007  3.0386\n",
      "     22                     \u001b[36m0.6131\u001b[0m        \u001b[32m0.6532\u001b[0m                     0.5989        0.6652  0.0004  3.0364\n",
      "     23                     \u001b[36m0.6149\u001b[0m        \u001b[32m0.6513\u001b[0m                     0.5929        0.6650  0.0002  3.0359\n",
      "     24                     \u001b[36m0.6167\u001b[0m        0.6519                     0.5983        0.6651  0.0000  3.0351\n",
      "     25                     \u001b[36m0.6168\u001b[0m        \u001b[32m0.6507\u001b[0m                     0.5953        0.6650  0.0000  3.0364\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5374\u001b[0m        \u001b[32m0.8537\u001b[0m                     \u001b[35m0.5555\u001b[0m        \u001b[31m0.7122\u001b[0m  0.0100  3.0175\n",
      "      2                     \u001b[36m0.5663\u001b[0m        \u001b[32m0.6947\u001b[0m                     \u001b[35m0.5648\u001b[0m        \u001b[31m0.6769\u001b[0m  0.0100  3.0274\n",
      "      3                     \u001b[36m0.5748\u001b[0m        \u001b[32m0.6778\u001b[0m                     \u001b[35m0.5991\u001b[0m        \u001b[31m0.6670\u001b[0m  0.0098  3.0271\n",
      "      4                     \u001b[36m0.5801\u001b[0m        \u001b[32m0.6760\u001b[0m                     0.5855        0.6700  0.0096  3.0284\n",
      "      5                     \u001b[36m0.5839\u001b[0m        \u001b[32m0.6758\u001b[0m                     0.5944        0.6685  0.0093  3.0280\n",
      "      6                     0.5825        \u001b[32m0.6752\u001b[0m                     0.5864        0.6691  0.0090  3.0607\n",
      "      7                     \u001b[36m0.5849\u001b[0m        \u001b[32m0.6734\u001b[0m                     \u001b[35m0.6049\u001b[0m        \u001b[31m0.6661\u001b[0m  0.0085  3.0292\n",
      "      8                     0.5838        \u001b[32m0.6729\u001b[0m                     0.5775        0.6726  0.0080  3.0304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     0.5837        0.6739                     0.5960        \u001b[31m0.6659\u001b[0m  0.0075  3.0291\n",
      "     10                     \u001b[36m0.5899\u001b[0m        \u001b[32m0.6709\u001b[0m                     0.5943        0.6672  0.0069  3.0307\n",
      "     11                     \u001b[36m0.5900\u001b[0m        0.6713                     0.5885        0.6717  0.0063  3.0288\n",
      "     12                     0.5890        \u001b[32m0.6698\u001b[0m                     0.5953        0.6680  0.0057  3.0324\n",
      "     13                     \u001b[36m0.5908\u001b[0m        0.6701                     0.5867        0.6703  0.0050  3.0311\n",
      "     14                     \u001b[36m0.5943\u001b[0m        \u001b[32m0.6668\u001b[0m                     0.6004        \u001b[31m0.6628\u001b[0m  0.0043  3.0318\n",
      "     15                     \u001b[36m0.5962\u001b[0m        \u001b[32m0.6657\u001b[0m                     0.5839        0.6716  0.0037  3.0314\n",
      "     16                     \u001b[36m0.6000\u001b[0m        \u001b[32m0.6637\u001b[0m                     0.6028        \u001b[31m0.6616\u001b[0m  0.0031  3.0332\n",
      "     17                     \u001b[36m0.6019\u001b[0m        \u001b[32m0.6609\u001b[0m                     0.6008        \u001b[31m0.6601\u001b[0m  0.0025  3.0308\n",
      "     18                     \u001b[36m0.6024\u001b[0m        \u001b[32m0.6581\u001b[0m                     \u001b[35m0.6095\u001b[0m        \u001b[31m0.6588\u001b[0m  0.0020  3.0322\n",
      "     19                     \u001b[36m0.6099\u001b[0m        \u001b[32m0.6571\u001b[0m                     \u001b[35m0.6107\u001b[0m        \u001b[31m0.6587\u001b[0m  0.0015  3.0284\n",
      "     20                     \u001b[36m0.6123\u001b[0m        \u001b[32m0.6552\u001b[0m                     0.6084        \u001b[31m0.6585\u001b[0m  0.0010  3.0314\n",
      "     21                     \u001b[36m0.6131\u001b[0m        \u001b[32m0.6541\u001b[0m                     0.6101        \u001b[31m0.6580\u001b[0m  0.0007  3.0317\n",
      "     22                     0.6129        \u001b[32m0.6521\u001b[0m                     0.6091        0.6581  0.0004  3.0325\n",
      "     23                     \u001b[36m0.6137\u001b[0m        \u001b[32m0.6513\u001b[0m                     0.6082        0.6581  0.0002  3.0321\n",
      "     24                     \u001b[36m0.6145\u001b[0m        0.6518                     0.6073        0.6593  0.0000  3.0337\n",
      "     25                     \u001b[36m0.6182\u001b[0m        \u001b[32m0.6504\u001b[0m                     0.6080        0.6581  0.0000  3.0350\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5352\u001b[0m        \u001b[32m0.8520\u001b[0m                     \u001b[35m0.5595\u001b[0m        \u001b[31m0.6936\u001b[0m  0.0100  3.0169\n",
      "      2                     \u001b[36m0.5625\u001b[0m        \u001b[32m0.6874\u001b[0m                     \u001b[35m0.5947\u001b[0m        \u001b[31m0.6687\u001b[0m  0.0100  3.0252\n",
      "      3                     \u001b[36m0.5721\u001b[0m        \u001b[32m0.6779\u001b[0m                     0.5885        0.6698  0.0098  3.0303\n",
      "      4                     \u001b[36m0.5794\u001b[0m        \u001b[32m0.6771\u001b[0m                     0.5843        0.6751  0.0096  3.0289\n",
      "      5                     \u001b[36m0.5800\u001b[0m        \u001b[32m0.6761\u001b[0m                     0.5847        0.6716  0.0093  3.0256\n",
      "      6                     \u001b[36m0.5831\u001b[0m        \u001b[32m0.6742\u001b[0m                     0.5906        0.6689  0.0090  3.0283\n",
      "      7                     0.5821        0.6748                     0.5945        0.6691  0.0085  3.0300\n",
      "      8                     \u001b[36m0.5856\u001b[0m        \u001b[32m0.6716\u001b[0m                     \u001b[35m0.5958\u001b[0m        \u001b[31m0.6678\u001b[0m  0.0080  3.0286\n",
      "      9                     \u001b[36m0.5858\u001b[0m        0.6724                     0.5813        0.6760  0.0075  3.0338\n",
      "     10                     \u001b[36m0.5896\u001b[0m        0.6719                     0.5835        0.6752  0.0069  3.0299\n",
      "     11                     0.5884        \u001b[32m0.6712\u001b[0m                     0.5878        0.6725  0.0063  3.0289\n",
      "     12                     \u001b[36m0.5919\u001b[0m        \u001b[32m0.6685\u001b[0m                     \u001b[35m0.5971\u001b[0m        0.6701  0.0057  3.0297\n",
      "     13                     \u001b[36m0.5966\u001b[0m        \u001b[32m0.6672\u001b[0m                     \u001b[35m0.5989\u001b[0m        \u001b[31m0.6643\u001b[0m  0.0050  3.0298\n",
      "     14                     \u001b[36m0.5978\u001b[0m        \u001b[32m0.6648\u001b[0m                     \u001b[35m0.5991\u001b[0m        0.6654  0.0043  3.0293\n",
      "     15                     \u001b[36m0.6014\u001b[0m        \u001b[32m0.6624\u001b[0m                     0.5953        0.6655  0.0037  3.0313\n",
      "     16                     \u001b[36m0.6041\u001b[0m        \u001b[32m0.6609\u001b[0m                     0.5986        0.6658  0.0031  3.0321\n",
      "     17                     0.6036        \u001b[32m0.6608\u001b[0m                     0.5933        0.6669  0.0025  3.0328\n",
      "     18                     \u001b[36m0.6100\u001b[0m        \u001b[32m0.6569\u001b[0m                     0.5931        0.6664  0.0020  3.0318\n",
      "     19                     \u001b[36m0.6112\u001b[0m        \u001b[32m0.6542\u001b[0m                     0.5935        0.6652  0.0015  3.0312\n",
      "     20                     \u001b[36m0.6129\u001b[0m        \u001b[32m0.6534\u001b[0m                     0.5948        0.6657  0.0010  3.0319\n",
      "     21                     \u001b[36m0.6153\u001b[0m        \u001b[32m0.6515\u001b[0m                     \u001b[35m0.6002\u001b[0m        \u001b[31m0.6642\u001b[0m  0.0007  3.0305\n",
      "     22                     \u001b[36m0.6206\u001b[0m        \u001b[32m0.6502\u001b[0m                     0.5971        0.6648  0.0004  3.0369\n",
      "     23                     0.6144        0.6503                     \u001b[35m0.6013\u001b[0m        0.6647  0.0002  3.0351\n",
      "     24                     0.6158        \u001b[32m0.6495\u001b[0m                     0.5957        \u001b[31m0.6640\u001b[0m  0.0000  3.0345\n",
      "     25                     0.6191        \u001b[32m0.6490\u001b[0m                     0.5980        \u001b[31m0.6639\u001b[0m  0.0000  3.0362\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5259\u001b[0m        \u001b[32m0.8754\u001b[0m                     \u001b[35m0.5208\u001b[0m        \u001b[31m0.6910\u001b[0m  0.0100  3.0208\n",
      "      2                     \u001b[36m0.5642\u001b[0m        \u001b[32m0.6850\u001b[0m                     \u001b[35m0.5812\u001b[0m        \u001b[31m0.6758\u001b[0m  0.0100  3.0276\n",
      "      3                     \u001b[36m0.5762\u001b[0m        \u001b[32m0.6786\u001b[0m                     \u001b[35m0.5910\u001b[0m        \u001b[31m0.6757\u001b[0m  0.0098  3.0290\n",
      "      4                     \u001b[36m0.5787\u001b[0m        \u001b[32m0.6772\u001b[0m                     0.5813        \u001b[31m0.6750\u001b[0m  0.0096  3.0320\n",
      "      5                     \u001b[36m0.5796\u001b[0m        \u001b[32m0.6768\u001b[0m                     \u001b[35m0.5962\u001b[0m        \u001b[31m0.6680\u001b[0m  0.0093  3.0292\n",
      "      6                     \u001b[36m0.5812\u001b[0m        \u001b[32m0.6750\u001b[0m                     0.5587        0.6794  0.0090  3.0279\n",
      "      7                     0.5789        \u001b[32m0.6744\u001b[0m                     0.5894        0.6730  0.0085  3.0292\n",
      "      8                     0.5811        \u001b[32m0.6742\u001b[0m                     0.5822        0.6749  0.0080  3.0298\n",
      "      9                     0.5798        0.6744                     \u001b[35m0.6049\u001b[0m        \u001b[31m0.6669\u001b[0m  0.0075  3.0289\n",
      "     10                     \u001b[36m0.5872\u001b[0m        \u001b[32m0.6711\u001b[0m                     0.5572        0.6821  0.0069  3.0304\n",
      "     11                     \u001b[36m0.5901\u001b[0m        \u001b[32m0.6703\u001b[0m                     0.5695        0.6786  0.0063  3.0292\n",
      "     12                     0.5864        \u001b[32m0.6688\u001b[0m                     \u001b[35m0.6075\u001b[0m        \u001b[31m0.6628\u001b[0m  0.0057  3.0292\n",
      "     13                     0.5901        \u001b[32m0.6672\u001b[0m                     \u001b[35m0.6112\u001b[0m        \u001b[31m0.6617\u001b[0m  0.0050  3.0305\n",
      "     14                     \u001b[36m0.5958\u001b[0m        \u001b[32m0.6649\u001b[0m                     0.5986        0.6663  0.0043  3.0320\n",
      "     15                     \u001b[36m0.5992\u001b[0m        \u001b[32m0.6626\u001b[0m                     0.5964        0.6674  0.0037  3.0324\n",
      "     16                     \u001b[36m0.6051\u001b[0m        \u001b[32m0.6598\u001b[0m                     0.5854        0.6696  0.0031  3.0347\n",
      "     17                     \u001b[36m0.6071\u001b[0m        \u001b[32m0.6580\u001b[0m                     0.6048        0.6627  0.0025  3.0303\n",
      "     18                     0.6062        \u001b[32m0.6570\u001b[0m                     0.5966        0.6639  0.0020  3.0358\n",
      "     19                     \u001b[36m0.6113\u001b[0m        \u001b[32m0.6542\u001b[0m                     0.6072        0.6619  0.0015  3.0314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                     \u001b[36m0.6177\u001b[0m        \u001b[32m0.6525\u001b[0m                     0.6062        0.6619  0.0010  3.0326\n",
      "     21                     0.6109        0.6537                     0.6045        \u001b[31m0.6615\u001b[0m  0.0007  3.0323\n",
      "     22                     0.6112        \u001b[32m0.6524\u001b[0m                     0.6044        \u001b[31m0.6615\u001b[0m  0.0004  3.0350\n",
      "     23                     0.6172        \u001b[32m0.6501\u001b[0m                     0.6047        \u001b[31m0.6614\u001b[0m  0.0002  3.0343\n",
      "     24                     0.6163        \u001b[32m0.6487\u001b[0m                     0.6035        0.6622  0.0000  3.0344\n",
      "     25                     0.6148        0.6496                     0.6041        0.6622  0.0000  3.0354\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5329\u001b[0m        \u001b[32m0.8550\u001b[0m                     \u001b[35m0.5575\u001b[0m        \u001b[31m0.6922\u001b[0m  0.0100  3.0194\n",
      "      2                     \u001b[36m0.5620\u001b[0m        \u001b[32m0.6929\u001b[0m                     \u001b[35m0.5843\u001b[0m        \u001b[31m0.6742\u001b[0m  0.0100  3.0271\n",
      "      3                     \u001b[36m0.5795\u001b[0m        \u001b[32m0.6763\u001b[0m                     \u001b[35m0.5871\u001b[0m        0.6752  0.0098  3.0272\n",
      "      4                     \u001b[36m0.5797\u001b[0m        \u001b[32m0.6755\u001b[0m                     \u001b[35m0.5889\u001b[0m        \u001b[31m0.6713\u001b[0m  0.0096  3.0280\n",
      "      5                     0.5790        \u001b[32m0.6752\u001b[0m                     \u001b[35m0.5896\u001b[0m        0.6731  0.0093  3.0295\n",
      "      6                     0.5796        0.6757                     0.5745        0.6769  0.0090  3.0294\n",
      "      7                     \u001b[36m0.5844\u001b[0m        \u001b[32m0.6718\u001b[0m                     \u001b[35m0.5920\u001b[0m        0.6727  0.0085  3.0276\n",
      "      8                     \u001b[36m0.5865\u001b[0m        \u001b[32m0.6716\u001b[0m                     0.5833        0.6754  0.0080  3.0280\n",
      "      9                     0.5860        0.6724                     0.5766        0.6756  0.0075  3.0288\n",
      "     10                     \u001b[36m0.5884\u001b[0m        \u001b[32m0.6699\u001b[0m                     0.5823        0.6784  0.0069  3.0300\n",
      "     11                     \u001b[36m0.5888\u001b[0m        \u001b[32m0.6693\u001b[0m                     \u001b[35m0.5940\u001b[0m        \u001b[31m0.6697\u001b[0m  0.0063  3.0266\n",
      "     12                     \u001b[36m0.5893\u001b[0m        \u001b[32m0.6681\u001b[0m                     0.5861        0.6724  0.0057  3.0319\n",
      "     13                     \u001b[36m0.5966\u001b[0m        \u001b[32m0.6651\u001b[0m                     \u001b[35m0.5977\u001b[0m        0.6742  0.0050  3.0291\n",
      "     14                     \u001b[36m0.5994\u001b[0m        \u001b[32m0.6637\u001b[0m                     0.5785        0.6777  0.0043  3.0317\n",
      "     15                     \u001b[36m0.6021\u001b[0m        \u001b[32m0.6619\u001b[0m                     0.5971        \u001b[31m0.6675\u001b[0m  0.0037  3.0339\n",
      "     16                     \u001b[36m0.6043\u001b[0m        \u001b[32m0.6589\u001b[0m                     0.5946        0.6686  0.0031  3.0298\n",
      "     17                     \u001b[36m0.6070\u001b[0m        \u001b[32m0.6582\u001b[0m                     0.5968        \u001b[31m0.6668\u001b[0m  0.0025  3.0342\n",
      "     18                     \u001b[36m0.6116\u001b[0m        \u001b[32m0.6552\u001b[0m                     0.5938        0.6686  0.0020  3.0380\n",
      "     19                     \u001b[36m0.6132\u001b[0m        \u001b[32m0.6541\u001b[0m                     0.5958        0.6676  0.0015  3.0296\n",
      "     20                     0.6113        \u001b[32m0.6531\u001b[0m                     \u001b[35m0.5994\u001b[0m        \u001b[31m0.6667\u001b[0m  0.0010  3.0310\n",
      "     21                     \u001b[36m0.6141\u001b[0m        \u001b[32m0.6515\u001b[0m                     \u001b[35m0.6012\u001b[0m        0.6672  0.0007  3.0346\n",
      "     22                     \u001b[36m0.6163\u001b[0m        \u001b[32m0.6515\u001b[0m                     0.5993        0.6668  0.0004  3.0339\n",
      "     23                     0.6154        \u001b[32m0.6503\u001b[0m                     0.5988        \u001b[31m0.6667\u001b[0m  0.0002  3.0349\n",
      "     24                     \u001b[36m0.6196\u001b[0m        \u001b[32m0.6496\u001b[0m                     0.5962        0.6668  0.0000  3.0359\n",
      "     25                     0.6157        0.6501                     \u001b[35m0.6016\u001b[0m        \u001b[31m0.6658\u001b[0m  0.0000  3.0312\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5247\u001b[0m        \u001b[32m0.8578\u001b[0m                     \u001b[35m0.5386\u001b[0m        \u001b[31m0.6949\u001b[0m  0.0100  3.0209\n",
      "      2                     \u001b[36m0.5621\u001b[0m        \u001b[32m0.6924\u001b[0m                     \u001b[35m0.5750\u001b[0m        \u001b[31m0.6783\u001b[0m  0.0100  3.0300\n",
      "      3                     \u001b[36m0.5732\u001b[0m        \u001b[32m0.6767\u001b[0m                     \u001b[35m0.5913\u001b[0m        \u001b[31m0.6707\u001b[0m  0.0098  3.0268\n",
      "      4                     \u001b[36m0.5759\u001b[0m        0.6768                     0.5823        0.6805  0.0096  3.0311\n",
      "      5                     \u001b[36m0.5798\u001b[0m        \u001b[32m0.6762\u001b[0m                     0.5868        0.6734  0.0093  3.0321\n",
      "      6                     0.5780        \u001b[32m0.6760\u001b[0m                     0.5694        0.6771  0.0090  3.0335\n",
      "      7                     \u001b[36m0.5836\u001b[0m        \u001b[32m0.6754\u001b[0m                     \u001b[35m0.6022\u001b[0m        \u001b[31m0.6673\u001b[0m  0.0085  3.0291\n",
      "      8                     \u001b[36m0.5872\u001b[0m        \u001b[32m0.6751\u001b[0m                     0.5786        0.6780  0.0080  3.0298\n",
      "      9                     0.5851        \u001b[32m0.6732\u001b[0m                     0.5756        0.6829  0.0075  3.0305\n",
      "     10                     0.5850        \u001b[32m0.6715\u001b[0m                     0.5923        0.6740  0.0069  3.0308\n",
      "     11                     \u001b[36m0.5875\u001b[0m        0.6723                     0.5903        0.6701  0.0063  3.0312\n",
      "     12                     \u001b[36m0.5912\u001b[0m        \u001b[32m0.6699\u001b[0m                     0.5948        0.6701  0.0057  3.0318\n",
      "     13                     0.5907        \u001b[32m0.6688\u001b[0m                     0.5961        \u001b[31m0.6670\u001b[0m  0.0050  3.0312\n",
      "     14                     \u001b[36m0.5913\u001b[0m        \u001b[32m0.6665\u001b[0m                     0.5863        0.6683  0.0043  3.0301\n",
      "     15                     \u001b[36m0.6033\u001b[0m        \u001b[32m0.6629\u001b[0m                     0.5898        \u001b[31m0.6654\u001b[0m  0.0037  3.0307\n",
      "     16                     \u001b[36m0.6039\u001b[0m        \u001b[32m0.6610\u001b[0m                     0.5957        \u001b[31m0.6647\u001b[0m  0.0031  3.0312\n",
      "     17                     0.6032        \u001b[32m0.6602\u001b[0m                     0.5969        0.6654  0.0025  3.0333\n",
      "     18                     \u001b[36m0.6070\u001b[0m        \u001b[32m0.6584\u001b[0m                     0.5992        \u001b[31m0.6622\u001b[0m  0.0020  3.0332\n",
      "     19                     \u001b[36m0.6087\u001b[0m        \u001b[32m0.6572\u001b[0m                     \u001b[35m0.6026\u001b[0m        0.6634  0.0015  3.0321\n",
      "     20                     \u001b[36m0.6092\u001b[0m        \u001b[32m0.6552\u001b[0m                     0.6004        \u001b[31m0.6620\u001b[0m  0.0010  3.0328\n",
      "     21                     \u001b[36m0.6134\u001b[0m        \u001b[32m0.6537\u001b[0m                     0.5998        0.6625  0.0007  3.0318\n",
      "     22                     0.6119        \u001b[32m0.6524\u001b[0m                     \u001b[35m0.6032\u001b[0m        \u001b[31m0.6616\u001b[0m  0.0004  3.0357\n",
      "     23                     \u001b[36m0.6171\u001b[0m        \u001b[32m0.6518\u001b[0m                     \u001b[35m0.6045\u001b[0m        0.6626  0.0002  3.0329\n",
      "     24                     \u001b[36m0.6220\u001b[0m        \u001b[32m0.6505\u001b[0m                     0.5997        0.6616  0.0000  3.0370\n",
      "     25                     0.6145        0.6514                     0.6016        0.6621  0.0000  3.0349\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5282\u001b[0m        \u001b[32m0.8659\u001b[0m                     \u001b[35m0.5598\u001b[0m        \u001b[31m0.6964\u001b[0m  0.0100  3.0216\n",
      "      2                     \u001b[36m0.5681\u001b[0m        \u001b[32m0.7006\u001b[0m                     0.5488        \u001b[31m0.6893\u001b[0m  0.0100  3.0268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3                     \u001b[36m0.5754\u001b[0m        \u001b[32m0.6793\u001b[0m                     \u001b[35m0.5859\u001b[0m        \u001b[31m0.6750\u001b[0m  0.0098  3.0301\n",
      "      4                     \u001b[36m0.5831\u001b[0m        \u001b[32m0.6758\u001b[0m                     0.5748        0.6833  0.0096  3.0269\n",
      "      5                     0.5797        0.6762                     0.5836        \u001b[31m0.6714\u001b[0m  0.0093  3.0301\n",
      "      6                     \u001b[36m0.5849\u001b[0m        \u001b[32m0.6744\u001b[0m                     0.5831        0.6716  0.0090  3.0314\n",
      "      7                     \u001b[36m0.5876\u001b[0m        \u001b[32m0.6728\u001b[0m                     0.5682        0.6860  0.0085  3.0314\n",
      "      8                     0.5854        \u001b[32m0.6727\u001b[0m                     0.5710        0.6872  0.0080  3.0305\n",
      "      9                     0.5863        \u001b[32m0.6718\u001b[0m                     0.5779        0.6760  0.0075  3.0308\n",
      "     10                     0.5847        \u001b[32m0.6709\u001b[0m                     \u001b[35m0.5864\u001b[0m        0.6715  0.0069  3.0294\n",
      "     11                     \u001b[36m0.5904\u001b[0m        \u001b[32m0.6688\u001b[0m                     \u001b[35m0.5972\u001b[0m        \u001b[31m0.6705\u001b[0m  0.0063  3.0280\n",
      "     12                     \u001b[36m0.5958\u001b[0m        \u001b[32m0.6678\u001b[0m                     0.5934        \u001b[31m0.6682\u001b[0m  0.0057  3.0310\n",
      "     13                     0.5873        0.6698                     0.5903        0.6706  0.0050  3.0316\n",
      "     14                     0.5949        \u001b[32m0.6648\u001b[0m                     0.5723        0.6857  0.0043  3.0307\n",
      "     15                     \u001b[36m0.6004\u001b[0m        \u001b[32m0.6626\u001b[0m                     0.5902        0.6700  0.0037  3.0311\n",
      "     16                     0.6001        \u001b[32m0.6606\u001b[0m                     0.5969        \u001b[31m0.6675\u001b[0m  0.0031  3.0303\n",
      "     17                     \u001b[36m0.6066\u001b[0m        \u001b[32m0.6601\u001b[0m                     0.5794        0.6742  0.0025  3.0310\n",
      "     18                     \u001b[36m0.6081\u001b[0m        \u001b[32m0.6570\u001b[0m                     0.5951        0.6689  0.0020  3.0327\n",
      "     19                     \u001b[36m0.6120\u001b[0m        \u001b[32m0.6544\u001b[0m                     0.5889        0.6681  0.0015  3.0379\n",
      "     20                     \u001b[36m0.6148\u001b[0m        \u001b[32m0.6535\u001b[0m                     0.5908        0.6687  0.0010  3.0325\n",
      "     21                     0.6124        \u001b[32m0.6515\u001b[0m                     0.5924        0.6682  0.0007  3.0336\n",
      "     22                     \u001b[36m0.6152\u001b[0m        \u001b[32m0.6512\u001b[0m                     0.5943        0.6685  0.0004  3.0350\n",
      "     23                     0.6142        \u001b[32m0.6503\u001b[0m                     \u001b[35m0.5990\u001b[0m        0.6675  0.0002  3.0363\n",
      "     24                     \u001b[36m0.6207\u001b[0m        \u001b[32m0.6489\u001b[0m                     0.5974        0.6679  0.0000  3.0341\n",
      "     25                     0.6199        0.6499                     0.5943        0.6684  0.0000  3.0399\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5294\u001b[0m        \u001b[32m0.8942\u001b[0m                     \u001b[35m0.5452\u001b[0m        \u001b[31m0.6844\u001b[0m  0.0100  3.0224\n",
      "      2                     \u001b[36m0.5635\u001b[0m        \u001b[32m0.6892\u001b[0m                     \u001b[35m0.5772\u001b[0m        \u001b[31m0.6747\u001b[0m  0.0100  3.0295\n",
      "      3                     \u001b[36m0.5752\u001b[0m        \u001b[32m0.6775\u001b[0m                     \u001b[35m0.5968\u001b[0m        \u001b[31m0.6673\u001b[0m  0.0098  3.0263\n",
      "      4                     \u001b[36m0.5776\u001b[0m        \u001b[32m0.6764\u001b[0m                     0.5815        0.6730  0.0096  3.0268\n",
      "      5                     \u001b[36m0.5804\u001b[0m        \u001b[32m0.6762\u001b[0m                     0.5783        0.6753  0.0093  3.0295\n",
      "      6                     \u001b[36m0.5850\u001b[0m        \u001b[32m0.6733\u001b[0m                     0.5832        0.6713  0.0090  3.0277\n",
      "      7                     \u001b[36m0.5888\u001b[0m        \u001b[32m0.6721\u001b[0m                     0.5772        0.6755  0.0085  3.0292\n",
      "      8                     0.5879        0.6732                     0.5866        0.6696  0.0080  3.0298\n",
      "      9                     0.5864        0.6723                     0.5904        0.6675  0.0075  3.0308\n",
      "     10                     0.5859        \u001b[32m0.6709\u001b[0m                     0.5857        0.6730  0.0069  3.0271\n",
      "     11                     \u001b[36m0.5918\u001b[0m        \u001b[32m0.6692\u001b[0m                     0.5843        0.6713  0.0063  3.0306\n",
      "     12                     \u001b[36m0.5942\u001b[0m        \u001b[32m0.6680\u001b[0m                     0.5873        0.6691  0.0057  3.0295\n",
      "     13                     0.5940        \u001b[32m0.6678\u001b[0m                     0.5889        0.6707  0.0050  3.0301\n",
      "     14                     \u001b[36m0.5980\u001b[0m        \u001b[32m0.6646\u001b[0m                     0.5913        \u001b[31m0.6672\u001b[0m  0.0043  3.0312\n",
      "     15                     \u001b[36m0.6015\u001b[0m        \u001b[32m0.6615\u001b[0m                     0.5852        0.6673  0.0037  3.0324\n",
      "     16                     \u001b[36m0.6052\u001b[0m        \u001b[32m0.6614\u001b[0m                     0.5887        \u001b[31m0.6670\u001b[0m  0.0031  3.0345\n",
      "     17                     \u001b[36m0.6060\u001b[0m        \u001b[32m0.6595\u001b[0m                     0.5897        0.6673  0.0025  3.0316\n",
      "     18                     0.6046        \u001b[32m0.6590\u001b[0m                     0.5917        \u001b[31m0.6647\u001b[0m  0.0020  3.0318\n",
      "     19                     \u001b[36m0.6103\u001b[0m        \u001b[32m0.6563\u001b[0m                     \u001b[35m0.5996\u001b[0m        \u001b[31m0.6639\u001b[0m  0.0015  3.0328\n",
      "     20                     \u001b[36m0.6116\u001b[0m        \u001b[32m0.6549\u001b[0m                     0.5953        0.6645  0.0010  3.0377\n",
      "     21                     \u001b[36m0.6128\u001b[0m        \u001b[32m0.6530\u001b[0m                     0.5967        \u001b[31m0.6638\u001b[0m  0.0007  3.0349\n",
      "     22                     \u001b[36m0.6177\u001b[0m        \u001b[32m0.6517\u001b[0m                     0.5906        0.6643  0.0004  3.0351\n",
      "     23                     \u001b[36m0.6188\u001b[0m        \u001b[32m0.6508\u001b[0m                     0.5917        0.6641  0.0002  3.0354\n",
      "     24                     0.6129        \u001b[32m0.6500\u001b[0m                     0.5961        \u001b[31m0.6634\u001b[0m  0.0000  3.0345\n",
      "     25                     0.6175        0.6510                     0.5919        0.6638  0.0000  3.0343\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5312\u001b[0m        \u001b[32m0.8960\u001b[0m                     \u001b[35m0.5727\u001b[0m        \u001b[31m0.6864\u001b[0m  0.0100  3.0227\n",
      "      2                     \u001b[36m0.5698\u001b[0m        \u001b[32m0.6868\u001b[0m                     0.5726        \u001b[31m0.6766\u001b[0m  0.0100  3.0266\n",
      "      3                     \u001b[36m0.5785\u001b[0m        \u001b[32m0.6766\u001b[0m                     0.5705        0.6817  0.0098  3.0299\n",
      "      4                     0.5781        \u001b[32m0.6757\u001b[0m                     \u001b[35m0.5775\u001b[0m        0.6798  0.0096  3.0303\n",
      "      5                     \u001b[36m0.5819\u001b[0m        \u001b[32m0.6745\u001b[0m                     \u001b[35m0.5932\u001b[0m        \u001b[31m0.6706\u001b[0m  0.0093  3.0292\n",
      "      6                     \u001b[36m0.5842\u001b[0m        \u001b[32m0.6730\u001b[0m                     0.5377        0.6868  0.0090  3.0296\n",
      "      7                     0.5786        0.6745                     0.5931        \u001b[31m0.6700\u001b[0m  0.0085  3.0339\n",
      "      8                     \u001b[36m0.5890\u001b[0m        \u001b[32m0.6717\u001b[0m                     \u001b[35m0.5973\u001b[0m        0.6717  0.0080  3.0303\n",
      "      9                     0.5834        0.6730                     0.5806        0.6791  0.0075  3.0311\n",
      "     10                     0.5860        \u001b[32m0.6716\u001b[0m                     0.5321        0.6910  0.0069  3.0283\n",
      "     11                     \u001b[36m0.5897\u001b[0m        \u001b[32m0.6705\u001b[0m                     0.5962        0.6705  0.0063  3.0336\n",
      "     12                     \u001b[36m0.5921\u001b[0m        \u001b[32m0.6681\u001b[0m                     0.5906        0.6700  0.0057  3.0317\n",
      "     13                     \u001b[36m0.5934\u001b[0m        \u001b[32m0.6662\u001b[0m                     \u001b[35m0.6040\u001b[0m        \u001b[31m0.6671\u001b[0m  0.0050  3.0323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14                     \u001b[36m0.6026\u001b[0m        \u001b[32m0.6629\u001b[0m                     0.5966        0.6678  0.0043  3.0339\n",
      "     15                     0.6005        \u001b[32m0.6628\u001b[0m                     0.5968        \u001b[31m0.6671\u001b[0m  0.0037  3.0310\n",
      "     16                     0.6009        0.6634                     0.5992        \u001b[31m0.6659\u001b[0m  0.0031  3.0301\n",
      "     17                     \u001b[36m0.6057\u001b[0m        \u001b[32m0.6590\u001b[0m                     0.5937        \u001b[31m0.6654\u001b[0m  0.0025  3.0313\n",
      "     18                     0.6051        \u001b[32m0.6571\u001b[0m                     0.5975        \u001b[31m0.6651\u001b[0m  0.0020  3.0330\n",
      "     19                     \u001b[36m0.6154\u001b[0m        \u001b[32m0.6550\u001b[0m                     0.6008        \u001b[31m0.6645\u001b[0m  0.0015  3.0326\n",
      "     20                     0.6143        \u001b[32m0.6539\u001b[0m                     0.5987        \u001b[31m0.6642\u001b[0m  0.0010  3.0328\n",
      "     21                     0.6120        \u001b[32m0.6529\u001b[0m                     0.5972        0.6648  0.0007  3.0345\n",
      "     22                     0.6134        \u001b[32m0.6526\u001b[0m                     0.5951        0.6654  0.0004  3.0321\n",
      "     23                     \u001b[36m0.6167\u001b[0m        \u001b[32m0.6492\u001b[0m                     0.5952        0.6649  0.0002  3.0337\n",
      "     24                     \u001b[36m0.6171\u001b[0m        0.6503                     0.5926        0.6655  0.0000  3.0327\n",
      "     25                     \u001b[36m0.6202\u001b[0m        0.6502                     0.5950        0.6655  0.0000  3.0356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[1 1 0 ... 0 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7667\u001b[0m        \u001b[32m0.8038\u001b[0m                     \u001b[35m0.7957\u001b[0m        \u001b[31m0.5487\u001b[0m  0.0100  1.4743\n",
      "      2                     \u001b[36m0.8205\u001b[0m        \u001b[32m0.4399\u001b[0m                     \u001b[35m0.8469\u001b[0m        \u001b[31m0.3579\u001b[0m  0.0100  1.2412\n",
      "      3                     \u001b[36m0.8409\u001b[0m        \u001b[32m0.3881\u001b[0m                     \u001b[35m0.8640\u001b[0m        \u001b[31m0.3354\u001b[0m  0.0098  1.2582\n",
      "      4                     \u001b[36m0.8621\u001b[0m        \u001b[32m0.3377\u001b[0m                     0.8603        \u001b[31m0.3262\u001b[0m  0.0096  1.2384\n",
      "      5                     \u001b[36m0.8648\u001b[0m        \u001b[32m0.3293\u001b[0m                     \u001b[35m0.8825\u001b[0m        \u001b[31m0.3112\u001b[0m  0.0093  1.2425\n",
      "      6                     \u001b[36m0.8718\u001b[0m        \u001b[32m0.3125\u001b[0m                     0.8745        0.3155  0.0090  1.2421\n",
      "      7                     \u001b[36m0.8770\u001b[0m        \u001b[32m0.2994\u001b[0m                     \u001b[35m0.8955\u001b[0m        \u001b[31m0.2739\u001b[0m  0.0085  1.2412\n",
      "      8                     \u001b[36m0.8800\u001b[0m        \u001b[32m0.2848\u001b[0m                     0.8865        0.2852  0.0080  1.2419\n",
      "      9                     \u001b[36m0.8873\u001b[0m        \u001b[32m0.2744\u001b[0m                     0.8901        \u001b[31m0.2714\u001b[0m  0.0075  1.2427\n",
      "     10                     \u001b[36m0.8911\u001b[0m        \u001b[32m0.2666\u001b[0m                     \u001b[35m0.8986\u001b[0m        \u001b[31m0.2586\u001b[0m  0.0069  1.2408\n",
      "     11                     \u001b[36m0.8949\u001b[0m        0.2699                     \u001b[35m0.9031\u001b[0m        \u001b[31m0.2577\u001b[0m  0.0063  1.2427\n",
      "     12                     \u001b[36m0.8982\u001b[0m        \u001b[32m0.2576\u001b[0m                     \u001b[35m0.9051\u001b[0m        \u001b[31m0.2527\u001b[0m  0.0057  1.2421\n",
      "     13                     \u001b[36m0.9015\u001b[0m        \u001b[32m0.2525\u001b[0m                     0.9048        \u001b[31m0.2469\u001b[0m  0.0050  1.2433\n",
      "     14                     \u001b[36m0.9019\u001b[0m        \u001b[32m0.2444\u001b[0m                     0.8992        0.2561  0.0043  1.2428\n",
      "     15                     \u001b[36m0.9086\u001b[0m        \u001b[32m0.2311\u001b[0m                     0.8989        0.2498  0.0037  1.2426\n",
      "     16                     \u001b[36m0.9098\u001b[0m        \u001b[32m0.2299\u001b[0m                     \u001b[35m0.9138\u001b[0m        \u001b[31m0.2361\u001b[0m  0.0031  1.2437\n",
      "     17                     \u001b[36m0.9169\u001b[0m        \u001b[32m0.2248\u001b[0m                     0.9137        \u001b[31m0.2350\u001b[0m  0.0025  1.2443\n",
      "     18                     0.9157        \u001b[32m0.2182\u001b[0m                     0.9101        \u001b[31m0.2345\u001b[0m  0.0020  1.2431\n",
      "     19                     \u001b[36m0.9177\u001b[0m        \u001b[32m0.2099\u001b[0m                     0.9124        \u001b[31m0.2338\u001b[0m  0.0015  1.2443\n",
      "     20                     \u001b[36m0.9182\u001b[0m        0.2120                     \u001b[35m0.9145\u001b[0m        \u001b[31m0.2311\u001b[0m  0.0010  1.2435\n",
      "     21                     \u001b[36m0.9237\u001b[0m        \u001b[32m0.1994\u001b[0m                     0.9086        0.2373  0.0007  1.2421\n",
      "     22                     0.9221        0.2001                     \u001b[35m0.9146\u001b[0m        \u001b[31m0.2286\u001b[0m  0.0004  1.2422\n",
      "     23                     \u001b[36m0.9257\u001b[0m        0.2002                     0.9119        0.2314  0.0002  1.2418\n",
      "     24                     \u001b[36m0.9279\u001b[0m        \u001b[32m0.1925\u001b[0m                     0.9132        \u001b[31m0.2271\u001b[0m  0.0000  1.2448\n",
      "     25                     0.9264        \u001b[32m0.1867\u001b[0m                     0.9139        0.2274  0.0000  1.2419\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7710\u001b[0m        \u001b[32m0.7257\u001b[0m                     \u001b[35m0.6462\u001b[0m        \u001b[31m0.7640\u001b[0m  0.0100  1.2382\n",
      "      2                     \u001b[36m0.8166\u001b[0m        \u001b[32m0.5007\u001b[0m                     \u001b[35m0.8391\u001b[0m        \u001b[31m0.3776\u001b[0m  0.0100  1.2427\n",
      "      3                     \u001b[36m0.8473\u001b[0m        \u001b[32m0.3652\u001b[0m                     \u001b[35m0.8565\u001b[0m        \u001b[31m0.3563\u001b[0m  0.0098  1.2402\n",
      "      4                     \u001b[36m0.8628\u001b[0m        \u001b[32m0.3299\u001b[0m                     \u001b[35m0.8737\u001b[0m        \u001b[31m0.3199\u001b[0m  0.0096  1.2425\n",
      "      5                     \u001b[36m0.8688\u001b[0m        \u001b[32m0.3115\u001b[0m                     0.8725        \u001b[31m0.3146\u001b[0m  0.0093  1.2417\n",
      "      6                     \u001b[36m0.8823\u001b[0m        \u001b[32m0.2926\u001b[0m                     \u001b[35m0.8771\u001b[0m        \u001b[31m0.3033\u001b[0m  0.0090  1.2420\n",
      "      7                     0.8766        0.2978                     0.8702        0.3131  0.0085  1.2417\n",
      "      8                     \u001b[36m0.8846\u001b[0m        \u001b[32m0.2867\u001b[0m                     \u001b[35m0.8876\u001b[0m        \u001b[31m0.2838\u001b[0m  0.0080  1.2417\n",
      "      9                     \u001b[36m0.8918\u001b[0m        \u001b[32m0.2708\u001b[0m                     \u001b[35m0.8925\u001b[0m        \u001b[31m0.2816\u001b[0m  0.0075  1.2426\n",
      "     10                     0.8893        \u001b[32m0.2680\u001b[0m                     0.8869        \u001b[31m0.2794\u001b[0m  0.0069  1.2436\n",
      "     11                     \u001b[36m0.8963\u001b[0m        \u001b[32m0.2616\u001b[0m                     \u001b[35m0.8964\u001b[0m        \u001b[31m0.2649\u001b[0m  0.0063  1.2421\n",
      "     12                     \u001b[36m0.8989\u001b[0m        \u001b[32m0.2551\u001b[0m                     \u001b[35m0.8977\u001b[0m        0.2729  0.0057  1.2422\n",
      "     13                     \u001b[36m0.9025\u001b[0m        \u001b[32m0.2468\u001b[0m                     \u001b[35m0.8977\u001b[0m        0.2710  0.0050  1.2430\n",
      "     14                     0.9020        \u001b[32m0.2417\u001b[0m                     \u001b[35m0.8983\u001b[0m        \u001b[31m0.2618\u001b[0m  0.0043  1.2407\n",
      "     15                     \u001b[36m0.9069\u001b[0m        \u001b[32m0.2338\u001b[0m                     0.8932        0.2644  0.0037  1.2415\n",
      "     16                     0.9066        \u001b[32m0.2318\u001b[0m                     0.8975        0.2678  0.0031  1.2427\n",
      "     17                     \u001b[36m0.9138\u001b[0m        \u001b[32m0.2234\u001b[0m                     0.8945        \u001b[31m0.2577\u001b[0m  0.0025  1.2427\n",
      "     18                     \u001b[36m0.9149\u001b[0m        \u001b[32m0.2119\u001b[0m                     \u001b[35m0.8987\u001b[0m        0.2674  0.0020  1.2427\n",
      "     19                     0.9136        0.2145                     0.8968        \u001b[31m0.2571\u001b[0m  0.0015  1.2421\n",
      "     20                     \u001b[36m0.9162\u001b[0m        0.2123                     \u001b[35m0.9026\u001b[0m        \u001b[31m0.2486\u001b[0m  0.0010  1.2435\n",
      "     21                     \u001b[36m0.9224\u001b[0m        \u001b[32m0.1979\u001b[0m                     \u001b[35m0.9056\u001b[0m        \u001b[31m0.2472\u001b[0m  0.0007  1.2427\n",
      "     22                     \u001b[36m0.9258\u001b[0m        \u001b[32m0.1975\u001b[0m                     0.9056        \u001b[31m0.2468\u001b[0m  0.0004  1.2433\n",
      "     23                     0.9229        0.1987                     0.9054        \u001b[31m0.2455\u001b[0m  0.0002  1.2457\n",
      "     24                     0.9230        \u001b[32m0.1932\u001b[0m                     0.9005        0.2516  0.0000  1.2438\n",
      "     25                     0.9237        0.1991                     0.9044        0.2472  0.0000  1.2461\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7690\u001b[0m        \u001b[32m0.7286\u001b[0m                     \u001b[35m0.8085\u001b[0m        \u001b[31m0.4790\u001b[0m  0.0100  1.2382\n",
      "      2                     \u001b[36m0.8315\u001b[0m        \u001b[32m0.4253\u001b[0m                     \u001b[35m0.8436\u001b[0m        \u001b[31m0.4128\u001b[0m  0.0100  1.2403\n",
      "      3                     \u001b[36m0.8531\u001b[0m        \u001b[32m0.3554\u001b[0m                     0.8256        0.4338  0.0098  1.2415\n",
      "      4                     \u001b[36m0.8549\u001b[0m        \u001b[32m0.3442\u001b[0m                     \u001b[35m0.8685\u001b[0m        \u001b[31m0.3130\u001b[0m  0.0096  1.2418\n",
      "      5                     \u001b[36m0.8750\u001b[0m        \u001b[32m0.3096\u001b[0m                     \u001b[35m0.8731\u001b[0m        \u001b[31m0.3010\u001b[0m  0.0093  1.2446\n",
      "      6                     \u001b[36m0.8848\u001b[0m        \u001b[32m0.2929\u001b[0m                     \u001b[35m0.8731\u001b[0m        \u001b[31m0.2919\u001b[0m  0.0090  1.2417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7                     0.8781        \u001b[32m0.2898\u001b[0m                     0.8668        0.3024  0.0085  1.2430\n",
      "      8                     \u001b[36m0.8900\u001b[0m        \u001b[32m0.2752\u001b[0m                     \u001b[35m0.8749\u001b[0m        0.2960  0.0080  1.2427\n",
      "      9                     \u001b[36m0.8962\u001b[0m        \u001b[32m0.2700\u001b[0m                     \u001b[35m0.8824\u001b[0m        \u001b[31m0.2903\u001b[0m  0.0075  1.2420\n",
      "     10                     0.8880        \u001b[32m0.2662\u001b[0m                     0.8652        0.2915  0.0069  1.2438\n",
      "     11                     \u001b[36m0.8983\u001b[0m        \u001b[32m0.2597\u001b[0m                     \u001b[35m0.8884\u001b[0m        \u001b[31m0.2644\u001b[0m  0.0063  1.2430\n",
      "     12                     0.8967        0.2599                     0.8813        0.2773  0.0057  1.2428\n",
      "     13                     \u001b[36m0.9047\u001b[0m        \u001b[32m0.2490\u001b[0m                     \u001b[35m0.8895\u001b[0m        0.2745  0.0050  1.2416\n",
      "     14                     \u001b[36m0.9050\u001b[0m        \u001b[32m0.2387\u001b[0m                     \u001b[35m0.8904\u001b[0m        \u001b[31m0.2538\u001b[0m  0.0043  1.2436\n",
      "     15                     0.9041        0.2388                     \u001b[35m0.8914\u001b[0m        0.2557  0.0037  1.2429\n",
      "     16                     \u001b[36m0.9134\u001b[0m        \u001b[32m0.2285\u001b[0m                     \u001b[35m0.9020\u001b[0m        \u001b[31m0.2483\u001b[0m  0.0031  1.2432\n",
      "     17                     0.9051        0.2306                     0.8966        \u001b[31m0.2416\u001b[0m  0.0025  1.2451\n",
      "     18                     \u001b[36m0.9156\u001b[0m        \u001b[32m0.2243\u001b[0m                     \u001b[35m0.9032\u001b[0m        0.2431  0.0020  1.2413\n",
      "     19                     0.9104        \u001b[32m0.2209\u001b[0m                     0.8899        0.2476  0.0015  1.2428\n",
      "     20                     \u001b[36m0.9195\u001b[0m        \u001b[32m0.2084\u001b[0m                     0.8978        0.2437  0.0010  1.2448\n",
      "     21                     \u001b[36m0.9205\u001b[0m        \u001b[32m0.2054\u001b[0m                     0.9000        \u001b[31m0.2342\u001b[0m  0.0007  1.2446\n",
      "     22                     0.9189        \u001b[32m0.2050\u001b[0m                     0.8990        0.2421  0.0004  1.2428\n",
      "     23                     \u001b[36m0.9253\u001b[0m        \u001b[32m0.1990\u001b[0m                     0.9011        \u001b[31m0.2317\u001b[0m  0.0002  1.2429\n",
      "     24                     0.9247        \u001b[32m0.1959\u001b[0m                     0.8990        \u001b[31m0.2307\u001b[0m  0.0000  1.2419\n",
      "     25                     0.9252        \u001b[32m0.1933\u001b[0m                     0.8988        0.2315  0.0000  1.2424\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7749\u001b[0m        \u001b[32m0.7602\u001b[0m                     \u001b[35m0.8330\u001b[0m        \u001b[31m0.4030\u001b[0m  0.0100  1.2367\n",
      "      2                     \u001b[36m0.8264\u001b[0m        \u001b[32m0.4273\u001b[0m                     \u001b[35m0.8517\u001b[0m        \u001b[31m0.3406\u001b[0m  0.0100  1.2412\n",
      "      3                     \u001b[36m0.8529\u001b[0m        \u001b[32m0.3473\u001b[0m                     \u001b[35m0.8735\u001b[0m        \u001b[31m0.3115\u001b[0m  0.0098  1.2409\n",
      "      4                     \u001b[36m0.8655\u001b[0m        \u001b[32m0.3197\u001b[0m                     0.8682        0.3171  0.0096  1.2413\n",
      "      5                     \u001b[36m0.8739\u001b[0m        \u001b[32m0.3135\u001b[0m                     \u001b[35m0.8790\u001b[0m        \u001b[31m0.3042\u001b[0m  0.0093  1.2423\n",
      "      6                     0.8704        \u001b[32m0.3016\u001b[0m                     \u001b[35m0.8898\u001b[0m        \u001b[31m0.2869\u001b[0m  0.0090  1.2412\n",
      "      7                     \u001b[36m0.8838\u001b[0m        \u001b[32m0.2872\u001b[0m                     0.8744        0.3010  0.0085  1.2418\n",
      "      8                     0.8808        \u001b[32m0.2860\u001b[0m                     \u001b[35m0.8903\u001b[0m        \u001b[31m0.2688\u001b[0m  0.0080  1.2420\n",
      "      9                     \u001b[36m0.8884\u001b[0m        \u001b[32m0.2723\u001b[0m                     0.8877        0.2845  0.0075  1.2401\n",
      "     10                     \u001b[36m0.8941\u001b[0m        \u001b[32m0.2669\u001b[0m                     0.8902        0.2842  0.0069  1.2423\n",
      "     11                     \u001b[36m0.8958\u001b[0m        \u001b[32m0.2589\u001b[0m                     \u001b[35m0.9003\u001b[0m        0.2729  0.0063  1.2408\n",
      "     12                     0.8955        0.2589                     0.8888        0.2818  0.0057  1.2427\n",
      "     13                     \u001b[36m0.9014\u001b[0m        \u001b[32m0.2555\u001b[0m                     0.8887        \u001b[31m0.2549\u001b[0m  0.0050  1.2418\n",
      "     14                     0.9011        \u001b[32m0.2451\u001b[0m                     \u001b[35m0.9009\u001b[0m        0.2676  0.0043  1.2429\n",
      "     15                     \u001b[36m0.9064\u001b[0m        \u001b[32m0.2365\u001b[0m                     \u001b[35m0.9049\u001b[0m        \u001b[31m0.2532\u001b[0m  0.0037  1.2434\n",
      "     16                     \u001b[36m0.9110\u001b[0m        \u001b[32m0.2307\u001b[0m                     \u001b[35m0.9054\u001b[0m        \u001b[31m0.2473\u001b[0m  0.0031  1.2422\n",
      "     17                     \u001b[36m0.9118\u001b[0m        \u001b[32m0.2273\u001b[0m                     0.8943        0.2717  0.0025  1.2427\n",
      "     18                     \u001b[36m0.9158\u001b[0m        \u001b[32m0.2153\u001b[0m                     \u001b[35m0.9069\u001b[0m        0.2479  0.0020  1.2418\n",
      "     19                     \u001b[36m0.9165\u001b[0m        \u001b[32m0.2137\u001b[0m                     0.9007        0.2509  0.0015  1.2437\n",
      "     20                     \u001b[36m0.9174\u001b[0m        \u001b[32m0.2112\u001b[0m                     0.9039        \u001b[31m0.2454\u001b[0m  0.0010  1.2415\n",
      "     21                     \u001b[36m0.9227\u001b[0m        \u001b[32m0.2003\u001b[0m                     0.9069        \u001b[31m0.2427\u001b[0m  0.0007  1.2410\n",
      "     22                     \u001b[36m0.9251\u001b[0m        \u001b[32m0.1993\u001b[0m                     \u001b[35m0.9099\u001b[0m        0.2432  0.0004  1.2427\n",
      "     23                     0.9237        0.2006                     0.9067        0.2430  0.0002  1.2437\n",
      "     24                     \u001b[36m0.9270\u001b[0m        \u001b[32m0.1936\u001b[0m                     0.9061        \u001b[31m0.2397\u001b[0m  0.0000  1.2447\n",
      "     25                     \u001b[36m0.9284\u001b[0m        \u001b[32m0.1911\u001b[0m                     0.9069        0.2398  0.0000  1.2430\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7752\u001b[0m        \u001b[32m0.6733\u001b[0m                     \u001b[35m0.8191\u001b[0m        \u001b[31m0.4366\u001b[0m  0.0100  1.2396\n",
      "      2                     \u001b[36m0.8164\u001b[0m        \u001b[32m0.4378\u001b[0m                     \u001b[35m0.8335\u001b[0m        0.4421  0.0100  1.2442\n",
      "      3                     \u001b[36m0.8375\u001b[0m        \u001b[32m0.3796\u001b[0m                     \u001b[35m0.8700\u001b[0m        \u001b[31m0.3226\u001b[0m  0.0098  1.2436\n",
      "      4                     \u001b[36m0.8605\u001b[0m        \u001b[32m0.3353\u001b[0m                     \u001b[35m0.8744\u001b[0m        \u001b[31m0.3097\u001b[0m  0.0096  1.2438\n",
      "      5                     \u001b[36m0.8663\u001b[0m        \u001b[32m0.3256\u001b[0m                     0.8551        0.3398  0.0093  1.2425\n",
      "      6                     \u001b[36m0.8713\u001b[0m        \u001b[32m0.3083\u001b[0m                     \u001b[35m0.8790\u001b[0m        \u001b[31m0.3047\u001b[0m  0.0090  1.2424\n",
      "      7                     \u001b[36m0.8742\u001b[0m        \u001b[32m0.3024\u001b[0m                     0.8747        0.3136  0.0085  1.2427\n",
      "      8                     \u001b[36m0.8798\u001b[0m        \u001b[32m0.2857\u001b[0m                     \u001b[35m0.8912\u001b[0m        \u001b[31m0.2847\u001b[0m  0.0080  1.2400\n",
      "      9                     \u001b[36m0.8891\u001b[0m        \u001b[32m0.2819\u001b[0m                     0.8865        \u001b[31m0.2832\u001b[0m  0.0075  1.2452\n",
      "     10                     0.8880        \u001b[32m0.2719\u001b[0m                     \u001b[35m0.8923\u001b[0m        \u001b[31m0.2745\u001b[0m  0.0069  1.2427\n",
      "     11                     \u001b[36m0.8920\u001b[0m        \u001b[32m0.2691\u001b[0m                     \u001b[35m0.8965\u001b[0m        \u001b[31m0.2673\u001b[0m  0.0063  1.2437\n",
      "     12                     \u001b[36m0.8993\u001b[0m        \u001b[32m0.2540\u001b[0m                     \u001b[35m0.8994\u001b[0m        0.2755  0.0057  1.2429\n",
      "     13                     \u001b[36m0.9014\u001b[0m        \u001b[32m0.2532\u001b[0m                     \u001b[35m0.9019\u001b[0m        0.2686  0.0050  1.2445\n",
      "     14                     \u001b[36m0.9033\u001b[0m        \u001b[32m0.2449\u001b[0m                     0.9018        \u001b[31m0.2632\u001b[0m  0.0043  1.2431\n",
      "     15                     \u001b[36m0.9103\u001b[0m        \u001b[32m0.2272\u001b[0m                     0.8796        0.3227  0.0037  1.2424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16                     0.9081        0.2327                     \u001b[35m0.9063\u001b[0m        0.2704  0.0031  1.2455\n",
      "     17                     0.9066        \u001b[32m0.2249\u001b[0m                     0.9043        0.2746  0.0025  1.2482\n",
      "     18                     \u001b[36m0.9125\u001b[0m        \u001b[32m0.2187\u001b[0m                     0.9024        0.2792  0.0020  1.2438\n",
      "     19                     \u001b[36m0.9154\u001b[0m        \u001b[32m0.2116\u001b[0m                     0.9056        0.2659  0.0015  1.2425\n",
      "     20                     \u001b[36m0.9200\u001b[0m        \u001b[32m0.2079\u001b[0m                     0.9051        0.2692  0.0010  1.2429\n",
      "     21                     \u001b[36m0.9227\u001b[0m        \u001b[32m0.1973\u001b[0m                     0.9028        0.2700  0.0007  1.2448\n",
      "     22                     0.9221        \u001b[32m0.1968\u001b[0m                     0.9038        0.2642  0.0004  1.2436\n",
      "     23                     \u001b[36m0.9270\u001b[0m        \u001b[32m0.1959\u001b[0m                     0.9059        0.2673  0.0002  1.2432\n",
      "     24                     0.9261        \u001b[32m0.1958\u001b[0m                     \u001b[35m0.9066\u001b[0m        0.2678  0.0000  1.2435\n",
      "     25                     0.9249        \u001b[32m0.1905\u001b[0m                     0.9061        0.2653  0.0000  1.2447\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7835\u001b[0m        \u001b[32m0.7254\u001b[0m                     \u001b[35m0.7327\u001b[0m        \u001b[31m0.5684\u001b[0m  0.0100  1.2397\n",
      "      2                     \u001b[36m0.8274\u001b[0m        \u001b[32m0.4437\u001b[0m                     \u001b[35m0.8612\u001b[0m        \u001b[31m0.3271\u001b[0m  0.0100  1.2406\n",
      "      3                     \u001b[36m0.8525\u001b[0m        \u001b[32m0.3507\u001b[0m                     0.8545        \u001b[31m0.3182\u001b[0m  0.0098  1.2418\n",
      "      4                     \u001b[36m0.8635\u001b[0m        \u001b[32m0.3196\u001b[0m                     \u001b[35m0.8660\u001b[0m        \u001b[31m0.3056\u001b[0m  0.0096  1.2427\n",
      "      5                     \u001b[36m0.8742\u001b[0m        \u001b[32m0.3089\u001b[0m                     \u001b[35m0.8748\u001b[0m        \u001b[31m0.2852\u001b[0m  0.0093  1.2413\n",
      "      6                     \u001b[36m0.8765\u001b[0m        \u001b[32m0.3019\u001b[0m                     \u001b[35m0.8824\u001b[0m        \u001b[31m0.2803\u001b[0m  0.0090  1.2419\n",
      "      7                     \u001b[36m0.8770\u001b[0m        \u001b[32m0.2968\u001b[0m                     0.8615        0.2972  0.0085  1.2425\n",
      "      8                     \u001b[36m0.8865\u001b[0m        \u001b[32m0.2814\u001b[0m                     \u001b[35m0.8863\u001b[0m        \u001b[31m0.2752\u001b[0m  0.0080  1.2411\n",
      "      9                     \u001b[36m0.8870\u001b[0m        \u001b[32m0.2761\u001b[0m                     \u001b[35m0.8871\u001b[0m        \u001b[31m0.2666\u001b[0m  0.0075  1.2406\n",
      "     10                     \u001b[36m0.8920\u001b[0m        \u001b[32m0.2687\u001b[0m                     \u001b[35m0.8921\u001b[0m        \u001b[31m0.2616\u001b[0m  0.0069  1.2416\n",
      "     11                     \u001b[36m0.8936\u001b[0m        \u001b[32m0.2595\u001b[0m                     0.8801        0.2689  0.0063  1.2412\n",
      "     12                     \u001b[36m0.9028\u001b[0m        \u001b[32m0.2488\u001b[0m                     \u001b[35m0.8983\u001b[0m        \u001b[31m0.2536\u001b[0m  0.0057  1.2408\n",
      "     13                     \u001b[36m0.9031\u001b[0m        \u001b[32m0.2486\u001b[0m                     0.8930        0.2614  0.0050  1.2405\n",
      "     14                     0.9024        \u001b[32m0.2426\u001b[0m                     \u001b[35m0.9002\u001b[0m        \u001b[31m0.2495\u001b[0m  0.0043  1.2420\n",
      "     15                     \u001b[36m0.9086\u001b[0m        \u001b[32m0.2332\u001b[0m                     0.8942        0.2582  0.0037  1.2432\n",
      "     16                     \u001b[36m0.9104\u001b[0m        \u001b[32m0.2300\u001b[0m                     0.8968        \u001b[31m0.2490\u001b[0m  0.0031  1.2427\n",
      "     17                     \u001b[36m0.9153\u001b[0m        \u001b[32m0.2225\u001b[0m                     0.8982        \u001b[31m0.2387\u001b[0m  0.0025  1.2444\n",
      "     18                     0.9139        \u001b[32m0.2177\u001b[0m                     \u001b[35m0.9029\u001b[0m        0.2453  0.0020  1.2438\n",
      "     19                     \u001b[36m0.9197\u001b[0m        \u001b[32m0.2085\u001b[0m                     \u001b[35m0.9033\u001b[0m        \u001b[31m0.2355\u001b[0m  0.0015  1.2425\n",
      "     20                     0.9196        \u001b[32m0.2070\u001b[0m                     \u001b[35m0.9034\u001b[0m        0.2393  0.0010  1.2430\n",
      "     21                     \u001b[36m0.9224\u001b[0m        \u001b[32m0.1997\u001b[0m                     \u001b[35m0.9068\u001b[0m        0.2371  0.0007  1.2435\n",
      "     22                     \u001b[36m0.9293\u001b[0m        \u001b[32m0.1945\u001b[0m                     0.9058        0.2361  0.0004  1.2437\n",
      "     23                     0.9250        0.1953                     0.9048        \u001b[31m0.2334\u001b[0m  0.0002  1.2453\n",
      "     24                     0.9286        \u001b[32m0.1881\u001b[0m                     0.9027        0.2347  0.0000  1.2451\n",
      "     25                     \u001b[36m0.9299\u001b[0m        0.1894                     0.9022        0.2345  0.0000  1.2437\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7699\u001b[0m        \u001b[32m0.7508\u001b[0m                     \u001b[35m0.7597\u001b[0m        \u001b[31m0.5383\u001b[0m  0.0100  1.2397\n",
      "      2                     \u001b[36m0.8276\u001b[0m        \u001b[32m0.4273\u001b[0m                     \u001b[35m0.8368\u001b[0m        \u001b[31m0.3777\u001b[0m  0.0100  1.2406\n",
      "      3                     \u001b[36m0.8517\u001b[0m        \u001b[32m0.3687\u001b[0m                     \u001b[35m0.8679\u001b[0m        \u001b[31m0.3129\u001b[0m  0.0098  1.2434\n",
      "      4                     \u001b[36m0.8649\u001b[0m        \u001b[32m0.3338\u001b[0m                     0.8570        0.3269  0.0096  1.2417\n",
      "      5                     \u001b[36m0.8679\u001b[0m        \u001b[32m0.3118\u001b[0m                     0.8635        0.3193  0.0093  1.2420\n",
      "      6                     \u001b[36m0.8732\u001b[0m        \u001b[32m0.3061\u001b[0m                     \u001b[35m0.8916\u001b[0m        \u001b[31m0.2674\u001b[0m  0.0090  1.2429\n",
      "      7                     \u001b[36m0.8777\u001b[0m        \u001b[32m0.2922\u001b[0m                     \u001b[35m0.8926\u001b[0m        0.2745  0.0085  1.2421\n",
      "      8                     \u001b[36m0.8837\u001b[0m        \u001b[32m0.2870\u001b[0m                     \u001b[35m0.8998\u001b[0m        0.2761  0.0080  1.2422\n",
      "      9                     \u001b[36m0.8844\u001b[0m        \u001b[32m0.2782\u001b[0m                     \u001b[35m0.9034\u001b[0m        \u001b[31m0.2626\u001b[0m  0.0075  1.2437\n",
      "     10                     \u001b[36m0.8892\u001b[0m        \u001b[32m0.2706\u001b[0m                     0.9008        0.2642  0.0069  1.2400\n",
      "     11                     \u001b[36m0.8929\u001b[0m        \u001b[32m0.2644\u001b[0m                     \u001b[35m0.9063\u001b[0m        \u001b[31m0.2566\u001b[0m  0.0063  1.2427\n",
      "     12                     0.8907        \u001b[32m0.2597\u001b[0m                     0.8996        \u001b[31m0.2509\u001b[0m  0.0057  1.2453\n",
      "     13                     \u001b[36m0.9031\u001b[0m        \u001b[32m0.2448\u001b[0m                     0.8954        0.2657  0.0050  1.2427\n",
      "     14                     \u001b[36m0.9079\u001b[0m        \u001b[32m0.2397\u001b[0m                     \u001b[35m0.9065\u001b[0m        \u001b[31m0.2450\u001b[0m  0.0043  1.2440\n",
      "     15                     0.9057        \u001b[32m0.2387\u001b[0m                     0.9021        0.2468  0.0037  1.2437\n",
      "     16                     0.9068        \u001b[32m0.2299\u001b[0m                     0.9051        0.2544  0.0031  1.2413\n",
      "     17                     \u001b[36m0.9125\u001b[0m        \u001b[32m0.2227\u001b[0m                     0.9038        0.2510  0.0025  1.2413\n",
      "     18                     0.9104        \u001b[32m0.2212\u001b[0m                     0.9043        0.2511  0.0020  1.2418\n",
      "     19                     \u001b[36m0.9172\u001b[0m        \u001b[32m0.2107\u001b[0m                     \u001b[35m0.9118\u001b[0m        0.2462  0.0015  1.2432\n",
      "     20                     \u001b[36m0.9197\u001b[0m        \u001b[32m0.2066\u001b[0m                     0.9084        \u001b[31m0.2430\u001b[0m  0.0010  1.2433\n",
      "     21                     0.9187        \u001b[32m0.2047\u001b[0m                     0.9091        0.2433  0.0007  1.2430\n",
      "     22                     \u001b[36m0.9244\u001b[0m        \u001b[32m0.1954\u001b[0m                     0.9090        0.2463  0.0004  1.2440\n",
      "     23                     0.9176        0.2026                     \u001b[35m0.9122\u001b[0m        \u001b[31m0.2425\u001b[0m  0.0002  1.2437\n",
      "     24                     0.9192        0.1991                     0.9110        0.2425  0.0000  1.2437\n",
      "     25                     0.9241        \u001b[32m0.1941\u001b[0m                     0.9114        0.2434  0.0000  1.2732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7762\u001b[0m        \u001b[32m0.7237\u001b[0m                     \u001b[35m0.7821\u001b[0m        \u001b[31m0.6263\u001b[0m  0.0100  1.2387\n",
      "      2                     \u001b[36m0.8289\u001b[0m        \u001b[32m0.4170\u001b[0m                     \u001b[35m0.8559\u001b[0m        \u001b[31m0.3962\u001b[0m  0.0100  1.2430\n",
      "      3                     \u001b[36m0.8500\u001b[0m        \u001b[32m0.3579\u001b[0m                     \u001b[35m0.8680\u001b[0m        \u001b[31m0.3424\u001b[0m  0.0098  1.2413\n",
      "      4                     \u001b[36m0.8631\u001b[0m        \u001b[32m0.3233\u001b[0m                     \u001b[35m0.8749\u001b[0m        \u001b[31m0.3106\u001b[0m  0.0096  1.2419\n",
      "      5                     \u001b[36m0.8690\u001b[0m        \u001b[32m0.3063\u001b[0m                     \u001b[35m0.8819\u001b[0m        \u001b[31m0.3044\u001b[0m  0.0093  1.2402\n",
      "      6                     \u001b[36m0.8763\u001b[0m        \u001b[32m0.3014\u001b[0m                     \u001b[35m0.8885\u001b[0m        \u001b[31m0.2800\u001b[0m  0.0090  1.2412\n",
      "      7                     \u001b[36m0.8769\u001b[0m        \u001b[32m0.2949\u001b[0m                     0.8805        0.3188  0.0085  1.2405\n",
      "      8                     \u001b[36m0.8858\u001b[0m        \u001b[32m0.2864\u001b[0m                     0.8825        0.2887  0.0080  1.2410\n",
      "      9                     \u001b[36m0.8889\u001b[0m        \u001b[32m0.2681\u001b[0m                     0.8840        0.3042  0.0075  1.2410\n",
      "     10                     0.8875        0.2713                     0.8821        0.2842  0.0069  1.2416\n",
      "     11                     \u001b[36m0.8929\u001b[0m        \u001b[32m0.2600\u001b[0m                     \u001b[35m0.8961\u001b[0m        \u001b[31m0.2761\u001b[0m  0.0063  1.2416\n",
      "     12                     \u001b[36m0.8944\u001b[0m        \u001b[32m0.2553\u001b[0m                     0.8923        \u001b[31m0.2722\u001b[0m  0.0057  1.2407\n",
      "     13                     \u001b[36m0.9000\u001b[0m        \u001b[32m0.2460\u001b[0m                     0.8946        0.2785  0.0050  1.2433\n",
      "     14                     \u001b[36m0.9037\u001b[0m        0.2473                     0.8901        0.2823  0.0043  1.2426\n",
      "     15                     \u001b[36m0.9054\u001b[0m        \u001b[32m0.2357\u001b[0m                     \u001b[35m0.8965\u001b[0m        \u001b[31m0.2580\u001b[0m  0.0037  1.2442\n",
      "     16                     \u001b[36m0.9087\u001b[0m        \u001b[32m0.2287\u001b[0m                     \u001b[35m0.8996\u001b[0m        0.2585  0.0031  1.2424\n",
      "     17                     \u001b[36m0.9131\u001b[0m        \u001b[32m0.2198\u001b[0m                     0.8936        0.2597  0.0025  1.2431\n",
      "     18                     0.9127        0.2198                     0.8984        \u001b[31m0.2539\u001b[0m  0.0020  1.2427\n",
      "     19                     \u001b[36m0.9163\u001b[0m        \u001b[32m0.2094\u001b[0m                     \u001b[35m0.9003\u001b[0m        0.2649  0.0015  1.2411\n",
      "     20                     \u001b[36m0.9189\u001b[0m        \u001b[32m0.2021\u001b[0m                     0.8981        0.2629  0.0010  1.2549\n",
      "     21                     \u001b[36m0.9231\u001b[0m        \u001b[32m0.1960\u001b[0m                     \u001b[35m0.9012\u001b[0m        0.2635  0.0007  1.2412\n",
      "     22                     \u001b[36m0.9253\u001b[0m        \u001b[32m0.1945\u001b[0m                     0.8995        0.2686  0.0004  1.2441\n",
      "     23                     0.9251        \u001b[32m0.1934\u001b[0m                     \u001b[35m0.9020\u001b[0m        0.2675  0.0002  1.2422\n",
      "     24                     \u001b[36m0.9269\u001b[0m        \u001b[32m0.1909\u001b[0m                     0.9012        0.2619  0.0000  1.2435\n",
      "     25                     0.9253        0.1962                     0.9014        0.2656  0.0000  1.2439\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7793\u001b[0m        \u001b[32m0.7115\u001b[0m                     \u001b[35m0.8331\u001b[0m        \u001b[31m0.3773\u001b[0m  0.0100  1.2427\n",
      "      2                     \u001b[36m0.8228\u001b[0m        \u001b[32m0.4460\u001b[0m                     \u001b[35m0.8474\u001b[0m        \u001b[31m0.3508\u001b[0m  0.0100  1.2404\n",
      "      3                     \u001b[36m0.8460\u001b[0m        \u001b[32m0.3693\u001b[0m                     0.8418        0.3696  0.0098  1.2418\n",
      "      4                     \u001b[36m0.8573\u001b[0m        \u001b[32m0.3419\u001b[0m                     \u001b[35m0.8657\u001b[0m        \u001b[31m0.3104\u001b[0m  0.0096  1.2412\n",
      "      5                     \u001b[36m0.8636\u001b[0m        \u001b[32m0.3197\u001b[0m                     \u001b[35m0.8844\u001b[0m        \u001b[31m0.3075\u001b[0m  0.0093  1.2444\n",
      "      6                     \u001b[36m0.8691\u001b[0m        \u001b[32m0.3152\u001b[0m                     \u001b[35m0.8870\u001b[0m        \u001b[31m0.2945\u001b[0m  0.0090  1.2420\n",
      "      7                     \u001b[36m0.8803\u001b[0m        \u001b[32m0.2943\u001b[0m                     \u001b[35m0.8974\u001b[0m        \u001b[31m0.2799\u001b[0m  0.0085  1.2450\n",
      "      8                     \u001b[36m0.8808\u001b[0m        \u001b[32m0.2869\u001b[0m                     0.8940        \u001b[31m0.2649\u001b[0m  0.0080  1.2427\n",
      "      9                     0.8786        0.2884                     0.8752        0.2976  0.0075  1.2427\n",
      "     10                     \u001b[36m0.8856\u001b[0m        \u001b[32m0.2754\u001b[0m                     \u001b[35m0.8982\u001b[0m        \u001b[31m0.2598\u001b[0m  0.0069  1.2424\n",
      "     11                     \u001b[36m0.8918\u001b[0m        \u001b[32m0.2630\u001b[0m                     0.8829        0.2927  0.0063  1.2439\n",
      "     12                     \u001b[36m0.8935\u001b[0m        0.2670                     \u001b[35m0.8995\u001b[0m        \u001b[31m0.2588\u001b[0m  0.0057  1.2432\n",
      "     13                     \u001b[36m0.8953\u001b[0m        \u001b[32m0.2518\u001b[0m                     \u001b[35m0.9135\u001b[0m        \u001b[31m0.2480\u001b[0m  0.0050  1.2427\n",
      "     14                     \u001b[36m0.8977\u001b[0m        0.2536                     0.9058        0.2562  0.0043  1.2434\n",
      "     15                     \u001b[36m0.8994\u001b[0m        \u001b[32m0.2419\u001b[0m                     0.9078        0.2485  0.0037  1.2436\n",
      "     16                     \u001b[36m0.9068\u001b[0m        \u001b[32m0.2310\u001b[0m                     0.9024        0.2534  0.0031  1.2427\n",
      "     17                     0.9031        0.2321                     0.9095        \u001b[31m0.2364\u001b[0m  0.0025  1.2427\n",
      "     18                     \u001b[36m0.9145\u001b[0m        \u001b[32m0.2231\u001b[0m                     0.9089        0.2426  0.0020  1.2446\n",
      "     19                     \u001b[36m0.9199\u001b[0m        \u001b[32m0.2115\u001b[0m                     \u001b[35m0.9172\u001b[0m        \u001b[31m0.2322\u001b[0m  0.0015  1.2451\n",
      "     20                     0.9121        0.2176                     0.9130        0.2413  0.0010  1.2436\n",
      "     21                     0.9182        \u001b[32m0.2074\u001b[0m                     0.9144        0.2340  0.0007  1.2428\n",
      "     22                     \u001b[36m0.9202\u001b[0m        \u001b[32m0.2003\u001b[0m                     \u001b[35m0.9201\u001b[0m        \u001b[31m0.2264\u001b[0m  0.0004  1.2442\n",
      "     23                     \u001b[36m0.9214\u001b[0m        \u001b[32m0.1975\u001b[0m                     0.9157        0.2294  0.0002  1.2433\n",
      "     24                     \u001b[36m0.9246\u001b[0m        \u001b[32m0.1970\u001b[0m                     0.9170        0.2292  0.0000  1.2423\n",
      "     25                     0.9228        0.1977                     0.9183        0.2276  0.0000  1.2439\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7676\u001b[0m        \u001b[32m0.7830\u001b[0m                     \u001b[35m0.8261\u001b[0m        \u001b[31m0.4439\u001b[0m  0.0100  1.2422\n",
      "      2                     \u001b[36m0.8256\u001b[0m        \u001b[32m0.4410\u001b[0m                     \u001b[35m0.8423\u001b[0m        \u001b[31m0.3668\u001b[0m  0.0100  1.2412\n",
      "      3                     \u001b[36m0.8378\u001b[0m        \u001b[32m0.3928\u001b[0m                     0.8124        0.4913  0.0098  1.2425\n",
      "      4                     \u001b[36m0.8568\u001b[0m        \u001b[32m0.3381\u001b[0m                     \u001b[35m0.8695\u001b[0m        \u001b[31m0.3178\u001b[0m  0.0096  1.2424\n",
      "      5                     \u001b[36m0.8695\u001b[0m        \u001b[32m0.3206\u001b[0m                     \u001b[35m0.8710\u001b[0m        \u001b[31m0.3140\u001b[0m  0.0093  1.2406\n",
      "      6                     \u001b[36m0.8743\u001b[0m        \u001b[32m0.3114\u001b[0m                     0.8623        0.3305  0.0090  1.2407\n",
      "      7                     \u001b[36m0.8764\u001b[0m        \u001b[32m0.2906\u001b[0m                     \u001b[35m0.8954\u001b[0m        \u001b[31m0.2824\u001b[0m  0.0085  1.2423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8                     \u001b[36m0.8866\u001b[0m        \u001b[32m0.2796\u001b[0m                     0.8928        \u001b[31m0.2764\u001b[0m  0.0080  1.2421\n",
      "      9                     \u001b[36m0.8926\u001b[0m        \u001b[32m0.2764\u001b[0m                     \u001b[35m0.9045\u001b[0m        \u001b[31m0.2662\u001b[0m  0.0075  1.2435\n",
      "     10                     \u001b[36m0.8931\u001b[0m        \u001b[32m0.2600\u001b[0m                     0.8883        0.2701  0.0069  1.2412\n",
      "     11                     \u001b[36m0.8959\u001b[0m        0.2622                     0.8829        0.2700  0.0063  1.2466\n",
      "     12                     0.8947        \u001b[32m0.2545\u001b[0m                     0.8981        \u001b[31m0.2553\u001b[0m  0.0057  1.2443\n",
      "     13                     \u001b[36m0.9031\u001b[0m        \u001b[32m0.2424\u001b[0m                     0.8911        0.2790  0.0050  1.2427\n",
      "     14                     0.9022        0.2428                     \u001b[35m0.9046\u001b[0m        \u001b[31m0.2467\u001b[0m  0.0043  1.2426\n",
      "     15                     \u001b[36m0.9066\u001b[0m        \u001b[32m0.2339\u001b[0m                     \u001b[35m0.9104\u001b[0m        0.2520  0.0037  1.2437\n",
      "     16                     \u001b[36m0.9089\u001b[0m        \u001b[32m0.2267\u001b[0m                     0.8910        0.2705  0.0031  1.2432\n",
      "     17                     \u001b[36m0.9175\u001b[0m        \u001b[32m0.2225\u001b[0m                     0.9021        \u001b[31m0.2370\u001b[0m  0.0025  1.2732\n",
      "     18                     \u001b[36m0.9210\u001b[0m        \u001b[32m0.2143\u001b[0m                     0.8976        0.2438  0.0020  1.2441\n",
      "     19                     0.9189        \u001b[32m0.2063\u001b[0m                     0.9011        0.2419  0.0015  1.2435\n",
      "     20                     \u001b[36m0.9239\u001b[0m        \u001b[32m0.1986\u001b[0m                     0.9062        0.2377  0.0010  1.2437\n",
      "     21                     \u001b[36m0.9241\u001b[0m        0.1987                     0.9049        \u001b[31m0.2367\u001b[0m  0.0007  1.2427\n",
      "     22                     0.9239        \u001b[32m0.1929\u001b[0m                     0.9063        \u001b[31m0.2359\u001b[0m  0.0004  1.2433\n",
      "     23                     \u001b[36m0.9247\u001b[0m        0.1930                     0.9052        0.2366  0.0002  1.2424\n",
      "     24                     \u001b[36m0.9265\u001b[0m        \u001b[32m0.1906\u001b[0m                     0.9045        \u001b[31m0.2359\u001b[0m  0.0000  1.2451\n",
      "     25                     \u001b[36m0.9299\u001b[0m        \u001b[32m0.1850\u001b[0m                     0.9064        0.2389  0.0000  1.2427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[1 1 0 ... 0 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6356\u001b[0m        \u001b[32m0.8937\u001b[0m                     \u001b[35m0.6812\u001b[0m        \u001b[31m0.6480\u001b[0m  0.0100  1.2428\n",
      "      2                     \u001b[36m0.7393\u001b[0m        \u001b[32m0.5555\u001b[0m                     \u001b[35m0.7806\u001b[0m        \u001b[31m0.4648\u001b[0m  0.0100  1.2397\n",
      "      3                     \u001b[36m0.7703\u001b[0m        \u001b[32m0.4980\u001b[0m                     \u001b[35m0.7867\u001b[0m        \u001b[31m0.4573\u001b[0m  0.0098  1.2432\n",
      "      4                     \u001b[36m0.7870\u001b[0m        \u001b[32m0.4600\u001b[0m                     \u001b[35m0.7880\u001b[0m        \u001b[31m0.4471\u001b[0m  0.0096  1.2408\n",
      "      5                     \u001b[36m0.7946\u001b[0m        \u001b[32m0.4463\u001b[0m                     \u001b[35m0.8024\u001b[0m        \u001b[31m0.4336\u001b[0m  0.0093  1.2408\n",
      "      6                     0.7901        \u001b[32m0.4431\u001b[0m                     0.7918        0.4457  0.0090  1.2367\n",
      "      7                     \u001b[36m0.7991\u001b[0m        \u001b[32m0.4358\u001b[0m                     0.7949        0.4450  0.0085  1.2427\n",
      "      8                     \u001b[36m0.8015\u001b[0m        \u001b[32m0.4283\u001b[0m                     0.7894        0.4426  0.0080  1.2408\n",
      "      9                     \u001b[36m0.8044\u001b[0m        \u001b[32m0.4243\u001b[0m                     \u001b[35m0.8026\u001b[0m        \u001b[31m0.4273\u001b[0m  0.0075  1.2428\n",
      "     10                     \u001b[36m0.8095\u001b[0m        \u001b[32m0.4174\u001b[0m                     \u001b[35m0.8039\u001b[0m        0.4275  0.0069  1.2427\n",
      "     11                     \u001b[36m0.8098\u001b[0m        \u001b[32m0.4123\u001b[0m                     \u001b[35m0.8050\u001b[0m        \u001b[31m0.4219\u001b[0m  0.0063  1.2423\n",
      "     12                     0.8088        \u001b[32m0.4106\u001b[0m                     \u001b[35m0.8107\u001b[0m        \u001b[31m0.4168\u001b[0m  0.0057  1.2416\n",
      "     13                     \u001b[36m0.8179\u001b[0m        \u001b[32m0.4032\u001b[0m                     \u001b[35m0.8137\u001b[0m        \u001b[31m0.4147\u001b[0m  0.0050  1.2427\n",
      "     14                     0.8156        \u001b[32m0.4008\u001b[0m                     \u001b[35m0.8152\u001b[0m        \u001b[31m0.4119\u001b[0m  0.0043  1.2410\n",
      "     15                     \u001b[36m0.8202\u001b[0m        \u001b[32m0.3959\u001b[0m                     0.8067        0.4174  0.0037  1.2427\n",
      "     16                     \u001b[36m0.8254\u001b[0m        \u001b[32m0.3888\u001b[0m                     0.8090        0.4127  0.0031  1.2416\n",
      "     17                     0.8252        \u001b[32m0.3855\u001b[0m                     \u001b[35m0.8155\u001b[0m        \u001b[31m0.4102\u001b[0m  0.0025  1.2437\n",
      "     18                     0.8249        \u001b[32m0.3819\u001b[0m                     0.8148        \u001b[31m0.4073\u001b[0m  0.0020  1.2427\n",
      "     19                     \u001b[36m0.8294\u001b[0m        \u001b[32m0.3795\u001b[0m                     0.8140        0.4096  0.0015  1.2437\n",
      "     20                     \u001b[36m0.8305\u001b[0m        \u001b[32m0.3750\u001b[0m                     \u001b[35m0.8160\u001b[0m        \u001b[31m0.4048\u001b[0m  0.0010  1.2412\n",
      "     21                     \u001b[36m0.8334\u001b[0m        \u001b[32m0.3689\u001b[0m                     \u001b[35m0.8168\u001b[0m        0.4065  0.0007  1.2437\n",
      "     22                     0.8326        \u001b[32m0.3686\u001b[0m                     \u001b[35m0.8185\u001b[0m        \u001b[31m0.4045\u001b[0m  0.0004  1.2457\n",
      "     23                     \u001b[36m0.8372\u001b[0m        \u001b[32m0.3645\u001b[0m                     0.8185        \u001b[31m0.4039\u001b[0m  0.0002  1.2447\n",
      "     24                     0.8350        \u001b[32m0.3639\u001b[0m                     \u001b[35m0.8188\u001b[0m        0.4045  0.0000  1.2440\n",
      "     25                     \u001b[36m0.8390\u001b[0m        0.3655                     0.8173        0.4046  0.0000  1.2434\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6439\u001b[0m        \u001b[32m0.8975\u001b[0m                     \u001b[35m0.7469\u001b[0m        \u001b[31m0.5175\u001b[0m  0.0100  1.2397\n",
      "      2                     \u001b[36m0.7397\u001b[0m        \u001b[32m0.5394\u001b[0m                     0.7380        0.5208  0.0100  1.2401\n",
      "      3                     \u001b[36m0.7615\u001b[0m        \u001b[32m0.5090\u001b[0m                     \u001b[35m0.7589\u001b[0m        \u001b[31m0.4941\u001b[0m  0.0098  1.2418\n",
      "      4                     \u001b[36m0.7855\u001b[0m        \u001b[32m0.4678\u001b[0m                     \u001b[35m0.8008\u001b[0m        \u001b[31m0.4277\u001b[0m  0.0096  1.2428\n",
      "      5                     \u001b[36m0.7865\u001b[0m        \u001b[32m0.4550\u001b[0m                     0.7929        0.4314  0.0093  1.2401\n",
      "      6                     \u001b[36m0.7914\u001b[0m        \u001b[32m0.4475\u001b[0m                     \u001b[35m0.8067\u001b[0m        \u001b[31m0.4258\u001b[0m  0.0090  1.2403\n",
      "      7                     \u001b[36m0.7944\u001b[0m        \u001b[32m0.4417\u001b[0m                     0.7967        0.4344  0.0085  1.2410\n",
      "      8                     \u001b[36m0.7984\u001b[0m        \u001b[32m0.4355\u001b[0m                     0.8022        \u001b[31m0.4172\u001b[0m  0.0080  1.2440\n",
      "      9                     \u001b[36m0.7999\u001b[0m        \u001b[32m0.4281\u001b[0m                     \u001b[35m0.8104\u001b[0m        \u001b[31m0.4114\u001b[0m  0.0075  1.2416\n",
      "     10                     \u001b[36m0.8029\u001b[0m        \u001b[32m0.4252\u001b[0m                     0.8101        0.4137  0.0069  1.2426\n",
      "     11                     \u001b[36m0.8118\u001b[0m        \u001b[32m0.4183\u001b[0m                     \u001b[35m0.8117\u001b[0m        \u001b[31m0.4076\u001b[0m  0.0063  1.2435\n",
      "     12                     0.8104        \u001b[32m0.4116\u001b[0m                     0.8077        \u001b[31m0.4076\u001b[0m  0.0057  1.2427\n",
      "     13                     \u001b[36m0.8127\u001b[0m        \u001b[32m0.4091\u001b[0m                     \u001b[35m0.8217\u001b[0m        \u001b[31m0.3931\u001b[0m  0.0050  1.2427\n",
      "     14                     \u001b[36m0.8136\u001b[0m        \u001b[32m0.4063\u001b[0m                     0.8211        \u001b[31m0.3915\u001b[0m  0.0043  1.2433\n",
      "     15                     \u001b[36m0.8179\u001b[0m        \u001b[32m0.3992\u001b[0m                     0.8167        0.3935  0.0037  1.2414\n",
      "     16                     \u001b[36m0.8201\u001b[0m        \u001b[32m0.3963\u001b[0m                     \u001b[35m0.8234\u001b[0m        \u001b[31m0.3871\u001b[0m  0.0031  1.2408\n",
      "     17                     \u001b[36m0.8229\u001b[0m        \u001b[32m0.3905\u001b[0m                     0.8211        \u001b[31m0.3859\u001b[0m  0.0025  1.2412\n",
      "     18                     \u001b[36m0.8274\u001b[0m        \u001b[32m0.3860\u001b[0m                     \u001b[35m0.8260\u001b[0m        \u001b[31m0.3832\u001b[0m  0.0020  1.2422\n",
      "     19                     \u001b[36m0.8300\u001b[0m        \u001b[32m0.3819\u001b[0m                     \u001b[35m0.8262\u001b[0m        \u001b[31m0.3802\u001b[0m  0.0015  1.2421\n",
      "     20                     \u001b[36m0.8302\u001b[0m        \u001b[32m0.3803\u001b[0m                     \u001b[35m0.8288\u001b[0m        \u001b[31m0.3798\u001b[0m  0.0010  1.2444\n",
      "     21                     \u001b[36m0.8326\u001b[0m        \u001b[32m0.3715\u001b[0m                     \u001b[35m0.8331\u001b[0m        \u001b[31m0.3769\u001b[0m  0.0007  1.2448\n",
      "     22                     0.8299        0.3738                     0.8310        \u001b[31m0.3762\u001b[0m  0.0004  1.2437\n",
      "     23                     0.8312        0.3734                     0.8325        \u001b[31m0.3760\u001b[0m  0.0002  1.2471\n",
      "     24                     \u001b[36m0.8337\u001b[0m        \u001b[32m0.3679\u001b[0m                     \u001b[35m0.8334\u001b[0m        0.3761  0.0000  1.2449\n",
      "     25                     \u001b[36m0.8363\u001b[0m        \u001b[32m0.3675\u001b[0m                     0.8325        0.3763  0.0000  1.2456\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6202\u001b[0m        \u001b[32m0.8677\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.7860\u001b[0m  0.0100  1.2380\n",
      "      2                     \u001b[36m0.7346\u001b[0m        \u001b[32m0.5783\u001b[0m                     \u001b[35m0.7628\u001b[0m        \u001b[31m0.4983\u001b[0m  0.0100  1.2403\n",
      "      3                     \u001b[36m0.7627\u001b[0m        \u001b[32m0.5090\u001b[0m                     \u001b[35m0.7772\u001b[0m        \u001b[31m0.4623\u001b[0m  0.0098  1.2425\n",
      "      4                     \u001b[36m0.7749\u001b[0m        \u001b[32m0.4755\u001b[0m                     \u001b[35m0.7915\u001b[0m        \u001b[31m0.4520\u001b[0m  0.0096  1.2409\n",
      "      5                     \u001b[36m0.7924\u001b[0m        \u001b[32m0.4543\u001b[0m                     0.7863        0.4521  0.0093  1.2427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6                     \u001b[36m0.7937\u001b[0m        \u001b[32m0.4434\u001b[0m                     0.7739        0.4692  0.0090  1.2427\n",
      "      7                     \u001b[36m0.8001\u001b[0m        \u001b[32m0.4356\u001b[0m                     \u001b[35m0.7928\u001b[0m        \u001b[31m0.4392\u001b[0m  0.0085  1.2407\n",
      "      8                     \u001b[36m0.8003\u001b[0m        \u001b[32m0.4339\u001b[0m                     \u001b[35m0.7966\u001b[0m        \u001b[31m0.4317\u001b[0m  0.0080  1.2429\n",
      "      9                     \u001b[36m0.8056\u001b[0m        \u001b[32m0.4249\u001b[0m                     \u001b[35m0.8041\u001b[0m        \u001b[31m0.4275\u001b[0m  0.0075  1.2435\n",
      "     10                     0.8042        0.4250                     0.8024        \u001b[31m0.4173\u001b[0m  0.0069  1.2412\n",
      "     11                     \u001b[36m0.8099\u001b[0m        \u001b[32m0.4134\u001b[0m                     \u001b[35m0.8077\u001b[0m        \u001b[31m0.4103\u001b[0m  0.0063  1.2429\n",
      "     12                     \u001b[36m0.8101\u001b[0m        \u001b[32m0.4118\u001b[0m                     0.8041        0.4125  0.0057  1.2425\n",
      "     13                     \u001b[36m0.8128\u001b[0m        \u001b[32m0.4076\u001b[0m                     0.8074        0.4144  0.0050  1.2437\n",
      "     14                     \u001b[36m0.8155\u001b[0m        \u001b[32m0.4005\u001b[0m                     0.8068        0.4190  0.0043  1.2417\n",
      "     15                     \u001b[36m0.8172\u001b[0m        \u001b[32m0.3991\u001b[0m                     \u001b[35m0.8085\u001b[0m        0.4109  0.0037  1.2434\n",
      "     16                     \u001b[36m0.8202\u001b[0m        \u001b[32m0.3913\u001b[0m                     \u001b[35m0.8089\u001b[0m        \u001b[31m0.4081\u001b[0m  0.0031  1.2437\n",
      "     17                     \u001b[36m0.8233\u001b[0m        \u001b[32m0.3867\u001b[0m                     \u001b[35m0.8101\u001b[0m        \u001b[31m0.4073\u001b[0m  0.0025  1.2420\n",
      "     18                     \u001b[36m0.8269\u001b[0m        \u001b[32m0.3800\u001b[0m                     0.8036        0.4085  0.0020  1.2432\n",
      "     19                     \u001b[36m0.8303\u001b[0m        \u001b[32m0.3752\u001b[0m                     \u001b[35m0.8112\u001b[0m        \u001b[31m0.4022\u001b[0m  0.0015  1.2436\n",
      "     20                     \u001b[36m0.8344\u001b[0m        \u001b[32m0.3714\u001b[0m                     0.8071        0.4036  0.0010  1.2447\n",
      "     21                     0.8332        \u001b[32m0.3698\u001b[0m                     0.8060        0.4059  0.0007  1.2436\n",
      "     22                     \u001b[36m0.8382\u001b[0m        \u001b[32m0.3654\u001b[0m                     0.8095        0.4031  0.0004  1.2437\n",
      "     23                     0.8346        0.3668                     0.8095        \u001b[31m0.4013\u001b[0m  0.0002  1.2431\n",
      "     24                     \u001b[36m0.8425\u001b[0m        \u001b[32m0.3624\u001b[0m                     0.8086        \u001b[31m0.4012\u001b[0m  0.0000  1.2429\n",
      "     25                     0.8345        0.3641                     0.8079        \u001b[31m0.4007\u001b[0m  0.0000  1.2450\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6331\u001b[0m        \u001b[32m0.8712\u001b[0m                     \u001b[35m0.7436\u001b[0m        \u001b[31m0.5137\u001b[0m  0.0100  1.2366\n",
      "      2                     \u001b[36m0.7377\u001b[0m        \u001b[32m0.5554\u001b[0m                     \u001b[35m0.7667\u001b[0m        \u001b[31m0.4781\u001b[0m  0.0100  1.2441\n",
      "      3                     \u001b[36m0.7655\u001b[0m        \u001b[32m0.4996\u001b[0m                     \u001b[35m0.7808\u001b[0m        \u001b[31m0.4729\u001b[0m  0.0098  1.2419\n",
      "      4                     \u001b[36m0.7773\u001b[0m        \u001b[32m0.4739\u001b[0m                     \u001b[35m0.7943\u001b[0m        \u001b[31m0.4378\u001b[0m  0.0096  1.2404\n",
      "      5                     \u001b[36m0.7843\u001b[0m        \u001b[32m0.4654\u001b[0m                     0.7859        0.4598  0.0093  1.2416\n",
      "      6                     \u001b[36m0.7885\u001b[0m        \u001b[32m0.4508\u001b[0m                     \u001b[35m0.7980\u001b[0m        0.4440  0.0090  1.2428\n",
      "      7                     \u001b[36m0.7938\u001b[0m        \u001b[32m0.4409\u001b[0m                     \u001b[35m0.8013\u001b[0m        \u001b[31m0.4345\u001b[0m  0.0085  1.2447\n",
      "      8                     \u001b[36m0.7988\u001b[0m        \u001b[32m0.4392\u001b[0m                     \u001b[35m0.8039\u001b[0m        \u001b[31m0.4256\u001b[0m  0.0080  1.2414\n",
      "      9                     \u001b[36m0.8007\u001b[0m        \u001b[32m0.4262\u001b[0m                     \u001b[35m0.8106\u001b[0m        \u001b[31m0.4188\u001b[0m  0.0075  1.2401\n",
      "     10                     0.7997        \u001b[32m0.4257\u001b[0m                     0.8062        \u001b[31m0.4187\u001b[0m  0.0069  1.2417\n",
      "     11                     \u001b[36m0.8034\u001b[0m        \u001b[32m0.4220\u001b[0m                     \u001b[35m0.8147\u001b[0m        \u001b[31m0.4151\u001b[0m  0.0063  1.2424\n",
      "     12                     \u001b[36m0.8085\u001b[0m        \u001b[32m0.4165\u001b[0m                     0.8135        \u001b[31m0.4065\u001b[0m  0.0057  1.2422\n",
      "     13                     \u001b[36m0.8120\u001b[0m        \u001b[32m0.4089\u001b[0m                     0.8128        0.4114  0.0050  1.2444\n",
      "     14                     0.8100        0.4090                     0.8081        0.4069  0.0043  1.2429\n",
      "     15                     \u001b[36m0.8134\u001b[0m        \u001b[32m0.3995\u001b[0m                     \u001b[35m0.8161\u001b[0m        \u001b[31m0.4000\u001b[0m  0.0037  1.2447\n",
      "     16                     \u001b[36m0.8187\u001b[0m        \u001b[32m0.3971\u001b[0m                     \u001b[35m0.8223\u001b[0m        \u001b[31m0.3997\u001b[0m  0.0031  1.2437\n",
      "     17                     \u001b[36m0.8215\u001b[0m        \u001b[32m0.3941\u001b[0m                     0.8169        \u001b[31m0.3971\u001b[0m  0.0025  1.2413\n",
      "     18                     \u001b[36m0.8246\u001b[0m        \u001b[32m0.3899\u001b[0m                     0.8116        0.4029  0.0020  1.2437\n",
      "     19                     0.8243        \u001b[32m0.3838\u001b[0m                     0.8162        \u001b[31m0.3959\u001b[0m  0.0015  1.2436\n",
      "     20                     0.8237        0.3844                     0.8141        \u001b[31m0.3942\u001b[0m  0.0010  1.2457\n",
      "     21                     \u001b[36m0.8317\u001b[0m        \u001b[32m0.3721\u001b[0m                     0.8185        \u001b[31m0.3903\u001b[0m  0.0007  1.2408\n",
      "     22                     0.8289        0.3754                     0.8184        \u001b[31m0.3881\u001b[0m  0.0004  1.2434\n",
      "     23                     0.8313        0.3753                     0.8171        0.3883  0.0002  1.2420\n",
      "     24                     \u001b[36m0.8336\u001b[0m        \u001b[32m0.3684\u001b[0m                     0.8165        \u001b[31m0.3876\u001b[0m  0.0000  1.2436\n",
      "     25                     0.8303        0.3738                     0.8153        0.3878  0.0000  1.2428\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6535\u001b[0m        \u001b[32m0.8681\u001b[0m                     \u001b[35m0.7384\u001b[0m        \u001b[31m0.5523\u001b[0m  0.0100  1.2389\n",
      "      2                     \u001b[36m0.7456\u001b[0m        \u001b[32m0.5448\u001b[0m                     \u001b[35m0.7756\u001b[0m        \u001b[31m0.4738\u001b[0m  0.0100  1.2398\n",
      "      3                     \u001b[36m0.7716\u001b[0m        \u001b[32m0.4824\u001b[0m                     \u001b[35m0.7937\u001b[0m        \u001b[31m0.4422\u001b[0m  0.0098  1.2417\n",
      "      4                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4633\u001b[0m                     \u001b[35m0.7966\u001b[0m        \u001b[31m0.4351\u001b[0m  0.0096  1.2429\n",
      "      5                     \u001b[36m0.7912\u001b[0m        \u001b[32m0.4518\u001b[0m                     0.7934        0.4431  0.0093  1.2448\n",
      "      6                     \u001b[36m0.7925\u001b[0m        \u001b[32m0.4463\u001b[0m                     0.7937        0.4451  0.0090  1.2391\n",
      "      7                     \u001b[36m0.7994\u001b[0m        \u001b[32m0.4390\u001b[0m                     \u001b[35m0.8082\u001b[0m        \u001b[31m0.4206\u001b[0m  0.0085  1.2413\n",
      "      8                     0.7975        \u001b[32m0.4355\u001b[0m                     0.7938        0.4412  0.0080  1.2427\n",
      "      9                     0.7992        \u001b[32m0.4321\u001b[0m                     \u001b[35m0.8112\u001b[0m        \u001b[31m0.4108\u001b[0m  0.0075  1.2430\n",
      "     10                     \u001b[36m0.8084\u001b[0m        \u001b[32m0.4214\u001b[0m                     0.8075        0.4210  0.0069  1.2405\n",
      "     11                     0.8068        \u001b[32m0.4185\u001b[0m                     \u001b[35m0.8117\u001b[0m        0.4131  0.0063  1.2404\n",
      "     12                     \u001b[36m0.8117\u001b[0m        \u001b[32m0.4118\u001b[0m                     0.8104        0.4132  0.0057  1.2417\n",
      "     13                     \u001b[36m0.8133\u001b[0m        \u001b[32m0.4101\u001b[0m                     \u001b[35m0.8129\u001b[0m        \u001b[31m0.4083\u001b[0m  0.0050  1.2425\n",
      "     14                     \u001b[36m0.8167\u001b[0m        \u001b[32m0.4035\u001b[0m                     0.8097        0.4097  0.0043  1.2422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15                     \u001b[36m0.8213\u001b[0m        \u001b[32m0.3999\u001b[0m                     0.8114        \u001b[31m0.4081\u001b[0m  0.0037  1.2435\n",
      "     16                     \u001b[36m0.8215\u001b[0m        \u001b[32m0.3939\u001b[0m                     \u001b[35m0.8130\u001b[0m        \u001b[31m0.4074\u001b[0m  0.0031  1.2437\n",
      "     17                     \u001b[36m0.8257\u001b[0m        \u001b[32m0.3869\u001b[0m                     0.8115        \u001b[31m0.4002\u001b[0m  0.0025  1.2476\n",
      "     18                     0.8219        \u001b[32m0.3868\u001b[0m                     0.8120        \u001b[31m0.3971\u001b[0m  0.0020  1.2437\n",
      "     19                     \u001b[36m0.8318\u001b[0m        \u001b[32m0.3782\u001b[0m                     \u001b[35m0.8174\u001b[0m        \u001b[31m0.3942\u001b[0m  0.0015  1.2431\n",
      "     20                     \u001b[36m0.8320\u001b[0m        \u001b[32m0.3738\u001b[0m                     0.8150        0.3956  0.0010  1.2435\n",
      "     21                     \u001b[36m0.8338\u001b[0m        \u001b[32m0.3693\u001b[0m                     0.8165        0.3964  0.0007  1.2437\n",
      "     22                     \u001b[36m0.8349\u001b[0m        0.3695                     0.8138        0.3951  0.0004  1.2447\n",
      "     23                     \u001b[36m0.8350\u001b[0m        \u001b[32m0.3676\u001b[0m                     0.8133        0.3973  0.0002  1.2447\n",
      "     24                     \u001b[36m0.8425\u001b[0m        \u001b[32m0.3636\u001b[0m                     \u001b[35m0.8180\u001b[0m        0.3958  0.0000  1.2428\n",
      "     25                     0.8339        0.3671                     0.8128        0.3960  0.0000  1.2453\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6315\u001b[0m        \u001b[32m0.8974\u001b[0m                     \u001b[35m0.7450\u001b[0m        \u001b[31m0.5264\u001b[0m  0.0100  1.2394\n",
      "      2                     \u001b[36m0.7475\u001b[0m        \u001b[32m0.5479\u001b[0m                     \u001b[35m0.7575\u001b[0m        \u001b[31m0.4906\u001b[0m  0.0100  1.2410\n",
      "      3                     \u001b[36m0.7729\u001b[0m        \u001b[32m0.4891\u001b[0m                     \u001b[35m0.7866\u001b[0m        \u001b[31m0.4482\u001b[0m  0.0098  1.2427\n",
      "      4                     \u001b[36m0.7812\u001b[0m        \u001b[32m0.4668\u001b[0m                     \u001b[35m0.7945\u001b[0m        \u001b[31m0.4418\u001b[0m  0.0096  1.2404\n",
      "      5                     \u001b[36m0.7853\u001b[0m        \u001b[32m0.4509\u001b[0m                     0.7942        \u001b[31m0.4407\u001b[0m  0.0093  1.2417\n",
      "      6                     \u001b[36m0.7931\u001b[0m        \u001b[32m0.4424\u001b[0m                     \u001b[35m0.7981\u001b[0m        \u001b[31m0.4333\u001b[0m  0.0090  1.2410\n",
      "      7                     \u001b[36m0.8016\u001b[0m        \u001b[32m0.4361\u001b[0m                     \u001b[35m0.8027\u001b[0m        \u001b[31m0.4257\u001b[0m  0.0085  1.2415\n",
      "      8                     \u001b[36m0.8034\u001b[0m        \u001b[32m0.4314\u001b[0m                     0.7984        \u001b[31m0.4251\u001b[0m  0.0080  1.2420\n",
      "      9                     \u001b[36m0.8049\u001b[0m        \u001b[32m0.4270\u001b[0m                     \u001b[35m0.8048\u001b[0m        \u001b[31m0.4202\u001b[0m  0.0075  1.2425\n",
      "     10                     \u001b[36m0.8070\u001b[0m        \u001b[32m0.4186\u001b[0m                     \u001b[35m0.8114\u001b[0m        \u001b[31m0.4075\u001b[0m  0.0069  1.2415\n",
      "     11                     \u001b[36m0.8076\u001b[0m        \u001b[32m0.4171\u001b[0m                     0.8053        0.4139  0.0063  1.2437\n",
      "     12                     \u001b[36m0.8100\u001b[0m        \u001b[32m0.4093\u001b[0m                     0.8043        0.4083  0.0057  1.2431\n",
      "     13                     \u001b[36m0.8130\u001b[0m        \u001b[32m0.4071\u001b[0m                     0.8070        0.4118  0.0050  1.2427\n",
      "     14                     \u001b[36m0.8142\u001b[0m        \u001b[32m0.4037\u001b[0m                     0.8017        0.4137  0.0043  1.2429\n",
      "     15                     \u001b[36m0.8176\u001b[0m        \u001b[32m0.3971\u001b[0m                     0.8099        \u001b[31m0.4073\u001b[0m  0.0037  1.2428\n",
      "     16                     \u001b[36m0.8218\u001b[0m        \u001b[32m0.3959\u001b[0m                     \u001b[35m0.8168\u001b[0m        \u001b[31m0.3983\u001b[0m  0.0031  1.2428\n",
      "     17                     \u001b[36m0.8229\u001b[0m        \u001b[32m0.3879\u001b[0m                     \u001b[35m0.8190\u001b[0m        \u001b[31m0.3967\u001b[0m  0.0025  1.2447\n",
      "     18                     \u001b[36m0.8252\u001b[0m        \u001b[32m0.3846\u001b[0m                     0.8157        \u001b[31m0.3940\u001b[0m  0.0020  1.2436\n",
      "     19                     0.8240        \u001b[32m0.3837\u001b[0m                     0.8087        0.4023  0.0015  1.2434\n",
      "     20                     \u001b[36m0.8302\u001b[0m        \u001b[32m0.3754\u001b[0m                     0.8174        0.3977  0.0010  1.2442\n",
      "     21                     \u001b[36m0.8311\u001b[0m        0.3756                     0.8144        0.3952  0.0007  1.2436\n",
      "     22                     \u001b[36m0.8317\u001b[0m        \u001b[32m0.3745\u001b[0m                     0.8123        0.3965  0.0004  1.2438\n",
      "     23                     \u001b[36m0.8321\u001b[0m        \u001b[32m0.3707\u001b[0m                     0.8148        \u001b[31m0.3924\u001b[0m  0.0002  1.2435\n",
      "     24                     \u001b[36m0.8340\u001b[0m        \u001b[32m0.3696\u001b[0m                     0.8135        0.3934  0.0000  1.2437\n",
      "     25                     0.8323        0.3700                     0.8138        \u001b[31m0.3923\u001b[0m  0.0000  1.2447\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6289\u001b[0m        \u001b[32m0.8760\u001b[0m                     \u001b[35m0.7454\u001b[0m        \u001b[31m0.5225\u001b[0m  0.0100  1.2388\n",
      "      2                     \u001b[36m0.7466\u001b[0m        \u001b[32m0.5491\u001b[0m                     \u001b[35m0.7752\u001b[0m        \u001b[31m0.4753\u001b[0m  0.0100  1.2429\n",
      "      3                     \u001b[36m0.7715\u001b[0m        \u001b[32m0.4871\u001b[0m                     0.7709        0.5157  0.0098  1.2414\n",
      "      4                     \u001b[36m0.7858\u001b[0m        \u001b[32m0.4627\u001b[0m                     \u001b[35m0.7917\u001b[0m        \u001b[31m0.4451\u001b[0m  0.0096  1.2412\n",
      "      5                     \u001b[36m0.7898\u001b[0m        \u001b[32m0.4513\u001b[0m                     0.7891        \u001b[31m0.4438\u001b[0m  0.0093  1.2431\n",
      "      6                     \u001b[36m0.7961\u001b[0m        \u001b[32m0.4425\u001b[0m                     \u001b[35m0.7951\u001b[0m        \u001b[31m0.4376\u001b[0m  0.0090  1.2428\n",
      "      7                     \u001b[36m0.7987\u001b[0m        \u001b[32m0.4350\u001b[0m                     0.7918        \u001b[31m0.4340\u001b[0m  0.0085  1.2423\n",
      "      8                     \u001b[36m0.8021\u001b[0m        \u001b[32m0.4287\u001b[0m                     \u001b[35m0.7996\u001b[0m        \u001b[31m0.4298\u001b[0m  0.0080  1.2410\n",
      "      9                     \u001b[36m0.8065\u001b[0m        \u001b[32m0.4220\u001b[0m                     0.7940        0.4455  0.0075  1.2426\n",
      "     10                     \u001b[36m0.8071\u001b[0m        \u001b[32m0.4189\u001b[0m                     \u001b[35m0.8035\u001b[0m        \u001b[31m0.4215\u001b[0m  0.0069  1.2420\n",
      "     11                     \u001b[36m0.8100\u001b[0m        \u001b[32m0.4132\u001b[0m                     \u001b[35m0.8049\u001b[0m        0.4225  0.0063  1.2432\n",
      "     12                     \u001b[36m0.8157\u001b[0m        \u001b[32m0.4092\u001b[0m                     0.8048        \u001b[31m0.4178\u001b[0m  0.0057  1.2414\n",
      "     13                     0.8156        \u001b[32m0.4058\u001b[0m                     \u001b[35m0.8176\u001b[0m        \u001b[31m0.4167\u001b[0m  0.0050  1.2442\n",
      "     14                     \u001b[36m0.8177\u001b[0m        \u001b[32m0.3999\u001b[0m                     0.8091        0.4183  0.0043  1.2439\n",
      "     15                     \u001b[36m0.8208\u001b[0m        \u001b[32m0.3982\u001b[0m                     0.8052        \u001b[31m0.4066\u001b[0m  0.0037  1.2451\n",
      "     16                     \u001b[36m0.8248\u001b[0m        \u001b[32m0.3901\u001b[0m                     0.8123        0.4104  0.0031  1.2427\n",
      "     17                     0.8246        \u001b[32m0.3889\u001b[0m                     0.8147        0.4072  0.0025  1.2440\n",
      "     18                     \u001b[36m0.8287\u001b[0m        \u001b[32m0.3801\u001b[0m                     \u001b[35m0.8180\u001b[0m        \u001b[31m0.4011\u001b[0m  0.0020  1.2425\n",
      "     19                     \u001b[36m0.8309\u001b[0m        \u001b[32m0.3781\u001b[0m                     0.8175        \u001b[31m0.3963\u001b[0m  0.0015  1.2428\n",
      "     20                     0.8291        \u001b[32m0.3763\u001b[0m                     0.8154        0.3969  0.0010  1.2420\n",
      "     21                     0.8307        \u001b[32m0.3729\u001b[0m                     0.8176        0.3981  0.0007  1.2439\n",
      "     22                     \u001b[36m0.8331\u001b[0m        0.3732                     0.8158        0.3978  0.0004  1.2463\n",
      "     23                     \u001b[36m0.8363\u001b[0m        \u001b[32m0.3656\u001b[0m                     \u001b[35m0.8188\u001b[0m        \u001b[31m0.3947\u001b[0m  0.0002  1.2442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24                     0.8360        \u001b[32m0.3649\u001b[0m                     \u001b[35m0.8201\u001b[0m        0.3948  0.0000  1.2448\n",
      "     25                     \u001b[36m0.8366\u001b[0m        \u001b[32m0.3643\u001b[0m                     \u001b[35m0.8201\u001b[0m        0.3955  0.0000  1.2450\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6266\u001b[0m        \u001b[32m0.9128\u001b[0m                     \u001b[35m0.7327\u001b[0m        \u001b[31m0.5523\u001b[0m  0.0100  1.2366\n",
      "      2                     \u001b[36m0.7511\u001b[0m        \u001b[32m0.5280\u001b[0m                     \u001b[35m0.7516\u001b[0m        \u001b[31m0.5186\u001b[0m  0.0100  1.2415\n",
      "      3                     \u001b[36m0.7716\u001b[0m        \u001b[32m0.4841\u001b[0m                     \u001b[35m0.7800\u001b[0m        \u001b[31m0.4623\u001b[0m  0.0098  1.2418\n",
      "      4                     \u001b[36m0.7838\u001b[0m        \u001b[32m0.4653\u001b[0m                     \u001b[35m0.7966\u001b[0m        \u001b[31m0.4483\u001b[0m  0.0096  1.2407\n",
      "      5                     \u001b[36m0.7889\u001b[0m        \u001b[32m0.4546\u001b[0m                     0.7913        \u001b[31m0.4429\u001b[0m  0.0093  1.2413\n",
      "      6                     \u001b[36m0.7926\u001b[0m        \u001b[32m0.4465\u001b[0m                     0.7885        0.4492  0.0090  1.2393\n",
      "      7                     \u001b[36m0.7994\u001b[0m        \u001b[32m0.4360\u001b[0m                     0.7859        \u001b[31m0.4399\u001b[0m  0.0085  1.2404\n",
      "      8                     0.7994        \u001b[32m0.4313\u001b[0m                     0.7930        \u001b[31m0.4311\u001b[0m  0.0080  1.2420\n",
      "      9                     \u001b[36m0.7997\u001b[0m        \u001b[32m0.4281\u001b[0m                     0.7960        0.4329  0.0075  1.2432\n",
      "     10                     \u001b[36m0.8052\u001b[0m        \u001b[32m0.4217\u001b[0m                     \u001b[35m0.8060\u001b[0m        \u001b[31m0.4236\u001b[0m  0.0069  1.2426\n",
      "     11                     \u001b[36m0.8088\u001b[0m        \u001b[32m0.4160\u001b[0m                     0.8036        \u001b[31m0.4204\u001b[0m  0.0063  1.2401\n",
      "     12                     \u001b[36m0.8116\u001b[0m        \u001b[32m0.4110\u001b[0m                     \u001b[35m0.8082\u001b[0m        \u001b[31m0.4110\u001b[0m  0.0057  1.2427\n",
      "     13                     0.8105        \u001b[32m0.4086\u001b[0m                     0.8059        0.4250  0.0050  1.2427\n",
      "     14                     \u001b[36m0.8137\u001b[0m        \u001b[32m0.4004\u001b[0m                     \u001b[35m0.8190\u001b[0m        \u001b[31m0.4060\u001b[0m  0.0043  1.2423\n",
      "     15                     \u001b[36m0.8215\u001b[0m        \u001b[32m0.3958\u001b[0m                     0.8131        0.4111  0.0037  1.2427\n",
      "     16                     \u001b[36m0.8238\u001b[0m        \u001b[32m0.3923\u001b[0m                     \u001b[35m0.8198\u001b[0m        \u001b[31m0.4017\u001b[0m  0.0031  1.2419\n",
      "     17                     \u001b[36m0.8276\u001b[0m        \u001b[32m0.3853\u001b[0m                     0.8103        0.4076  0.0025  1.2429\n",
      "     18                     0.8245        \u001b[32m0.3850\u001b[0m                     \u001b[35m0.8222\u001b[0m        \u001b[31m0.3954\u001b[0m  0.0020  1.2440\n",
      "     19                     0.8224        \u001b[32m0.3819\u001b[0m                     0.8156        0.3966  0.0015  1.2423\n",
      "     20                     \u001b[36m0.8307\u001b[0m        \u001b[32m0.3753\u001b[0m                     0.8179        0.3986  0.0010  1.2429\n",
      "     21                     \u001b[36m0.8316\u001b[0m        \u001b[32m0.3724\u001b[0m                     0.8183        0.3965  0.0007  1.2437\n",
      "     22                     0.8307        \u001b[32m0.3719\u001b[0m                     0.8213        0.3975  0.0004  1.2436\n",
      "     23                     \u001b[36m0.8326\u001b[0m        \u001b[32m0.3677\u001b[0m                     0.8214        \u001b[31m0.3948\u001b[0m  0.0002  1.2410\n",
      "     24                     0.8313        0.3721                     \u001b[35m0.8241\u001b[0m        0.3963  0.0000  1.2437\n",
      "     25                     \u001b[36m0.8340\u001b[0m        \u001b[32m0.3670\u001b[0m                     0.8222        0.3952  0.0000  1.2442\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6439\u001b[0m        \u001b[32m0.8835\u001b[0m                     \u001b[35m0.6320\u001b[0m        \u001b[31m0.8729\u001b[0m  0.0100  1.2387\n",
      "      2                     \u001b[36m0.7430\u001b[0m        \u001b[32m0.5491\u001b[0m                     \u001b[35m0.7557\u001b[0m        \u001b[31m0.4959\u001b[0m  0.0100  1.2406\n",
      "      3                     \u001b[36m0.7687\u001b[0m        \u001b[32m0.4889\u001b[0m                     \u001b[35m0.7928\u001b[0m        \u001b[31m0.4554\u001b[0m  0.0098  1.2427\n",
      "      4                     \u001b[36m0.7788\u001b[0m        \u001b[32m0.4696\u001b[0m                     \u001b[35m0.7937\u001b[0m        \u001b[31m0.4436\u001b[0m  0.0096  1.2417\n",
      "      5                     \u001b[36m0.7911\u001b[0m        \u001b[32m0.4548\u001b[0m                     0.7882        0.4625  0.0093  1.2424\n",
      "      6                     \u001b[36m0.7945\u001b[0m        \u001b[32m0.4420\u001b[0m                     \u001b[35m0.7946\u001b[0m        \u001b[31m0.4400\u001b[0m  0.0090  1.2433\n",
      "      7                     \u001b[36m0.7951\u001b[0m        \u001b[32m0.4415\u001b[0m                     \u001b[35m0.7992\u001b[0m        \u001b[31m0.4382\u001b[0m  0.0085  1.2429\n",
      "      8                     \u001b[36m0.8000\u001b[0m        \u001b[32m0.4366\u001b[0m                     0.7939        0.4420  0.0080  1.2418\n",
      "      9                     \u001b[36m0.8075\u001b[0m        \u001b[32m0.4234\u001b[0m                     0.7959        \u001b[31m0.4316\u001b[0m  0.0075  1.2414\n",
      "     10                     0.8068        \u001b[32m0.4220\u001b[0m                     \u001b[35m0.8117\u001b[0m        \u001b[31m0.4220\u001b[0m  0.0069  1.2415\n",
      "     11                     0.8074        \u001b[32m0.4179\u001b[0m                     0.8099        0.4247  0.0063  1.2431\n",
      "     12                     \u001b[36m0.8084\u001b[0m        \u001b[32m0.4139\u001b[0m                     0.8070        \u001b[31m0.4135\u001b[0m  0.0057  1.2418\n",
      "     13                     \u001b[36m0.8155\u001b[0m        \u001b[32m0.4067\u001b[0m                     0.8085        \u001b[31m0.4122\u001b[0m  0.0050  1.2430\n",
      "     14                     \u001b[36m0.8175\u001b[0m        \u001b[32m0.4002\u001b[0m                     \u001b[35m0.8137\u001b[0m        \u001b[31m0.4118\u001b[0m  0.0043  1.2437\n",
      "     15                     0.8161        \u001b[32m0.3992\u001b[0m                     \u001b[35m0.8143\u001b[0m        \u001b[31m0.4060\u001b[0m  0.0037  1.2434\n",
      "     16                     \u001b[36m0.8219\u001b[0m        \u001b[32m0.3931\u001b[0m                     \u001b[35m0.8219\u001b[0m        \u001b[31m0.4018\u001b[0m  0.0031  1.2427\n",
      "     17                     0.8205        \u001b[32m0.3916\u001b[0m                     0.8094        0.4082  0.0025  1.2431\n",
      "     18                     \u001b[36m0.8220\u001b[0m        \u001b[32m0.3859\u001b[0m                     0.8171        \u001b[31m0.4003\u001b[0m  0.0020  1.2427\n",
      "     19                     \u001b[36m0.8253\u001b[0m        \u001b[32m0.3841\u001b[0m                     0.8189        0.4059  0.0015  1.2411\n",
      "     20                     \u001b[36m0.8255\u001b[0m        \u001b[32m0.3769\u001b[0m                     0.8184        \u001b[31m0.3989\u001b[0m  0.0010  1.2424\n",
      "     21                     \u001b[36m0.8343\u001b[0m        \u001b[32m0.3734\u001b[0m                     0.8183        \u001b[31m0.3977\u001b[0m  0.0007  1.2429\n",
      "     22                     \u001b[36m0.8367\u001b[0m        \u001b[32m0.3697\u001b[0m                     0.8196        0.3982  0.0004  1.2437\n",
      "     23                     0.8349        \u001b[32m0.3687\u001b[0m                     0.8189        0.3982  0.0002  1.2460\n",
      "     24                     0.8318        0.3715                     0.8174        0.3985  0.0000  1.2434\n",
      "     25                     0.8342        \u001b[32m0.3680\u001b[0m                     0.8193        \u001b[31m0.3976\u001b[0m  0.0000  1.2447\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6470\u001b[0m        \u001b[32m0.8373\u001b[0m                     \u001b[35m0.6924\u001b[0m        \u001b[31m0.7046\u001b[0m  0.0100  1.2394\n",
      "      2                     \u001b[36m0.7453\u001b[0m        \u001b[32m0.5434\u001b[0m                     \u001b[35m0.7673\u001b[0m        \u001b[31m0.4837\u001b[0m  0.0100  1.2416\n",
      "      3                     \u001b[36m0.7728\u001b[0m        \u001b[32m0.4775\u001b[0m                     \u001b[35m0.7887\u001b[0m        \u001b[31m0.4462\u001b[0m  0.0098  1.2413\n",
      "      4                     \u001b[36m0.7826\u001b[0m        \u001b[32m0.4619\u001b[0m                     \u001b[35m0.7893\u001b[0m        0.4525  0.0096  1.2421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5                     \u001b[36m0.7897\u001b[0m        \u001b[32m0.4527\u001b[0m                     \u001b[35m0.7901\u001b[0m        0.4474  0.0093  1.2435\n",
      "      6                     \u001b[36m0.7952\u001b[0m        \u001b[32m0.4406\u001b[0m                     \u001b[35m0.7907\u001b[0m        \u001b[31m0.4418\u001b[0m  0.0090  1.2436\n",
      "      7                     \u001b[36m0.7994\u001b[0m        \u001b[32m0.4370\u001b[0m                     \u001b[35m0.8034\u001b[0m        \u001b[31m0.4299\u001b[0m  0.0085  1.2411\n",
      "      8                     0.7984        \u001b[32m0.4337\u001b[0m                     0.8013        \u001b[31m0.4283\u001b[0m  0.0080  1.2425\n",
      "      9                     \u001b[36m0.8027\u001b[0m        \u001b[32m0.4264\u001b[0m                     \u001b[35m0.8045\u001b[0m        0.4315  0.0075  1.2421\n",
      "     10                     \u001b[36m0.8077\u001b[0m        \u001b[32m0.4198\u001b[0m                     \u001b[35m0.8068\u001b[0m        \u001b[31m0.4212\u001b[0m  0.0069  1.2414\n",
      "     11                     \u001b[36m0.8093\u001b[0m        \u001b[32m0.4182\u001b[0m                     0.8049        \u001b[31m0.4145\u001b[0m  0.0063  1.2446\n",
      "     12                     \u001b[36m0.8113\u001b[0m        \u001b[32m0.4140\u001b[0m                     0.8047        0.4222  0.0057  1.2429\n",
      "     13                     \u001b[36m0.8144\u001b[0m        \u001b[32m0.4049\u001b[0m                     \u001b[35m0.8116\u001b[0m        \u001b[31m0.4080\u001b[0m  0.0050  1.2457\n",
      "     14                     \u001b[36m0.8182\u001b[0m        \u001b[32m0.3979\u001b[0m                     0.8070        0.4088  0.0043  1.2437\n",
      "     15                     0.8152        0.3980                     \u001b[35m0.8135\u001b[0m        \u001b[31m0.4046\u001b[0m  0.0037  1.2428\n",
      "     16                     0.8166        \u001b[32m0.3929\u001b[0m                     0.8133        \u001b[31m0.4027\u001b[0m  0.0031  1.2433\n",
      "     17                     \u001b[36m0.8282\u001b[0m        \u001b[32m0.3856\u001b[0m                     \u001b[35m0.8163\u001b[0m        \u001b[31m0.4017\u001b[0m  0.0025  1.2446\n",
      "     18                     0.8264        \u001b[32m0.3850\u001b[0m                     0.8090        0.4027  0.0020  1.2441\n",
      "     19                     0.8249        \u001b[32m0.3834\u001b[0m                     0.8156        \u001b[31m0.3982\u001b[0m  0.0015  1.2437\n",
      "     20                     \u001b[36m0.8317\u001b[0m        \u001b[32m0.3757\u001b[0m                     0.8135        \u001b[31m0.3979\u001b[0m  0.0010  1.2451\n",
      "     21                     0.8317        \u001b[32m0.3719\u001b[0m                     0.8140        \u001b[31m0.3978\u001b[0m  0.0007  1.2442\n",
      "     22                     0.8311        \u001b[32m0.3711\u001b[0m                     0.8153        \u001b[31m0.3952\u001b[0m  0.0004  1.2426\n",
      "     23                     \u001b[36m0.8361\u001b[0m        \u001b[32m0.3679\u001b[0m                     0.8156        0.3958  0.0002  1.2454\n",
      "     24                     \u001b[36m0.8374\u001b[0m        \u001b[32m0.3670\u001b[0m                     0.8144        0.3952  0.0000  1.2448\n",
      "     25                     0.8339        0.3686                     0.8147        0.3953  0.0000  1.2437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5823\u001b[0m        \u001b[32m0.6966\u001b[0m                     \u001b[35m0.6637\u001b[0m        \u001b[31m0.6373\u001b[0m  0.0100  1.4371\n",
      "      2                     \u001b[36m0.6577\u001b[0m        \u001b[32m0.6293\u001b[0m                     \u001b[35m0.6746\u001b[0m        \u001b[31m0.6153\u001b[0m  0.0100  1.3632\n",
      "      3                     \u001b[36m0.6853\u001b[0m        \u001b[32m0.5942\u001b[0m                     \u001b[35m0.7054\u001b[0m        \u001b[31m0.5756\u001b[0m  0.0098  1.3633\n",
      "      4                     \u001b[36m0.7017\u001b[0m        \u001b[32m0.5796\u001b[0m                     \u001b[35m0.7174\u001b[0m        \u001b[31m0.5626\u001b[0m  0.0096  1.3634\n",
      "      5                     \u001b[36m0.7078\u001b[0m        \u001b[32m0.5701\u001b[0m                     0.6990        0.5792  0.0093  1.3634\n",
      "      6                     \u001b[36m0.7246\u001b[0m        \u001b[32m0.5542\u001b[0m                     \u001b[35m0.7321\u001b[0m        \u001b[31m0.5552\u001b[0m  0.0090  1.3631\n",
      "      7                     \u001b[36m0.7383\u001b[0m        \u001b[32m0.5357\u001b[0m                     \u001b[35m0.7389\u001b[0m        \u001b[31m0.5386\u001b[0m  0.0085  1.3632\n",
      "      8                     \u001b[36m0.7399\u001b[0m        \u001b[32m0.5291\u001b[0m                     0.7314        0.5644  0.0080  1.3634\n",
      "      9                     \u001b[36m0.7494\u001b[0m        \u001b[32m0.5204\u001b[0m                     0.7012        0.5717  0.0075  1.3628\n",
      "     10                     0.7457        \u001b[32m0.5153\u001b[0m                     0.7379        0.5423  0.0069  1.3628\n",
      "     11                     \u001b[36m0.7526\u001b[0m        \u001b[32m0.5091\u001b[0m                     0.7077        0.5739  0.0063  1.3631\n",
      "     12                     \u001b[36m0.7638\u001b[0m        \u001b[32m0.4996\u001b[0m                     \u001b[35m0.7641\u001b[0m        \u001b[31m0.5146\u001b[0m  0.0057  1.3643\n",
      "     13                     0.7584        0.4999                     0.7423        0.5359  0.0050  1.3628\n",
      "     14                     \u001b[36m0.7732\u001b[0m        \u001b[32m0.4850\u001b[0m                     \u001b[35m0.7691\u001b[0m        \u001b[31m0.5033\u001b[0m  0.0043  1.3634\n",
      "     15                     0.7680        0.4903                     0.7571        0.5195  0.0037  1.3661\n",
      "     16                     0.7672        \u001b[32m0.4827\u001b[0m                     \u001b[35m0.7691\u001b[0m        \u001b[31m0.4909\u001b[0m  0.0031  1.3637\n",
      "     17                     \u001b[36m0.7813\u001b[0m        \u001b[32m0.4748\u001b[0m                     \u001b[35m0.7700\u001b[0m        0.4964  0.0025  1.3641\n",
      "     18                     \u001b[36m0.7829\u001b[0m        \u001b[32m0.4739\u001b[0m                     0.7611        0.5067  0.0020  1.3631\n",
      "     19                     \u001b[36m0.7848\u001b[0m        \u001b[32m0.4654\u001b[0m                     \u001b[35m0.7843\u001b[0m        \u001b[31m0.4852\u001b[0m  0.0015  1.3639\n",
      "     20                     0.7773        0.4678                     0.7841        \u001b[31m0.4833\u001b[0m  0.0010  1.3634\n",
      "     21                     0.7840        \u001b[32m0.4605\u001b[0m                     \u001b[35m0.7927\u001b[0m        0.4842  0.0007  1.3631\n",
      "     22                     \u001b[36m0.7932\u001b[0m        \u001b[32m0.4485\u001b[0m                     \u001b[35m0.7934\u001b[0m        \u001b[31m0.4832\u001b[0m  0.0004  1.3634\n",
      "     23                     0.7911        0.4537                     0.7920        \u001b[31m0.4816\u001b[0m  0.0002  1.3623\n",
      "     24                     0.7925        0.4508                     0.7869        0.4817  0.0000  1.3641\n",
      "     25                     \u001b[36m0.7956\u001b[0m        0.4499                     0.7879        0.4817  0.0000  1.3640\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5831\u001b[0m        \u001b[32m0.6970\u001b[0m                     \u001b[35m0.6666\u001b[0m        \u001b[31m0.6173\u001b[0m  0.0100  1.3612\n",
      "      2                     \u001b[36m0.6645\u001b[0m        \u001b[32m0.6136\u001b[0m                     \u001b[35m0.6909\u001b[0m        \u001b[31m0.5835\u001b[0m  0.0100  1.3634\n",
      "      3                     \u001b[36m0.6873\u001b[0m        \u001b[32m0.5975\u001b[0m                     \u001b[35m0.7222\u001b[0m        \u001b[31m0.5480\u001b[0m  0.0098  1.3631\n",
      "      4                     \u001b[36m0.7076\u001b[0m        \u001b[32m0.5740\u001b[0m                     \u001b[35m0.7327\u001b[0m        \u001b[31m0.5235\u001b[0m  0.0096  1.3634\n",
      "      5                     \u001b[36m0.7231\u001b[0m        \u001b[32m0.5549\u001b[0m                     \u001b[35m0.7373\u001b[0m        0.5282  0.0093  1.3652\n",
      "      6                     \u001b[36m0.7270\u001b[0m        \u001b[32m0.5513\u001b[0m                     \u001b[35m0.7564\u001b[0m        \u001b[31m0.5047\u001b[0m  0.0090  1.3628\n",
      "      7                     \u001b[36m0.7491\u001b[0m        \u001b[32m0.5329\u001b[0m                     0.7274        0.5459  0.0085  1.3635\n",
      "      8                     0.7406        \u001b[32m0.5301\u001b[0m                     0.7511        0.5088  0.0080  1.3633\n",
      "      9                     0.7462        \u001b[32m0.5201\u001b[0m                     0.7496        0.5072  0.0075  1.3634\n",
      "     10                     \u001b[36m0.7540\u001b[0m        \u001b[32m0.5022\u001b[0m                     0.7412        0.5143  0.0069  1.3628\n",
      "     11                     0.7525        0.5045                     \u001b[35m0.7571\u001b[0m        \u001b[31m0.5001\u001b[0m  0.0063  1.3633\n",
      "     12                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4978\u001b[0m                     0.6875        0.5959  0.0057  1.3634\n",
      "     13                     \u001b[36m0.7677\u001b[0m        \u001b[32m0.4916\u001b[0m                     0.7439        0.5192  0.0050  1.3639\n",
      "     14                     0.7663        \u001b[32m0.4905\u001b[0m                     0.7495        0.5083  0.0043  1.3632\n",
      "     15                     \u001b[36m0.7703\u001b[0m        \u001b[32m0.4889\u001b[0m                     0.6406        0.6051  0.0037  1.3626\n",
      "     16                     \u001b[36m0.7774\u001b[0m        \u001b[32m0.4768\u001b[0m                     0.7439        0.5115  0.0031  1.3627\n",
      "     17                     0.7759        \u001b[32m0.4752\u001b[0m                     \u001b[35m0.7628\u001b[0m        \u001b[31m0.4887\u001b[0m  0.0025  1.3633\n",
      "     18                     \u001b[36m0.7875\u001b[0m        \u001b[32m0.4587\u001b[0m                     \u001b[35m0.7639\u001b[0m        \u001b[31m0.4778\u001b[0m  0.0020  1.3633\n",
      "     19                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4587\u001b[0m                     0.7207        0.5275  0.0015  1.3629\n",
      "     20                     0.7876        \u001b[32m0.4553\u001b[0m                     0.7555        0.4845  0.0010  1.3632\n",
      "     21                     \u001b[36m0.7932\u001b[0m        \u001b[32m0.4537\u001b[0m                     \u001b[35m0.7753\u001b[0m        \u001b[31m0.4692\u001b[0m  0.0007  1.3630\n",
      "     22                     \u001b[36m0.7955\u001b[0m        0.4538                     0.7719        \u001b[31m0.4641\u001b[0m  0.0004  1.3629\n",
      "     23                     0.7833        0.4556                     0.7626        0.4664  0.0002  1.3631\n",
      "     24                     \u001b[36m0.7988\u001b[0m        \u001b[32m0.4452\u001b[0m                     0.7645        0.4673  0.0000  1.3637\n",
      "     25                     0.7949        0.4471                     0.7629        0.4682  0.0000  1.3642\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5796\u001b[0m        \u001b[32m0.6896\u001b[0m                     \u001b[35m0.6501\u001b[0m        \u001b[31m0.6248\u001b[0m  0.0100  1.3614\n",
      "      2                     \u001b[36m0.6654\u001b[0m        \u001b[32m0.6132\u001b[0m                     \u001b[35m0.6779\u001b[0m        \u001b[31m0.6084\u001b[0m  0.0100  1.3664\n",
      "      3                     \u001b[36m0.6890\u001b[0m        \u001b[32m0.5909\u001b[0m                     \u001b[35m0.7054\u001b[0m        \u001b[31m0.5927\u001b[0m  0.0098  1.3694\n",
      "      4                     \u001b[36m0.7110\u001b[0m        \u001b[32m0.5712\u001b[0m                     \u001b[35m0.7185\u001b[0m        \u001b[31m0.5659\u001b[0m  0.0096  1.3684\n",
      "      5                     \u001b[36m0.7258\u001b[0m        \u001b[32m0.5448\u001b[0m                     0.7010        0.6018  0.0093  1.3664\n",
      "      6                     0.7249        0.5473                     \u001b[35m0.7392\u001b[0m        \u001b[31m0.5347\u001b[0m  0.0090  1.3674\n",
      "      7                     \u001b[36m0.7426\u001b[0m        \u001b[32m0.5357\u001b[0m                     0.7385        \u001b[31m0.5287\u001b[0m  0.0085  1.3703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.5154\u001b[0m                     0.7295        0.5436  0.0080  1.3673\n",
      "      9                     \u001b[36m0.7605\u001b[0m        0.5167                     0.7316        0.5313  0.0075  1.3693\n",
      "     10                     \u001b[36m0.7649\u001b[0m        \u001b[32m0.4989\u001b[0m                     0.7330        0.5346  0.0069  1.3673\n",
      "     11                     \u001b[36m0.7662\u001b[0m        0.5018                     0.6644        0.6358  0.0063  1.3683\n",
      "     12                     \u001b[36m0.7725\u001b[0m        \u001b[32m0.4848\u001b[0m                     \u001b[35m0.7549\u001b[0m        \u001b[31m0.5071\u001b[0m  0.0057  1.3634\n",
      "     13                     0.7642        0.4883                     0.7483        0.5159  0.0050  1.3644\n",
      "     14                     \u001b[36m0.7737\u001b[0m        \u001b[32m0.4797\u001b[0m                     \u001b[35m0.7780\u001b[0m        0.5118  0.0043  1.3683\n",
      "     15                     \u001b[36m0.7796\u001b[0m        \u001b[32m0.4712\u001b[0m                     0.7457        0.5303  0.0037  1.3673\n",
      "     16                     \u001b[36m0.7800\u001b[0m        0.4738                     0.7432        0.5285  0.0031  1.3684\n",
      "     17                     \u001b[36m0.7903\u001b[0m        \u001b[32m0.4525\u001b[0m                     0.7561        \u001b[31m0.4985\u001b[0m  0.0025  1.3694\n",
      "     18                     \u001b[36m0.7966\u001b[0m        \u001b[32m0.4494\u001b[0m                     0.7692        \u001b[31m0.4897\u001b[0m  0.0020  1.3663\n",
      "     19                     0.7931        0.4516                     0.7536        0.5057  0.0015  1.3693\n",
      "     20                     0.7948        0.4502                     \u001b[35m0.7781\u001b[0m        \u001b[31m0.4873\u001b[0m  0.0010  1.3683\n",
      "     21                     \u001b[36m0.7974\u001b[0m        \u001b[32m0.4409\u001b[0m                     0.7708        0.4905  0.0007  1.3703\n",
      "     22                     \u001b[36m0.7986\u001b[0m        0.4415                     \u001b[35m0.7894\u001b[0m        \u001b[31m0.4820\u001b[0m  0.0004  1.3683\n",
      "     23                     0.7960        \u001b[32m0.4394\u001b[0m                     0.7849        \u001b[31m0.4809\u001b[0m  0.0002  1.3694\n",
      "     24                     0.7946        0.4466                     0.7825        0.4816  0.0000  1.3674\n",
      "     25                     \u001b[36m0.8036\u001b[0m        \u001b[32m0.4360\u001b[0m                     0.7842        0.4815  0.0000  1.3693\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6010\u001b[0m        \u001b[32m0.6749\u001b[0m                     \u001b[35m0.6664\u001b[0m        \u001b[31m0.6162\u001b[0m  0.0100  1.3693\n",
      "      2                     \u001b[36m0.6755\u001b[0m        \u001b[32m0.6095\u001b[0m                     0.6455        0.6500  0.0100  1.3713\n",
      "      3                     \u001b[36m0.6905\u001b[0m        \u001b[32m0.5961\u001b[0m                     \u001b[35m0.6949\u001b[0m        \u001b[31m0.5979\u001b[0m  0.0098  1.3713\n",
      "      4                     \u001b[36m0.6978\u001b[0m        \u001b[32m0.5840\u001b[0m                     \u001b[35m0.7085\u001b[0m        0.6157  0.0096  1.3693\n",
      "      5                     \u001b[36m0.7120\u001b[0m        \u001b[32m0.5674\u001b[0m                     0.7082        0.6243  0.0093  1.3663\n",
      "      6                     \u001b[36m0.7189\u001b[0m        \u001b[32m0.5590\u001b[0m                     \u001b[35m0.7213\u001b[0m        \u001b[31m0.5895\u001b[0m  0.0090  1.3653\n",
      "      7                     \u001b[36m0.7221\u001b[0m        \u001b[32m0.5514\u001b[0m                     \u001b[35m0.7381\u001b[0m        0.6355  0.0085  1.3704\n",
      "      8                     \u001b[36m0.7331\u001b[0m        \u001b[32m0.5349\u001b[0m                     0.7275        0.5926  0.0080  1.3713\n",
      "      9                     \u001b[36m0.7386\u001b[0m        \u001b[32m0.5285\u001b[0m                     0.7170        0.6241  0.0075  1.3687\n",
      "     10                     \u001b[36m0.7503\u001b[0m        \u001b[32m0.5182\u001b[0m                     \u001b[35m0.7529\u001b[0m        0.5939  0.0069  1.3683\n",
      "     11                     \u001b[36m0.7537\u001b[0m        \u001b[32m0.5096\u001b[0m                     \u001b[35m0.7538\u001b[0m        \u001b[31m0.5700\u001b[0m  0.0063  1.3673\n",
      "     12                     \u001b[36m0.7627\u001b[0m        \u001b[32m0.5023\u001b[0m                     0.7290        0.6228  0.0057  1.3723\n",
      "     13                     \u001b[36m0.7635\u001b[0m        \u001b[32m0.4959\u001b[0m                     0.7523        0.5807  0.0050  1.3663\n",
      "     14                     \u001b[36m0.7708\u001b[0m        \u001b[32m0.4848\u001b[0m                     0.6244        0.7609  0.0043  1.3654\n",
      "     15                     0.7705        \u001b[32m0.4804\u001b[0m                     0.7504        0.5937  0.0037  1.3703\n",
      "     16                     \u001b[36m0.7766\u001b[0m        \u001b[32m0.4761\u001b[0m                     \u001b[35m0.7746\u001b[0m        0.5907  0.0031  1.3724\n",
      "     17                     \u001b[36m0.7781\u001b[0m        \u001b[32m0.4734\u001b[0m                     0.7659        0.5788  0.0025  1.3714\n",
      "     18                     \u001b[36m0.7836\u001b[0m        \u001b[32m0.4601\u001b[0m                     0.7678        0.5781  0.0020  1.3763\n",
      "     19                     \u001b[36m0.7837\u001b[0m        \u001b[32m0.4575\u001b[0m                     \u001b[35m0.7773\u001b[0m        \u001b[31m0.5657\u001b[0m  0.0015  1.3723\n",
      "     20                     \u001b[36m0.7906\u001b[0m        \u001b[32m0.4535\u001b[0m                     0.7632        0.5873  0.0010  1.3743\n",
      "     21                     0.7870        0.4564                     0.7602        0.5733  0.0007  1.3733\n",
      "     22                     \u001b[36m0.7925\u001b[0m        \u001b[32m0.4526\u001b[0m                     \u001b[35m0.7789\u001b[0m        0.5722  0.0004  1.3733\n",
      "     23                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4475\u001b[0m                     0.7772        0.5698  0.0002  1.3714\n",
      "     24                     0.7922        0.4478                     \u001b[35m0.7791\u001b[0m        0.5702  0.0000  1.3713\n",
      "     25                     0.7925        0.4495                     \u001b[35m0.7799\u001b[0m        0.5701  0.0000  1.3708\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5635\u001b[0m        \u001b[32m0.7038\u001b[0m                     \u001b[35m0.6443\u001b[0m        \u001b[31m0.6360\u001b[0m  0.0100  1.3624\n",
      "      2                     \u001b[36m0.6575\u001b[0m        \u001b[32m0.6231\u001b[0m                     \u001b[35m0.6748\u001b[0m        \u001b[31m0.6044\u001b[0m  0.0100  1.3638\n",
      "      3                     \u001b[36m0.6725\u001b[0m        \u001b[32m0.6076\u001b[0m                     \u001b[35m0.6975\u001b[0m        \u001b[31m0.5907\u001b[0m  0.0098  1.3634\n",
      "      4                     \u001b[36m0.6845\u001b[0m        \u001b[32m0.5924\u001b[0m                     \u001b[35m0.7054\u001b[0m        \u001b[31m0.5685\u001b[0m  0.0096  1.3632\n",
      "      5                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.5768\u001b[0m                     \u001b[35m0.7371\u001b[0m        \u001b[31m0.5567\u001b[0m  0.0093  1.3631\n",
      "      6                     \u001b[36m0.7184\u001b[0m        \u001b[32m0.5589\u001b[0m                     0.6848        0.5904  0.0090  1.3639\n",
      "      7                     \u001b[36m0.7224\u001b[0m        \u001b[32m0.5487\u001b[0m                     0.7246        \u001b[31m0.5481\u001b[0m  0.0085  1.3632\n",
      "      8                     \u001b[36m0.7273\u001b[0m        \u001b[32m0.5460\u001b[0m                     \u001b[35m0.7402\u001b[0m        \u001b[31m0.5424\u001b[0m  0.0080  1.3633\n",
      "      9                     \u001b[36m0.7302\u001b[0m        \u001b[32m0.5386\u001b[0m                     0.7340        0.5529  0.0075  1.3631\n",
      "     10                     \u001b[36m0.7387\u001b[0m        \u001b[32m0.5312\u001b[0m                     \u001b[35m0.7567\u001b[0m        \u001b[31m0.5234\u001b[0m  0.0069  1.3633\n",
      "     11                     \u001b[36m0.7439\u001b[0m        \u001b[32m0.5219\u001b[0m                     0.7387        0.5373  0.0063  1.3635\n",
      "     12                     \u001b[36m0.7467\u001b[0m        \u001b[32m0.5186\u001b[0m                     0.7404        0.5324  0.0057  1.3633\n",
      "     13                     \u001b[36m0.7486\u001b[0m        \u001b[32m0.5117\u001b[0m                     0.7139        0.5556  0.0050  1.3634\n",
      "     14                     \u001b[36m0.7593\u001b[0m        \u001b[32m0.4990\u001b[0m                     0.7505        \u001b[31m0.5219\u001b[0m  0.0043  1.3632\n",
      "     15                     \u001b[36m0.7616\u001b[0m        \u001b[32m0.4966\u001b[0m                     0.7565        \u001b[31m0.5175\u001b[0m  0.0037  1.3635\n",
      "     16                     0.7607        \u001b[32m0.4949\u001b[0m                     \u001b[35m0.7619\u001b[0m        \u001b[31m0.5032\u001b[0m  0.0031  1.3630\n",
      "     17                     \u001b[36m0.7665\u001b[0m        \u001b[32m0.4848\u001b[0m                     0.7591        0.5071  0.0025  1.3655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18                     \u001b[36m0.7720\u001b[0m        \u001b[32m0.4770\u001b[0m                     0.7543        0.5179  0.0020  1.3634\n",
      "     19                     \u001b[36m0.7752\u001b[0m        0.4788                     0.7617        \u001b[31m0.4989\u001b[0m  0.0015  1.3637\n",
      "     20                     \u001b[36m0.7849\u001b[0m        \u001b[32m0.4724\u001b[0m                     \u001b[35m0.7629\u001b[0m        0.5011  0.0010  1.3650\n",
      "     21                     0.7823        \u001b[32m0.4631\u001b[0m                     0.7602        0.5000  0.0007  1.3634\n",
      "     22                     0.7800        0.4642                     \u001b[35m0.7663\u001b[0m        \u001b[31m0.4979\u001b[0m  0.0004  1.3635\n",
      "     23                     0.7831        \u001b[32m0.4615\u001b[0m                     \u001b[35m0.7681\u001b[0m        \u001b[31m0.4947\u001b[0m  0.0002  1.3634\n",
      "     24                     0.7846        0.4638                     0.7655        0.4949  0.0000  1.3643\n",
      "     25                     0.7789        0.4738                     0.7646        \u001b[31m0.4937\u001b[0m  0.0000  1.3637\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5728\u001b[0m        \u001b[32m0.7007\u001b[0m                     \u001b[35m0.6171\u001b[0m        \u001b[31m0.6681\u001b[0m  0.0100  1.3614\n",
      "      2                     \u001b[36m0.6414\u001b[0m        \u001b[32m0.6389\u001b[0m                     \u001b[35m0.6864\u001b[0m        \u001b[31m0.6061\u001b[0m  0.0100  1.3624\n",
      "      3                     \u001b[36m0.6851\u001b[0m        \u001b[32m0.5996\u001b[0m                     \u001b[35m0.7341\u001b[0m        \u001b[31m0.5411\u001b[0m  0.0098  1.3628\n",
      "      4                     \u001b[36m0.7119\u001b[0m        \u001b[32m0.5665\u001b[0m                     0.7319        \u001b[31m0.5325\u001b[0m  0.0096  1.3635\n",
      "      5                     \u001b[36m0.7225\u001b[0m        \u001b[32m0.5500\u001b[0m                     \u001b[35m0.7552\u001b[0m        \u001b[31m0.5161\u001b[0m  0.0093  1.3632\n",
      "      6                     \u001b[36m0.7252\u001b[0m        \u001b[32m0.5460\u001b[0m                     0.7402        \u001b[31m0.5158\u001b[0m  0.0090  1.3629\n",
      "      7                     \u001b[36m0.7269\u001b[0m        \u001b[32m0.5403\u001b[0m                     0.5306        0.8971  0.0085  1.3641\n",
      "      8                     \u001b[36m0.7388\u001b[0m        \u001b[32m0.5371\u001b[0m                     \u001b[35m0.7814\u001b[0m        \u001b[31m0.4903\u001b[0m  0.0080  1.3630\n",
      "      9                     \u001b[36m0.7478\u001b[0m        \u001b[32m0.5187\u001b[0m                     0.7614        0.4998  0.0075  1.3636\n",
      "     10                     \u001b[36m0.7502\u001b[0m        \u001b[32m0.5080\u001b[0m                     0.7638        0.5041  0.0069  1.3628\n",
      "     11                     \u001b[36m0.7556\u001b[0m        0.5133                     0.7413        0.5125  0.0063  1.3634\n",
      "     12                     \u001b[36m0.7625\u001b[0m        \u001b[32m0.4993\u001b[0m                     0.7631        0.5051  0.0057  1.3627\n",
      "     13                     \u001b[36m0.7686\u001b[0m        \u001b[32m0.4969\u001b[0m                     0.7605        \u001b[31m0.4861\u001b[0m  0.0050  1.3641\n",
      "     14                     \u001b[36m0.7703\u001b[0m        \u001b[32m0.4897\u001b[0m                     0.7726        \u001b[31m0.4765\u001b[0m  0.0043  1.3625\n",
      "     15                     0.7683        \u001b[32m0.4831\u001b[0m                     0.7777        0.4791  0.0037  1.3638\n",
      "     16                     \u001b[36m0.7797\u001b[0m        \u001b[32m0.4809\u001b[0m                     0.7274        0.5263  0.0031  1.3640\n",
      "     17                     0.7762        \u001b[32m0.4752\u001b[0m                     0.7517        0.4976  0.0025  1.3634\n",
      "     18                     \u001b[36m0.7871\u001b[0m        \u001b[32m0.4663\u001b[0m                     0.7778        \u001b[31m0.4692\u001b[0m  0.0020  1.3643\n",
      "     19                     0.7828        \u001b[32m0.4630\u001b[0m                     0.7728        \u001b[31m0.4690\u001b[0m  0.0015  1.3644\n",
      "     20                     \u001b[36m0.7938\u001b[0m        \u001b[32m0.4610\u001b[0m                     0.7804        \u001b[31m0.4657\u001b[0m  0.0010  1.3640\n",
      "     21                     \u001b[36m0.7968\u001b[0m        \u001b[32m0.4520\u001b[0m                     0.7694        0.4741  0.0007  1.3634\n",
      "     22                     0.7915        \u001b[32m0.4516\u001b[0m                     \u001b[35m0.7860\u001b[0m        \u001b[31m0.4644\u001b[0m  0.0004  1.3641\n",
      "     23                     \u001b[36m0.8003\u001b[0m        \u001b[32m0.4468\u001b[0m                     0.7836        \u001b[31m0.4642\u001b[0m  0.0002  1.3634\n",
      "     24                     0.7915        0.4478                     0.7836        \u001b[31m0.4631\u001b[0m  0.0000  1.3640\n",
      "     25                     0.7984        \u001b[32m0.4449\u001b[0m                     0.7828        0.4632  0.0000  1.3631\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5745\u001b[0m        \u001b[32m0.6960\u001b[0m                     \u001b[35m0.6709\u001b[0m        \u001b[31m0.6177\u001b[0m  0.0100  1.3621\n",
      "      2                     \u001b[36m0.6588\u001b[0m        \u001b[32m0.6348\u001b[0m                     \u001b[35m0.6803\u001b[0m        \u001b[31m0.6173\u001b[0m  0.0100  1.3641\n",
      "      3                     \u001b[36m0.6783\u001b[0m        \u001b[32m0.5952\u001b[0m                     \u001b[35m0.7126\u001b[0m        \u001b[31m0.5740\u001b[0m  0.0098  1.3618\n",
      "      4                     \u001b[36m0.7048\u001b[0m        \u001b[32m0.5825\u001b[0m                     0.7034        0.5862  0.0096  1.3623\n",
      "      5                     \u001b[36m0.7148\u001b[0m        \u001b[32m0.5655\u001b[0m                     \u001b[35m0.7175\u001b[0m        \u001b[31m0.5617\u001b[0m  0.0093  1.3625\n",
      "      6                     \u001b[36m0.7230\u001b[0m        \u001b[32m0.5521\u001b[0m                     \u001b[35m0.7241\u001b[0m        \u001b[31m0.5495\u001b[0m  0.0090  1.3620\n",
      "      7                     \u001b[36m0.7318\u001b[0m        \u001b[32m0.5434\u001b[0m                     \u001b[35m0.7285\u001b[0m        0.5570  0.0085  1.3620\n",
      "      8                     0.7315        \u001b[32m0.5348\u001b[0m                     \u001b[35m0.7334\u001b[0m        \u001b[31m0.5418\u001b[0m  0.0080  1.3619\n",
      "      9                     \u001b[36m0.7421\u001b[0m        \u001b[32m0.5268\u001b[0m                     0.7166        0.5626  0.0075  1.3624\n",
      "     10                     0.7415        \u001b[32m0.5198\u001b[0m                     \u001b[35m0.7521\u001b[0m        \u001b[31m0.5198\u001b[0m  0.0069  1.3641\n",
      "     11                     \u001b[36m0.7502\u001b[0m        \u001b[32m0.5142\u001b[0m                     0.7089        0.5600  0.0063  1.3632\n",
      "     12                     \u001b[36m0.7611\u001b[0m        \u001b[32m0.4983\u001b[0m                     0.7325        0.5513  0.0057  1.3624\n",
      "     13                     \u001b[36m0.7640\u001b[0m        \u001b[32m0.4945\u001b[0m                     \u001b[35m0.7552\u001b[0m        \u001b[31m0.5182\u001b[0m  0.0050  1.3624\n",
      "     14                     \u001b[36m0.7711\u001b[0m        \u001b[32m0.4904\u001b[0m                     0.7431        0.5241  0.0043  1.3628\n",
      "     15                     \u001b[36m0.7737\u001b[0m        \u001b[32m0.4776\u001b[0m                     \u001b[35m0.7555\u001b[0m        \u001b[31m0.5120\u001b[0m  0.0037  1.3625\n",
      "     16                     \u001b[36m0.7748\u001b[0m        \u001b[32m0.4763\u001b[0m                     \u001b[35m0.7600\u001b[0m        \u001b[31m0.5107\u001b[0m  0.0031  1.3633\n",
      "     17                     0.7740        0.4766                     0.7557        0.5172  0.0025  1.3629\n",
      "     18                     \u001b[36m0.7810\u001b[0m        \u001b[32m0.4641\u001b[0m                     \u001b[35m0.7746\u001b[0m        \u001b[31m0.4995\u001b[0m  0.0020  1.3623\n",
      "     19                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4626\u001b[0m                     0.7482        0.5109  0.0015  1.3626\n",
      "     20                     0.7790        \u001b[32m0.4609\u001b[0m                     \u001b[35m0.7922\u001b[0m        \u001b[31m0.4920\u001b[0m  0.0010  1.3632\n",
      "     21                     \u001b[36m0.7893\u001b[0m        \u001b[32m0.4549\u001b[0m                     \u001b[35m0.7928\u001b[0m        \u001b[31m0.4909\u001b[0m  0.0007  1.3620\n",
      "     22                     \u001b[36m0.7951\u001b[0m        \u001b[32m0.4534\u001b[0m                     0.7872        0.4933  0.0004  1.3629\n",
      "     23                     \u001b[36m0.7996\u001b[0m        \u001b[32m0.4534\u001b[0m                     0.7879        \u001b[31m0.4909\u001b[0m  0.0002  1.3636\n",
      "     24                     0.7944        \u001b[32m0.4511\u001b[0m                     0.7878        \u001b[31m0.4894\u001b[0m  0.0000  1.3626\n",
      "     25                     0.7955        \u001b[32m0.4474\u001b[0m                     0.7913        0.4894  0.0000  1.3620\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5843\u001b[0m        \u001b[32m0.6980\u001b[0m                     \u001b[35m0.6571\u001b[0m        \u001b[31m0.6302\u001b[0m  0.0100  1.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                     \u001b[36m0.6624\u001b[0m        \u001b[32m0.6229\u001b[0m                     \u001b[35m0.6655\u001b[0m        0.6384  0.0100  1.3624\n",
      "      3                     \u001b[36m0.6806\u001b[0m        \u001b[32m0.6066\u001b[0m                     \u001b[35m0.6718\u001b[0m        \u001b[31m0.6282\u001b[0m  0.0098  1.3624\n",
      "      4                     \u001b[36m0.7008\u001b[0m        \u001b[32m0.5756\u001b[0m                     \u001b[35m0.7131\u001b[0m        \u001b[31m0.5744\u001b[0m  0.0096  1.3613\n",
      "      5                     \u001b[36m0.7141\u001b[0m        \u001b[32m0.5642\u001b[0m                     0.6712        0.6091  0.0093  1.3627\n",
      "      6                     \u001b[36m0.7206\u001b[0m        \u001b[32m0.5530\u001b[0m                     0.7131        \u001b[31m0.5658\u001b[0m  0.0090  1.3618\n",
      "      7                     \u001b[36m0.7321\u001b[0m        \u001b[32m0.5437\u001b[0m                     \u001b[35m0.7642\u001b[0m        \u001b[31m0.5428\u001b[0m  0.0085  1.3628\n",
      "      8                     \u001b[36m0.7409\u001b[0m        \u001b[32m0.5282\u001b[0m                     0.7489        0.5482  0.0080  1.3618\n",
      "      9                     \u001b[36m0.7467\u001b[0m        \u001b[32m0.5263\u001b[0m                     0.7175        0.5923  0.0075  1.3615\n",
      "     10                     \u001b[36m0.7522\u001b[0m        \u001b[32m0.5097\u001b[0m                     0.7548        \u001b[31m0.5298\u001b[0m  0.0069  1.3624\n",
      "     11                     \u001b[36m0.7543\u001b[0m        \u001b[32m0.5072\u001b[0m                     0.7442        0.5590  0.0063  1.3624\n",
      "     12                     0.7539        \u001b[32m0.5040\u001b[0m                     0.7531        0.5570  0.0057  1.3624\n",
      "     13                     \u001b[36m0.7655\u001b[0m        \u001b[32m0.4945\u001b[0m                     \u001b[35m0.7666\u001b[0m        0.5503  0.0050  1.3620\n",
      "     14                     \u001b[36m0.7690\u001b[0m        \u001b[32m0.4885\u001b[0m                     0.7528        0.5329  0.0043  1.3639\n",
      "     15                     \u001b[36m0.7734\u001b[0m        \u001b[32m0.4869\u001b[0m                     \u001b[35m0.7713\u001b[0m        \u001b[31m0.5059\u001b[0m  0.0037  1.3624\n",
      "     16                     0.7699        \u001b[32m0.4789\u001b[0m                     0.7497        0.5565  0.0031  1.3634\n",
      "     17                     0.7717        0.4801                     0.7642        0.5295  0.0025  1.3626\n",
      "     18                     \u001b[36m0.7777\u001b[0m        \u001b[32m0.4656\u001b[0m                     0.7249        0.5784  0.0020  1.3624\n",
      "     19                     0.7769        \u001b[32m0.4639\u001b[0m                     \u001b[35m0.7740\u001b[0m        0.5155  0.0015  1.3632\n",
      "     20                     \u001b[36m0.7861\u001b[0m        \u001b[32m0.4582\u001b[0m                     \u001b[35m0.7765\u001b[0m        0.5230  0.0010  1.3632\n",
      "     21                     \u001b[36m0.7888\u001b[0m        0.4591                     0.7673        0.5331  0.0007  1.3619\n",
      "     22                     \u001b[36m0.7947\u001b[0m        \u001b[32m0.4546\u001b[0m                     0.7764        0.5169  0.0004  1.3624\n",
      "     23                     0.7828        0.4567                     0.7747        0.5165  0.0002  1.3620\n",
      "     24                     0.7843        \u001b[32m0.4494\u001b[0m                     0.7757        0.5188  0.0000  1.3625\n",
      "     25                     0.7876        0.4531                     0.7731        0.5195  0.0000  1.3627\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5874\u001b[0m        \u001b[32m0.6924\u001b[0m                     \u001b[35m0.6251\u001b[0m        \u001b[31m0.6448\u001b[0m  0.0100  1.3604\n",
      "      2                     \u001b[36m0.6564\u001b[0m        \u001b[32m0.6216\u001b[0m                     \u001b[35m0.6892\u001b[0m        \u001b[31m0.5987\u001b[0m  0.0100  1.3622\n",
      "      3                     \u001b[36m0.6955\u001b[0m        \u001b[32m0.5850\u001b[0m                     \u001b[35m0.7071\u001b[0m        \u001b[31m0.5811\u001b[0m  0.0098  1.3618\n",
      "      4                     \u001b[36m0.7116\u001b[0m        \u001b[32m0.5660\u001b[0m                     \u001b[35m0.7078\u001b[0m        0.5963  0.0096  1.3645\n",
      "      5                     \u001b[36m0.7255\u001b[0m        \u001b[32m0.5490\u001b[0m                     0.7056        0.6213  0.0093  1.3624\n",
      "      6                     \u001b[36m0.7379\u001b[0m        \u001b[32m0.5401\u001b[0m                     \u001b[35m0.7166\u001b[0m        0.5830  0.0090  1.3620\n",
      "      7                     \u001b[36m0.7507\u001b[0m        \u001b[32m0.5180\u001b[0m                     \u001b[35m0.7355\u001b[0m        \u001b[31m0.5604\u001b[0m  0.0085  1.3620\n",
      "      8                     \u001b[36m0.7568\u001b[0m        \u001b[32m0.5144\u001b[0m                     0.7283        0.5906  0.0080  1.3617\n",
      "      9                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.5057\u001b[0m                     0.7322        0.5678  0.0075  1.3621\n",
      "     10                     \u001b[36m0.7723\u001b[0m        \u001b[32m0.4878\u001b[0m                     0.7347        0.5836  0.0069  1.3618\n",
      "     11                     0.7721        0.4937                     \u001b[35m0.7382\u001b[0m        \u001b[31m0.5514\u001b[0m  0.0063  1.3624\n",
      "     12                     0.7662        \u001b[32m0.4847\u001b[0m                     \u001b[35m0.7470\u001b[0m        \u001b[31m0.5380\u001b[0m  0.0057  1.3622\n",
      "     13                     \u001b[36m0.7740\u001b[0m        0.4867                     0.6582        0.6492  0.0050  1.3630\n",
      "     14                     \u001b[36m0.7776\u001b[0m        \u001b[32m0.4726\u001b[0m                     0.7397        0.5458  0.0043  1.3621\n",
      "     15                     \u001b[36m0.7787\u001b[0m        0.4749                     \u001b[35m0.7527\u001b[0m        \u001b[31m0.5169\u001b[0m  0.0037  1.3634\n",
      "     16                     \u001b[36m0.7838\u001b[0m        \u001b[32m0.4691\u001b[0m                     \u001b[35m0.7530\u001b[0m        0.5261  0.0031  1.3623\n",
      "     17                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4598\u001b[0m                     0.7517        0.5263  0.0025  1.3634\n",
      "     18                     0.7904        \u001b[32m0.4536\u001b[0m                     \u001b[35m0.7577\u001b[0m        0.5238  0.0020  1.3621\n",
      "     19                     0.7901        \u001b[32m0.4506\u001b[0m                     \u001b[35m0.7598\u001b[0m        \u001b[31m0.5112\u001b[0m  0.0015  1.3616\n",
      "     20                     \u001b[36m0.8005\u001b[0m        \u001b[32m0.4366\u001b[0m                     \u001b[35m0.7648\u001b[0m        0.5153  0.0010  1.3634\n",
      "     21                     \u001b[36m0.8012\u001b[0m        0.4392                     \u001b[35m0.7663\u001b[0m        0.5168  0.0007  1.3624\n",
      "     22                     \u001b[36m0.8018\u001b[0m        0.4443                     0.7577        0.5123  0.0004  1.3629\n",
      "     23                     \u001b[36m0.8053\u001b[0m        \u001b[32m0.4358\u001b[0m                     0.7630        \u001b[31m0.5106\u001b[0m  0.0002  1.3637\n",
      "     24                     \u001b[36m0.8061\u001b[0m        0.4389                     0.7639        \u001b[31m0.5097\u001b[0m  0.0000  1.3627\n",
      "     25                     0.7991        0.4359                     0.7631        \u001b[31m0.5093\u001b[0m  0.0000  1.3621\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5909\u001b[0m        \u001b[32m0.6868\u001b[0m                     \u001b[35m0.6548\u001b[0m        \u001b[31m0.6337\u001b[0m  0.0100  1.3611\n",
      "      2                     \u001b[36m0.6737\u001b[0m        \u001b[32m0.6147\u001b[0m                     \u001b[35m0.6799\u001b[0m        \u001b[31m0.6163\u001b[0m  0.0100  1.3627\n",
      "      3                     \u001b[36m0.7062\u001b[0m        \u001b[32m0.5809\u001b[0m                     \u001b[35m0.6930\u001b[0m        \u001b[31m0.5894\u001b[0m  0.0098  1.3634\n",
      "      4                     \u001b[36m0.7121\u001b[0m        \u001b[32m0.5627\u001b[0m                     \u001b[35m0.7046\u001b[0m        \u001b[31m0.5748\u001b[0m  0.0096  1.3639\n",
      "      5                     \u001b[36m0.7278\u001b[0m        \u001b[32m0.5539\u001b[0m                     0.6828        0.6155  0.0093  1.3637\n",
      "      6                     \u001b[36m0.7373\u001b[0m        \u001b[32m0.5379\u001b[0m                     \u001b[35m0.7238\u001b[0m        \u001b[31m0.5608\u001b[0m  0.0090  1.3627\n",
      "      7                     \u001b[36m0.7466\u001b[0m        \u001b[32m0.5231\u001b[0m                     0.7208        0.5775  0.0085  1.3625\n",
      "      8                     \u001b[36m0.7507\u001b[0m        \u001b[32m0.5172\u001b[0m                     0.7227        0.5782  0.0080  1.3634\n",
      "      9                     0.7461        \u001b[32m0.5149\u001b[0m                     0.7186        0.5767  0.0075  1.3635\n",
      "     10                     \u001b[36m0.7546\u001b[0m        \u001b[32m0.5064\u001b[0m                     0.7214        \u001b[31m0.5597\u001b[0m  0.0069  1.3627\n",
      "     11                     \u001b[36m0.7649\u001b[0m        0.5074                     0.7195        0.5743  0.0063  1.3640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     12                     0.7626        \u001b[32m0.4962\u001b[0m                     0.6913        0.6324  0.0057  1.3631\n",
      "     13                     \u001b[36m0.7662\u001b[0m        \u001b[32m0.4873\u001b[0m                     0.7174        0.6021  0.0050  1.3624\n",
      "     14                     \u001b[36m0.7735\u001b[0m        \u001b[32m0.4840\u001b[0m                     0.7235        0.5941  0.0043  1.3644\n",
      "     15                     0.7693        0.4846                     \u001b[35m0.7364\u001b[0m        0.5638  0.0037  1.3629\n",
      "     16                     \u001b[36m0.7753\u001b[0m        \u001b[32m0.4758\u001b[0m                     \u001b[35m0.7503\u001b[0m        \u001b[31m0.5271\u001b[0m  0.0031  1.3629\n",
      "     17                     \u001b[36m0.7824\u001b[0m        \u001b[32m0.4743\u001b[0m                     0.7433        0.5591  0.0025  1.3630\n",
      "     18                     \u001b[36m0.7836\u001b[0m        \u001b[32m0.4572\u001b[0m                     \u001b[35m0.7535\u001b[0m        0.5425  0.0020  1.3642\n",
      "     19                     \u001b[36m0.7860\u001b[0m        0.4620                     0.7500        0.5482  0.0015  1.3639\n",
      "     20                     \u001b[36m0.7889\u001b[0m        \u001b[32m0.4564\u001b[0m                     \u001b[35m0.7614\u001b[0m        0.5503  0.0010  1.3633\n",
      "     21                     0.7862        0.4567                     0.7353        0.5667  0.0007  1.3643\n",
      "     22                     \u001b[36m0.7911\u001b[0m        \u001b[32m0.4530\u001b[0m                     0.7505        0.5542  0.0004  1.3638\n",
      "     23                     \u001b[36m0.7947\u001b[0m        \u001b[32m0.4524\u001b[0m                     \u001b[35m0.7622\u001b[0m        0.5453  0.0002  1.3642\n",
      "     24                     0.7869        0.4568                     \u001b[35m0.7637\u001b[0m        0.5417  0.0000  1.3637\n",
      "     25                     0.7893        0.4535                     0.7593        0.5405  0.0000  1.3640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 1 1 ... 1 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6414\u001b[0m        \u001b[32m0.6521\u001b[0m                     \u001b[35m0.7080\u001b[0m        \u001b[31m0.5852\u001b[0m  0.0100  0.8256\n",
      "      2                     \u001b[36m0.7123\u001b[0m        \u001b[32m0.5697\u001b[0m                     \u001b[35m0.7235\u001b[0m        \u001b[31m0.5451\u001b[0m  0.0100  0.8128\n",
      "      3                     \u001b[36m0.7281\u001b[0m        \u001b[32m0.5399\u001b[0m                     \u001b[35m0.7241\u001b[0m        0.5568  0.0098  0.8119\n",
      "      4                     \u001b[36m0.7316\u001b[0m        0.5506                     0.7119        0.5810  0.0096  0.8141\n",
      "      5                     \u001b[36m0.7436\u001b[0m        \u001b[32m0.5172\u001b[0m                     \u001b[35m0.7323\u001b[0m        \u001b[31m0.5291\u001b[0m  0.0093  0.8128\n",
      "      6                     \u001b[36m0.7500\u001b[0m        \u001b[32m0.5125\u001b[0m                     0.6873        0.6494  0.0090  0.8134\n",
      "      7                     \u001b[36m0.7514\u001b[0m        \u001b[32m0.5026\u001b[0m                     \u001b[35m0.7451\u001b[0m        0.5297  0.0085  0.8130\n",
      "      8                     \u001b[36m0.7563\u001b[0m        0.5027                     0.7265        0.5432  0.0080  0.8129\n",
      "      9                     0.7508        0.5049                     0.7168        0.5757  0.0075  0.8145\n",
      "     10                     \u001b[36m0.7612\u001b[0m        \u001b[32m0.4922\u001b[0m                     \u001b[35m0.7479\u001b[0m        0.5452  0.0069  0.8141\n",
      "     11                     \u001b[36m0.7677\u001b[0m        \u001b[32m0.4836\u001b[0m                     0.5641        1.0155  0.0063  0.8139\n",
      "     12                     0.7598        0.4867                     0.7231        0.5750  0.0057  0.8129\n",
      "     13                     \u001b[36m0.7746\u001b[0m        \u001b[32m0.4757\u001b[0m                     \u001b[35m0.7537\u001b[0m        \u001b[31m0.4995\u001b[0m  0.0050  0.8124\n",
      "     14                     0.7679        0.4833                     0.7405        0.5252  0.0043  0.8124\n",
      "     15                     \u001b[36m0.7759\u001b[0m        \u001b[32m0.4636\u001b[0m                     \u001b[35m0.7659\u001b[0m        \u001b[31m0.4970\u001b[0m  0.0037  0.8135\n",
      "     16                     \u001b[36m0.7771\u001b[0m        \u001b[32m0.4574\u001b[0m                     0.7474        0.5049  0.0031  0.8138\n",
      "     17                     0.7756        0.4624                     0.7342        0.5397  0.0025  0.8139\n",
      "     18                     \u001b[36m0.7785\u001b[0m        0.4619                     0.7433        0.5079  0.0020  0.8137\n",
      "     19                     \u001b[36m0.7852\u001b[0m        \u001b[32m0.4544\u001b[0m                     0.7475        0.5154  0.0015  0.8138\n",
      "     20                     0.7813        \u001b[32m0.4429\u001b[0m                     0.7521        0.5024  0.0010  0.8137\n",
      "     21                     \u001b[36m0.7950\u001b[0m        \u001b[32m0.4397\u001b[0m                     0.7524        \u001b[31m0.4949\u001b[0m  0.0007  0.8134\n",
      "     22                     0.7945        \u001b[32m0.4369\u001b[0m                     0.7566        \u001b[31m0.4933\u001b[0m  0.0004  0.8135\n",
      "     23                     0.7905        0.4429                     0.7635        \u001b[31m0.4911\u001b[0m  0.0002  0.8136\n",
      "     24                     0.7884        0.4455                     0.7653        0.4914  0.0000  0.8131\n",
      "     25                     0.7847        0.4396                     \u001b[35m0.7672\u001b[0m        0.4917  0.0000  0.8148\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6472\u001b[0m        \u001b[32m0.6606\u001b[0m                     \u001b[35m0.6194\u001b[0m        \u001b[31m0.6993\u001b[0m  0.0100  0.8127\n",
      "      2                     \u001b[36m0.7038\u001b[0m        \u001b[32m0.5723\u001b[0m                     \u001b[35m0.7327\u001b[0m        \u001b[31m0.5672\u001b[0m  0.0100  0.8149\n",
      "      3                     \u001b[36m0.7150\u001b[0m        \u001b[32m0.5713\u001b[0m                     0.7066        0.5758  0.0098  0.8141\n",
      "      4                     \u001b[36m0.7273\u001b[0m        \u001b[32m0.5548\u001b[0m                     0.7247        0.5681  0.0096  0.8148\n",
      "      5                     \u001b[36m0.7392\u001b[0m        \u001b[32m0.5236\u001b[0m                     0.7323        \u001b[31m0.5356\u001b[0m  0.0093  0.8143\n",
      "      6                     \u001b[36m0.7481\u001b[0m        \u001b[32m0.5116\u001b[0m                     0.7031        0.5944  0.0090  0.8145\n",
      "      7                     0.7441        0.5256                     0.7203        0.5627  0.0085  0.8140\n",
      "      8                     \u001b[36m0.7543\u001b[0m        0.5161                     \u001b[35m0.7423\u001b[0m        0.5408  0.0080  0.8139\n",
      "      9                     \u001b[36m0.7624\u001b[0m        \u001b[32m0.4953\u001b[0m                     0.7258        0.5514  0.0075  0.8129\n",
      "     10                     0.7590        \u001b[32m0.4951\u001b[0m                     0.7037        0.5880  0.0069  0.8148\n",
      "     11                     \u001b[36m0.7661\u001b[0m        \u001b[32m0.4898\u001b[0m                     \u001b[35m0.7424\u001b[0m        \u001b[31m0.5258\u001b[0m  0.0063  0.8139\n",
      "     12                     0.7575        \u001b[32m0.4828\u001b[0m                     0.7162        0.5541  0.0057  0.8140\n",
      "     13                     \u001b[36m0.7709\u001b[0m        \u001b[32m0.4726\u001b[0m                     0.7404        \u001b[31m0.5257\u001b[0m  0.0050  0.8137\n",
      "     14                     0.7554        0.4981                     0.6927        0.6222  0.0043  0.8146\n",
      "     15                     \u001b[36m0.7721\u001b[0m        0.4803                     0.7210        0.5491  0.0037  0.8135\n",
      "     16                     \u001b[36m0.7728\u001b[0m        \u001b[32m0.4669\u001b[0m                     \u001b[35m0.7471\u001b[0m        0.5273  0.0031  0.8146\n",
      "     17                     \u001b[36m0.7887\u001b[0m        \u001b[32m0.4538\u001b[0m                     0.7361        0.5271  0.0025  0.8158\n",
      "     18                     0.7867        \u001b[32m0.4488\u001b[0m                     0.7382        0.5326  0.0020  0.8151\n",
      "     19                     0.7871        0.4518                     \u001b[35m0.7479\u001b[0m        0.5353  0.0015  0.8148\n",
      "     20                     \u001b[36m0.7922\u001b[0m        \u001b[32m0.4485\u001b[0m                     0.7439        \u001b[31m0.5254\u001b[0m  0.0010  0.8141\n",
      "     21                     0.7834        0.4496                     \u001b[35m0.7541\u001b[0m        \u001b[31m0.5196\u001b[0m  0.0007  0.8138\n",
      "     22                     0.7918        0.4491                     0.7508        \u001b[31m0.5178\u001b[0m  0.0004  0.8138\n",
      "     23                     \u001b[36m0.7933\u001b[0m        \u001b[32m0.4421\u001b[0m                     0.7502        0.5195  0.0002  0.8146\n",
      "     24                     0.7915        0.4470                     0.7489        0.5195  0.0000  0.8138\n",
      "     25                     \u001b[36m0.7981\u001b[0m        \u001b[32m0.4337\u001b[0m                     0.7511        0.5188  0.0000  0.8139\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6556\u001b[0m        \u001b[32m0.6389\u001b[0m                     \u001b[35m0.6870\u001b[0m        \u001b[31m0.6211\u001b[0m  0.0100  0.8119\n",
      "      2                     \u001b[36m0.7063\u001b[0m        \u001b[32m0.5663\u001b[0m                     \u001b[35m0.7083\u001b[0m        \u001b[31m0.5983\u001b[0m  0.0100  0.8131\n",
      "      3                     \u001b[36m0.7296\u001b[0m        \u001b[32m0.5587\u001b[0m                     \u001b[35m0.7311\u001b[0m        \u001b[31m0.5474\u001b[0m  0.0098  0.8136\n",
      "      4                     0.7273        \u001b[32m0.5453\u001b[0m                     \u001b[35m0.7326\u001b[0m        0.5622  0.0096  0.8145\n",
      "      5                     \u001b[36m0.7422\u001b[0m        \u001b[32m0.5162\u001b[0m                     0.7012        0.6158  0.0093  0.8147\n",
      "      6                     0.7407        0.5257                     0.7159        0.6213  0.0090  0.8139\n",
      "      7                     \u001b[36m0.7553\u001b[0m        \u001b[32m0.4993\u001b[0m                     0.7194        0.5663  0.0085  0.8138\n",
      "      8                     0.7513        0.5032                     0.7236        0.5475  0.0080  0.8141\n",
      "      9                     \u001b[36m0.7774\u001b[0m        \u001b[32m0.4877\u001b[0m                     0.7235        0.6039  0.0075  0.8138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     0.7439        0.5228                     0.7286        0.5918  0.0069  0.8140\n",
      "     11                     0.7649        \u001b[32m0.4856\u001b[0m                     0.7207        0.5608  0.0063  0.8136\n",
      "     12                     0.7536        0.4895                     0.7182        0.5601  0.0057  0.8130\n",
      "     13                     \u001b[36m0.7794\u001b[0m        \u001b[32m0.4740\u001b[0m                     0.7319        \u001b[31m0.5377\u001b[0m  0.0050  0.8144\n",
      "     14                     0.7691        0.4901                     0.7087        0.5977  0.0043  0.8138\n",
      "     15                     \u001b[36m0.7803\u001b[0m        \u001b[32m0.4644\u001b[0m                     0.7142        0.5574  0.0037  0.8132\n",
      "     16                     0.7668        0.4676                     0.6433        0.6772  0.0031  0.8139\n",
      "     17                     0.7782        \u001b[32m0.4633\u001b[0m                     0.6685        0.6555  0.0025  0.8140\n",
      "     18                     0.7802        \u001b[32m0.4627\u001b[0m                     0.6666        0.6468  0.0020  0.8155\n",
      "     19                     \u001b[36m0.7915\u001b[0m        \u001b[32m0.4558\u001b[0m                     \u001b[35m0.7505\u001b[0m        \u001b[31m0.5181\u001b[0m  0.0015  0.8141\n",
      "     20                     \u001b[36m0.7922\u001b[0m        \u001b[32m0.4459\u001b[0m                     \u001b[35m0.7580\u001b[0m        \u001b[31m0.5111\u001b[0m  0.0010  0.8148\n",
      "     21                     \u001b[36m0.7946\u001b[0m        \u001b[32m0.4455\u001b[0m                     0.7530        \u001b[31m0.5099\u001b[0m  0.0007  0.8142\n",
      "     22                     0.7925        0.4478                     0.7558        0.5101  0.0004  0.8149\n",
      "     23                     \u001b[36m0.7961\u001b[0m        \u001b[32m0.4359\u001b[0m                     0.7522        0.5106  0.0002  0.8138\n",
      "     24                     0.7922        0.4462                     0.7549        0.5111  0.0000  0.8138\n",
      "     25                     \u001b[36m0.8014\u001b[0m        0.4418                     0.7539        0.5114  0.0000  0.8139\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6403\u001b[0m        \u001b[32m0.6582\u001b[0m                     \u001b[35m0.7011\u001b[0m        \u001b[31m0.5726\u001b[0m  0.0100  0.8108\n",
      "      2                     \u001b[36m0.7165\u001b[0m        \u001b[32m0.5680\u001b[0m                     0.6855        0.5729  0.0100  0.8129\n",
      "      3                     \u001b[36m0.7188\u001b[0m        \u001b[32m0.5650\u001b[0m                     0.6952        0.5771  0.0098  0.8132\n",
      "      4                     0.7160        \u001b[32m0.5588\u001b[0m                     \u001b[35m0.7216\u001b[0m        \u001b[31m0.5631\u001b[0m  0.0096  0.8128\n",
      "      5                     0.7164        \u001b[32m0.5580\u001b[0m                     \u001b[35m0.7348\u001b[0m        \u001b[31m0.5561\u001b[0m  0.0093  0.8134\n",
      "      6                     \u001b[36m0.7476\u001b[0m        \u001b[32m0.5212\u001b[0m                     \u001b[35m0.7565\u001b[0m        \u001b[31m0.5206\u001b[0m  0.0090  0.8140\n",
      "      7                     0.7451        \u001b[32m0.5162\u001b[0m                     \u001b[35m0.7615\u001b[0m        0.5524  0.0085  0.8130\n",
      "      8                     0.7378        0.5282                     0.7408        0.5225  0.0080  0.8128\n",
      "      9                     0.7446        \u001b[32m0.5159\u001b[0m                     0.7481        \u001b[31m0.4976\u001b[0m  0.0075  0.8129\n",
      "     10                     \u001b[36m0.7516\u001b[0m        \u001b[32m0.5014\u001b[0m                     0.7147        0.5476  0.0069  0.8129\n",
      "     11                     \u001b[36m0.7519\u001b[0m        \u001b[32m0.4988\u001b[0m                     0.7484        0.4985  0.0063  0.8130\n",
      "     12                     \u001b[36m0.7613\u001b[0m        \u001b[32m0.4945\u001b[0m                     0.7546        \u001b[31m0.4912\u001b[0m  0.0057  0.8128\n",
      "     13                     0.7559        0.5074                     0.7462        \u001b[31m0.4893\u001b[0m  0.0050  0.8136\n",
      "     14                     \u001b[36m0.7667\u001b[0m        \u001b[32m0.4799\u001b[0m                     0.7289        0.5017  0.0043  0.8125\n",
      "     15                     0.7662        0.4903                     0.7110        0.5317  0.0037  0.8125\n",
      "     16                     \u001b[36m0.7779\u001b[0m        \u001b[32m0.4714\u001b[0m                     0.6886        0.5591  0.0031  0.8140\n",
      "     17                     \u001b[36m0.7862\u001b[0m        \u001b[32m0.4667\u001b[0m                     0.7505        \u001b[31m0.4889\u001b[0m  0.0025  0.8131\n",
      "     18                     \u001b[36m0.7877\u001b[0m        \u001b[32m0.4577\u001b[0m                     0.7367        \u001b[31m0.4873\u001b[0m  0.0020  0.8131\n",
      "     19                     0.7780        0.4644                     0.7569        \u001b[31m0.4832\u001b[0m  0.0015  0.8131\n",
      "     20                     0.7870        0.4588                     \u001b[35m0.7665\u001b[0m        \u001b[31m0.4812\u001b[0m  0.0010  0.8134\n",
      "     21                     \u001b[36m0.7985\u001b[0m        \u001b[32m0.4496\u001b[0m                     \u001b[35m0.7685\u001b[0m        0.4825  0.0007  0.8129\n",
      "     22                     0.7824        0.4555                     0.7525        \u001b[31m0.4800\u001b[0m  0.0004  0.8139\n",
      "     23                     0.7855        \u001b[32m0.4491\u001b[0m                     0.7585        0.4801  0.0002  0.8139\n",
      "     24                     0.7947        \u001b[32m0.4475\u001b[0m                     0.7536        0.4802  0.0000  0.8137\n",
      "     25                     0.7882        \u001b[32m0.4471\u001b[0m                     0.7522        0.4802  0.0000  0.8135\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6428\u001b[0m        \u001b[32m0.6547\u001b[0m                     \u001b[35m0.7055\u001b[0m        \u001b[31m0.6016\u001b[0m  0.0100  0.8112\n",
      "      2                     \u001b[36m0.7088\u001b[0m        \u001b[32m0.5702\u001b[0m                     0.7030        \u001b[31m0.5674\u001b[0m  0.0100  0.8131\n",
      "      3                     0.6955        0.5968                     0.6924        0.6519  0.0098  0.8124\n",
      "      4                     \u001b[36m0.7240\u001b[0m        \u001b[32m0.5592\u001b[0m                     \u001b[35m0.7222\u001b[0m        \u001b[31m0.5539\u001b[0m  0.0096  0.8126\n",
      "      5                     \u001b[36m0.7409\u001b[0m        \u001b[32m0.5350\u001b[0m                     0.7127        \u001b[31m0.5514\u001b[0m  0.0093  0.8122\n",
      "      6                     0.7330        0.5461                     \u001b[35m0.7316\u001b[0m        \u001b[31m0.5367\u001b[0m  0.0090  0.8130\n",
      "      7                     0.7138        0.5447                     0.7097        0.5573  0.0085  0.8128\n",
      "      8                     0.7368        \u001b[32m0.5173\u001b[0m                     0.7289        \u001b[31m0.5249\u001b[0m  0.0080  0.8129\n",
      "      9                     \u001b[36m0.7517\u001b[0m        \u001b[32m0.5170\u001b[0m                     \u001b[35m0.7547\u001b[0m        \u001b[31m0.5147\u001b[0m  0.0075  0.8121\n",
      "     10                     0.7410        \u001b[32m0.5029\u001b[0m                     0.7456        \u001b[31m0.5086\u001b[0m  0.0069  0.8134\n",
      "     11                     \u001b[36m0.7585\u001b[0m        \u001b[32m0.4961\u001b[0m                     0.7332        0.5233  0.0063  0.8129\n",
      "     12                     \u001b[36m0.7588\u001b[0m        \u001b[32m0.4956\u001b[0m                     0.7150        0.5474  0.0057  0.8129\n",
      "     13                     0.7522        0.5163                     0.7244        0.5225  0.0050  0.8138\n",
      "     14                     0.7580        \u001b[32m0.4876\u001b[0m                     0.7355        \u001b[31m0.5048\u001b[0m  0.0043  0.8135\n",
      "     15                     0.7587        0.4933                     0.7351        0.5060  0.0037  0.8127\n",
      "     16                     \u001b[36m0.7610\u001b[0m        \u001b[32m0.4800\u001b[0m                     0.7388        \u001b[31m0.4971\u001b[0m  0.0031  0.8130\n",
      "     17                     \u001b[36m0.7717\u001b[0m        0.4802                     0.7445        \u001b[31m0.4920\u001b[0m  0.0025  0.8128\n",
      "     18                     0.7661        \u001b[32m0.4759\u001b[0m                     0.7459        \u001b[31m0.4872\u001b[0m  0.0020  0.8134\n",
      "     19                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4667\u001b[0m                     0.7350        0.5017  0.0015  0.8138\n",
      "     20                     0.7785        0.4691                     0.7456        \u001b[31m0.4845\u001b[0m  0.0010  0.8129\n",
      "     21                     0.7751        0.4676                     0.7376        0.4929  0.0007  0.8139\n",
      "     22                     0.7807        \u001b[32m0.4625\u001b[0m                     0.7493        0.4898  0.0004  0.8126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23                     \u001b[36m0.7854\u001b[0m        \u001b[32m0.4614\u001b[0m                     0.7449        0.4869  0.0002  0.8138\n",
      "     24                     0.7777        0.4621                     0.7436        0.4872  0.0000  0.8139\n",
      "     25                     0.7839        \u001b[32m0.4562\u001b[0m                     0.7396        0.4877  0.0000  0.8129\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6538\u001b[0m        \u001b[32m0.6589\u001b[0m                     \u001b[35m0.7190\u001b[0m        \u001b[31m0.5534\u001b[0m  0.0100  0.8114\n",
      "      2                     \u001b[36m0.7203\u001b[0m        \u001b[32m0.5646\u001b[0m                     0.7077        0.5962  0.0100  0.8140\n",
      "      3                     \u001b[36m0.7230\u001b[0m        \u001b[32m0.5581\u001b[0m                     \u001b[35m0.7276\u001b[0m        \u001b[31m0.5466\u001b[0m  0.0098  0.8128\n",
      "      4                     0.7014        0.5683                     \u001b[35m0.7283\u001b[0m        0.5807  0.0096  0.8142\n",
      "      5                     \u001b[36m0.7335\u001b[0m        \u001b[32m0.5364\u001b[0m                     \u001b[35m0.7292\u001b[0m        \u001b[31m0.5400\u001b[0m  0.0093  0.8148\n",
      "      6                     \u001b[36m0.7444\u001b[0m        \u001b[32m0.5228\u001b[0m                     \u001b[35m0.7379\u001b[0m        \u001b[31m0.5269\u001b[0m  0.0090  0.8138\n",
      "      7                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5089\u001b[0m                     0.7285        0.5601  0.0085  0.8140\n",
      "      8                     0.7467        0.5197                     \u001b[35m0.7440\u001b[0m        0.5324  0.0080  0.8130\n",
      "      9                     \u001b[36m0.7584\u001b[0m        \u001b[32m0.4933\u001b[0m                     0.7037        0.5943  0.0075  0.8133\n",
      "     10                     0.7528        0.5151                     0.7313        0.5487  0.0069  0.8136\n",
      "     11                     \u001b[36m0.7633\u001b[0m        0.4999                     0.6774        0.5946  0.0063  0.8138\n",
      "     12                     \u001b[36m0.7695\u001b[0m        \u001b[32m0.4925\u001b[0m                     0.7209        0.5477  0.0057  0.8138\n",
      "     13                     \u001b[36m0.7727\u001b[0m        \u001b[32m0.4837\u001b[0m                     0.6478        0.6636  0.0050  0.8136\n",
      "     14                     \u001b[36m0.7818\u001b[0m        \u001b[32m0.4669\u001b[0m                     0.7383        0.5313  0.0043  0.8148\n",
      "     15                     0.7715        0.4686                     0.7357        0.5536  0.0037  0.8135\n",
      "     16                     \u001b[36m0.7866\u001b[0m        \u001b[32m0.4556\u001b[0m                     0.6447        0.6621  0.0031  0.8163\n",
      "     17                     0.7840        0.4609                     \u001b[35m0.7521\u001b[0m        \u001b[31m0.5146\u001b[0m  0.0025  0.8142\n",
      "     18                     0.7855        0.4582                     \u001b[35m0.7588\u001b[0m        \u001b[31m0.5120\u001b[0m  0.0020  0.8138\n",
      "     19                     0.7860        \u001b[32m0.4484\u001b[0m                     0.7330        0.5351  0.0015  0.8146\n",
      "     20                     0.7815        0.4564                     0.7568        \u001b[31m0.5085\u001b[0m  0.0010  0.8138\n",
      "     21                     \u001b[36m0.7906\u001b[0m        0.4485                     0.7555        0.5117  0.0007  0.8152\n",
      "     22                     \u001b[36m0.8040\u001b[0m        \u001b[32m0.4415\u001b[0m                     \u001b[35m0.7618\u001b[0m        0.5110  0.0004  0.8147\n",
      "     23                     0.7912        0.4442                     0.7580        0.5115  0.0002  0.8150\n",
      "     24                     0.8020        \u001b[32m0.4342\u001b[0m                     0.7618        0.5108  0.0000  0.8142\n",
      "     25                     0.7965        0.4409                     0.7585        0.5106  0.0000  0.8139\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6505\u001b[0m        \u001b[32m0.6466\u001b[0m                     \u001b[35m0.6408\u001b[0m        \u001b[31m0.6726\u001b[0m  0.0100  0.8114\n",
      "      2                     \u001b[36m0.7172\u001b[0m        \u001b[32m0.5721\u001b[0m                     \u001b[35m0.7370\u001b[0m        \u001b[31m0.5249\u001b[0m  0.0100  0.8139\n",
      "      3                     \u001b[36m0.7184\u001b[0m        \u001b[32m0.5515\u001b[0m                     \u001b[35m0.7374\u001b[0m        \u001b[31m0.5114\u001b[0m  0.0098  0.8138\n",
      "      4                     \u001b[36m0.7299\u001b[0m        \u001b[32m0.5442\u001b[0m                     \u001b[35m0.7468\u001b[0m        \u001b[31m0.5049\u001b[0m  0.0096  0.8128\n",
      "      5                     \u001b[36m0.7375\u001b[0m        \u001b[32m0.5348\u001b[0m                     \u001b[35m0.7632\u001b[0m        \u001b[31m0.4808\u001b[0m  0.0093  0.8135\n",
      "      6                     \u001b[36m0.7403\u001b[0m        \u001b[32m0.5253\u001b[0m                     0.7629        0.5071  0.0090  0.8139\n",
      "      7                     0.7353        0.5355                     0.7437        0.4959  0.0085  0.8141\n",
      "      8                     0.7385        0.5301                     0.7062        0.6426  0.0080  0.8129\n",
      "      9                     \u001b[36m0.7499\u001b[0m        \u001b[32m0.5123\u001b[0m                     0.7483        0.4928  0.0075  0.8135\n",
      "     10                     \u001b[36m0.7507\u001b[0m        \u001b[32m0.5082\u001b[0m                     0.7483        0.4894  0.0069  0.8139\n",
      "     11                     0.7416        0.5192                     0.7469        0.5007  0.0063  0.8135\n",
      "     12                     \u001b[36m0.7542\u001b[0m        \u001b[32m0.5005\u001b[0m                     0.6559        0.6251  0.0057  0.8134\n",
      "     13                     \u001b[36m0.7694\u001b[0m        \u001b[32m0.4935\u001b[0m                     0.6465        0.6233  0.0050  0.8138\n",
      "     14                     0.7648        0.4937                     0.6115        0.8582  0.0043  0.8141\n",
      "     15                     \u001b[36m0.7723\u001b[0m        \u001b[32m0.4841\u001b[0m                     0.7508        0.4964  0.0037  0.8138\n",
      "     16                     \u001b[36m0.7759\u001b[0m        \u001b[32m0.4716\u001b[0m                     \u001b[35m0.7662\u001b[0m        \u001b[31m0.4803\u001b[0m  0.0031  0.8135\n",
      "     17                     \u001b[36m0.7799\u001b[0m        0.4721                     \u001b[35m0.7845\u001b[0m        \u001b[31m0.4591\u001b[0m  0.0025  0.8142\n",
      "     18                     0.7734        \u001b[32m0.4679\u001b[0m                     0.7307        0.5234  0.0020  0.8137\n",
      "     19                     0.7799        \u001b[32m0.4642\u001b[0m                     0.7842        \u001b[31m0.4580\u001b[0m  0.0015  0.8138\n",
      "     20                     0.7786        0.4658                     0.7717        0.4647  0.0010  0.8138\n",
      "     21                     \u001b[36m0.7806\u001b[0m        \u001b[32m0.4558\u001b[0m                     0.7716        0.4630  0.0007  0.8134\n",
      "     22                     \u001b[36m0.7856\u001b[0m        0.4594                     \u001b[35m0.7874\u001b[0m        \u001b[31m0.4539\u001b[0m  0.0004  0.8135\n",
      "     23                     \u001b[36m0.7960\u001b[0m        \u001b[32m0.4525\u001b[0m                     0.7810        \u001b[31m0.4538\u001b[0m  0.0002  0.8148\n",
      "     24                     0.7942        \u001b[32m0.4496\u001b[0m                     \u001b[35m0.7896\u001b[0m        \u001b[31m0.4533\u001b[0m  0.0000  0.8148\n",
      "     25                     0.7861        0.4527                     0.7886        0.4535  0.0000  0.8137\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6639\u001b[0m        \u001b[32m0.6322\u001b[0m                     \u001b[35m0.6873\u001b[0m        \u001b[31m0.6026\u001b[0m  0.0100  0.8130\n",
      "      2                     \u001b[36m0.6932\u001b[0m        \u001b[32m0.5844\u001b[0m                     \u001b[35m0.7247\u001b[0m        \u001b[31m0.5462\u001b[0m  0.0100  0.8141\n",
      "      3                     \u001b[36m0.7055\u001b[0m        \u001b[32m0.5739\u001b[0m                     0.7146        0.5648  0.0098  0.8138\n",
      "      4                     \u001b[36m0.7244\u001b[0m        \u001b[32m0.5402\u001b[0m                     0.7182        0.5576  0.0096  0.8139\n",
      "      5                     \u001b[36m0.7413\u001b[0m        \u001b[32m0.5212\u001b[0m                     \u001b[35m0.7283\u001b[0m        \u001b[31m0.5349\u001b[0m  0.0093  0.8132\n",
      "      6                     0.7390        0.5231                     0.7273        \u001b[31m0.5219\u001b[0m  0.0090  0.8141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7                     \u001b[36m0.7493\u001b[0m        \u001b[32m0.5063\u001b[0m                     \u001b[35m0.7332\u001b[0m        0.5428  0.0085  0.8139\n",
      "      8                     \u001b[36m0.7548\u001b[0m        \u001b[32m0.4995\u001b[0m                     \u001b[35m0.7414\u001b[0m        \u001b[31m0.5159\u001b[0m  0.0080  0.8128\n",
      "      9                     \u001b[36m0.7672\u001b[0m        \u001b[32m0.4842\u001b[0m                     0.6937        0.6429  0.0075  0.8132\n",
      "     10                     0.7495        0.5206                     0.6643        0.6779  0.0069  0.8138\n",
      "     11                     \u001b[36m0.7699\u001b[0m        0.4891                     0.7235        0.5486  0.0063  0.8139\n",
      "     12                     0.7565        0.5001                     0.7398        0.5264  0.0057  0.8135\n",
      "     13                     \u001b[36m0.7743\u001b[0m        \u001b[32m0.4765\u001b[0m                     0.7097        0.5886  0.0050  0.8163\n",
      "     14                     0.7682        0.4814                     0.6771        0.6818  0.0043  0.8142\n",
      "     15                     \u001b[36m0.7758\u001b[0m        \u001b[32m0.4703\u001b[0m                     0.7401        \u001b[31m0.5032\u001b[0m  0.0037  0.8136\n",
      "     16                     \u001b[36m0.7849\u001b[0m        \u001b[32m0.4606\u001b[0m                     0.7042        0.5867  0.0031  0.8147\n",
      "     17                     0.7732        0.4681                     0.7363        0.5275  0.0025  0.8148\n",
      "     18                     0.7741        0.4620                     0.7333        0.5071  0.0020  0.8145\n",
      "     19                     0.7827        0.4618                     0.7299        0.5223  0.0015  0.8139\n",
      "     20                     \u001b[36m0.7905\u001b[0m        \u001b[32m0.4536\u001b[0m                     0.7297        0.5119  0.0010  0.8148\n",
      "     21                     \u001b[36m0.8001\u001b[0m        \u001b[32m0.4421\u001b[0m                     0.7275        0.5068  0.0007  0.8133\n",
      "     22                     0.7875        0.4539                     0.7363        \u001b[31m0.5020\u001b[0m  0.0004  0.8141\n",
      "     23                     0.7983        0.4455                     0.7349        \u001b[31m0.5017\u001b[0m  0.0002  0.8146\n",
      "     24                     0.7900        0.4495                     0.7322        0.5018  0.0000  0.8137\n",
      "     25                     0.7934        0.4423                     0.7311        0.5017  0.0000  0.8132\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6588\u001b[0m        \u001b[32m0.6414\u001b[0m                     \u001b[35m0.6692\u001b[0m        \u001b[31m0.6311\u001b[0m  0.0100  0.8112\n",
      "      2                     \u001b[36m0.6939\u001b[0m        \u001b[32m0.5790\u001b[0m                     0.6616        0.6333  0.0100  0.8140\n",
      "      3                     \u001b[36m0.7306\u001b[0m        \u001b[32m0.5418\u001b[0m                     \u001b[35m0.7622\u001b[0m        \u001b[31m0.5125\u001b[0m  0.0098  0.8123\n",
      "      4                     \u001b[36m0.7350\u001b[0m        \u001b[32m0.5301\u001b[0m                     0.7609        0.5283  0.0096  0.8129\n",
      "      5                     \u001b[36m0.7444\u001b[0m        \u001b[32m0.5199\u001b[0m                     0.7213        0.5636  0.0093  0.8130\n",
      "      6                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.5025\u001b[0m                     0.7474        0.5155  0.0090  0.8130\n",
      "      7                     0.7447        0.5104                     0.7523        0.5347  0.0085  0.8136\n",
      "      8                     0.7513        0.5044                     0.7577        \u001b[31m0.5105\u001b[0m  0.0080  0.8130\n",
      "      9                     0.7505        0.5065                     0.7608        0.5564  0.0075  0.8134\n",
      "     10                     0.7548        \u001b[32m0.4977\u001b[0m                     0.6980        0.5865  0.0069  0.8125\n",
      "     11                     0.7506        0.4980                     0.7552        0.5112  0.0063  0.8136\n",
      "     12                     \u001b[36m0.7664\u001b[0m        \u001b[32m0.4817\u001b[0m                     0.7364        0.5334  0.0057  0.8128\n",
      "     13                     0.7557        0.4897                     0.6205        0.7505  0.0050  0.8135\n",
      "     14                     0.7608        \u001b[32m0.4786\u001b[0m                     0.7518        \u001b[31m0.5085\u001b[0m  0.0043  0.8152\n",
      "     15                     0.7633        \u001b[32m0.4645\u001b[0m                     0.7256        0.5654  0.0037  0.8128\n",
      "     16                     \u001b[36m0.7768\u001b[0m        \u001b[32m0.4546\u001b[0m                     \u001b[35m0.7622\u001b[0m        \u001b[31m0.4960\u001b[0m  0.0031  0.8129\n",
      "     17                     0.7697        0.4684                     0.7494        0.5030  0.0025  0.8131\n",
      "     18                     0.7724        0.4698                     \u001b[35m0.7735\u001b[0m        0.4974  0.0020  0.8131\n",
      "     19                     \u001b[36m0.7824\u001b[0m        \u001b[32m0.4536\u001b[0m                     0.7631        0.5049  0.0015  0.8133\n",
      "     20                     0.7778        0.4601                     0.7690        0.5009  0.0010  0.8129\n",
      "     21                     \u001b[36m0.7935\u001b[0m        \u001b[32m0.4470\u001b[0m                     0.7688        0.5017  0.0007  0.8139\n",
      "     22                     0.7903        \u001b[32m0.4450\u001b[0m                     \u001b[35m0.7770\u001b[0m        0.5000  0.0004  0.8134\n",
      "     23                     0.7898        0.4502                     \u001b[35m0.7798\u001b[0m        \u001b[31m0.4932\u001b[0m  0.0002  0.8129\n",
      "     24                     \u001b[36m0.7991\u001b[0m        \u001b[32m0.4384\u001b[0m                     0.7786        \u001b[31m0.4913\u001b[0m  0.0000  0.8125\n",
      "     25                     0.7913        0.4495                     0.7780        \u001b[31m0.4905\u001b[0m  0.0000  0.8135\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6567\u001b[0m        \u001b[32m0.6421\u001b[0m                     \u001b[35m0.6791\u001b[0m        \u001b[31m0.6838\u001b[0m  0.0100  0.8125\n",
      "      2                     \u001b[36m0.7037\u001b[0m        \u001b[32m0.5654\u001b[0m                     \u001b[35m0.7018\u001b[0m        \u001b[31m0.6316\u001b[0m  0.0100  0.8139\n",
      "      3                     \u001b[36m0.7276\u001b[0m        \u001b[32m0.5550\u001b[0m                     \u001b[35m0.7291\u001b[0m        \u001b[31m0.5482\u001b[0m  0.0098  0.8136\n",
      "      4                     0.7156        0.5593                     0.7036        0.5778  0.0096  0.8138\n",
      "      5                     \u001b[36m0.7358\u001b[0m        \u001b[32m0.5380\u001b[0m                     0.7266        0.5527  0.0093  0.8129\n",
      "      6                     \u001b[36m0.7539\u001b[0m        \u001b[32m0.5311\u001b[0m                     \u001b[35m0.7346\u001b[0m        \u001b[31m0.5451\u001b[0m  0.0090  0.8139\n",
      "      7                     \u001b[36m0.7616\u001b[0m        \u001b[32m0.5144\u001b[0m                     0.7216        \u001b[31m0.5352\u001b[0m  0.0085  0.8138\n",
      "      8                     0.7509        \u001b[32m0.5070\u001b[0m                     0.7295        0.5401  0.0080  0.8139\n",
      "      9                     0.7498        \u001b[32m0.4983\u001b[0m                     0.7137        0.5417  0.0075  0.8141\n",
      "     10                     0.7506        0.5050                     0.7077        0.5908  0.0069  0.8127\n",
      "     11                     \u001b[36m0.7698\u001b[0m        \u001b[32m0.4835\u001b[0m                     \u001b[35m0.7382\u001b[0m        \u001b[31m0.5198\u001b[0m  0.0063  0.8139\n",
      "     12                     0.7628        \u001b[32m0.4806\u001b[0m                     0.7022        0.5637  0.0057  0.8131\n",
      "     13                     0.7617        0.4930                     \u001b[35m0.7412\u001b[0m        \u001b[31m0.5193\u001b[0m  0.0050  0.8136\n",
      "     14                     \u001b[36m0.7717\u001b[0m        0.4830                     0.7385        \u001b[31m0.5076\u001b[0m  0.0043  0.8144\n",
      "     15                     \u001b[36m0.7764\u001b[0m        \u001b[32m0.4714\u001b[0m                     0.7364        0.5118  0.0037  0.8139\n",
      "     16                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4603\u001b[0m                     0.7161        0.5523  0.0031  0.8137\n",
      "     17                     \u001b[36m0.7877\u001b[0m        0.4610                     0.7095        0.5992  0.0025  0.8138\n",
      "     18                     0.7814        \u001b[32m0.4520\u001b[0m                     0.7339        0.5239  0.0020  0.8139\n",
      "     19                     0.7785        0.4528                     \u001b[35m0.7426\u001b[0m        \u001b[31m0.4976\u001b[0m  0.0015  0.8139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                     \u001b[36m0.7899\u001b[0m        \u001b[32m0.4501\u001b[0m                     \u001b[35m0.7558\u001b[0m        0.5014  0.0010  0.8139\n",
      "     21                     \u001b[36m0.7911\u001b[0m        \u001b[32m0.4439\u001b[0m                     0.7495        0.4979  0.0007  0.8149\n",
      "     22                     0.7908        0.4451                     0.7503        0.5068  0.0004  0.8141\n",
      "     23                     0.7895        \u001b[32m0.4435\u001b[0m                     0.7467        0.5086  0.0002  0.8138\n",
      "     24                     \u001b[36m0.7928\u001b[0m        \u001b[32m0.4406\u001b[0m                     0.7459        0.5039  0.0000  0.8148\n",
      "     25                     0.7831        0.4421                     0.7458        0.5010  0.0000  0.8136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[1 0 0 ... 0 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6465\u001b[0m        \u001b[32m0.6548\u001b[0m                     \u001b[35m0.6712\u001b[0m        \u001b[31m0.6305\u001b[0m  0.0100  1.7897\n",
      "      2                     \u001b[36m0.6964\u001b[0m        \u001b[32m0.5882\u001b[0m                     \u001b[35m0.6727\u001b[0m        \u001b[31m0.5958\u001b[0m  0.0100  1.7769\n",
      "      3                     \u001b[36m0.7060\u001b[0m        \u001b[32m0.5684\u001b[0m                     \u001b[35m0.6823\u001b[0m        0.5987  0.0098  1.7772\n",
      "      4                     \u001b[36m0.7242\u001b[0m        \u001b[32m0.5552\u001b[0m                     0.5841        0.8292  0.0096  1.7766\n",
      "      5                     0.7209        \u001b[32m0.5426\u001b[0m                     0.6296        0.6913  0.0093  1.7769\n",
      "      6                     \u001b[36m0.7282\u001b[0m        \u001b[32m0.5366\u001b[0m                     \u001b[35m0.7434\u001b[0m        \u001b[31m0.5229\u001b[0m  0.0090  1.7764\n",
      "      7                     \u001b[36m0.7415\u001b[0m        \u001b[32m0.5187\u001b[0m                     0.7295        0.5356  0.0085  1.7766\n",
      "      8                     \u001b[36m0.7421\u001b[0m        0.5246                     0.6420        0.6541  0.0080  1.7766\n",
      "      9                     0.7410        0.5309                     0.6956        0.5906  0.0075  1.7764\n",
      "     10                     0.7294        0.5324                     0.5313        1.0371  0.0069  1.7763\n",
      "     11                     \u001b[36m0.7493\u001b[0m        \u001b[32m0.5100\u001b[0m                     0.5182        1.0883  0.0063  1.7772\n",
      "     12                     0.7466        \u001b[32m0.5064\u001b[0m                     0.7402        0.5433  0.0057  1.7761\n",
      "     13                     \u001b[36m0.7606\u001b[0m        \u001b[32m0.4957\u001b[0m                     0.6157        0.6990  0.0050  1.7759\n",
      "     14                     \u001b[36m0.7681\u001b[0m        \u001b[32m0.4925\u001b[0m                     0.7013        0.5694  0.0043  1.7767\n",
      "     15                     \u001b[36m0.7747\u001b[0m        \u001b[32m0.4882\u001b[0m                     0.6505        0.6410  0.0037  1.7761\n",
      "     16                     0.7694        \u001b[32m0.4824\u001b[0m                     0.7238        0.5578  0.0031  1.7772\n",
      "     17                     0.7736        \u001b[32m0.4733\u001b[0m                     0.7364        0.5372  0.0025  1.7781\n",
      "     18                     0.7709        0.4776                     0.7291        0.5430  0.0020  1.7774\n",
      "     19                     0.7740        \u001b[32m0.4684\u001b[0m                     0.7102        0.5753  0.0015  1.7776\n",
      "     20                     \u001b[36m0.7800\u001b[0m        \u001b[32m0.4630\u001b[0m                     \u001b[35m0.7522\u001b[0m        0.5270  0.0010  1.7773\n",
      "     21                     \u001b[36m0.7867\u001b[0m        \u001b[32m0.4577\u001b[0m                     \u001b[35m0.7552\u001b[0m        \u001b[31m0.5135\u001b[0m  0.0007  1.7769\n",
      "     22                     0.7754        0.4596                     \u001b[35m0.7564\u001b[0m        0.5143  0.0004  1.7779\n",
      "     23                     \u001b[36m0.7913\u001b[0m        \u001b[32m0.4473\u001b[0m                     0.7561        \u001b[31m0.5127\u001b[0m  0.0002  1.7767\n",
      "     24                     0.7847        0.4522                     0.7563        0.5130  0.0000  1.7777\n",
      "     25                     0.7883        0.4546                     0.7547        0.5137  0.0000  1.7771\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6382\u001b[0m        \u001b[32m0.6569\u001b[0m                     \u001b[35m0.6613\u001b[0m        \u001b[31m0.6295\u001b[0m  0.0100  1.7754\n",
      "      2                     \u001b[36m0.6866\u001b[0m        \u001b[32m0.5998\u001b[0m                     \u001b[35m0.6798\u001b[0m        \u001b[31m0.5887\u001b[0m  0.0100  1.7783\n",
      "      3                     \u001b[36m0.7059\u001b[0m        \u001b[32m0.5727\u001b[0m                     \u001b[35m0.7075\u001b[0m        \u001b[31m0.5555\u001b[0m  0.0098  1.7779\n",
      "      4                     \u001b[36m0.7335\u001b[0m        \u001b[32m0.5486\u001b[0m                     \u001b[35m0.7337\u001b[0m        \u001b[31m0.5531\u001b[0m  0.0096  1.7777\n",
      "      5                     0.7294        \u001b[32m0.5420\u001b[0m                     0.6960        0.5690  0.0093  1.7777\n",
      "      6                     0.7312        0.5444                     0.6662        0.6874  0.0090  1.7774\n",
      "      7                     \u001b[36m0.7387\u001b[0m        \u001b[32m0.5334\u001b[0m                     0.7277        \u001b[31m0.5493\u001b[0m  0.0085  1.7776\n",
      "      8                     0.7352        \u001b[32m0.5313\u001b[0m                     0.6507        0.6819  0.0080  1.7773\n",
      "      9                     0.7373        \u001b[32m0.5222\u001b[0m                     0.6719        0.6163  0.0075  1.7779\n",
      "     10                     \u001b[36m0.7526\u001b[0m        \u001b[32m0.5182\u001b[0m                     0.5443        0.9307  0.0069  1.7770\n",
      "     11                     0.7517        \u001b[32m0.5149\u001b[0m                     0.6560        0.6397  0.0063  1.7780\n",
      "     12                     0.7493        \u001b[32m0.5115\u001b[0m                     0.6732        0.6162  0.0057  1.7794\n",
      "     13                     0.7443        0.5237                     0.6740        0.5716  0.0050  1.7807\n",
      "     14                     0.7482        \u001b[32m0.5032\u001b[0m                     0.7072        0.6068  0.0043  1.7793\n",
      "     15                     \u001b[36m0.7618\u001b[0m        \u001b[32m0.4987\u001b[0m                     0.6394        0.6550  0.0037  1.7793\n",
      "     16                     \u001b[36m0.7675\u001b[0m        \u001b[32m0.4865\u001b[0m                     0.7192        0.5594  0.0031  1.7789\n",
      "     17                     0.7642        0.4938                     0.5974        0.7259  0.0025  1.7787\n",
      "     18                     \u001b[36m0.7710\u001b[0m        \u001b[32m0.4813\u001b[0m                     0.6788        0.5891  0.0020  1.7782\n",
      "     19                     \u001b[36m0.7716\u001b[0m        \u001b[32m0.4809\u001b[0m                     0.6939        0.5645  0.0015  1.7794\n",
      "     20                     \u001b[36m0.7755\u001b[0m        \u001b[32m0.4776\u001b[0m                     0.7282        \u001b[31m0.5205\u001b[0m  0.0010  1.7793\n",
      "     21                     \u001b[36m0.7798\u001b[0m        \u001b[32m0.4720\u001b[0m                     0.7198        0.5283  0.0007  1.7793\n",
      "     22                     0.7794        \u001b[32m0.4689\u001b[0m                     0.6723        0.6132  0.0004  1.7788\n",
      "     23                     \u001b[36m0.7810\u001b[0m        \u001b[32m0.4645\u001b[0m                     0.6881        0.5795  0.0002  1.7791\n",
      "     24                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4631\u001b[0m                     0.7130        0.5402  0.0000  1.7796\n",
      "     25                     0.7809        0.4667                     0.7210        0.5331  0.0000  1.7814\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6500\u001b[0m        \u001b[32m0.6512\u001b[0m                     \u001b[35m0.6646\u001b[0m        \u001b[31m0.6000\u001b[0m  0.0100  1.7743\n",
      "      2                     \u001b[36m0.6988\u001b[0m        \u001b[32m0.5845\u001b[0m                     0.5572        0.7994  0.0100  1.7764\n",
      "      3                     \u001b[36m0.7080\u001b[0m        \u001b[32m0.5668\u001b[0m                     \u001b[35m0.6671\u001b[0m        0.6163  0.0098  1.7758\n",
      "      4                     \u001b[36m0.7186\u001b[0m        \u001b[32m0.5516\u001b[0m                     \u001b[35m0.7245\u001b[0m        \u001b[31m0.5589\u001b[0m  0.0096  1.7772\n",
      "      5                     \u001b[36m0.7194\u001b[0m        \u001b[32m0.5461\u001b[0m                     \u001b[35m0.7374\u001b[0m        \u001b[31m0.5424\u001b[0m  0.0093  1.7762\n",
      "      6                     \u001b[36m0.7334\u001b[0m        \u001b[32m0.5398\u001b[0m                     0.6734        0.6658  0.0090  1.7764\n",
      "      7                     \u001b[36m0.7415\u001b[0m        \u001b[32m0.5335\u001b[0m                     0.6372        0.6066  0.0085  1.7758\n",
      "      8                     \u001b[36m0.7474\u001b[0m        \u001b[32m0.5179\u001b[0m                     0.7113        0.5539  0.0080  1.7767\n",
      "      9                     0.7435        0.5209                     0.7305        0.5499  0.0075  1.7766\n",
      "     10                     0.7444        0.5206                     0.7327        \u001b[31m0.5334\u001b[0m  0.0069  1.7773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     \u001b[36m0.7513\u001b[0m        \u001b[32m0.5167\u001b[0m                     0.6976        0.6022  0.0063  1.7760\n",
      "     12                     \u001b[36m0.7525\u001b[0m        \u001b[32m0.5075\u001b[0m                     0.5562        0.8432  0.0057  1.7762\n",
      "     13                     \u001b[36m0.7582\u001b[0m        \u001b[32m0.4996\u001b[0m                     0.5918        0.7232  0.0050  1.7766\n",
      "     14                     0.7559        0.5025                     0.7305        0.5459  0.0043  1.7764\n",
      "     15                     0.7575        \u001b[32m0.4928\u001b[0m                     0.6901        0.5800  0.0037  1.7767\n",
      "     16                     \u001b[36m0.7710\u001b[0m        \u001b[32m0.4832\u001b[0m                     0.7336        0.5379  0.0031  1.7772\n",
      "     17                     0.7688        \u001b[32m0.4783\u001b[0m                     0.7330        0.5579  0.0025  1.7769\n",
      "     18                     \u001b[36m0.7813\u001b[0m        \u001b[32m0.4744\u001b[0m                     0.7360        0.5357  0.0020  1.7776\n",
      "     19                     0.7749        \u001b[32m0.4672\u001b[0m                     \u001b[35m0.7484\u001b[0m        \u001b[31m0.5297\u001b[0m  0.0015  1.7774\n",
      "     20                     0.7722        0.4796                     0.6988        0.5903  0.0010  1.7783\n",
      "     21                     0.7762        0.4708                     \u001b[35m0.7503\u001b[0m        \u001b[31m0.5259\u001b[0m  0.0007  1.7787\n",
      "     22                     0.7812        0.4691                     0.7449        \u001b[31m0.5190\u001b[0m  0.0004  1.7778\n",
      "     23                     \u001b[36m0.7818\u001b[0m        \u001b[32m0.4628\u001b[0m                     0.7483        0.5198  0.0002  1.7772\n",
      "     24                     0.7814        0.4639                     0.7488        0.5209  0.0000  1.7775\n",
      "     25                     \u001b[36m0.7831\u001b[0m        \u001b[32m0.4596\u001b[0m                     0.7413        0.5221  0.0000  1.7765\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6425\u001b[0m        \u001b[32m0.6602\u001b[0m                     \u001b[35m0.6464\u001b[0m        \u001b[31m0.6621\u001b[0m  0.0100  1.7753\n",
      "      2                     \u001b[36m0.6687\u001b[0m        \u001b[32m0.6129\u001b[0m                     \u001b[35m0.7047\u001b[0m        \u001b[31m0.5801\u001b[0m  0.0100  1.7782\n",
      "      3                     \u001b[36m0.6853\u001b[0m        \u001b[32m0.5964\u001b[0m                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5621\u001b[0m  0.0098  1.7780\n",
      "      4                     \u001b[36m0.7032\u001b[0m        \u001b[32m0.5683\u001b[0m                     0.7162        0.5624  0.0096  1.7789\n",
      "      5                     \u001b[36m0.7077\u001b[0m        \u001b[32m0.5629\u001b[0m                     0.7291        \u001b[31m0.5612\u001b[0m  0.0093  1.7798\n",
      "      6                     0.7024        0.5693                     \u001b[35m0.7434\u001b[0m        \u001b[31m0.5455\u001b[0m  0.0090  1.7778\n",
      "      7                     \u001b[36m0.7211\u001b[0m        \u001b[32m0.5563\u001b[0m                     0.7272        0.5560  0.0085  1.7787\n",
      "      8                     0.7177        \u001b[32m0.5518\u001b[0m                     0.6642        0.6433  0.0080  1.7774\n",
      "      9                     \u001b[36m0.7230\u001b[0m        \u001b[32m0.5404\u001b[0m                     0.6651        0.6314  0.0075  1.7790\n",
      "     10                     \u001b[36m0.7286\u001b[0m        \u001b[32m0.5364\u001b[0m                     \u001b[35m0.7516\u001b[0m        \u001b[31m0.5229\u001b[0m  0.0069  1.7774\n",
      "     11                     \u001b[36m0.7327\u001b[0m        \u001b[32m0.5275\u001b[0m                     0.5178        1.1390  0.0063  1.7789\n",
      "     12                     \u001b[36m0.7415\u001b[0m        0.5293                     0.6060        0.6789  0.0057  1.7780\n",
      "     13                     0.7396        \u001b[32m0.5122\u001b[0m                     0.5334        1.0616  0.0050  1.7780\n",
      "     14                     \u001b[36m0.7472\u001b[0m        \u001b[32m0.5089\u001b[0m                     0.6647        0.6120  0.0043  1.7782\n",
      "     15                     0.7436        0.5187                     \u001b[35m0.7556\u001b[0m        \u001b[31m0.5189\u001b[0m  0.0037  1.7782\n",
      "     16                     \u001b[36m0.7549\u001b[0m        \u001b[32m0.5057\u001b[0m                     \u001b[35m0.7569\u001b[0m        0.5253  0.0031  1.7789\n",
      "     17                     0.7513        0.5063                     \u001b[35m0.7801\u001b[0m        \u001b[31m0.5012\u001b[0m  0.0025  1.7782\n",
      "     18                     \u001b[36m0.7556\u001b[0m        \u001b[32m0.4964\u001b[0m                     0.7128        0.5591  0.0020  1.7785\n",
      "     19                     \u001b[36m0.7617\u001b[0m        \u001b[32m0.4907\u001b[0m                     0.7540        0.5051  0.0015  1.7785\n",
      "     20                     \u001b[36m0.7654\u001b[0m        \u001b[32m0.4860\u001b[0m                     0.6467        0.6367  0.0010  1.7792\n",
      "     21                     \u001b[36m0.7665\u001b[0m        \u001b[32m0.4809\u001b[0m                     0.7789        \u001b[31m0.4948\u001b[0m  0.0007  1.7787\n",
      "     22                     0.7594        0.4882                     0.7776        \u001b[31m0.4903\u001b[0m  0.0004  1.7793\n",
      "     23                     0.7660        \u001b[32m0.4737\u001b[0m                     \u001b[35m0.7885\u001b[0m        \u001b[31m0.4857\u001b[0m  0.0002  1.7783\n",
      "     24                     \u001b[36m0.7712\u001b[0m        0.4768                     0.7814        0.4865  0.0000  1.7787\n",
      "     25                     0.7709        0.4756                     0.7824        0.4863  0.0000  1.7786\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6370\u001b[0m        \u001b[32m0.6478\u001b[0m                     \u001b[35m0.6110\u001b[0m        \u001b[31m0.6774\u001b[0m  0.0100  1.7760\n",
      "      2                     \u001b[36m0.6828\u001b[0m        \u001b[32m0.5890\u001b[0m                     \u001b[35m0.7054\u001b[0m        \u001b[31m0.5665\u001b[0m  0.0100  1.7779\n",
      "      3                     \u001b[36m0.7026\u001b[0m        \u001b[32m0.5767\u001b[0m                     0.6853        0.5821  0.0098  1.7777\n",
      "      4                     \u001b[36m0.7180\u001b[0m        \u001b[32m0.5547\u001b[0m                     0.6897        0.6086  0.0096  1.7780\n",
      "      5                     \u001b[36m0.7188\u001b[0m        \u001b[32m0.5543\u001b[0m                     \u001b[35m0.7082\u001b[0m        0.5833  0.0093  1.7804\n",
      "      6                     \u001b[36m0.7350\u001b[0m        \u001b[32m0.5370\u001b[0m                     \u001b[35m0.7145\u001b[0m        \u001b[31m0.5564\u001b[0m  0.0090  1.7793\n",
      "      7                     \u001b[36m0.7358\u001b[0m        0.5458                     0.5837        0.7799  0.0085  1.7782\n",
      "      8                     0.7312        \u001b[32m0.5326\u001b[0m                     \u001b[35m0.7301\u001b[0m        \u001b[31m0.5456\u001b[0m  0.0080  1.7784\n",
      "      9                     \u001b[36m0.7423\u001b[0m        \u001b[32m0.5191\u001b[0m                     \u001b[35m0.7310\u001b[0m        \u001b[31m0.5326\u001b[0m  0.0075  1.7774\n",
      "     10                     \u001b[36m0.7498\u001b[0m        0.5194                     0.7216        0.5461  0.0069  1.7792\n",
      "     11                     \u001b[36m0.7561\u001b[0m        \u001b[32m0.5120\u001b[0m                     0.7240        0.5512  0.0063  1.7779\n",
      "     12                     0.7501        0.5172                     0.5141        1.1776  0.0057  1.7786\n",
      "     13                     \u001b[36m0.7563\u001b[0m        \u001b[32m0.5005\u001b[0m                     0.7197        0.5393  0.0050  1.7780\n",
      "     14                     \u001b[36m0.7596\u001b[0m        \u001b[32m0.5000\u001b[0m                     \u001b[35m0.7396\u001b[0m        \u001b[31m0.5210\u001b[0m  0.0043  1.7790\n",
      "     15                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4912\u001b[0m                     0.7181        0.5775  0.0037  1.7780\n",
      "     16                     0.7624        0.4916                     0.7389        0.5387  0.0031  1.7783\n",
      "     17                     \u001b[36m0.7739\u001b[0m        \u001b[32m0.4844\u001b[0m                     0.7322        0.5307  0.0025  1.7797\n",
      "     18                     0.7728        \u001b[32m0.4775\u001b[0m                     0.7326        0.5280  0.0020  1.7782\n",
      "     19                     0.7739        \u001b[32m0.4690\u001b[0m                     0.7179        0.5467  0.0015  1.7788\n",
      "     20                     \u001b[36m0.7762\u001b[0m        \u001b[32m0.4665\u001b[0m                     0.7313        0.5370  0.0010  1.7794\n",
      "     21                     \u001b[36m0.7822\u001b[0m        \u001b[32m0.4585\u001b[0m                     \u001b[35m0.7415\u001b[0m        0.5235  0.0007  1.7787\n",
      "     22                     0.7726        0.4640                     \u001b[35m0.7442\u001b[0m        \u001b[31m0.5195\u001b[0m  0.0004  1.7795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23                     \u001b[36m0.7826\u001b[0m        \u001b[32m0.4544\u001b[0m                     \u001b[35m0.7525\u001b[0m        0.5209  0.0002  1.7805\n",
      "     24                     \u001b[36m0.7863\u001b[0m        0.4575                     0.7510        0.5212  0.0000  1.7783\n",
      "     25                     \u001b[36m0.7874\u001b[0m        0.4561                     0.7518        0.5215  0.0000  1.7797\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6381\u001b[0m        \u001b[32m0.6611\u001b[0m                     \u001b[35m0.6503\u001b[0m        \u001b[31m0.6485\u001b[0m  0.0100  1.7749\n",
      "      2                     \u001b[36m0.6716\u001b[0m        \u001b[32m0.6141\u001b[0m                     \u001b[35m0.6590\u001b[0m        \u001b[31m0.6128\u001b[0m  0.0100  1.7780\n",
      "      3                     \u001b[36m0.7001\u001b[0m        \u001b[32m0.5703\u001b[0m                     \u001b[35m0.6655\u001b[0m        0.6168  0.0098  1.7775\n",
      "      4                     \u001b[36m0.7163\u001b[0m        \u001b[32m0.5543\u001b[0m                     0.6328        0.6618  0.0096  1.7765\n",
      "      5                     \u001b[36m0.7220\u001b[0m        \u001b[32m0.5459\u001b[0m                     0.6221        0.7811  0.0093  1.7776\n",
      "      6                     \u001b[36m0.7234\u001b[0m        \u001b[32m0.5445\u001b[0m                     \u001b[35m0.7358\u001b[0m        \u001b[31m0.5342\u001b[0m  0.0090  1.7787\n",
      "      7                     \u001b[36m0.7301\u001b[0m        \u001b[32m0.5345\u001b[0m                     0.7071        0.5715  0.0085  1.7781\n",
      "      8                     0.7237        0.5461                     0.7280        0.5777  0.0080  1.7778\n",
      "      9                     \u001b[36m0.7370\u001b[0m        \u001b[32m0.5320\u001b[0m                     0.6900        0.6296  0.0075  1.7764\n",
      "     10                     \u001b[36m0.7480\u001b[0m        \u001b[32m0.5264\u001b[0m                     0.5772        0.7336  0.0069  1.7776\n",
      "     11                     0.7397        \u001b[32m0.5261\u001b[0m                     0.7070        0.5672  0.0063  1.7777\n",
      "     12                     \u001b[36m0.7554\u001b[0m        \u001b[32m0.5151\u001b[0m                     0.7024        0.5750  0.0057  1.7780\n",
      "     13                     0.7485        \u001b[32m0.5100\u001b[0m                     0.7169        0.5577  0.0050  1.7782\n",
      "     14                     \u001b[36m0.7571\u001b[0m        \u001b[32m0.5031\u001b[0m                     0.5742        0.8663  0.0043  1.7773\n",
      "     15                     \u001b[36m0.7619\u001b[0m        0.5050                     0.6225        0.7063  0.0037  1.7782\n",
      "     16                     \u001b[36m0.7648\u001b[0m        \u001b[32m0.4955\u001b[0m                     0.7234        \u001b[31m0.5339\u001b[0m  0.0031  1.7784\n",
      "     17                     0.7610        \u001b[32m0.4868\u001b[0m                     0.7217        0.5534  0.0025  1.7780\n",
      "     18                     0.7625        0.4903                     0.6001        0.7097  0.0020  1.7792\n",
      "     19                     \u001b[36m0.7707\u001b[0m        \u001b[32m0.4840\u001b[0m                     \u001b[35m0.7401\u001b[0m        0.5345  0.0015  1.7774\n",
      "     20                     0.7654        \u001b[32m0.4802\u001b[0m                     0.7134        0.5614  0.0010  1.7776\n",
      "     21                     \u001b[36m0.7723\u001b[0m        \u001b[32m0.4688\u001b[0m                     \u001b[35m0.7459\u001b[0m        \u001b[31m0.5334\u001b[0m  0.0007  1.7793\n",
      "     22                     \u001b[36m0.7820\u001b[0m        0.4701                     0.7404        \u001b[31m0.5231\u001b[0m  0.0004  1.7777\n",
      "     23                     0.7765        0.4733                     \u001b[35m0.7475\u001b[0m        \u001b[31m0.5182\u001b[0m  0.0002  1.7778\n",
      "     24                     0.7772        \u001b[32m0.4643\u001b[0m                     0.7471        0.5187  0.0000  1.7786\n",
      "     25                     0.7718        0.4756                     0.7458        0.5190  0.0000  1.7793\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6386\u001b[0m        \u001b[32m0.6578\u001b[0m                     \u001b[35m0.6693\u001b[0m        \u001b[31m0.5948\u001b[0m  0.0100  1.7761\n",
      "      2                     \u001b[36m0.6776\u001b[0m        \u001b[32m0.5984\u001b[0m                     \u001b[35m0.6747\u001b[0m        0.6099  0.0100  1.7771\n",
      "      3                     \u001b[36m0.7114\u001b[0m        \u001b[32m0.5658\u001b[0m                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5580\u001b[0m  0.0098  1.7783\n",
      "      4                     \u001b[36m0.7181\u001b[0m        \u001b[32m0.5547\u001b[0m                     0.6999        0.6338  0.0096  1.7792\n",
      "      5                     \u001b[36m0.7339\u001b[0m        \u001b[32m0.5468\u001b[0m                     0.6735        0.6349  0.0093  1.7782\n",
      "      6                     0.7251        \u001b[32m0.5381\u001b[0m                     0.6279        0.7508  0.0090  1.7775\n",
      "      7                     \u001b[36m0.7351\u001b[0m        \u001b[32m0.5283\u001b[0m                     0.7111        0.5636  0.0085  1.7770\n",
      "      8                     \u001b[36m0.7406\u001b[0m        \u001b[32m0.5215\u001b[0m                     0.6583        0.6201  0.0080  1.7781\n",
      "      9                     0.7154        0.5562                     0.6813        0.6604  0.0075  1.7779\n",
      "     10                     0.7336        \u001b[32m0.5201\u001b[0m                     \u001b[35m0.7321\u001b[0m        0.5759  0.0069  1.7773\n",
      "     11                     \u001b[36m0.7487\u001b[0m        \u001b[32m0.5086\u001b[0m                     0.5398        1.3577  0.0063  1.7775\n",
      "     12                     \u001b[36m0.7523\u001b[0m        0.5105                     0.6637        0.7027  0.0057  1.7777\n",
      "     13                     0.7412        0.5190                     0.5766        0.9437  0.0050  1.7784\n",
      "     14                     \u001b[36m0.7576\u001b[0m        \u001b[32m0.5049\u001b[0m                     0.7075        0.5773  0.0043  1.7778\n",
      "     15                     \u001b[36m0.7612\u001b[0m        \u001b[32m0.4945\u001b[0m                     0.6553        0.7399  0.0037  1.7780\n",
      "     16                     \u001b[36m0.7689\u001b[0m        \u001b[32m0.4816\u001b[0m                     0.7099        0.5635  0.0031  1.7790\n",
      "     17                     0.7642        \u001b[32m0.4790\u001b[0m                     0.7251        0.5691  0.0025  1.7792\n",
      "     18                     \u001b[36m0.7707\u001b[0m        0.4810                     0.6958        0.5962  0.0020  1.7783\n",
      "     19                     0.7651        \u001b[32m0.4778\u001b[0m                     \u001b[35m0.7322\u001b[0m        \u001b[31m0.5522\u001b[0m  0.0015  1.7784\n",
      "     20                     \u001b[36m0.7803\u001b[0m        \u001b[32m0.4740\u001b[0m                     \u001b[35m0.7373\u001b[0m        \u001b[31m0.5437\u001b[0m  0.0010  1.7783\n",
      "     21                     \u001b[36m0.7812\u001b[0m        \u001b[32m0.4690\u001b[0m                     0.7295        0.5469  0.0007  1.7784\n",
      "     22                     \u001b[36m0.7866\u001b[0m        \u001b[32m0.4615\u001b[0m                     0.7350        0.5474  0.0004  1.7822\n",
      "     23                     \u001b[36m0.7870\u001b[0m        0.4666                     0.7325        \u001b[31m0.5408\u001b[0m  0.0002  1.7784\n",
      "     24                     0.7804        0.4658                     0.7308        0.5432  0.0000  1.7792\n",
      "     25                     0.7858        \u001b[32m0.4578\u001b[0m                     0.7323        0.5436  0.0000  1.7785\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6324\u001b[0m        \u001b[32m0.6636\u001b[0m                     \u001b[35m0.6708\u001b[0m        \u001b[31m0.5925\u001b[0m  0.0100  1.7743\n",
      "      2                     \u001b[36m0.6841\u001b[0m        \u001b[32m0.5972\u001b[0m                     \u001b[35m0.7173\u001b[0m        \u001b[31m0.5757\u001b[0m  0.0100  1.7775\n",
      "      3                     \u001b[36m0.7034\u001b[0m        \u001b[32m0.5796\u001b[0m                     0.7163        0.5803  0.0098  1.7774\n",
      "      4                     \u001b[36m0.7110\u001b[0m        \u001b[32m0.5641\u001b[0m                     0.6364        0.6354  0.0096  1.7773\n",
      "      5                     \u001b[36m0.7170\u001b[0m        \u001b[32m0.5520\u001b[0m                     0.6528        0.6136  0.0093  1.7773\n",
      "      6                     \u001b[36m0.7261\u001b[0m        \u001b[32m0.5410\u001b[0m                     0.5532        0.9203  0.0090  1.7770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7                     \u001b[36m0.7339\u001b[0m        \u001b[32m0.5355\u001b[0m                     0.6799        0.6357  0.0085  1.7776\n",
      "      8                     0.7214        0.5425                     \u001b[35m0.7197\u001b[0m        \u001b[31m0.5576\u001b[0m  0.0080  1.7781\n",
      "      9                     \u001b[36m0.7377\u001b[0m        \u001b[32m0.5237\u001b[0m                     0.7147        0.6098  0.0075  1.7798\n",
      "     10                     \u001b[36m0.7451\u001b[0m        \u001b[32m0.5162\u001b[0m                     0.5512        0.8932  0.0069  1.7779\n",
      "     11                     \u001b[36m0.7650\u001b[0m        \u001b[32m0.4967\u001b[0m                     0.6716        0.6315  0.0063  1.7773\n",
      "     12                     0.7552        0.5077                     0.6216        0.6931  0.0057  1.7772\n",
      "     13                     0.7580        0.5080                     0.5563        0.9116  0.0050  1.7773\n",
      "     14                     0.7643        0.4977                     \u001b[35m0.7432\u001b[0m        \u001b[31m0.5295\u001b[0m  0.0043  1.7778\n",
      "     15                     \u001b[36m0.7657\u001b[0m        \u001b[32m0.4943\u001b[0m                     0.6389        0.6448  0.0037  1.7787\n",
      "     16                     0.7559        0.4981                     0.7286        0.5390  0.0031  1.7774\n",
      "     17                     \u001b[36m0.7727\u001b[0m        \u001b[32m0.4784\u001b[0m                     0.6687        0.6434  0.0025  1.7774\n",
      "     18                     \u001b[36m0.7738\u001b[0m        \u001b[32m0.4743\u001b[0m                     0.7373        \u001b[31m0.5137\u001b[0m  0.0020  1.7779\n",
      "     19                     \u001b[36m0.7749\u001b[0m        0.4746                     \u001b[35m0.7647\u001b[0m        0.5156  0.0015  1.7782\n",
      "     20                     \u001b[36m0.7804\u001b[0m        \u001b[32m0.4599\u001b[0m                     0.6900        0.5987  0.0010  1.7772\n",
      "     21                     \u001b[36m0.7858\u001b[0m        0.4602                     0.7532        \u001b[31m0.5108\u001b[0m  0.0007  1.7780\n",
      "     22                     0.7781        0.4618                     0.7515        0.5128  0.0004  1.7773\n",
      "     23                     0.7854        \u001b[32m0.4587\u001b[0m                     \u001b[35m0.7658\u001b[0m        \u001b[31m0.5108\u001b[0m  0.0002  1.7789\n",
      "     24                     0.7807        0.4618                     0.7596        0.5129  0.0000  1.7779\n",
      "     25                     0.7836        0.4601                     0.7636        0.5131  0.0000  1.7774\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6339\u001b[0m        \u001b[32m0.6658\u001b[0m                     \u001b[35m0.6770\u001b[0m        \u001b[31m0.5971\u001b[0m  0.0100  1.7756\n",
      "      2                     \u001b[36m0.6821\u001b[0m        \u001b[32m0.6059\u001b[0m                     0.6718        0.6129  0.0100  1.7779\n",
      "      3                     \u001b[36m0.6930\u001b[0m        \u001b[32m0.5897\u001b[0m                     \u001b[35m0.7046\u001b[0m        \u001b[31m0.5798\u001b[0m  0.0098  1.7773\n",
      "      4                     \u001b[36m0.7072\u001b[0m        \u001b[32m0.5716\u001b[0m                     \u001b[35m0.7145\u001b[0m        0.5844  0.0096  1.7778\n",
      "      5                     \u001b[36m0.7153\u001b[0m        \u001b[32m0.5643\u001b[0m                     \u001b[35m0.7208\u001b[0m        \u001b[31m0.5517\u001b[0m  0.0093  1.7781\n",
      "      6                     \u001b[36m0.7207\u001b[0m        \u001b[32m0.5541\u001b[0m                     0.7186        \u001b[31m0.5490\u001b[0m  0.0090  1.7774\n",
      "      7                     \u001b[36m0.7302\u001b[0m        \u001b[32m0.5435\u001b[0m                     \u001b[35m0.7234\u001b[0m        0.5512  0.0085  1.7784\n",
      "      8                     0.7273        0.5439                     0.7227        0.5593  0.0080  1.7784\n",
      "      9                     \u001b[36m0.7404\u001b[0m        \u001b[32m0.5331\u001b[0m                     \u001b[35m0.7262\u001b[0m        \u001b[31m0.5388\u001b[0m  0.0075  1.7795\n",
      "     10                     0.7364        \u001b[32m0.5263\u001b[0m                     \u001b[35m0.7276\u001b[0m        0.5458  0.0069  1.7791\n",
      "     11                     0.7397        \u001b[32m0.5225\u001b[0m                     0.6864        0.5840  0.0063  1.7785\n",
      "     12                     \u001b[36m0.7505\u001b[0m        \u001b[32m0.5128\u001b[0m                     0.6855        0.5934  0.0057  1.7793\n",
      "     13                     \u001b[36m0.7513\u001b[0m        0.5162                     0.6993        0.5893  0.0050  1.7794\n",
      "     14                     0.7485        \u001b[32m0.5091\u001b[0m                     0.6558        0.6264  0.0043  1.7787\n",
      "     15                     \u001b[36m0.7530\u001b[0m        \u001b[32m0.5064\u001b[0m                     0.7176        \u001b[31m0.5311\u001b[0m  0.0037  1.7788\n",
      "     16                     \u001b[36m0.7637\u001b[0m        \u001b[32m0.5008\u001b[0m                     0.6932        0.5613  0.0031  1.7787\n",
      "     17                     0.7626        \u001b[32m0.4951\u001b[0m                     \u001b[35m0.7400\u001b[0m        \u001b[31m0.5235\u001b[0m  0.0025  1.7806\n",
      "     18                     \u001b[36m0.7684\u001b[0m        \u001b[32m0.4770\u001b[0m                     0.7107        0.5434  0.0020  1.7799\n",
      "     19                     \u001b[36m0.7745\u001b[0m        0.4830                     0.7272        0.5377  0.0015  1.7787\n",
      "     20                     0.7654        0.4813                     0.7288        0.5256  0.0010  1.7797\n",
      "     21                     \u001b[36m0.7766\u001b[0m        \u001b[32m0.4738\u001b[0m                     0.7234        0.5295  0.0007  1.7792\n",
      "     22                     \u001b[36m0.7792\u001b[0m        \u001b[32m0.4692\u001b[0m                     0.7322        \u001b[31m0.5208\u001b[0m  0.0004  1.7783\n",
      "     23                     \u001b[36m0.7871\u001b[0m        \u001b[32m0.4635\u001b[0m                     0.7377        \u001b[31m0.5163\u001b[0m  0.0002  1.7801\n",
      "     24                     0.7815        0.4733                     0.7391        \u001b[31m0.5130\u001b[0m  0.0000  1.7802\n",
      "     25                     0.7834        0.4673                     0.7386        \u001b[31m0.5114\u001b[0m  0.0000  1.7792\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6304\u001b[0m        \u001b[32m0.6534\u001b[0m                     \u001b[35m0.6828\u001b[0m        \u001b[31m0.6408\u001b[0m  0.0100  1.7752\n",
      "      2                     \u001b[36m0.6985\u001b[0m        \u001b[32m0.5909\u001b[0m                     \u001b[35m0.6914\u001b[0m        \u001b[31m0.5959\u001b[0m  0.0100  1.7774\n",
      "      3                     \u001b[36m0.7147\u001b[0m        \u001b[32m0.5560\u001b[0m                     \u001b[35m0.6983\u001b[0m        0.6144  0.0098  1.7771\n",
      "      4                     \u001b[36m0.7183\u001b[0m        \u001b[32m0.5553\u001b[0m                     \u001b[35m0.7247\u001b[0m        \u001b[31m0.5742\u001b[0m  0.0096  1.7771\n",
      "      5                     \u001b[36m0.7256\u001b[0m        \u001b[32m0.5444\u001b[0m                     0.6864        0.5973  0.0093  1.7768\n",
      "      6                     \u001b[36m0.7325\u001b[0m        \u001b[32m0.5398\u001b[0m                     0.7165        \u001b[31m0.5732\u001b[0m  0.0090  1.7775\n",
      "      7                     \u001b[36m0.7332\u001b[0m        \u001b[32m0.5373\u001b[0m                     0.6387        0.6595  0.0085  1.7767\n",
      "      8                     \u001b[36m0.7471\u001b[0m        \u001b[32m0.5182\u001b[0m                     0.6839        0.6195  0.0080  1.7770\n",
      "      9                     0.7389        0.5253                     0.6611        0.6431  0.0075  1.7772\n",
      "     10                     0.7426        0.5189                     0.7180        \u001b[31m0.5573\u001b[0m  0.0069  1.7777\n",
      "     11                     \u001b[36m0.7548\u001b[0m        \u001b[32m0.5124\u001b[0m                     \u001b[35m0.7311\u001b[0m        0.5581  0.0063  1.7775\n",
      "     12                     0.7509        \u001b[32m0.5056\u001b[0m                     0.6833        0.6283  0.0057  1.7773\n",
      "     13                     \u001b[36m0.7581\u001b[0m        \u001b[32m0.5045\u001b[0m                     0.6315        0.7114  0.0050  1.7775\n",
      "     14                     \u001b[36m0.7686\u001b[0m        \u001b[32m0.4895\u001b[0m                     0.6802        0.6342  0.0043  1.7792\n",
      "     15                     0.7665        \u001b[32m0.4888\u001b[0m                     0.6991        0.5855  0.0037  1.7777\n",
      "     16                     \u001b[36m0.7704\u001b[0m        \u001b[32m0.4850\u001b[0m                     0.7097        0.5684  0.0031  1.7776\n",
      "     17                     \u001b[36m0.7783\u001b[0m        \u001b[32m0.4767\u001b[0m                     0.6846        0.6448  0.0025  1.7771\n",
      "     18                     0.7775        \u001b[32m0.4684\u001b[0m                     \u001b[35m0.7416\u001b[0m        \u001b[31m0.5567\u001b[0m  0.0020  1.7779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19                     \u001b[36m0.7796\u001b[0m        0.4711                     0.7246        0.5577  0.0015  1.7783\n",
      "     20                     \u001b[36m0.7810\u001b[0m        \u001b[32m0.4610\u001b[0m                     0.7205        0.5763  0.0010  1.7786\n",
      "     21                     \u001b[36m0.7890\u001b[0m        \u001b[32m0.4599\u001b[0m                     0.7321        0.5609  0.0007  1.7781\n",
      "     22                     0.7818        \u001b[32m0.4591\u001b[0m                     0.7292        \u001b[31m0.5510\u001b[0m  0.0004  1.7781\n",
      "     23                     \u001b[36m0.7911\u001b[0m        \u001b[32m0.4579\u001b[0m                     0.7297        0.5537  0.0002  1.7782\n",
      "     24                     0.7850        0.4598                     0.7203        0.5545  0.0000  1.7782\n",
      "     25                     0.7807        \u001b[32m0.4568\u001b[0m                     0.7224        0.5552  0.0000  1.7771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 0 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5936\u001b[0m        \u001b[32m0.6858\u001b[0m                     \u001b[35m0.6338\u001b[0m        \u001b[31m0.6483\u001b[0m  0.0100  2.5721\n",
      "      2                     \u001b[36m0.6393\u001b[0m        \u001b[32m0.6420\u001b[0m                     0.6328        \u001b[31m0.6410\u001b[0m  0.0100  2.5524\n",
      "      3                     \u001b[36m0.6502\u001b[0m        \u001b[32m0.6283\u001b[0m                     \u001b[35m0.6447\u001b[0m        \u001b[31m0.6346\u001b[0m  0.0098  2.5521\n",
      "      4                     \u001b[36m0.6625\u001b[0m        \u001b[32m0.6209\u001b[0m                     \u001b[35m0.6694\u001b[0m        \u001b[31m0.6239\u001b[0m  0.0096  2.5515\n",
      "      5                     0.6609        \u001b[32m0.6190\u001b[0m                     0.6329        0.6454  0.0093  2.5529\n",
      "      6                     \u001b[36m0.6640\u001b[0m        \u001b[32m0.6159\u001b[0m                     0.6553        \u001b[31m0.6228\u001b[0m  0.0090  2.5516\n",
      "      7                     \u001b[36m0.6725\u001b[0m        \u001b[32m0.6119\u001b[0m                     0.6614        \u001b[31m0.6220\u001b[0m  0.0085  2.5521\n",
      "      8                     0.6698        \u001b[32m0.6114\u001b[0m                     0.6687        0.6249  0.0080  2.5522\n",
      "      9                     0.6707        0.6116                     0.6615        0.6249  0.0075  2.5512\n",
      "     10                     0.6685        \u001b[32m0.6068\u001b[0m                     0.6180        0.6584  0.0069  2.5527\n",
      "     11                     \u001b[36m0.6751\u001b[0m        \u001b[32m0.6066\u001b[0m                     0.5273        0.7596  0.0063  2.5523\n",
      "     12                     0.6745        \u001b[32m0.6020\u001b[0m                     0.5696        0.7058  0.0057  2.5522\n",
      "     13                     \u001b[36m0.6801\u001b[0m        0.6021                     0.6422        0.6279  0.0050  2.5521\n",
      "     14                     0.6785        \u001b[32m0.5990\u001b[0m                     0.5869        0.6626  0.0043  2.5519\n",
      "     15                     0.6797        0.6006                     0.6626        \u001b[31m0.6149\u001b[0m  0.0037  2.5523\n",
      "     16                     \u001b[36m0.6877\u001b[0m        \u001b[32m0.5958\u001b[0m                     0.6388        0.6379  0.0031  2.5522\n",
      "     17                     0.6856        \u001b[32m0.5919\u001b[0m                     0.6653        0.6163  0.0025  2.5521\n",
      "     18                     0.6866        \u001b[32m0.5915\u001b[0m                     0.6514        0.6288  0.0020  2.5514\n",
      "     19                     \u001b[36m0.6899\u001b[0m        \u001b[32m0.5854\u001b[0m                     0.6677        0.6172  0.0015  2.5532\n",
      "     20                     0.6899        \u001b[32m0.5851\u001b[0m                     \u001b[35m0.6714\u001b[0m        \u001b[31m0.6130\u001b[0m  0.0010  2.5518\n",
      "     21                     \u001b[36m0.6946\u001b[0m        \u001b[32m0.5828\u001b[0m                     0.6688        \u001b[31m0.6100\u001b[0m  0.0007  2.5522\n",
      "     22                     \u001b[36m0.6974\u001b[0m        \u001b[32m0.5806\u001b[0m                     0.6688        0.6103  0.0004  2.5522\n",
      "     23                     0.6958        0.5810                     0.6714        \u001b[31m0.6080\u001b[0m  0.0002  2.5537\n",
      "     24                     0.6973        \u001b[32m0.5790\u001b[0m                     0.6700        0.6080  0.0000  2.5536\n",
      "     25                     0.6951        0.5803                     0.6700        0.6083  0.0000  2.5532\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5758\u001b[0m        \u001b[32m0.7004\u001b[0m                     \u001b[35m0.6260\u001b[0m        \u001b[31m0.6475\u001b[0m  0.0100  2.5504\n",
      "      2                     \u001b[36m0.6418\u001b[0m        \u001b[32m0.6371\u001b[0m                     \u001b[35m0.6537\u001b[0m        \u001b[31m0.6420\u001b[0m  0.0100  2.5526\n",
      "      3                     \u001b[36m0.6549\u001b[0m        \u001b[32m0.6246\u001b[0m                     \u001b[35m0.6696\u001b[0m        \u001b[31m0.6247\u001b[0m  0.0098  2.5539\n",
      "      4                     \u001b[36m0.6616\u001b[0m        \u001b[32m0.6175\u001b[0m                     0.6494        0.6413  0.0096  2.5542\n",
      "      5                     \u001b[36m0.6616\u001b[0m        0.6196                     0.6485        0.6342  0.0093  2.5536\n",
      "      6                     \u001b[36m0.6659\u001b[0m        \u001b[32m0.6126\u001b[0m                     \u001b[35m0.6710\u001b[0m        \u001b[31m0.6218\u001b[0m  0.0090  2.5531\n",
      "      7                     \u001b[36m0.6664\u001b[0m        \u001b[32m0.6121\u001b[0m                     \u001b[35m0.6742\u001b[0m        \u001b[31m0.6204\u001b[0m  0.0085  2.5546\n",
      "      8                     \u001b[36m0.6668\u001b[0m        \u001b[32m0.6102\u001b[0m                     0.6406        0.6398  0.0080  2.5532\n",
      "      9                     \u001b[36m0.6713\u001b[0m        \u001b[32m0.6066\u001b[0m                     0.6688        \u001b[31m0.6181\u001b[0m  0.0075  2.5545\n",
      "     10                     \u001b[36m0.6744\u001b[0m        \u001b[32m0.6035\u001b[0m                     0.6431        0.6442  0.0069  2.5539\n",
      "     11                     0.6720        0.6040                     0.6656        0.6222  0.0063  2.5537\n",
      "     12                     \u001b[36m0.6776\u001b[0m        \u001b[32m0.6007\u001b[0m                     0.6500        0.6284  0.0057  2.5540\n",
      "     13                     0.6771        0.6023                     0.6666        0.6356  0.0050  2.5552\n",
      "     14                     \u001b[36m0.6792\u001b[0m        \u001b[32m0.5986\u001b[0m                     0.6453        0.6377  0.0043  2.5558\n",
      "     15                     \u001b[36m0.6845\u001b[0m        \u001b[32m0.5954\u001b[0m                     0.6607        0.6259  0.0037  2.5549\n",
      "     16                     \u001b[36m0.6869\u001b[0m        \u001b[32m0.5937\u001b[0m                     0.6659        0.6216  0.0031  2.5552\n",
      "     17                     0.6840        \u001b[32m0.5934\u001b[0m                     0.6701        0.6185  0.0025  2.5550\n",
      "     18                     \u001b[36m0.6951\u001b[0m        \u001b[32m0.5884\u001b[0m                     0.6740        \u001b[31m0.6143\u001b[0m  0.0020  2.5536\n",
      "     19                     0.6869        \u001b[32m0.5867\u001b[0m                     \u001b[35m0.6768\u001b[0m        0.6197  0.0015  2.5546\n",
      "     20                     0.6900        0.5882                     0.6752        0.6168  0.0010  2.5539\n",
      "     21                     0.6913        \u001b[32m0.5824\u001b[0m                     0.6689        0.6212  0.0007  2.5543\n",
      "     22                     0.6905        \u001b[32m0.5821\u001b[0m                     0.6671        0.6164  0.0004  2.5566\n",
      "     23                     0.6892        \u001b[32m0.5818\u001b[0m                     0.6726        0.6162  0.0002  2.5553\n",
      "     24                     \u001b[36m0.6962\u001b[0m        \u001b[32m0.5783\u001b[0m                     0.6703        0.6161  0.0000  2.5545\n",
      "     25                     0.6961        0.5785                     0.6699        0.6163  0.0000  2.5553\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6005\u001b[0m        \u001b[32m0.6814\u001b[0m                     \u001b[35m0.6448\u001b[0m        \u001b[31m0.6393\u001b[0m  0.0100  2.5487\n",
      "      2                     \u001b[36m0.6442\u001b[0m        \u001b[32m0.6342\u001b[0m                     \u001b[35m0.6553\u001b[0m        \u001b[31m0.6280\u001b[0m  0.0100  2.5515\n",
      "      3                     \u001b[36m0.6563\u001b[0m        \u001b[32m0.6237\u001b[0m                     \u001b[35m0.6728\u001b[0m        \u001b[31m0.6216\u001b[0m  0.0098  2.5505\n",
      "      4                     \u001b[36m0.6568\u001b[0m        \u001b[32m0.6232\u001b[0m                     \u001b[35m0.6741\u001b[0m        \u001b[31m0.6073\u001b[0m  0.0096  2.5506\n",
      "      5                     0.6555        \u001b[32m0.6177\u001b[0m                     0.6630        \u001b[31m0.6065\u001b[0m  0.0093  2.5516\n",
      "      6                     \u001b[36m0.6630\u001b[0m        \u001b[32m0.6143\u001b[0m                     0.6733        0.6185  0.0090  2.5508\n",
      "      7                     \u001b[36m0.6643\u001b[0m        0.6152                     0.6593        0.6196  0.0085  2.5522\n",
      "      8                     \u001b[36m0.6654\u001b[0m        \u001b[32m0.6118\u001b[0m                     0.6675        0.6149  0.0080  2.5522\n",
      "      9                     \u001b[36m0.6718\u001b[0m        \u001b[32m0.6107\u001b[0m                     0.6497        0.6266  0.0075  2.5530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     0.6698        \u001b[32m0.6106\u001b[0m                     0.6629        0.6216  0.0069  2.5523\n",
      "     11                     0.6701        \u001b[32m0.6057\u001b[0m                     0.6655        0.6164  0.0063  2.5522\n",
      "     12                     \u001b[36m0.6784\u001b[0m        \u001b[32m0.6026\u001b[0m                     0.6459        0.6351  0.0057  2.5521\n",
      "     13                     \u001b[36m0.6814\u001b[0m        \u001b[32m0.5968\u001b[0m                     0.6145        0.6518  0.0050  2.5509\n",
      "     14                     0.6781        0.5993                     \u001b[35m0.6773\u001b[0m        0.6080  0.0043  2.5513\n",
      "     15                     0.6787        0.5987                     0.6577        0.6143  0.0037  2.5529\n",
      "     16                     \u001b[36m0.6877\u001b[0m        \u001b[32m0.5906\u001b[0m                     0.6101        0.6663  0.0031  2.5521\n",
      "     17                     \u001b[36m0.6966\u001b[0m        \u001b[32m0.5862\u001b[0m                     0.6390        0.6405  0.0025  2.5527\n",
      "     18                     0.6901        \u001b[32m0.5856\u001b[0m                     0.6770        0.6094  0.0020  2.5527\n",
      "     19                     0.6894        \u001b[32m0.5834\u001b[0m                     0.6320        0.6510  0.0015  2.5522\n",
      "     20                     0.6951        \u001b[32m0.5806\u001b[0m                     0.6722        \u001b[31m0.6045\u001b[0m  0.0010  2.5526\n",
      "     21                     \u001b[36m0.6994\u001b[0m        \u001b[32m0.5779\u001b[0m                     \u001b[35m0.6816\u001b[0m        \u001b[31m0.6023\u001b[0m  0.0007  2.5543\n",
      "     22                     0.6947        \u001b[32m0.5778\u001b[0m                     0.6787        0.6028  0.0004  2.5518\n",
      "     23                     \u001b[36m0.7001\u001b[0m        \u001b[32m0.5767\u001b[0m                     \u001b[35m0.6823\u001b[0m        \u001b[31m0.5986\u001b[0m  0.0002  2.5532\n",
      "     24                     \u001b[36m0.7004\u001b[0m        \u001b[32m0.5748\u001b[0m                     0.6822        \u001b[31m0.5978\u001b[0m  0.0000  2.5533\n",
      "     25                     0.6969        0.5758                     0.6794        \u001b[31m0.5975\u001b[0m  0.0000  2.5534\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5885\u001b[0m        \u001b[32m0.6847\u001b[0m                     \u001b[35m0.6398\u001b[0m        \u001b[31m0.6380\u001b[0m  0.0100  2.5487\n",
      "      2                     \u001b[36m0.6416\u001b[0m        \u001b[32m0.6364\u001b[0m                     \u001b[35m0.6812\u001b[0m        \u001b[31m0.6110\u001b[0m  0.0100  2.5518\n",
      "      3                     \u001b[36m0.6600\u001b[0m        \u001b[32m0.6220\u001b[0m                     0.6720        0.6126  0.0098  2.5521\n",
      "      4                     \u001b[36m0.6612\u001b[0m        \u001b[32m0.6215\u001b[0m                     0.6592        0.6221  0.0096  2.5522\n",
      "      5                     \u001b[36m0.6639\u001b[0m        \u001b[32m0.6157\u001b[0m                     0.6657        \u001b[31m0.6104\u001b[0m  0.0093  2.5515\n",
      "      6                     0.6634        0.6172                     0.6753        \u001b[31m0.6094\u001b[0m  0.0090  2.5523\n",
      "      7                     0.6636        0.6166                     0.6497        0.6444  0.0085  2.5506\n",
      "      8                     0.6634        \u001b[32m0.6146\u001b[0m                     0.6742        0.6118  0.0080  2.5536\n",
      "      9                     \u001b[36m0.6680\u001b[0m        \u001b[32m0.6100\u001b[0m                     0.6805        0.6141  0.0075  2.5514\n",
      "     10                     0.6674        0.6109                     0.6772        0.6108  0.0069  2.5543\n",
      "     11                     \u001b[36m0.6762\u001b[0m        \u001b[32m0.6047\u001b[0m                     0.6802        \u001b[31m0.6064\u001b[0m  0.0063  2.5519\n",
      "     12                     \u001b[36m0.6817\u001b[0m        \u001b[32m0.6009\u001b[0m                     0.6595        0.6222  0.0057  2.5517\n",
      "     13                     0.6778        \u001b[32m0.6008\u001b[0m                     0.6548        0.6257  0.0050  2.5518\n",
      "     14                     \u001b[36m0.6886\u001b[0m        \u001b[32m0.5947\u001b[0m                     0.6395        0.6401  0.0043  2.5531\n",
      "     15                     0.6855        \u001b[32m0.5944\u001b[0m                     0.6586        0.6274  0.0037  2.5527\n",
      "     16                     0.6833        \u001b[32m0.5940\u001b[0m                     0.6393        0.6399  0.0031  2.5520\n",
      "     17                     0.6859        \u001b[32m0.5910\u001b[0m                     0.6675        0.6213  0.0025  2.5530\n",
      "     18                     0.6870        \u001b[32m0.5882\u001b[0m                     0.6575        0.6199  0.0020  2.5535\n",
      "     19                     \u001b[36m0.6893\u001b[0m        \u001b[32m0.5851\u001b[0m                     0.6777        \u001b[31m0.5944\u001b[0m  0.0015  2.5528\n",
      "     20                     \u001b[36m0.6941\u001b[0m        \u001b[32m0.5820\u001b[0m                     \u001b[35m0.6917\u001b[0m        \u001b[31m0.5910\u001b[0m  0.0010  2.5527\n",
      "     21                     \u001b[36m0.6988\u001b[0m        \u001b[32m0.5806\u001b[0m                     0.6889        0.5972  0.0007  2.5522\n",
      "     22                     0.6969        \u001b[32m0.5758\u001b[0m                     0.6904        \u001b[31m0.5894\u001b[0m  0.0004  2.5522\n",
      "     23                     \u001b[36m0.6988\u001b[0m        0.5765                     0.6885        \u001b[31m0.5892\u001b[0m  0.0002  2.5527\n",
      "     24                     0.6985        0.5785                     0.6901        \u001b[31m0.5887\u001b[0m  0.0000  2.5529\n",
      "     25                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.5749\u001b[0m                     0.6874        \u001b[31m0.5887\u001b[0m  0.0000  2.5532\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5864\u001b[0m        \u001b[32m0.6844\u001b[0m                     \u001b[35m0.6171\u001b[0m        \u001b[31m0.6488\u001b[0m  0.0100  2.5496\n",
      "      2                     \u001b[36m0.6393\u001b[0m        \u001b[32m0.6400\u001b[0m                     \u001b[35m0.6494\u001b[0m        \u001b[31m0.6271\u001b[0m  0.0100  2.5525\n",
      "      3                     \u001b[36m0.6537\u001b[0m        \u001b[32m0.6296\u001b[0m                     0.6404        \u001b[31m0.6234\u001b[0m  0.0098  2.5525\n",
      "      4                     \u001b[36m0.6586\u001b[0m        \u001b[32m0.6232\u001b[0m                     0.6486        \u001b[31m0.6209\u001b[0m  0.0096  2.5538\n",
      "      5                     \u001b[36m0.6614\u001b[0m        \u001b[32m0.6207\u001b[0m                     0.6322        0.6307  0.0093  2.5534\n",
      "      6                     \u001b[36m0.6691\u001b[0m        \u001b[32m0.6153\u001b[0m                     0.5820        0.6740  0.0090  2.5533\n",
      "      7                     \u001b[36m0.6701\u001b[0m        \u001b[32m0.6109\u001b[0m                     0.5953        0.6617  0.0085  2.5539\n",
      "      8                     0.6682        0.6130                     0.6028        0.6582  0.0080  2.5532\n",
      "      9                     \u001b[36m0.6749\u001b[0m        \u001b[32m0.6088\u001b[0m                     0.6073        0.6592  0.0075  2.5531\n",
      "     10                     0.6651        \u001b[32m0.6081\u001b[0m                     \u001b[35m0.6619\u001b[0m        \u001b[31m0.6201\u001b[0m  0.0069  2.5539\n",
      "     11                     \u001b[36m0.6763\u001b[0m        \u001b[32m0.6051\u001b[0m                     0.6520        0.6296  0.0063  2.5532\n",
      "     12                     \u001b[36m0.6772\u001b[0m        \u001b[32m0.6036\u001b[0m                     0.5583        0.7628  0.0057  2.5556\n",
      "     13                     \u001b[36m0.6830\u001b[0m        \u001b[32m0.5987\u001b[0m                     0.6039        0.6538  0.0050  2.5540\n",
      "     14                     0.6788        0.6001                     0.6180        0.6450  0.0043  2.5532\n",
      "     15                     \u001b[36m0.6857\u001b[0m        \u001b[32m0.5946\u001b[0m                     0.5379        0.8270  0.0037  2.5536\n",
      "     16                     \u001b[36m0.6961\u001b[0m        \u001b[32m0.5906\u001b[0m                     0.6283        0.6303  0.0031  2.5532\n",
      "     17                     0.6899        \u001b[32m0.5877\u001b[0m                     0.6445        0.6297  0.0025  2.5549\n",
      "     18                     0.6913        \u001b[32m0.5864\u001b[0m                     \u001b[35m0.6663\u001b[0m        \u001b[31m0.6101\u001b[0m  0.0020  2.5550\n",
      "     19                     0.6939        \u001b[32m0.5814\u001b[0m                     0.6496        0.6162  0.0015  2.5549\n",
      "     20                     \u001b[36m0.6970\u001b[0m        0.5822                     0.6591        0.6250  0.0010  2.5547\n",
      "     21                     \u001b[36m0.6988\u001b[0m        \u001b[32m0.5755\u001b[0m                     0.6628        0.6160  0.0007  2.5537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22                     \u001b[36m0.7042\u001b[0m        \u001b[32m0.5739\u001b[0m                     \u001b[35m0.6744\u001b[0m        \u001b[31m0.6088\u001b[0m  0.0004  2.5534\n",
      "     23                     0.7013        0.5753                     \u001b[35m0.6773\u001b[0m        \u001b[31m0.5997\u001b[0m  0.0002  2.5532\n",
      "     24                     \u001b[36m0.7088\u001b[0m        \u001b[32m0.5718\u001b[0m                     0.6765        0.5998  0.0000  2.5537\n",
      "     25                     0.7051        0.5728                     \u001b[35m0.6774\u001b[0m        0.6001  0.0000  2.5544\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5844\u001b[0m        \u001b[32m0.6907\u001b[0m                     \u001b[35m0.6471\u001b[0m        \u001b[31m0.6380\u001b[0m  0.0100  2.5477\n",
      "      2                     \u001b[36m0.6325\u001b[0m        \u001b[32m0.6396\u001b[0m                     \u001b[35m0.6632\u001b[0m        \u001b[31m0.6205\u001b[0m  0.0100  2.5520\n",
      "      3                     \u001b[36m0.6512\u001b[0m        \u001b[32m0.6287\u001b[0m                     \u001b[35m0.6658\u001b[0m        \u001b[31m0.6170\u001b[0m  0.0098  2.5528\n",
      "      4                     \u001b[36m0.6551\u001b[0m        \u001b[32m0.6218\u001b[0m                     0.6581        0.6233  0.0096  2.5519\n",
      "      5                     \u001b[36m0.6634\u001b[0m        \u001b[32m0.6193\u001b[0m                     \u001b[35m0.6674\u001b[0m        \u001b[31m0.6128\u001b[0m  0.0093  2.5521\n",
      "      6                     \u001b[36m0.6665\u001b[0m        \u001b[32m0.6156\u001b[0m                     \u001b[35m0.6678\u001b[0m        \u001b[31m0.6047\u001b[0m  0.0090  2.5521\n",
      "      7                     0.6624        0.6166                     0.6511        0.6303  0.0085  2.5540\n",
      "      8                     \u001b[36m0.6747\u001b[0m        \u001b[32m0.6090\u001b[0m                     0.6648        0.6150  0.0080  2.5511\n",
      "      9                     0.6696        0.6116                     \u001b[35m0.6779\u001b[0m        0.6081  0.0075  2.5526\n",
      "     10                     0.6737        \u001b[32m0.6086\u001b[0m                     0.6590        0.6134  0.0069  2.5517\n",
      "     11                     \u001b[36m0.6796\u001b[0m        \u001b[32m0.6047\u001b[0m                     0.6610        0.6079  0.0063  2.5523\n",
      "     12                     0.6755        \u001b[32m0.6046\u001b[0m                     0.6500        0.6201  0.0057  2.5519\n",
      "     13                     \u001b[36m0.6829\u001b[0m        \u001b[32m0.6001\u001b[0m                     0.6724        0.6065  0.0050  2.5556\n",
      "     14                     \u001b[36m0.6848\u001b[0m        \u001b[32m0.5967\u001b[0m                     0.6759        0.6099  0.0043  2.5534\n",
      "     15                     0.6829        0.5968                     0.6704        0.6125  0.0037  2.5541\n",
      "     16                     0.6848        \u001b[32m0.5937\u001b[0m                     0.6526        0.6080  0.0031  2.5531\n",
      "     17                     \u001b[36m0.6959\u001b[0m        \u001b[32m0.5877\u001b[0m                     0.6624        0.6159  0.0025  2.5518\n",
      "     18                     0.6923        0.5884                     0.6698        0.6135  0.0020  2.5521\n",
      "     19                     0.6894        \u001b[32m0.5870\u001b[0m                     0.6711        0.6122  0.0015  2.5532\n",
      "     20                     0.6949        \u001b[32m0.5823\u001b[0m                     0.6578        0.6211  0.0010  2.5532\n",
      "     21                     \u001b[36m0.6976\u001b[0m        \u001b[32m0.5790\u001b[0m                     0.6702        0.6063  0.0007  2.5525\n",
      "     22                     \u001b[36m0.6996\u001b[0m        0.5807                     \u001b[35m0.6780\u001b[0m        \u001b[31m0.5964\u001b[0m  0.0004  2.5533\n",
      "     23                     0.6973        0.5806                     0.6771        \u001b[31m0.5954\u001b[0m  0.0002  2.5533\n",
      "     24                     \u001b[36m0.7000\u001b[0m        \u001b[32m0.5769\u001b[0m                     0.6729        \u001b[31m0.5949\u001b[0m  0.0000  2.5539\n",
      "     25                     \u001b[36m0.7014\u001b[0m        0.5794                     0.6771        0.5951  0.0000  2.5527\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5853\u001b[0m        \u001b[32m0.6879\u001b[0m                     \u001b[35m0.6423\u001b[0m        \u001b[31m0.6307\u001b[0m  0.0100  2.5509\n",
      "      2                     \u001b[36m0.6406\u001b[0m        \u001b[32m0.6366\u001b[0m                     \u001b[35m0.6653\u001b[0m        0.6335  0.0100  2.5527\n",
      "      3                     \u001b[36m0.6536\u001b[0m        \u001b[32m0.6271\u001b[0m                     0.6597        \u001b[31m0.6213\u001b[0m  0.0098  2.5529\n",
      "      4                     \u001b[36m0.6564\u001b[0m        \u001b[32m0.6205\u001b[0m                     0.6448        0.6332  0.0096  2.5530\n",
      "      5                     \u001b[36m0.6625\u001b[0m        \u001b[32m0.6157\u001b[0m                     \u001b[35m0.6702\u001b[0m        \u001b[31m0.6155\u001b[0m  0.0093  2.5536\n",
      "      6                     \u001b[36m0.6637\u001b[0m        \u001b[32m0.6135\u001b[0m                     \u001b[35m0.6702\u001b[0m        0.6256  0.0090  2.5528\n",
      "      7                     \u001b[36m0.6648\u001b[0m        \u001b[32m0.6121\u001b[0m                     0.6503        0.6237  0.0085  2.5530\n",
      "      8                     \u001b[36m0.6718\u001b[0m        \u001b[32m0.6080\u001b[0m                     0.6535        0.6273  0.0080  2.5533\n",
      "      9                     0.6708        0.6096                     0.6523        0.6164  0.0075  2.5536\n",
      "     10                     \u001b[36m0.6758\u001b[0m        \u001b[32m0.6053\u001b[0m                     0.6521        0.6266  0.0069  2.5529\n",
      "     11                     \u001b[36m0.6787\u001b[0m        \u001b[32m0.6049\u001b[0m                     0.6612        0.6265  0.0063  2.5530\n",
      "     12                     0.6763        \u001b[32m0.6000\u001b[0m                     0.6200        0.6400  0.0057  2.5539\n",
      "     13                     0.6773        \u001b[32m0.5985\u001b[0m                     0.6676        0.6233  0.0050  2.5564\n",
      "     14                     \u001b[36m0.6801\u001b[0m        \u001b[32m0.5968\u001b[0m                     0.6602        \u001b[31m0.6153\u001b[0m  0.0043  2.5549\n",
      "     15                     \u001b[36m0.6874\u001b[0m        \u001b[32m0.5926\u001b[0m                     \u001b[35m0.6787\u001b[0m        0.6205  0.0037  2.5545\n",
      "     16                     0.6858        0.5929                     0.5924        0.6872  0.0031  2.5544\n",
      "     17                     0.6872        \u001b[32m0.5906\u001b[0m                     0.6459        0.6257  0.0025  2.5541\n",
      "     18                     \u001b[36m0.6900\u001b[0m        \u001b[32m0.5861\u001b[0m                     0.6768        \u001b[31m0.6109\u001b[0m  0.0020  2.5558\n",
      "     19                     \u001b[36m0.6935\u001b[0m        \u001b[32m0.5834\u001b[0m                     0.6646        \u001b[31m0.6102\u001b[0m  0.0015  2.5553\n",
      "     20                     \u001b[36m0.6975\u001b[0m        \u001b[32m0.5799\u001b[0m                     0.6676        0.6141  0.0010  2.5548\n",
      "     21                     0.6929        0.5808                     0.6695        \u001b[31m0.6070\u001b[0m  0.0007  2.5544\n",
      "     22                     0.6968        \u001b[32m0.5784\u001b[0m                     0.6730        0.6144  0.0004  2.5537\n",
      "     23                     0.6938        0.5800                     \u001b[35m0.6888\u001b[0m        \u001b[31m0.6009\u001b[0m  0.0002  2.5547\n",
      "     24                     0.6932        0.5803                     0.6878        \u001b[31m0.6001\u001b[0m  0.0000  2.5598\n",
      "     25                     0.6972        \u001b[32m0.5740\u001b[0m                     0.6863        \u001b[31m0.6001\u001b[0m  0.0000  2.5551\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5849\u001b[0m        \u001b[32m0.6848\u001b[0m                     \u001b[35m0.6453\u001b[0m        \u001b[31m0.6335\u001b[0m  0.0100  2.5492\n",
      "      2                     \u001b[36m0.6391\u001b[0m        \u001b[32m0.6365\u001b[0m                     \u001b[35m0.6573\u001b[0m        \u001b[31m0.6205\u001b[0m  0.0100  2.5519\n",
      "      3                     \u001b[36m0.6496\u001b[0m        \u001b[32m0.6316\u001b[0m                     0.6499        \u001b[31m0.6189\u001b[0m  0.0098  2.5531\n",
      "      4                     \u001b[36m0.6535\u001b[0m        \u001b[32m0.6248\u001b[0m                     0.6537        \u001b[31m0.6127\u001b[0m  0.0096  2.5511\n",
      "      5                     \u001b[36m0.6575\u001b[0m        \u001b[32m0.6235\u001b[0m                     0.6440        0.6282  0.0093  2.5507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6                     \u001b[36m0.6600\u001b[0m        \u001b[32m0.6208\u001b[0m                     0.6075        0.6681  0.0090  2.5509\n",
      "      7                     \u001b[36m0.6616\u001b[0m        \u001b[32m0.6187\u001b[0m                     \u001b[35m0.6724\u001b[0m        0.6145  0.0085  2.5523\n",
      "      8                     0.6615        0.6188                     0.6627        0.6198  0.0080  2.5516\n",
      "      9                     \u001b[36m0.6652\u001b[0m        \u001b[32m0.6165\u001b[0m                     0.6724        \u001b[31m0.6097\u001b[0m  0.0075  2.5515\n",
      "     10                     0.6604        \u001b[32m0.6135\u001b[0m                     0.5370        0.6937  0.0069  2.5503\n",
      "     11                     \u001b[36m0.6675\u001b[0m        \u001b[32m0.6099\u001b[0m                     \u001b[35m0.6768\u001b[0m        \u001b[31m0.6044\u001b[0m  0.0063  2.5534\n",
      "     12                     \u001b[36m0.6705\u001b[0m        \u001b[32m0.6069\u001b[0m                     0.6565        0.6069  0.0057  2.5531\n",
      "     13                     \u001b[36m0.6745\u001b[0m        \u001b[32m0.6044\u001b[0m                     0.6488        0.6275  0.0050  2.5524\n",
      "     14                     0.6737        0.6054                     0.6368        0.6379  0.0043  2.5518\n",
      "     15                     \u001b[36m0.6821\u001b[0m        \u001b[32m0.6033\u001b[0m                     0.6751        0.6095  0.0037  2.5522\n",
      "     16                     \u001b[36m0.6833\u001b[0m        \u001b[32m0.5943\u001b[0m                     0.6407        0.6319  0.0031  2.5534\n",
      "     17                     0.6797        0.5979                     0.6453        0.6229  0.0025  2.5522\n",
      "     18                     \u001b[36m0.6871\u001b[0m        0.5955                     0.6538        0.6222  0.0020  2.5551\n",
      "     19                     0.6851        \u001b[32m0.5912\u001b[0m                     0.6664        \u001b[31m0.6030\u001b[0m  0.0015  2.5528\n",
      "     20                     \u001b[36m0.6915\u001b[0m        \u001b[32m0.5883\u001b[0m                     0.6763        \u001b[31m0.6003\u001b[0m  0.0010  2.5532\n",
      "     21                     \u001b[36m0.6916\u001b[0m        \u001b[32m0.5879\u001b[0m                     0.6723        0.6004  0.0007  2.5514\n",
      "     22                     \u001b[36m0.6927\u001b[0m        \u001b[32m0.5850\u001b[0m                     \u001b[35m0.6784\u001b[0m        \u001b[31m0.5944\u001b[0m  0.0004  2.5533\n",
      "     23                     \u001b[36m0.6941\u001b[0m        \u001b[32m0.5816\u001b[0m                     \u001b[35m0.6796\u001b[0m        \u001b[31m0.5931\u001b[0m  0.0002  2.5538\n",
      "     24                     0.6894        0.5840                     0.6753        \u001b[31m0.5930\u001b[0m  0.0000  2.5539\n",
      "     25                     0.6917        0.5855                     \u001b[35m0.6801\u001b[0m        \u001b[31m0.5925\u001b[0m  0.0000  2.5528\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5813\u001b[0m        \u001b[32m0.6905\u001b[0m                     \u001b[35m0.6390\u001b[0m        \u001b[31m0.6382\u001b[0m  0.0100  2.5501\n",
      "      2                     \u001b[36m0.6451\u001b[0m        \u001b[32m0.6342\u001b[0m                     \u001b[35m0.6394\u001b[0m        0.6464  0.0100  2.5542\n",
      "      3                     \u001b[36m0.6504\u001b[0m        \u001b[32m0.6247\u001b[0m                     \u001b[35m0.6437\u001b[0m        0.6494  0.0098  2.5532\n",
      "      4                     \u001b[36m0.6616\u001b[0m        \u001b[32m0.6199\u001b[0m                     \u001b[35m0.6490\u001b[0m        \u001b[31m0.6313\u001b[0m  0.0096  2.5535\n",
      "      5                     0.6608        \u001b[32m0.6147\u001b[0m                     0.6202        0.6552  0.0093  2.5538\n",
      "      6                     0.6580        0.6193                     \u001b[35m0.6544\u001b[0m        \u001b[31m0.6204\u001b[0m  0.0090  2.5536\n",
      "      7                     \u001b[36m0.6652\u001b[0m        0.6149                     \u001b[35m0.6562\u001b[0m        0.6286  0.0085  2.5532\n",
      "      8                     \u001b[36m0.6729\u001b[0m        \u001b[32m0.6120\u001b[0m                     0.6500        0.6300  0.0080  2.5530\n",
      "      9                     \u001b[36m0.6737\u001b[0m        \u001b[32m0.6097\u001b[0m                     0.6223        0.6510  0.0075  2.5542\n",
      "     10                     0.6703        \u001b[32m0.6094\u001b[0m                     0.6415        0.6323  0.0069  2.5590\n",
      "     11                     \u001b[36m0.6744\u001b[0m        \u001b[32m0.6064\u001b[0m                     0.6334        0.6355  0.0063  2.5553\n",
      "     12                     \u001b[36m0.6807\u001b[0m        \u001b[32m0.6028\u001b[0m                     \u001b[35m0.6580\u001b[0m        0.6252  0.0057  2.5546\n",
      "     13                     0.6761        \u001b[32m0.5988\u001b[0m                     0.6553        0.6244  0.0050  2.5544\n",
      "     14                     \u001b[36m0.6840\u001b[0m        \u001b[32m0.5981\u001b[0m                     0.6240        0.6480  0.0043  2.5539\n",
      "     15                     0.6823        \u001b[32m0.5971\u001b[0m                     0.6423        0.6332  0.0037  2.5551\n",
      "     16                     0.6830        \u001b[32m0.5952\u001b[0m                     0.6515        0.6231  0.0031  2.5543\n",
      "     17                     \u001b[36m0.6918\u001b[0m        \u001b[32m0.5897\u001b[0m                     0.5979        0.6747  0.0025  2.5559\n",
      "     18                     0.6906        \u001b[32m0.5879\u001b[0m                     \u001b[35m0.6618\u001b[0m        \u001b[31m0.6149\u001b[0m  0.0020  2.5546\n",
      "     19                     0.6876        \u001b[32m0.5859\u001b[0m                     0.6610        \u001b[31m0.6148\u001b[0m  0.0015  2.5544\n",
      "     20                     0.6864        0.5879                     0.6607        0.6149  0.0010  2.5558\n",
      "     21                     \u001b[36m0.6947\u001b[0m        \u001b[32m0.5828\u001b[0m                     0.6600        \u001b[31m0.6118\u001b[0m  0.0007  2.5549\n",
      "     22                     \u001b[36m0.6966\u001b[0m        \u001b[32m0.5793\u001b[0m                     0.6563        \u001b[31m0.6082\u001b[0m  0.0004  2.5592\n",
      "     23                     0.6965        \u001b[32m0.5773\u001b[0m                     0.6584        \u001b[31m0.6072\u001b[0m  0.0002  2.5548\n",
      "     24                     0.6958        0.5789                     0.6593        0.6072  0.0000  2.5557\n",
      "     25                     \u001b[36m0.6978\u001b[0m        \u001b[32m0.5748\u001b[0m                     0.6598        0.6072  0.0000  2.5581\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5855\u001b[0m        \u001b[32m0.6894\u001b[0m                     \u001b[35m0.6420\u001b[0m        \u001b[31m0.6375\u001b[0m  0.0100  2.5501\n",
      "      2                     \u001b[36m0.6404\u001b[0m        \u001b[32m0.6380\u001b[0m                     \u001b[35m0.6509\u001b[0m        \u001b[31m0.6252\u001b[0m  0.0100  2.5527\n",
      "      3                     \u001b[36m0.6506\u001b[0m        \u001b[32m0.6269\u001b[0m                     \u001b[35m0.6525\u001b[0m        0.6286  0.0098  2.5525\n",
      "      4                     \u001b[36m0.6559\u001b[0m        \u001b[32m0.6207\u001b[0m                     \u001b[35m0.6667\u001b[0m        \u001b[31m0.6184\u001b[0m  0.0096  2.5531\n",
      "      5                     0.6558        \u001b[32m0.6187\u001b[0m                     0.6631        0.6194  0.0093  2.5536\n",
      "      6                     \u001b[36m0.6611\u001b[0m        \u001b[32m0.6158\u001b[0m                     0.6660        \u001b[31m0.6119\u001b[0m  0.0090  2.5531\n",
      "      7                     \u001b[36m0.6670\u001b[0m        \u001b[32m0.6135\u001b[0m                     0.6640        0.6196  0.0085  2.5531\n",
      "      8                     0.6602        0.6146                     \u001b[35m0.6829\u001b[0m        0.6122  0.0080  2.5543\n",
      "      9                     0.6656        \u001b[32m0.6071\u001b[0m                     0.6680        0.6159  0.0075  2.5532\n",
      "     10                     \u001b[36m0.6687\u001b[0m        0.6093                     0.6380        0.6395  0.0069  2.5525\n",
      "     11                     \u001b[36m0.6766\u001b[0m        \u001b[32m0.6035\u001b[0m                     0.6632        0.6180  0.0063  2.5532\n",
      "     12                     \u001b[36m0.6787\u001b[0m        \u001b[32m0.6020\u001b[0m                     0.6612        0.6186  0.0057  2.5522\n",
      "     13                     \u001b[36m0.6815\u001b[0m        \u001b[32m0.5976\u001b[0m                     0.6692        0.6170  0.0050  2.5526\n",
      "     14                     0.6777        0.6026                     \u001b[35m0.6863\u001b[0m        \u001b[31m0.6073\u001b[0m  0.0043  2.5519\n",
      "     15                     \u001b[36m0.6817\u001b[0m        \u001b[32m0.5939\u001b[0m                     0.6838        \u001b[31m0.6066\u001b[0m  0.0037  2.5544\n",
      "     16                     \u001b[36m0.6838\u001b[0m        \u001b[32m0.5907\u001b[0m                     0.6392        0.6379  0.0031  2.5532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17                     \u001b[36m0.6894\u001b[0m        \u001b[32m0.5892\u001b[0m                     0.6666        0.6111  0.0025  2.5541\n",
      "     18                     0.6875        0.5900                     0.6795        0.6069  0.0020  2.5540\n",
      "     19                     \u001b[36m0.6911\u001b[0m        \u001b[32m0.5851\u001b[0m                     0.6765        \u001b[31m0.6058\u001b[0m  0.0015  2.5533\n",
      "     20                     \u001b[36m0.6984\u001b[0m        \u001b[32m0.5820\u001b[0m                     0.6768        \u001b[31m0.6031\u001b[0m  0.0010  2.5551\n",
      "     21                     0.6968        \u001b[32m0.5800\u001b[0m                     0.6781        0.6075  0.0007  2.5542\n",
      "     22                     0.6920        0.5812                     0.6790        0.6043  0.0004  2.5542\n",
      "     23                     \u001b[36m0.7025\u001b[0m        \u001b[32m0.5796\u001b[0m                     0.6842        \u001b[31m0.5995\u001b[0m  0.0002  2.5555\n",
      "     24                     0.6973        \u001b[32m0.5793\u001b[0m                     0.6821        \u001b[31m0.5993\u001b[0m  0.0000  2.5542\n",
      "     25                     0.6975        \u001b[32m0.5778\u001b[0m                     0.6821        \u001b[31m0.5991\u001b[0m  0.0000  2.5544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 0 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5486\u001b[0m        \u001b[32m0.6973\u001b[0m                     \u001b[35m0.5816\u001b[0m        \u001b[31m0.6784\u001b[0m  0.0100  9.2879\n",
      "      2                     \u001b[36m0.5775\u001b[0m        \u001b[32m0.6788\u001b[0m                     0.5379        0.6856  0.0100  9.1734\n",
      "      3                     0.5707        \u001b[32m0.6776\u001b[0m                     \u001b[35m0.5929\u001b[0m        \u001b[31m0.6718\u001b[0m  0.0098  9.1712\n",
      "      4                     \u001b[36m0.5829\u001b[0m        \u001b[32m0.6752\u001b[0m                     0.5614        0.6834  0.0096  9.1707\n",
      "      5                     \u001b[36m0.5840\u001b[0m        \u001b[32m0.6746\u001b[0m                     0.5883        0.6735  0.0093  9.1707\n",
      "      6                     \u001b[36m0.5865\u001b[0m        \u001b[32m0.6741\u001b[0m                     0.5778        0.6762  0.0090  9.1761\n",
      "      7                     0.5823        \u001b[32m0.6730\u001b[0m                     0.5102        0.7551  0.0085  9.1738\n",
      "      8                     0.5808        0.6736                     0.5897        0.6723  0.0080  9.1747\n",
      "      9                     \u001b[36m0.5884\u001b[0m        \u001b[32m0.6727\u001b[0m                     0.5838        0.6735  0.0075  9.2098\n",
      "     10                     \u001b[36m0.5886\u001b[0m        \u001b[32m0.6716\u001b[0m                     0.5609        0.6807  0.0069  9.1723\n",
      "     11                     \u001b[36m0.5941\u001b[0m        \u001b[32m0.6696\u001b[0m                     0.5012        0.7327  0.0063  9.1724\n",
      "     12                     \u001b[36m0.5942\u001b[0m        \u001b[32m0.6695\u001b[0m                     0.5558        0.6834  0.0057  9.1716\n",
      "     13                     0.5917        \u001b[32m0.6680\u001b[0m                     0.4996        0.8102  0.0050  9.1711\n",
      "     14                     \u001b[36m0.6010\u001b[0m        \u001b[32m0.6655\u001b[0m                     0.5875        0.6759  0.0043  9.1735\n",
      "     15                     0.5986        0.6663                     0.5465        0.6895  0.0037  9.1751\n",
      "     16                     0.5964        \u001b[32m0.6642\u001b[0m                     0.5159        0.7783  0.0031  9.1730\n",
      "     17                     \u001b[36m0.6036\u001b[0m        \u001b[32m0.6619\u001b[0m                     0.5455        0.6918  0.0025  9.1760\n",
      "     18                     \u001b[36m0.6055\u001b[0m        \u001b[32m0.6611\u001b[0m                     0.5011        0.8031  0.0020  9.1846\n",
      "     19                     \u001b[36m0.6119\u001b[0m        \u001b[32m0.6588\u001b[0m                     \u001b[35m0.6036\u001b[0m        \u001b[31m0.6689\u001b[0m  0.0015  9.1882\n",
      "     20                     \u001b[36m0.6126\u001b[0m        \u001b[32m0.6569\u001b[0m                     0.5864        0.6708  0.0010  9.1754\n",
      "     21                     0.6092        \u001b[32m0.6563\u001b[0m                     0.5504        0.7039  0.0007  9.1795\n",
      "     22                     \u001b[36m0.6167\u001b[0m        \u001b[32m0.6536\u001b[0m                     0.5824        0.6707  0.0004  9.1767\n",
      "     23                     0.6151        0.6542                     0.6020        \u001b[31m0.6678\u001b[0m  0.0002  9.1767\n",
      "     24                     0.6154        0.6546                     0.5997        \u001b[31m0.6673\u001b[0m  0.0000  9.1751\n",
      "     25                     0.6159        0.6541                     0.6011        \u001b[31m0.6673\u001b[0m  0.0000  9.2092\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5563\u001b[0m        \u001b[32m0.6939\u001b[0m                     \u001b[35m0.5745\u001b[0m        \u001b[31m0.6788\u001b[0m  0.0100  9.1649\n",
      "      2                     \u001b[36m0.5764\u001b[0m        \u001b[32m0.6772\u001b[0m                     \u001b[35m0.5876\u001b[0m        \u001b[31m0.6742\u001b[0m  0.0100  9.1698\n",
      "      3                     \u001b[36m0.5847\u001b[0m        \u001b[32m0.6750\u001b[0m                     0.5778        0.6774  0.0098  9.1707\n",
      "      4                     0.5821        \u001b[32m0.6747\u001b[0m                     0.5855        0.6782  0.0096  9.1709\n",
      "      5                     0.5811        \u001b[32m0.6740\u001b[0m                     0.5669        0.6833  0.0093  9.1679\n",
      "      6                     \u001b[36m0.5847\u001b[0m        \u001b[32m0.6727\u001b[0m                     0.5712        0.6783  0.0090  9.1696\n",
      "      7                     \u001b[36m0.5896\u001b[0m        \u001b[32m0.6724\u001b[0m                     0.5812        0.6765  0.0085  9.1724\n",
      "      8                     0.5857        0.6741                     0.5001        0.7863  0.0080  9.1705\n",
      "      9                     0.5860        \u001b[32m0.6720\u001b[0m                     0.5362        0.7033  0.0075  9.1736\n",
      "     10                     \u001b[36m0.5898\u001b[0m        \u001b[32m0.6710\u001b[0m                     0.5768        0.6799  0.0069  9.1722\n",
      "     11                     0.5877        \u001b[32m0.6700\u001b[0m                     0.5046        0.7051  0.0063  9.1716\n",
      "     12                     \u001b[36m0.5945\u001b[0m        \u001b[32m0.6685\u001b[0m                     0.5424        0.6892  0.0057  9.1711\n",
      "     13                     \u001b[36m0.5959\u001b[0m        \u001b[32m0.6675\u001b[0m                     0.5832        \u001b[31m0.6731\u001b[0m  0.0050  9.1720\n",
      "     14                     \u001b[36m0.5987\u001b[0m        \u001b[32m0.6658\u001b[0m                     0.5843        0.6764  0.0043  9.1713\n",
      "     15                     \u001b[36m0.6009\u001b[0m        \u001b[32m0.6645\u001b[0m                     0.5726        0.6802  0.0037  9.1725\n",
      "     16                     \u001b[36m0.6022\u001b[0m        \u001b[32m0.6643\u001b[0m                     0.5380        0.7019  0.0031  9.1764\n",
      "     17                     \u001b[36m0.6054\u001b[0m        \u001b[32m0.6617\u001b[0m                     0.5329        0.7372  0.0025  9.1755\n",
      "     18                     0.6044        0.6619                     \u001b[35m0.5952\u001b[0m        \u001b[31m0.6698\u001b[0m  0.0020  9.1731\n",
      "     19                     \u001b[36m0.6081\u001b[0m        \u001b[32m0.6594\u001b[0m                     0.5903        0.6726  0.0015  9.1755\n",
      "     20                     \u001b[36m0.6107\u001b[0m        \u001b[32m0.6591\u001b[0m                     0.5846        0.6737  0.0010  9.1777\n",
      "     21                     0.6102        \u001b[32m0.6569\u001b[0m                     0.5930        \u001b[31m0.6688\u001b[0m  0.0007  9.1739\n",
      "     22                     \u001b[36m0.6125\u001b[0m        \u001b[32m0.6560\u001b[0m                     0.5939        \u001b[31m0.6681\u001b[0m  0.0004  9.1798\n",
      "     23                     \u001b[36m0.6138\u001b[0m        \u001b[32m0.6549\u001b[0m                     \u001b[35m0.5969\u001b[0m        \u001b[31m0.6647\u001b[0m  0.0002  9.1739\n",
      "     24                     \u001b[36m0.6150\u001b[0m        \u001b[32m0.6546\u001b[0m                     0.5967        \u001b[31m0.6640\u001b[0m  0.0000  9.1769\n",
      "     25                     0.6147        0.6550                     \u001b[35m0.5973\u001b[0m        0.6642  0.0000  9.1729\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5570\u001b[0m        \u001b[32m0.6957\u001b[0m                     \u001b[35m0.5727\u001b[0m        \u001b[31m0.6760\u001b[0m  0.0100  9.1652\n",
      "      2                     \u001b[36m0.5773\u001b[0m        \u001b[32m0.6772\u001b[0m                     \u001b[35m0.5814\u001b[0m        \u001b[31m0.6735\u001b[0m  0.0100  9.1708\n",
      "      3                     0.5769        \u001b[32m0.6769\u001b[0m                     0.5611        0.6786  0.0098  9.1734\n",
      "      4                     \u001b[36m0.5809\u001b[0m        \u001b[32m0.6755\u001b[0m                     \u001b[35m0.5867\u001b[0m        \u001b[31m0.6723\u001b[0m  0.0096  9.1737\n",
      "      5                     0.5773        \u001b[32m0.6754\u001b[0m                     0.5737        0.6808  0.0093  9.1745\n",
      "      6                     \u001b[36m0.5827\u001b[0m        \u001b[32m0.6744\u001b[0m                     0.5618        0.6813  0.0090  9.1735\n",
      "      7                     0.5824        \u001b[32m0.6737\u001b[0m                     0.5436        0.6905  0.0085  9.1729\n",
      "      8                     \u001b[36m0.5834\u001b[0m        \u001b[32m0.6729\u001b[0m                     0.5806        0.6742  0.0080  9.1765\n",
      "      9                     \u001b[36m0.5848\u001b[0m        \u001b[32m0.6726\u001b[0m                     0.5012        0.6966  0.0075  9.1729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     \u001b[36m0.5854\u001b[0m        \u001b[32m0.6721\u001b[0m                     0.5332        0.7031  0.0069  9.1748\n",
      "     11                     0.5844        0.6721                     0.5413        0.6856  0.0063  9.1746\n",
      "     12                     \u001b[36m0.5879\u001b[0m        \u001b[32m0.6712\u001b[0m                     0.5800        0.6753  0.0057  9.1724\n",
      "     13                     \u001b[36m0.5958\u001b[0m        \u001b[32m0.6678\u001b[0m                     0.5025        0.7682  0.0050  9.1748\n",
      "     14                     \u001b[36m0.5976\u001b[0m        \u001b[32m0.6675\u001b[0m                     0.5733        0.6779  0.0043  9.1747\n",
      "     15                     0.5955        \u001b[32m0.6667\u001b[0m                     0.5838        0.6760  0.0037  9.1743\n",
      "     16                     \u001b[36m0.5995\u001b[0m        \u001b[32m0.6650\u001b[0m                     0.5297        0.6956  0.0031  9.1755\n",
      "     17                     \u001b[36m0.6004\u001b[0m        \u001b[32m0.6644\u001b[0m                     0.5840        \u001b[31m0.6722\u001b[0m  0.0025  9.1763\n",
      "     18                     \u001b[36m0.6037\u001b[0m        \u001b[32m0.6619\u001b[0m                     0.5717        0.6781  0.0020  9.1767\n",
      "     19                     \u001b[36m0.6067\u001b[0m        \u001b[32m0.6610\u001b[0m                     \u001b[35m0.5911\u001b[0m        0.6724  0.0015  9.1741\n",
      "     20                     0.6058        \u001b[32m0.6603\u001b[0m                     0.5893        \u001b[31m0.6701\u001b[0m  0.0010  9.1768\n",
      "     21                     \u001b[36m0.6100\u001b[0m        \u001b[32m0.6582\u001b[0m                     \u001b[35m0.6040\u001b[0m        \u001b[31m0.6656\u001b[0m  0.0007  9.1773\n",
      "     22                     \u001b[36m0.6108\u001b[0m        \u001b[32m0.6570\u001b[0m                     0.5951        \u001b[31m0.6649\u001b[0m  0.0004  9.1765\n",
      "     23                     \u001b[36m0.6128\u001b[0m        0.6571                     0.6008        \u001b[31m0.6630\u001b[0m  0.0002  9.1792\n",
      "     24                     0.6105        \u001b[32m0.6557\u001b[0m                     \u001b[35m0.6060\u001b[0m        \u001b[31m0.6606\u001b[0m  0.0000  9.1783\n",
      "     25                     \u001b[36m0.6130\u001b[0m        0.6561                     \u001b[35m0.6084\u001b[0m        \u001b[31m0.6606\u001b[0m  0.0000  9.1783\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5563\u001b[0m        \u001b[32m0.6934\u001b[0m                     \u001b[35m0.5784\u001b[0m        \u001b[31m0.6757\u001b[0m  0.0100  9.1626\n",
      "      2                     \u001b[36m0.5784\u001b[0m        \u001b[32m0.6773\u001b[0m                     0.5659        0.6840  0.0100  9.1700\n",
      "      3                     \u001b[36m0.5808\u001b[0m        \u001b[32m0.6757\u001b[0m                     \u001b[35m0.5821\u001b[0m        \u001b[31m0.6742\u001b[0m  0.0098  9.1720\n",
      "      4                     \u001b[36m0.5841\u001b[0m        \u001b[32m0.6738\u001b[0m                     0.5296        0.6889  0.0096  9.1737\n",
      "      5                     \u001b[36m0.5873\u001b[0m        \u001b[32m0.6722\u001b[0m                     \u001b[35m0.5834\u001b[0m        \u001b[31m0.6738\u001b[0m  0.0093  9.1710\n",
      "      6                     \u001b[36m0.5886\u001b[0m        0.6727                     \u001b[35m0.5860\u001b[0m        0.6758  0.0090  9.1703\n",
      "      7                     0.5851        \u001b[32m0.6722\u001b[0m                     0.5854        \u001b[31m0.6734\u001b[0m  0.0085  9.1739\n",
      "      8                     0.5876        0.6723                     \u001b[35m0.5862\u001b[0m        \u001b[31m0.6708\u001b[0m  0.0080  9.1727\n",
      "      9                     0.5873        \u001b[32m0.6716\u001b[0m                     0.5856        0.6731  0.0075  9.1725\n",
      "     10                     \u001b[36m0.5922\u001b[0m        \u001b[32m0.6706\u001b[0m                     0.5684        0.6832  0.0069  9.1733\n",
      "     11                     \u001b[36m0.5935\u001b[0m        \u001b[32m0.6693\u001b[0m                     0.5848        0.6732  0.0063  9.1738\n",
      "     12                     \u001b[36m0.5961\u001b[0m        \u001b[32m0.6679\u001b[0m                     0.5003        0.7641  0.0057  9.1748\n",
      "     13                     \u001b[36m0.5964\u001b[0m        \u001b[32m0.6665\u001b[0m                     \u001b[35m0.5923\u001b[0m        0.6714  0.0050  9.1755\n",
      "     14                     0.5937        0.6667                     0.5631        0.6811  0.0043  9.1737\n",
      "     15                     0.5954        \u001b[32m0.6655\u001b[0m                     0.5053        0.7200  0.0037  9.1770\n",
      "     16                     \u001b[36m0.5984\u001b[0m        \u001b[32m0.6646\u001b[0m                     \u001b[35m0.6041\u001b[0m        \u001b[31m0.6681\u001b[0m  0.0031  9.1761\n",
      "     17                     \u001b[36m0.6041\u001b[0m        \u001b[32m0.6619\u001b[0m                     0.5810        0.6749  0.0025  9.1785\n",
      "     18                     0.6034        \u001b[32m0.6616\u001b[0m                     0.5928        0.6721  0.0020  9.1750\n",
      "     19                     \u001b[36m0.6067\u001b[0m        \u001b[32m0.6596\u001b[0m                     0.5842        0.6781  0.0015  9.1760\n",
      "     20                     \u001b[36m0.6126\u001b[0m        \u001b[32m0.6582\u001b[0m                     0.5326        0.6955  0.0010  9.1772\n",
      "     21                     \u001b[36m0.6127\u001b[0m        \u001b[32m0.6563\u001b[0m                     0.5918        0.6712  0.0007  9.1762\n",
      "     22                     0.6110        \u001b[32m0.6560\u001b[0m                     0.5960        0.6700  0.0004  9.1796\n",
      "     23                     \u001b[36m0.6138\u001b[0m        \u001b[32m0.6546\u001b[0m                     0.5978        \u001b[31m0.6667\u001b[0m  0.0002  9.1780\n",
      "     24                     0.6138        0.6548                     0.5990        0.6670  0.0000  9.1779\n",
      "     25                     \u001b[36m0.6171\u001b[0m        \u001b[32m0.6540\u001b[0m                     0.5976        \u001b[31m0.6665\u001b[0m  0.0000  9.2092\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5606\u001b[0m        \u001b[32m0.6910\u001b[0m                     \u001b[35m0.5748\u001b[0m        \u001b[31m0.6770\u001b[0m  0.0100  9.1609\n",
      "      2                     \u001b[36m0.5750\u001b[0m        \u001b[32m0.6766\u001b[0m                     \u001b[35m0.5871\u001b[0m        \u001b[31m0.6766\u001b[0m  0.0100  9.1654\n",
      "      3                     \u001b[36m0.5782\u001b[0m        \u001b[32m0.6753\u001b[0m                     0.5702        0.6853  0.0098  9.1670\n",
      "      4                     \u001b[36m0.5804\u001b[0m        0.6754                     0.5631        0.6826  0.0096  9.1670\n",
      "      5                     0.5785        \u001b[32m0.6747\u001b[0m                     \u001b[35m0.5925\u001b[0m        \u001b[31m0.6756\u001b[0m  0.0093  9.1665\n",
      "      6                     \u001b[36m0.5859\u001b[0m        0.6751                     0.5633        0.6782  0.0090  9.1743\n",
      "      7                     0.5821        \u001b[32m0.6744\u001b[0m                     0.5874        \u001b[31m0.6729\u001b[0m  0.0085  9.1672\n",
      "      8                     \u001b[36m0.5859\u001b[0m        \u001b[32m0.6736\u001b[0m                     \u001b[35m0.6004\u001b[0m        \u001b[31m0.6684\u001b[0m  0.0080  9.1687\n",
      "      9                     0.5832        \u001b[32m0.6728\u001b[0m                     0.5571        0.6997  0.0075  9.1700\n",
      "     10                     0.5798        \u001b[32m0.6726\u001b[0m                     0.5915        0.6761  0.0069  9.1688\n",
      "     11                     \u001b[36m0.5917\u001b[0m        \u001b[32m0.6709\u001b[0m                     0.5704        0.6852  0.0063  9.1664\n",
      "     12                     0.5906        \u001b[32m0.6702\u001b[0m                     \u001b[35m0.6049\u001b[0m        \u001b[31m0.6655\u001b[0m  0.0057  9.1701\n",
      "     13                     0.5897        \u001b[32m0.6690\u001b[0m                     0.5785        0.6782  0.0050  9.1665\n",
      "     14                     \u001b[36m0.5959\u001b[0m        \u001b[32m0.6682\u001b[0m                     0.5329        0.6889  0.0043  9.1649\n",
      "     15                     \u001b[36m0.5963\u001b[0m        \u001b[32m0.6670\u001b[0m                     0.5355        0.6879  0.0037  9.1673\n",
      "     16                     \u001b[36m0.5977\u001b[0m        \u001b[32m0.6664\u001b[0m                     \u001b[35m0.6059\u001b[0m        0.6667  0.0031  9.1674\n",
      "     17                     \u001b[36m0.5995\u001b[0m        \u001b[32m0.6655\u001b[0m                     0.6019        0.6670  0.0025  9.1687\n",
      "     18                     \u001b[36m0.6026\u001b[0m        \u001b[32m0.6620\u001b[0m                     \u001b[35m0.6064\u001b[0m        \u001b[31m0.6648\u001b[0m  0.0020  9.1717\n",
      "     19                     0.6026        0.6622                     0.6049        \u001b[31m0.6640\u001b[0m  0.0015  9.1710\n",
      "     20                     \u001b[36m0.6089\u001b[0m        \u001b[32m0.6594\u001b[0m                     0.6058        0.6668  0.0010  9.1691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21                     0.6076        \u001b[32m0.6587\u001b[0m                     0.5735        0.6776  0.0007  9.1704\n",
      "     22                     0.6073        0.6587                     0.5966        0.6712  0.0004  9.1717\n",
      "     23                     \u001b[36m0.6127\u001b[0m        \u001b[32m0.6576\u001b[0m                     \u001b[35m0.6093\u001b[0m        \u001b[31m0.6609\u001b[0m  0.0002  9.1728\n",
      "     24                     \u001b[36m0.6128\u001b[0m        \u001b[32m0.6562\u001b[0m                     \u001b[35m0.6110\u001b[0m        \u001b[31m0.6609\u001b[0m  0.0000  9.1728\n",
      "     25                     \u001b[36m0.6135\u001b[0m        \u001b[32m0.6558\u001b[0m                     0.6097        \u001b[31m0.6608\u001b[0m  0.0000  9.1720\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5545\u001b[0m        \u001b[32m0.6968\u001b[0m                     \u001b[35m0.5774\u001b[0m        \u001b[31m0.6832\u001b[0m  0.0100  9.1641\n",
      "      2                     \u001b[36m0.5811\u001b[0m        \u001b[32m0.6761\u001b[0m                     \u001b[35m0.5846\u001b[0m        \u001b[31m0.6758\u001b[0m  0.0100  9.1716\n",
      "      3                     \u001b[36m0.5828\u001b[0m        \u001b[32m0.6754\u001b[0m                     0.5838        \u001b[31m0.6741\u001b[0m  0.0098  9.1703\n",
      "      4                     \u001b[36m0.5845\u001b[0m        \u001b[32m0.6749\u001b[0m                     0.5323        0.6988  0.0096  9.1716\n",
      "      5                     0.5825        \u001b[32m0.6730\u001b[0m                     0.5785        0.6796  0.0093  9.1722\n",
      "      6                     0.5833        \u001b[32m0.6724\u001b[0m                     0.5006        0.7318  0.0090  9.1738\n",
      "      7                     \u001b[36m0.5872\u001b[0m        \u001b[32m0.6721\u001b[0m                     0.5812        0.6785  0.0085  9.1731\n",
      "      8                     0.5822        0.6728                     0.5709        0.6805  0.0080  9.1746\n",
      "      9                     0.5861        \u001b[32m0.6718\u001b[0m                     0.5384        0.6896  0.0075  9.1702\n",
      "     10                     \u001b[36m0.5877\u001b[0m        \u001b[32m0.6714\u001b[0m                     0.5776        0.6761  0.0069  9.1706\n",
      "     11                     \u001b[36m0.5906\u001b[0m        \u001b[32m0.6707\u001b[0m                     0.5485        0.6815  0.0063  9.1743\n",
      "     12                     \u001b[36m0.5938\u001b[0m        \u001b[32m0.6690\u001b[0m                     \u001b[35m0.5892\u001b[0m        \u001b[31m0.6729\u001b[0m  0.0057  9.1731\n",
      "     13                     \u001b[36m0.5967\u001b[0m        \u001b[32m0.6685\u001b[0m                     0.5563        0.6857  0.0050  9.1738\n",
      "     14                     0.5899        \u001b[32m0.6683\u001b[0m                     0.5704        0.6812  0.0043  9.1744\n",
      "     15                     \u001b[36m0.5992\u001b[0m        \u001b[32m0.6667\u001b[0m                     0.5870        0.6800  0.0037  9.1737\n",
      "     16                     0.5968        \u001b[32m0.6649\u001b[0m                     0.5490        0.6974  0.0031  9.1735\n",
      "     17                     0.5986        \u001b[32m0.6644\u001b[0m                     0.5759        0.6782  0.0025  9.1763\n",
      "     18                     \u001b[36m0.6036\u001b[0m        \u001b[32m0.6614\u001b[0m                     0.5409        0.6976  0.0020  9.1736\n",
      "     19                     \u001b[36m0.6072\u001b[0m        \u001b[32m0.6600\u001b[0m                     0.5570        0.6888  0.0015  9.1774\n",
      "     20                     0.6065        \u001b[32m0.6590\u001b[0m                     0.5843        0.6770  0.0010  9.1764\n",
      "     21                     \u001b[36m0.6085\u001b[0m        \u001b[32m0.6577\u001b[0m                     0.5762        0.6768  0.0007  9.1782\n",
      "     22                     \u001b[36m0.6121\u001b[0m        0.6590                     0.5847        0.6738  0.0004  9.1769\n",
      "     23                     0.6121        \u001b[32m0.6563\u001b[0m                     \u001b[35m0.5913\u001b[0m        \u001b[31m0.6720\u001b[0m  0.0002  9.1769\n",
      "     24                     \u001b[36m0.6142\u001b[0m        \u001b[32m0.6562\u001b[0m                     \u001b[35m0.5928\u001b[0m        \u001b[31m0.6712\u001b[0m  0.0000  9.1793\n",
      "     25                     0.6139        \u001b[32m0.6552\u001b[0m                     0.5918        0.6713  0.0000  9.1781\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5572\u001b[0m        \u001b[32m0.6945\u001b[0m                     \u001b[35m0.5656\u001b[0m        \u001b[31m0.6800\u001b[0m  0.0100  9.1577\n",
      "      2                     \u001b[36m0.5682\u001b[0m        \u001b[32m0.6791\u001b[0m                     \u001b[35m0.5773\u001b[0m        \u001b[31m0.6785\u001b[0m  0.0100  9.1949\n",
      "      3                     \u001b[36m0.5802\u001b[0m        \u001b[32m0.6761\u001b[0m                     0.5500        0.6873  0.0098  9.1632\n",
      "      4                     0.5799        \u001b[32m0.6753\u001b[0m                     \u001b[35m0.5889\u001b[0m        \u001b[31m0.6705\u001b[0m  0.0096  9.1657\n",
      "      5                     0.5788        \u001b[32m0.6752\u001b[0m                     \u001b[35m0.5934\u001b[0m        0.6740  0.0093  9.1636\n",
      "      6                     \u001b[36m0.5863\u001b[0m        \u001b[32m0.6731\u001b[0m                     0.5422        0.6968  0.0090  9.1668\n",
      "      7                     \u001b[36m0.5877\u001b[0m        \u001b[32m0.6730\u001b[0m                     \u001b[35m0.5939\u001b[0m        0.6726  0.0085  9.1689\n",
      "      8                     0.5832        0.6732                     0.5669        0.6792  0.0080  9.1705\n",
      "      9                     \u001b[36m0.5886\u001b[0m        \u001b[32m0.6716\u001b[0m                     0.5145        0.6965  0.0075  9.1681\n",
      "     10                     \u001b[36m0.5904\u001b[0m        \u001b[32m0.6702\u001b[0m                     0.5600        0.6815  0.0069  9.1696\n",
      "     11                     0.5900        0.6711                     0.5848        0.6723  0.0063  9.1673\n",
      "     12                     \u001b[36m0.5914\u001b[0m        \u001b[32m0.6687\u001b[0m                     0.5040        0.8031  0.0057  9.1710\n",
      "     13                     \u001b[36m0.5934\u001b[0m        \u001b[32m0.6685\u001b[0m                     0.5922        \u001b[31m0.6692\u001b[0m  0.0050  9.1673\n",
      "     14                     \u001b[36m0.5944\u001b[0m        \u001b[32m0.6673\u001b[0m                     0.5127        0.7197  0.0043  9.1703\n",
      "     15                     \u001b[36m0.5946\u001b[0m        \u001b[32m0.6658\u001b[0m                     0.5773        0.6756  0.0037  9.1690\n",
      "     16                     \u001b[36m0.6013\u001b[0m        \u001b[32m0.6644\u001b[0m                     0.5330        0.6993  0.0031  9.1678\n",
      "     17                     0.5998        \u001b[32m0.6635\u001b[0m                     0.5421        0.6963  0.0025  9.1711\n",
      "     18                     \u001b[36m0.6055\u001b[0m        \u001b[32m0.6614\u001b[0m                     0.5676        0.6786  0.0020  9.1691\n",
      "     19                     \u001b[36m0.6072\u001b[0m        \u001b[32m0.6600\u001b[0m                     0.5161        0.7015  0.0015  9.1711\n",
      "     20                     \u001b[36m0.6084\u001b[0m        \u001b[32m0.6596\u001b[0m                     \u001b[35m0.5941\u001b[0m        0.6733  0.0010  9.1745\n",
      "     21                     \u001b[36m0.6091\u001b[0m        \u001b[32m0.6576\u001b[0m                     \u001b[35m0.6021\u001b[0m        \u001b[31m0.6663\u001b[0m  0.0007  9.1715\n",
      "     22                     \u001b[36m0.6125\u001b[0m        \u001b[32m0.6562\u001b[0m                     0.5965        0.6692  0.0004  9.1738\n",
      "     23                     0.6103        \u001b[32m0.6556\u001b[0m                     0.6014        \u001b[31m0.6659\u001b[0m  0.0002  9.1726\n",
      "     24                     0.6111        0.6563                     0.5981        \u001b[31m0.6648\u001b[0m  0.0000  9.1786\n",
      "     25                     0.6108        0.6568                     0.5971        0.6649  0.0000  9.1732\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5579\u001b[0m        \u001b[32m0.6961\u001b[0m                     \u001b[35m0.5701\u001b[0m        \u001b[31m0.6794\u001b[0m  0.0100  9.1579\n",
      "      2                     \u001b[36m0.5807\u001b[0m        \u001b[32m0.6764\u001b[0m                     \u001b[35m0.5824\u001b[0m        \u001b[31m0.6767\u001b[0m  0.0100  9.1668\n",
      "      3                     \u001b[36m0.5840\u001b[0m        \u001b[32m0.6744\u001b[0m                     0.5736        0.6785  0.0098  9.1654\n",
      "      4                     0.5808        \u001b[32m0.6743\u001b[0m                     0.5517        0.6866  0.0096  9.1655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5                     0.5794        \u001b[32m0.6735\u001b[0m                     0.5255        0.6929  0.0093  9.1667\n",
      "      6                     0.5797        0.6736                     \u001b[35m0.5852\u001b[0m        \u001b[31m0.6755\u001b[0m  0.0090  9.1662\n",
      "      7                     \u001b[36m0.5848\u001b[0m        \u001b[32m0.6720\u001b[0m                     0.5633        0.6817  0.0085  9.1650\n",
      "      8                     \u001b[36m0.5859\u001b[0m        \u001b[32m0.6717\u001b[0m                     0.5254        0.6998  0.0080  9.1703\n",
      "      9                     \u001b[36m0.5911\u001b[0m        \u001b[32m0.6706\u001b[0m                     0.5288        0.6876  0.0075  9.1688\n",
      "     10                     \u001b[36m0.5912\u001b[0m        \u001b[32m0.6691\u001b[0m                     0.5813        0.6775  0.0069  9.1675\n",
      "     11                     0.5855        0.6694                     0.5771        0.6794  0.0063  9.1691\n",
      "     12                     \u001b[36m0.5926\u001b[0m        \u001b[32m0.6687\u001b[0m                     0.5053        0.7022  0.0057  9.1736\n",
      "     13                     \u001b[36m0.5942\u001b[0m        \u001b[32m0.6681\u001b[0m                     0.5582        0.6850  0.0050  9.1694\n",
      "     14                     \u001b[36m0.5991\u001b[0m        \u001b[32m0.6653\u001b[0m                     \u001b[35m0.5910\u001b[0m        \u001b[31m0.6751\u001b[0m  0.0043  9.1690\n",
      "     15                     0.5972        0.6657                     0.5782        0.6788  0.0037  9.1730\n",
      "     16                     \u001b[36m0.6018\u001b[0m        \u001b[32m0.6625\u001b[0m                     0.5600        0.6809  0.0031  9.1708\n",
      "     17                     0.5999        0.6626                     0.5317        0.7132  0.0025  9.1689\n",
      "     18                     \u001b[36m0.6039\u001b[0m        \u001b[32m0.6597\u001b[0m                     0.5888        \u001b[31m0.6735\u001b[0m  0.0020  9.1672\n",
      "     19                     \u001b[36m0.6055\u001b[0m        0.6599                     0.5428        0.6893  0.0015  9.1706\n",
      "     20                     \u001b[36m0.6088\u001b[0m        \u001b[32m0.6573\u001b[0m                     \u001b[35m0.6027\u001b[0m        \u001b[31m0.6698\u001b[0m  0.0010  9.1713\n",
      "     21                     0.6072        \u001b[32m0.6564\u001b[0m                     0.5908        0.6727  0.0007  9.1737\n",
      "     22                     \u001b[36m0.6118\u001b[0m        \u001b[32m0.6556\u001b[0m                     0.6002        \u001b[31m0.6697\u001b[0m  0.0004  9.1728\n",
      "     23                     \u001b[36m0.6182\u001b[0m        \u001b[32m0.6541\u001b[0m                     \u001b[35m0.6044\u001b[0m        \u001b[31m0.6690\u001b[0m  0.0002  9.1734\n",
      "     24                     0.6130        0.6541                     0.6030        \u001b[31m0.6684\u001b[0m  0.0000  9.1733\n",
      "     25                     0.6102        0.6550                     0.6017        0.6690  0.0000  9.1703\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5500\u001b[0m        \u001b[32m0.6971\u001b[0m                     \u001b[35m0.5402\u001b[0m        \u001b[31m0.6848\u001b[0m  0.0100  9.1601\n",
      "      2                     \u001b[36m0.5795\u001b[0m        \u001b[32m0.6770\u001b[0m                     \u001b[35m0.5926\u001b[0m        \u001b[31m0.6673\u001b[0m  0.0100  9.1660\n",
      "      3                     \u001b[36m0.5809\u001b[0m        \u001b[32m0.6747\u001b[0m                     0.5855        0.6700  0.0098  9.1704\n",
      "      4                     \u001b[36m0.5834\u001b[0m        \u001b[32m0.6746\u001b[0m                     0.5730        0.6780  0.0096  9.1704\n",
      "      5                     0.5800        \u001b[32m0.6744\u001b[0m                     0.5833        0.6698  0.0093  9.1679\n",
      "      6                     0.5821        \u001b[32m0.6736\u001b[0m                     0.5335        0.6903  0.0090  9.1696\n",
      "      7                     \u001b[36m0.5840\u001b[0m        \u001b[32m0.6732\u001b[0m                     0.5549        0.6818  0.0085  9.1671\n",
      "      8                     \u001b[36m0.5853\u001b[0m        0.6743                     0.5073        0.7058  0.0080  9.1708\n",
      "      9                     \u001b[36m0.5870\u001b[0m        \u001b[32m0.6727\u001b[0m                     0.5553        0.6868  0.0075  9.1709\n",
      "     10                     \u001b[36m0.5878\u001b[0m        \u001b[32m0.6718\u001b[0m                     0.5008        0.6978  0.0069  9.1679\n",
      "     11                     \u001b[36m0.5886\u001b[0m        \u001b[32m0.6703\u001b[0m                     0.5078        0.7692  0.0063  9.1658\n",
      "     12                     \u001b[36m0.5954\u001b[0m        \u001b[32m0.6687\u001b[0m                     0.5617        0.6828  0.0057  9.1701\n",
      "     13                     0.5927        \u001b[32m0.6687\u001b[0m                     0.5481        0.6867  0.0050  9.1694\n",
      "     14                     \u001b[36m0.5974\u001b[0m        \u001b[32m0.6674\u001b[0m                     0.5402        0.7195  0.0043  9.1720\n",
      "     15                     \u001b[36m0.5984\u001b[0m        \u001b[32m0.6647\u001b[0m                     0.5389        0.6986  0.0037  9.1691\n",
      "     16                     \u001b[36m0.6012\u001b[0m        \u001b[32m0.6637\u001b[0m                     0.5056        0.7464  0.0031  9.1728\n",
      "     17                     \u001b[36m0.6041\u001b[0m        \u001b[32m0.6616\u001b[0m                     0.5494        0.6879  0.0025  9.1719\n",
      "     18                     0.6041        \u001b[32m0.6608\u001b[0m                     \u001b[35m0.6027\u001b[0m        \u001b[31m0.6658\u001b[0m  0.0020  9.1711\n",
      "     19                     \u001b[36m0.6076\u001b[0m        \u001b[32m0.6597\u001b[0m                     0.5593        0.6781  0.0015  9.1711\n",
      "     20                     \u001b[36m0.6087\u001b[0m        \u001b[32m0.6589\u001b[0m                     0.5816        0.6779  0.0010  9.1721\n",
      "     21                     \u001b[36m0.6108\u001b[0m        \u001b[32m0.6584\u001b[0m                     0.5970        0.6690  0.0007  9.1704\n",
      "     22                     \u001b[36m0.6155\u001b[0m        \u001b[32m0.6557\u001b[0m                     0.5907        0.6691  0.0004  9.1739\n",
      "     23                     0.6151        \u001b[32m0.6553\u001b[0m                     \u001b[35m0.6068\u001b[0m        \u001b[31m0.6633\u001b[0m  0.0002  9.1739\n",
      "     24                     0.6152        \u001b[32m0.6550\u001b[0m                     0.6041        \u001b[31m0.6618\u001b[0m  0.0000  9.1735\n",
      "     25                     0.6135        0.6552                     0.6040        0.6620  0.0000  9.1729\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5547\u001b[0m        \u001b[32m0.6926\u001b[0m                     \u001b[35m0.5498\u001b[0m        \u001b[31m0.6883\u001b[0m  0.0100  9.1699\n",
      "      2                     \u001b[36m0.5722\u001b[0m        \u001b[32m0.6775\u001b[0m                     \u001b[35m0.5748\u001b[0m        \u001b[31m0.6752\u001b[0m  0.0100  9.1756\n",
      "      3                     \u001b[36m0.5808\u001b[0m        \u001b[32m0.6744\u001b[0m                     \u001b[35m0.5830\u001b[0m        0.6767  0.0098  9.1765\n",
      "      4                     \u001b[36m0.5837\u001b[0m        0.6748                     0.5607        0.6806  0.0096  9.1764\n",
      "      5                     0.5833        \u001b[32m0.6731\u001b[0m                     0.5633        0.6827  0.0093  9.1757\n",
      "      6                     \u001b[36m0.5849\u001b[0m        \u001b[32m0.6730\u001b[0m                     0.5816        0.6758  0.0090  9.1759\n",
      "      7                     \u001b[36m0.5863\u001b[0m        \u001b[32m0.6730\u001b[0m                     0.5753        \u001b[31m0.6749\u001b[0m  0.0085  9.1743\n",
      "      8                     0.5855        \u001b[32m0.6723\u001b[0m                     0.5742        0.6773  0.0080  9.1774\n",
      "      9                     \u001b[36m0.5903\u001b[0m        \u001b[32m0.6711\u001b[0m                     0.5793        0.6781  0.0075  9.1774\n",
      "     10                     0.5899        \u001b[32m0.6705\u001b[0m                     \u001b[35m0.5895\u001b[0m        \u001b[31m0.6716\u001b[0m  0.0069  9.1775\n",
      "     11                     \u001b[36m0.5916\u001b[0m        \u001b[32m0.6691\u001b[0m                     0.5826        0.6730  0.0063  9.1752\n",
      "     12                     \u001b[36m0.5922\u001b[0m        \u001b[32m0.6688\u001b[0m                     0.5137        0.6986  0.0057  9.1788\n",
      "     13                     \u001b[36m0.6004\u001b[0m        \u001b[32m0.6670\u001b[0m                     0.5749        0.6776  0.0050  9.1795\n",
      "     14                     0.5941        \u001b[32m0.6670\u001b[0m                     0.5683        0.6799  0.0043  9.1847\n",
      "     15                     \u001b[36m0.6020\u001b[0m        \u001b[32m0.6649\u001b[0m                     0.5199        0.6961  0.0037  9.1818\n",
      "     16                     \u001b[36m0.6036\u001b[0m        \u001b[32m0.6637\u001b[0m                     0.5768        0.6755  0.0031  9.1799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17                     0.6029        \u001b[32m0.6619\u001b[0m                     0.5492        0.6838  0.0025  9.1818\n",
      "     18                     \u001b[36m0.6042\u001b[0m        \u001b[32m0.6607\u001b[0m                     \u001b[35m0.5950\u001b[0m        \u001b[31m0.6702\u001b[0m  0.0020  9.1793\n",
      "     19                     \u001b[36m0.6078\u001b[0m        \u001b[32m0.6587\u001b[0m                     0.5814        0.6733  0.0015  9.1789\n",
      "     20                     \u001b[36m0.6129\u001b[0m        \u001b[32m0.6575\u001b[0m                     0.5692        0.6773  0.0010  9.1793\n",
      "     21                     0.6088        \u001b[32m0.6569\u001b[0m                     0.5925        \u001b[31m0.6695\u001b[0m  0.0007  9.1851\n",
      "     22                     \u001b[36m0.6150\u001b[0m        \u001b[32m0.6564\u001b[0m                     \u001b[35m0.5968\u001b[0m        \u001b[31m0.6677\u001b[0m  0.0004  9.1827\n",
      "     23                     \u001b[36m0.6159\u001b[0m        \u001b[32m0.6553\u001b[0m                     \u001b[35m0.6058\u001b[0m        \u001b[31m0.6667\u001b[0m  0.0002  9.1827\n",
      "     24                     \u001b[36m0.6163\u001b[0m        \u001b[32m0.6541\u001b[0m                     0.6019        \u001b[31m0.6652\u001b[0m  0.0000  9.2119\n",
      "     25                     \u001b[36m0.6169\u001b[0m        0.6550                     0.6005        \u001b[31m0.6652\u001b[0m  0.0000  9.1819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[1 1 0 ... 0 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.8249\u001b[0m        \u001b[32m0.4034\u001b[0m                     \u001b[35m0.8695\u001b[0m        \u001b[31m0.3347\u001b[0m  0.0100  3.7846\n",
      "      2                     \u001b[36m0.8562\u001b[0m        \u001b[32m0.3469\u001b[0m                     0.8249        0.3893  0.0100  3.7499\n",
      "      3                     \u001b[36m0.8690\u001b[0m        \u001b[32m0.3182\u001b[0m                     0.8372        0.3612  0.0098  3.7508\n",
      "      4                     \u001b[36m0.8758\u001b[0m        \u001b[32m0.3088\u001b[0m                     \u001b[35m0.8754\u001b[0m        \u001b[31m0.3344\u001b[0m  0.0096  3.7534\n",
      "      5                     \u001b[36m0.8761\u001b[0m        \u001b[32m0.3019\u001b[0m                     0.8276        0.4037  0.0093  3.7515\n",
      "      6                     \u001b[36m0.8769\u001b[0m        0.3036                     0.8476        0.3569  0.0090  3.7504\n",
      "      7                     \u001b[36m0.8795\u001b[0m        \u001b[32m0.2975\u001b[0m                     0.7886        0.4284  0.0085  3.7519\n",
      "      8                     \u001b[36m0.8848\u001b[0m        \u001b[32m0.2854\u001b[0m                     0.8573        0.3591  0.0080  3.7514\n",
      "      9                     0.8831        \u001b[32m0.2816\u001b[0m                     0.8426        0.3667  0.0075  3.7526\n",
      "     10                     \u001b[36m0.8855\u001b[0m        \u001b[32m0.2752\u001b[0m                     \u001b[35m0.8794\u001b[0m        \u001b[31m0.2984\u001b[0m  0.0069  3.7542\n",
      "     11                     \u001b[36m0.8871\u001b[0m        \u001b[32m0.2701\u001b[0m                     0.6808        0.6833  0.0063  3.7518\n",
      "     12                     \u001b[36m0.8902\u001b[0m        \u001b[32m0.2689\u001b[0m                     0.8071        0.3899  0.0057  3.7512\n",
      "     13                     \u001b[36m0.8975\u001b[0m        \u001b[32m0.2604\u001b[0m                     0.8744        0.3278  0.0050  3.7521\n",
      "     14                     \u001b[36m0.9000\u001b[0m        \u001b[32m0.2546\u001b[0m                     0.8794        \u001b[31m0.2959\u001b[0m  0.0043  3.7520\n",
      "     15                     \u001b[36m0.9005\u001b[0m        \u001b[32m0.2514\u001b[0m                     0.7901        0.4748  0.0037  3.7513\n",
      "     16                     \u001b[36m0.9032\u001b[0m        \u001b[32m0.2482\u001b[0m                     0.7208        0.5250  0.0031  3.7518\n",
      "     17                     \u001b[36m0.9044\u001b[0m        \u001b[32m0.2384\u001b[0m                     \u001b[35m0.8930\u001b[0m        \u001b[31m0.2888\u001b[0m  0.0025  3.7527\n",
      "     18                     \u001b[36m0.9077\u001b[0m        \u001b[32m0.2351\u001b[0m                     0.6303        1.0962  0.0020  3.7529\n",
      "     19                     \u001b[36m0.9086\u001b[0m        \u001b[32m0.2276\u001b[0m                     \u001b[35m0.8966\u001b[0m        \u001b[31m0.2666\u001b[0m  0.0015  3.7507\n",
      "     20                     \u001b[36m0.9090\u001b[0m        0.2308                     0.8865        0.2736  0.0010  3.7530\n",
      "     21                     \u001b[36m0.9115\u001b[0m        \u001b[32m0.2218\u001b[0m                     0.8965        \u001b[31m0.2599\u001b[0m  0.0007  3.7512\n",
      "     22                     \u001b[36m0.9125\u001b[0m        0.2219                     \u001b[35m0.9035\u001b[0m        \u001b[31m0.2529\u001b[0m  0.0004  3.7529\n",
      "     23                     \u001b[36m0.9154\u001b[0m        \u001b[32m0.2211\u001b[0m                     0.8924        0.2558  0.0002  3.7530\n",
      "     24                     \u001b[36m0.9192\u001b[0m        \u001b[32m0.2144\u001b[0m                     0.8984        \u001b[31m0.2522\u001b[0m  0.0000  3.7545\n",
      "     25                     0.9139        0.2164                     0.8965        0.2524  0.0000  3.7531\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.8181\u001b[0m        \u001b[32m0.4132\u001b[0m                     \u001b[35m0.8530\u001b[0m        \u001b[31m0.3702\u001b[0m  0.0100  3.7502\n",
      "      2                     \u001b[36m0.8613\u001b[0m        \u001b[32m0.3357\u001b[0m                     \u001b[35m0.8754\u001b[0m        \u001b[31m0.3362\u001b[0m  0.0100  3.7500\n",
      "      3                     \u001b[36m0.8718\u001b[0m        \u001b[32m0.3189\u001b[0m                     0.8670        0.3499  0.0098  3.7484\n",
      "      4                     0.8663        \u001b[32m0.3183\u001b[0m                     0.8499        0.3669  0.0096  3.7479\n",
      "      5                     \u001b[36m0.8734\u001b[0m        \u001b[32m0.3087\u001b[0m                     0.8702        \u001b[31m0.3314\u001b[0m  0.0093  3.7513\n",
      "      6                     \u001b[36m0.8803\u001b[0m        \u001b[32m0.3032\u001b[0m                     \u001b[35m0.8821\u001b[0m        \u001b[31m0.2927\u001b[0m  0.0090  3.7506\n",
      "      7                     \u001b[36m0.8825\u001b[0m        \u001b[32m0.2936\u001b[0m                     0.7745        0.5314  0.0085  3.7488\n",
      "      8                     0.8820        \u001b[32m0.2885\u001b[0m                     0.7689        0.4998  0.0080  3.7500\n",
      "      9                     \u001b[36m0.8828\u001b[0m        0.2889                     0.8584        0.3380  0.0075  3.7497\n",
      "     10                     \u001b[36m0.8911\u001b[0m        \u001b[32m0.2779\u001b[0m                     0.6524        0.7426  0.0069  3.7500\n",
      "     11                     \u001b[36m0.8918\u001b[0m        \u001b[32m0.2738\u001b[0m                     0.5980        0.9538  0.0063  3.7529\n",
      "     12                     0.8857        0.2756                     0.8223        0.4143  0.0057  3.7491\n",
      "     13                     \u001b[36m0.8959\u001b[0m        \u001b[32m0.2617\u001b[0m                     0.8658        0.3251  0.0050  3.7502\n",
      "     14                     \u001b[36m0.8967\u001b[0m        0.2637                     \u001b[35m0.8863\u001b[0m        \u001b[31m0.2855\u001b[0m  0.0043  3.7506\n",
      "     15                     \u001b[36m0.8973\u001b[0m        \u001b[32m0.2602\u001b[0m                     0.7664        0.4879  0.0037  3.7501\n",
      "     16                     0.8946        \u001b[32m0.2562\u001b[0m                     0.8758        0.2941  0.0031  3.7519\n",
      "     17                     \u001b[36m0.8995\u001b[0m        \u001b[32m0.2484\u001b[0m                     0.8644        0.3340  0.0025  3.7527\n",
      "     18                     \u001b[36m0.9081\u001b[0m        \u001b[32m0.2369\u001b[0m                     0.7761        0.5233  0.0020  3.7507\n",
      "     19                     0.9008        0.2462                     0.8767        0.3072  0.0015  3.7510\n",
      "     20                     0.9058        \u001b[32m0.2346\u001b[0m                     \u001b[35m0.8922\u001b[0m        \u001b[31m0.2756\u001b[0m  0.0010  3.7511\n",
      "     21                     \u001b[36m0.9084\u001b[0m        \u001b[32m0.2322\u001b[0m                     \u001b[35m0.8939\u001b[0m        0.2808  0.0007  3.7505\n",
      "     22                     \u001b[36m0.9102\u001b[0m        \u001b[32m0.2293\u001b[0m                     \u001b[35m0.9042\u001b[0m        \u001b[31m0.2576\u001b[0m  0.0004  3.7497\n",
      "     23                     \u001b[36m0.9106\u001b[0m        \u001b[32m0.2283\u001b[0m                     0.9027        \u001b[31m0.2537\u001b[0m  0.0002  3.7503\n",
      "     24                     \u001b[36m0.9126\u001b[0m        \u001b[32m0.2250\u001b[0m                     \u001b[35m0.9051\u001b[0m        0.2543  0.0000  3.7521\n",
      "     25                     \u001b[36m0.9139\u001b[0m        \u001b[32m0.2239\u001b[0m                     0.9044        0.2543  0.0000  3.7518\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.8133\u001b[0m        \u001b[32m0.4266\u001b[0m                     \u001b[35m0.8567\u001b[0m        \u001b[31m0.3738\u001b[0m  0.0100  3.7477\n",
      "      2                     \u001b[36m0.8571\u001b[0m        \u001b[32m0.3428\u001b[0m                     \u001b[35m0.8644\u001b[0m        \u001b[31m0.3300\u001b[0m  0.0100  3.7495\n",
      "      3                     \u001b[36m0.8669\u001b[0m        \u001b[32m0.3216\u001b[0m                     0.8367        0.3737  0.0098  3.7498\n",
      "      4                     \u001b[36m0.8720\u001b[0m        \u001b[32m0.3129\u001b[0m                     0.8491        0.3350  0.0096  3.7496\n",
      "      5                     \u001b[36m0.8765\u001b[0m        \u001b[32m0.3029\u001b[0m                     0.8132        0.4368  0.0093  3.7497\n",
      "      6                     \u001b[36m0.8783\u001b[0m        \u001b[32m0.2933\u001b[0m                     0.8351        0.3883  0.0090  3.7505\n",
      "      7                     \u001b[36m0.8794\u001b[0m        0.2971                     \u001b[35m0.8658\u001b[0m        \u001b[31m0.3099\u001b[0m  0.0085  3.7503\n",
      "      8                     \u001b[36m0.8840\u001b[0m        0.2945                     0.8530        0.3509  0.0080  3.7519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.8900\u001b[0m        \u001b[32m0.2831\u001b[0m                     0.8379        0.3737  0.0075  3.7508\n",
      "     10                     0.8859        0.2863                     0.8418        0.3729  0.0069  3.7496\n",
      "     11                     0.8894        \u001b[32m0.2786\u001b[0m                     0.8622        0.3190  0.0063  3.7498\n",
      "     12                     \u001b[36m0.8908\u001b[0m        \u001b[32m0.2753\u001b[0m                     0.8584        0.3240  0.0057  3.7503\n",
      "     13                     \u001b[36m0.8974\u001b[0m        \u001b[32m0.2682\u001b[0m                     0.6003        0.9564  0.0050  3.7517\n",
      "     14                     0.8922        0.2698                     0.6891        0.8072  0.0043  3.7508\n",
      "     15                     \u001b[36m0.9025\u001b[0m        \u001b[32m0.2589\u001b[0m                     \u001b[35m0.8666\u001b[0m        0.3207  0.0037  3.7501\n",
      "     16                     0.8992        0.2613                     0.7500        0.5484  0.0031  3.7524\n",
      "     17                     \u001b[36m0.9045\u001b[0m        \u001b[32m0.2499\u001b[0m                     \u001b[35m0.8855\u001b[0m        \u001b[31m0.2849\u001b[0m  0.0025  3.7501\n",
      "     18                     0.9039        0.2514                     0.8570        0.3359  0.0020  3.7531\n",
      "     19                     \u001b[36m0.9072\u001b[0m        \u001b[32m0.2465\u001b[0m                     0.8762        0.3015  0.0015  3.7532\n",
      "     20                     \u001b[36m0.9093\u001b[0m        \u001b[32m0.2391\u001b[0m                     \u001b[35m0.8973\u001b[0m        \u001b[31m0.2537\u001b[0m  0.0010  3.7501\n",
      "     21                     \u001b[36m0.9125\u001b[0m        \u001b[32m0.2371\u001b[0m                     0.8231        0.3952  0.0007  3.7500\n",
      "     22                     0.9095        0.2375                     \u001b[35m0.9023\u001b[0m        \u001b[31m0.2466\u001b[0m  0.0004  3.7499\n",
      "     23                     0.9099        \u001b[32m0.2296\u001b[0m                     0.8912        0.2720  0.0002  3.7533\n",
      "     24                     \u001b[36m0.9131\u001b[0m        0.2298                     0.8969        0.2497  0.0000  3.7510\n",
      "     25                     0.9123        0.2301                     0.8967        0.2490  0.0000  3.7523\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.8173\u001b[0m        \u001b[32m0.4146\u001b[0m                     \u001b[35m0.8609\u001b[0m        \u001b[31m0.3686\u001b[0m  0.0100  3.7476\n",
      "      2                     \u001b[36m0.8512\u001b[0m        \u001b[32m0.3524\u001b[0m                     \u001b[35m0.8636\u001b[0m        \u001b[31m0.3368\u001b[0m  0.0100  3.7497\n",
      "      3                     \u001b[36m0.8630\u001b[0m        \u001b[32m0.3240\u001b[0m                     0.8590        \u001b[31m0.3187\u001b[0m  0.0098  3.7503\n",
      "      4                     \u001b[36m0.8676\u001b[0m        \u001b[32m0.3180\u001b[0m                     0.8054        0.4214  0.0096  3.7499\n",
      "      5                     \u001b[36m0.8741\u001b[0m        \u001b[32m0.3147\u001b[0m                     0.8544        0.3251  0.0093  3.7493\n",
      "      6                     \u001b[36m0.8807\u001b[0m        \u001b[32m0.2976\u001b[0m                     0.8117        0.4273  0.0090  3.7505\n",
      "      7                     0.8790        \u001b[32m0.2951\u001b[0m                     0.8200        0.3685  0.0085  3.7507\n",
      "      8                     \u001b[36m0.8815\u001b[0m        \u001b[32m0.2897\u001b[0m                     0.8477        0.3290  0.0080  3.7515\n",
      "      9                     \u001b[36m0.8855\u001b[0m        0.2898                     0.6346        0.7478  0.0075  3.7511\n",
      "     10                     0.8834        \u001b[32m0.2854\u001b[0m                     0.8575        0.3266  0.0069  3.7513\n",
      "     11                     \u001b[36m0.8895\u001b[0m        \u001b[32m0.2783\u001b[0m                     0.8460        0.3784  0.0063  3.7519\n",
      "     12                     0.8883        \u001b[32m0.2723\u001b[0m                     \u001b[35m0.8917\u001b[0m        \u001b[31m0.2665\u001b[0m  0.0057  3.7512\n",
      "     13                     \u001b[36m0.8962\u001b[0m        \u001b[32m0.2695\u001b[0m                     0.8893        0.2684  0.0050  3.7518\n",
      "     14                     \u001b[36m0.8974\u001b[0m        \u001b[32m0.2581\u001b[0m                     0.8459        0.3409  0.0043  3.7531\n",
      "     15                     \u001b[36m0.9017\u001b[0m        \u001b[32m0.2492\u001b[0m                     0.7792        0.4740  0.0037  3.7512\n",
      "     16                     \u001b[36m0.9061\u001b[0m        \u001b[32m0.2457\u001b[0m                     0.8836        0.3002  0.0031  3.7505\n",
      "     17                     0.9024        \u001b[32m0.2433\u001b[0m                     0.6118        1.0677  0.0025  3.7501\n",
      "     18                     \u001b[36m0.9065\u001b[0m        \u001b[32m0.2406\u001b[0m                     \u001b[35m0.9000\u001b[0m        \u001b[31m0.2563\u001b[0m  0.0020  3.7520\n",
      "     19                     0.9060        0.2439                     0.8671        0.3269  0.0015  3.7532\n",
      "     20                     \u001b[36m0.9104\u001b[0m        \u001b[32m0.2312\u001b[0m                     \u001b[35m0.9093\u001b[0m        \u001b[31m0.2427\u001b[0m  0.0010  3.7515\n",
      "     21                     \u001b[36m0.9141\u001b[0m        \u001b[32m0.2283\u001b[0m                     \u001b[35m0.9183\u001b[0m        \u001b[31m0.2369\u001b[0m  0.0007  3.7522\n",
      "     22                     \u001b[36m0.9144\u001b[0m        \u001b[32m0.2246\u001b[0m                     0.9070        0.2369  0.0004  3.7514\n",
      "     23                     \u001b[36m0.9174\u001b[0m        \u001b[32m0.2207\u001b[0m                     0.9100        \u001b[31m0.2341\u001b[0m  0.0002  3.7551\n",
      "     24                     \u001b[36m0.9177\u001b[0m        \u001b[32m0.2161\u001b[0m                     0.9119        \u001b[31m0.2329\u001b[0m  0.0000  3.7530\n",
      "     25                     0.9177        0.2193                     0.9130        \u001b[31m0.2327\u001b[0m  0.0000  3.7530\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.8123\u001b[0m        \u001b[32m0.4277\u001b[0m                     \u001b[35m0.8620\u001b[0m        \u001b[31m0.3458\u001b[0m  0.0100  3.7468\n",
      "      2                     \u001b[36m0.8558\u001b[0m        \u001b[32m0.3459\u001b[0m                     \u001b[35m0.8656\u001b[0m        \u001b[31m0.3277\u001b[0m  0.0100  3.7496\n",
      "      3                     \u001b[36m0.8661\u001b[0m        \u001b[32m0.3254\u001b[0m                     \u001b[35m0.8800\u001b[0m        \u001b[31m0.3225\u001b[0m  0.0098  3.7497\n",
      "      4                     \u001b[36m0.8667\u001b[0m        \u001b[32m0.3173\u001b[0m                     0.8388        0.3960  0.0096  3.7494\n",
      "      5                     \u001b[36m0.8766\u001b[0m        \u001b[32m0.3030\u001b[0m                     0.8420        0.3644  0.0093  3.7534\n",
      "      6                     \u001b[36m0.8782\u001b[0m        \u001b[32m0.2986\u001b[0m                     0.8513        0.3509  0.0090  3.7499\n",
      "      7                     \u001b[36m0.8862\u001b[0m        \u001b[32m0.2922\u001b[0m                     0.8451        0.3724  0.0085  3.7506\n",
      "      8                     \u001b[36m0.8870\u001b[0m        \u001b[32m0.2824\u001b[0m                     0.8078        0.4267  0.0080  3.7512\n",
      "      9                     \u001b[36m0.8907\u001b[0m        \u001b[32m0.2809\u001b[0m                     0.5676        1.0269  0.0075  3.7513\n",
      "     10                     0.8781        0.2910                     0.8388        0.3717  0.0069  3.7515\n",
      "     11                     0.8895        \u001b[32m0.2773\u001b[0m                     0.8554        0.3508  0.0063  3.7503\n",
      "     12                     \u001b[36m0.8949\u001b[0m        \u001b[32m0.2691\u001b[0m                     0.8699        0.3260  0.0057  3.7500\n",
      "     13                     \u001b[36m0.9000\u001b[0m        \u001b[32m0.2581\u001b[0m                     0.7180        0.6524  0.0050  3.7509\n",
      "     14                     0.8954        0.2641                     0.5376        1.7735  0.0043  3.7512\n",
      "     15                     0.8981        \u001b[32m0.2520\u001b[0m                     0.8274        0.3983  0.0037  3.7516\n",
      "     16                     \u001b[36m0.9006\u001b[0m        0.2564                     0.8212        0.4079  0.0031  3.7510\n",
      "     17                     \u001b[36m0.9035\u001b[0m        \u001b[32m0.2435\u001b[0m                     0.8195        0.4268  0.0025  3.7515\n",
      "     18                     \u001b[36m0.9069\u001b[0m        \u001b[32m0.2411\u001b[0m                     0.8480        0.3568  0.0020  3.7512\n",
      "     19                     \u001b[36m0.9084\u001b[0m        \u001b[32m0.2364\u001b[0m                     0.8095        0.4273  0.0015  3.7519\n",
      "     20                     \u001b[36m0.9128\u001b[0m        \u001b[32m0.2280\u001b[0m                     \u001b[35m0.9056\u001b[0m        \u001b[31m0.2718\u001b[0m  0.0010  3.7523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21                     0.9121        \u001b[32m0.2277\u001b[0m                     0.8211        0.4138  0.0007  3.7544\n",
      "     22                     0.9096        0.2279                     0.9032        \u001b[31m0.2666\u001b[0m  0.0004  3.7518\n",
      "     23                     \u001b[36m0.9141\u001b[0m        \u001b[32m0.2231\u001b[0m                     0.8952        0.2700  0.0002  3.7529\n",
      "     24                     0.9125        \u001b[32m0.2226\u001b[0m                     0.8923        0.2699  0.0000  3.7514\n",
      "     25                     0.9133        0.2253                     0.8952        0.2687  0.0000  3.7518\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.8251\u001b[0m        \u001b[32m0.4100\u001b[0m                     \u001b[35m0.8244\u001b[0m        \u001b[31m0.3970\u001b[0m  0.0100  3.7467\n",
      "      2                     \u001b[36m0.8622\u001b[0m        \u001b[32m0.3383\u001b[0m                     \u001b[35m0.8551\u001b[0m        \u001b[31m0.3534\u001b[0m  0.0100  3.7497\n",
      "      3                     \u001b[36m0.8753\u001b[0m        \u001b[32m0.3095\u001b[0m                     0.8266        0.4007  0.0098  3.7497\n",
      "      4                     0.8725        0.3105                     \u001b[35m0.8716\u001b[0m        \u001b[31m0.3097\u001b[0m  0.0096  3.7498\n",
      "      5                     \u001b[36m0.8827\u001b[0m        \u001b[32m0.2919\u001b[0m                     0.8268        0.4184  0.0093  3.7495\n",
      "      6                     \u001b[36m0.8840\u001b[0m        0.2929                     0.7950        0.4368  0.0090  3.7521\n",
      "      7                     \u001b[36m0.8847\u001b[0m        \u001b[32m0.2853\u001b[0m                     \u001b[35m0.8859\u001b[0m        \u001b[31m0.2823\u001b[0m  0.0085  3.7505\n",
      "      8                     \u001b[36m0.8895\u001b[0m        \u001b[32m0.2799\u001b[0m                     0.8538        0.3290  0.0080  3.7501\n",
      "      9                     0.8894        \u001b[32m0.2789\u001b[0m                     0.8453        0.3361  0.0075  3.7497\n",
      "     10                     \u001b[36m0.8912\u001b[0m        \u001b[32m0.2732\u001b[0m                     0.6133        0.8772  0.0069  3.7514\n",
      "     11                     \u001b[36m0.8938\u001b[0m        \u001b[32m0.2721\u001b[0m                     0.6471        0.9190  0.0063  3.7515\n",
      "     12                     0.8898        \u001b[32m0.2714\u001b[0m                     0.8329        0.3742  0.0057  3.7517\n",
      "     13                     \u001b[36m0.8955\u001b[0m        \u001b[32m0.2685\u001b[0m                     0.6127        0.9927  0.0050  3.7511\n",
      "     14                     \u001b[36m0.9050\u001b[0m        \u001b[32m0.2509\u001b[0m                     \u001b[35m0.8935\u001b[0m        \u001b[31m0.2713\u001b[0m  0.0043  3.7503\n",
      "     15                     \u001b[36m0.9065\u001b[0m        \u001b[32m0.2484\u001b[0m                     0.8856        0.2826  0.0037  3.7506\n",
      "     16                     0.9038        \u001b[32m0.2443\u001b[0m                     0.8508        0.3433  0.0031  3.7509\n",
      "     17                     0.9061        \u001b[32m0.2401\u001b[0m                     0.7841        0.4649  0.0025  3.7504\n",
      "     18                     \u001b[36m0.9109\u001b[0m        \u001b[32m0.2372\u001b[0m                     0.8716        0.3069  0.0020  3.7520\n",
      "     19                     \u001b[36m0.9123\u001b[0m        \u001b[32m0.2271\u001b[0m                     0.8348        0.3967  0.0015  3.7512\n",
      "     20                     0.9095        0.2279                     0.8840        0.2889  0.0010  3.7516\n",
      "     21                     \u001b[36m0.9167\u001b[0m        0.2286                     \u001b[35m0.8982\u001b[0m        \u001b[31m0.2590\u001b[0m  0.0007  3.7503\n",
      "     22                     0.9139        \u001b[32m0.2231\u001b[0m                     \u001b[35m0.9018\u001b[0m        \u001b[31m0.2537\u001b[0m  0.0004  3.7515\n",
      "     23                     0.9166        0.2245                     0.9011        0.2563  0.0002  3.7517\n",
      "     24                     0.9140        \u001b[32m0.2225\u001b[0m                     \u001b[35m0.9064\u001b[0m        0.2548  0.0000  3.7525\n",
      "     25                     0.9162        \u001b[32m0.2201\u001b[0m                     0.9041        \u001b[31m0.2529\u001b[0m  0.0000  3.7509\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.8143\u001b[0m        \u001b[32m0.4263\u001b[0m                     \u001b[35m0.8507\u001b[0m        \u001b[31m0.3559\u001b[0m  0.0100  3.7485\n",
      "      2                     \u001b[36m0.8569\u001b[0m        \u001b[32m0.3483\u001b[0m                     0.8477        \u001b[31m0.3500\u001b[0m  0.0100  3.7493\n",
      "      3                     \u001b[36m0.8648\u001b[0m        \u001b[32m0.3275\u001b[0m                     \u001b[35m0.8847\u001b[0m        \u001b[31m0.2975\u001b[0m  0.0098  3.7510\n",
      "      4                     \u001b[36m0.8667\u001b[0m        \u001b[32m0.3206\u001b[0m                     \u001b[35m0.8928\u001b[0m        \u001b[31m0.2867\u001b[0m  0.0096  3.7509\n",
      "      5                     \u001b[36m0.8791\u001b[0m        \u001b[32m0.3023\u001b[0m                     0.8107        0.4023  0.0093  3.7486\n",
      "      6                     0.8725        0.3075                     0.8842        0.2907  0.0090  3.7501\n",
      "      7                     0.8787        \u001b[32m0.3006\u001b[0m                     0.5511        1.2457  0.0085  3.7494\n",
      "      8                     \u001b[36m0.8853\u001b[0m        \u001b[32m0.2883\u001b[0m                     0.8848        0.2989  0.0080  3.7493\n",
      "      9                     \u001b[36m0.8879\u001b[0m        \u001b[32m0.2799\u001b[0m                     0.8474        0.3568  0.0075  3.7483\n",
      "     10                     0.8857        0.2802                     0.7739        0.4972  0.0069  3.7499\n",
      "     11                     \u001b[36m0.8909\u001b[0m        \u001b[32m0.2798\u001b[0m                     0.8366        0.3483  0.0063  3.7494\n",
      "     12                     \u001b[36m0.8910\u001b[0m        \u001b[32m0.2774\u001b[0m                     0.8684        0.3134  0.0057  3.7495\n",
      "     13                     0.8910        \u001b[32m0.2663\u001b[0m                     0.8417        0.4041  0.0050  3.7520\n",
      "     14                     \u001b[36m0.8964\u001b[0m        \u001b[32m0.2616\u001b[0m                     0.6388        0.6690  0.0043  3.7499\n",
      "     15                     0.8938        \u001b[32m0.2569\u001b[0m                     0.7198        0.5452  0.0037  3.7488\n",
      "     16                     \u001b[36m0.8993\u001b[0m        \u001b[32m0.2486\u001b[0m                     0.8088        0.3898  0.0031  3.7514\n",
      "     17                     \u001b[36m0.9039\u001b[0m        \u001b[32m0.2476\u001b[0m                     0.8840        \u001b[31m0.2761\u001b[0m  0.0025  3.7502\n",
      "     18                     0.9014        0.2477                     0.8905        0.2859  0.0020  3.7518\n",
      "     19                     0.9027        \u001b[32m0.2425\u001b[0m                     \u001b[35m0.9085\u001b[0m        \u001b[31m0.2584\u001b[0m  0.0015  3.7508\n",
      "     20                     \u001b[36m0.9095\u001b[0m        \u001b[32m0.2306\u001b[0m                     0.9061        \u001b[31m0.2535\u001b[0m  0.0010  3.7503\n",
      "     21                     \u001b[36m0.9105\u001b[0m        \u001b[32m0.2268\u001b[0m                     0.8856        0.2924  0.0007  3.7504\n",
      "     22                     0.9071        0.2341                     \u001b[35m0.9146\u001b[0m        \u001b[31m0.2402\u001b[0m  0.0004  3.7512\n",
      "     23                     \u001b[36m0.9112\u001b[0m        \u001b[32m0.2255\u001b[0m                     \u001b[35m0.9179\u001b[0m        \u001b[31m0.2389\u001b[0m  0.0002  3.7508\n",
      "     24                     \u001b[36m0.9161\u001b[0m        \u001b[32m0.2208\u001b[0m                     0.9176        \u001b[31m0.2376\u001b[0m  0.0000  3.7513\n",
      "     25                     0.9131        0.2238                     0.9174        \u001b[31m0.2374\u001b[0m  0.0000  3.7505\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.8083\u001b[0m        \u001b[32m0.4236\u001b[0m                     \u001b[35m0.8499\u001b[0m        \u001b[31m0.3815\u001b[0m  0.0100  3.7442\n",
      "      2                     \u001b[36m0.8463\u001b[0m        \u001b[32m0.3632\u001b[0m                     \u001b[35m0.8611\u001b[0m        \u001b[31m0.3539\u001b[0m  0.0100  3.7456\n",
      "      3                     \u001b[36m0.8564\u001b[0m        \u001b[32m0.3425\u001b[0m                     0.8428        0.3996  0.0098  3.7481\n",
      "      4                     \u001b[36m0.8698\u001b[0m        \u001b[32m0.3242\u001b[0m                     \u001b[35m0.8712\u001b[0m        \u001b[31m0.3303\u001b[0m  0.0096  3.7488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5                     \u001b[36m0.8703\u001b[0m        \u001b[32m0.3132\u001b[0m                     0.8499        0.3557  0.0093  3.7482\n",
      "      6                     \u001b[36m0.8748\u001b[0m        \u001b[32m0.3073\u001b[0m                     0.8648        0.3375  0.0090  3.7470\n",
      "      7                     \u001b[36m0.8791\u001b[0m        \u001b[32m0.3007\u001b[0m                     0.7768        0.4403  0.0085  3.7488\n",
      "      8                     0.8777        \u001b[32m0.2935\u001b[0m                     0.7985        0.4170  0.0080  3.7490\n",
      "      9                     \u001b[36m0.8810\u001b[0m        \u001b[32m0.2922\u001b[0m                     0.8331        0.4104  0.0075  3.7492\n",
      "     10                     \u001b[36m0.8832\u001b[0m        \u001b[32m0.2822\u001b[0m                     0.7612        0.4579  0.0069  3.7486\n",
      "     11                     \u001b[36m0.8936\u001b[0m        \u001b[32m0.2713\u001b[0m                     0.7277        0.5837  0.0063  3.7494\n",
      "     12                     \u001b[36m0.8938\u001b[0m        \u001b[32m0.2677\u001b[0m                     0.6733        0.7156  0.0057  3.7471\n",
      "     13                     \u001b[36m0.8964\u001b[0m        \u001b[32m0.2641\u001b[0m                     \u001b[35m0.8757\u001b[0m        \u001b[31m0.3186\u001b[0m  0.0050  3.7482\n",
      "     14                     \u001b[36m0.8990\u001b[0m        \u001b[32m0.2570\u001b[0m                     0.7637        0.4600  0.0043  3.7489\n",
      "     15                     \u001b[36m0.9013\u001b[0m        \u001b[32m0.2543\u001b[0m                     0.5069        2.2875  0.0037  3.7479\n",
      "     16                     \u001b[36m0.9071\u001b[0m        \u001b[32m0.2428\u001b[0m                     0.8291        0.4042  0.0031  3.7496\n",
      "     17                     0.9060        0.2444                     \u001b[35m0.8809\u001b[0m        \u001b[31m0.2908\u001b[0m  0.0025  3.7479\n",
      "     18                     \u001b[36m0.9143\u001b[0m        \u001b[32m0.2355\u001b[0m                     0.6967        0.6610  0.0020  3.7485\n",
      "     19                     0.9133        \u001b[32m0.2339\u001b[0m                     0.8009        0.4790  0.0015  3.7495\n",
      "     20                     0.9074        \u001b[32m0.2304\u001b[0m                     0.8316        0.3886  0.0010  3.7510\n",
      "     21                     \u001b[36m0.9149\u001b[0m        \u001b[32m0.2270\u001b[0m                     \u001b[35m0.8936\u001b[0m        \u001b[31m0.2813\u001b[0m  0.0007  3.7495\n",
      "     22                     \u001b[36m0.9154\u001b[0m        \u001b[32m0.2249\u001b[0m                     0.8918        \u001b[31m0.2701\u001b[0m  0.0004  3.7496\n",
      "     23                     0.9151        \u001b[32m0.2212\u001b[0m                     \u001b[35m0.9033\u001b[0m        \u001b[31m0.2503\u001b[0m  0.0002  3.7498\n",
      "     24                     \u001b[36m0.9182\u001b[0m        \u001b[32m0.2189\u001b[0m                     \u001b[35m0.9046\u001b[0m        \u001b[31m0.2499\u001b[0m  0.0000  3.7490\n",
      "     25                     \u001b[36m0.9188\u001b[0m        0.2193                     0.9041        \u001b[31m0.2498\u001b[0m  0.0000  3.7518\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.8110\u001b[0m        \u001b[32m0.4268\u001b[0m                     \u001b[35m0.8466\u001b[0m        \u001b[31m0.3878\u001b[0m  0.0100  3.7437\n",
      "      2                     \u001b[36m0.8515\u001b[0m        \u001b[32m0.3531\u001b[0m                     \u001b[35m0.8657\u001b[0m        \u001b[31m0.3344\u001b[0m  0.0100  3.7471\n",
      "      3                     \u001b[36m0.8687\u001b[0m        \u001b[32m0.3268\u001b[0m                     \u001b[35m0.8871\u001b[0m        \u001b[31m0.2884\u001b[0m  0.0098  3.7483\n",
      "      4                     \u001b[36m0.8714\u001b[0m        \u001b[32m0.3137\u001b[0m                     0.8551        0.3423  0.0096  3.7478\n",
      "      5                     \u001b[36m0.8737\u001b[0m        \u001b[32m0.3102\u001b[0m                     0.8854        0.3018  0.0093  3.7481\n",
      "      6                     \u001b[36m0.8754\u001b[0m        \u001b[32m0.3009\u001b[0m                     0.7943        0.4178  0.0090  3.7487\n",
      "      7                     0.8719        0.3055                     0.8757        0.3047  0.0085  3.7484\n",
      "      8                     \u001b[36m0.8802\u001b[0m        \u001b[32m0.2947\u001b[0m                     \u001b[35m0.8908\u001b[0m        \u001b[31m0.2830\u001b[0m  0.0080  3.7491\n",
      "      9                     \u001b[36m0.8845\u001b[0m        \u001b[32m0.2904\u001b[0m                     0.8551        0.3451  0.0075  3.7480\n",
      "     10                     \u001b[36m0.8936\u001b[0m        \u001b[32m0.2733\u001b[0m                     0.7634        0.4822  0.0069  3.7481\n",
      "     11                     0.8862        0.2798                     0.8712        0.3158  0.0063  3.7492\n",
      "     12                     0.8906        \u001b[32m0.2702\u001b[0m                     0.8459        0.3361  0.0057  3.7474\n",
      "     13                     \u001b[36m0.8949\u001b[0m        \u001b[32m0.2668\u001b[0m                     0.8649        0.3443  0.0050  3.7482\n",
      "     14                     \u001b[36m0.8958\u001b[0m        \u001b[32m0.2620\u001b[0m                     0.5844        1.0247  0.0043  3.7501\n",
      "     15                     \u001b[36m0.8995\u001b[0m        \u001b[32m0.2581\u001b[0m                     0.8723        0.3067  0.0037  3.7491\n",
      "     16                     0.8939        \u001b[32m0.2496\u001b[0m                     0.7016        0.5377  0.0031  3.7511\n",
      "     17                     \u001b[36m0.9057\u001b[0m        \u001b[32m0.2461\u001b[0m                     0.7462        0.6004  0.0025  3.7511\n",
      "     18                     0.9014        \u001b[32m0.2429\u001b[0m                     0.8844        0.2924  0.0020  3.7497\n",
      "     19                     \u001b[36m0.9088\u001b[0m        \u001b[32m0.2351\u001b[0m                     0.7711        0.5244  0.0015  3.7500\n",
      "     20                     \u001b[36m0.9161\u001b[0m        \u001b[32m0.2281\u001b[0m                     \u001b[35m0.8956\u001b[0m        \u001b[31m0.2685\u001b[0m  0.0010  3.7481\n",
      "     21                     0.9093        0.2294                     \u001b[35m0.9013\u001b[0m        \u001b[31m0.2652\u001b[0m  0.0007  3.7494\n",
      "     22                     0.9137        \u001b[32m0.2241\u001b[0m                     0.9005        \u001b[31m0.2642\u001b[0m  0.0004  3.7491\n",
      "     23                     0.9114        \u001b[32m0.2237\u001b[0m                     0.9013        \u001b[31m0.2606\u001b[0m  0.0002  3.7505\n",
      "     24                     \u001b[36m0.9171\u001b[0m        \u001b[32m0.2202\u001b[0m                     \u001b[35m0.9045\u001b[0m        \u001b[31m0.2583\u001b[0m  0.0000  3.7490\n",
      "     25                     0.9157        0.2220                     0.9011        0.2595  0.0000  3.7502\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.8129\u001b[0m        \u001b[32m0.4222\u001b[0m                     \u001b[35m0.8643\u001b[0m        \u001b[31m0.3535\u001b[0m  0.0100  3.7451\n",
      "      2                     \u001b[36m0.8512\u001b[0m        \u001b[32m0.3492\u001b[0m                     \u001b[35m0.8701\u001b[0m        \u001b[31m0.3248\u001b[0m  0.0100  3.7464\n",
      "      3                     \u001b[36m0.8671\u001b[0m        \u001b[32m0.3254\u001b[0m                     \u001b[35m0.8734\u001b[0m        \u001b[31m0.3089\u001b[0m  0.0098  3.7475\n",
      "      4                     0.8660        \u001b[32m0.3208\u001b[0m                     0.8409        0.3848  0.0096  3.7465\n",
      "      5                     \u001b[36m0.8768\u001b[0m        \u001b[32m0.3087\u001b[0m                     0.8390        0.3785  0.0093  3.7478\n",
      "      6                     0.8714        0.3113                     0.7003        0.6537  0.0090  3.7482\n",
      "      7                     \u001b[36m0.8829\u001b[0m        \u001b[32m0.2954\u001b[0m                     0.8667        0.3163  0.0085  3.7498\n",
      "      8                     0.8806        \u001b[32m0.2906\u001b[0m                     0.8544        0.3498  0.0080  3.7481\n",
      "      9                     0.8810        0.2936                     0.7758        0.4892  0.0075  3.7481\n",
      "     10                     \u001b[36m0.8917\u001b[0m        \u001b[32m0.2746\u001b[0m                     0.6528        0.9053  0.0069  3.7475\n",
      "     11                     0.8882        0.2748                     0.6872        0.6772  0.0063  3.7481\n",
      "     12                     \u001b[36m0.8943\u001b[0m        \u001b[32m0.2724\u001b[0m                     0.6311        0.7684  0.0057  3.7468\n",
      "     13                     0.8935        \u001b[32m0.2701\u001b[0m                     0.5468        1.5690  0.0050  3.7479\n",
      "     14                     \u001b[36m0.8973\u001b[0m        \u001b[32m0.2625\u001b[0m                     0.6241        1.1040  0.0043  3.7489\n",
      "     15                     \u001b[36m0.8990\u001b[0m        \u001b[32m0.2538\u001b[0m                     0.8100        0.4252  0.0037  3.7484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16                     \u001b[36m0.9011\u001b[0m        \u001b[32m0.2499\u001b[0m                     \u001b[35m0.8812\u001b[0m        0.3150  0.0031  3.7483\n",
      "     17                     \u001b[36m0.9025\u001b[0m        \u001b[32m0.2427\u001b[0m                     0.8169        0.3892  0.0025  3.7494\n",
      "     18                     \u001b[36m0.9069\u001b[0m        \u001b[32m0.2426\u001b[0m                     \u001b[35m0.8815\u001b[0m        \u001b[31m0.2941\u001b[0m  0.0020  3.7478\n",
      "     19                     \u001b[36m0.9099\u001b[0m        \u001b[32m0.2367\u001b[0m                     \u001b[35m0.8870\u001b[0m        \u001b[31m0.2914\u001b[0m  0.0015  3.7490\n",
      "     20                     0.9050        \u001b[32m0.2351\u001b[0m                     0.8599        0.3248  0.0010  3.7501\n",
      "     21                     \u001b[36m0.9100\u001b[0m        \u001b[32m0.2296\u001b[0m                     \u001b[35m0.9036\u001b[0m        \u001b[31m0.2588\u001b[0m  0.0007  3.7499\n",
      "     22                     \u001b[36m0.9112\u001b[0m        0.2297                     0.8988        0.2669  0.0004  3.7493\n",
      "     23                     \u001b[36m0.9132\u001b[0m        \u001b[32m0.2241\u001b[0m                     0.9009        0.2617  0.0002  3.7517\n",
      "     24                     \u001b[36m0.9146\u001b[0m        0.2255                     0.9008        0.2620  0.0000  3.7491\n",
      "     25                     \u001b[36m0.9174\u001b[0m        \u001b[32m0.2192\u001b[0m                     0.8996        0.2598  0.0000  3.7503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[1 1 0 ... 0 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7180\u001b[0m        \u001b[32m0.5604\u001b[0m                     \u001b[35m0.7625\u001b[0m        \u001b[31m0.5087\u001b[0m  0.0100  3.7490\n",
      "      2                     \u001b[36m0.7754\u001b[0m        \u001b[32m0.4793\u001b[0m                     0.7600        0.5090  0.0100  3.7504\n",
      "      3                     \u001b[36m0.7816\u001b[0m        \u001b[32m0.4620\u001b[0m                     \u001b[35m0.7926\u001b[0m        \u001b[31m0.4480\u001b[0m  0.0098  3.7501\n",
      "      4                     \u001b[36m0.7918\u001b[0m        \u001b[32m0.4499\u001b[0m                     0.7647        0.4810  0.0096  3.7518\n",
      "      5                     0.7883        0.4526                     0.7897        0.4599  0.0093  3.7501\n",
      "      6                     \u001b[36m0.7948\u001b[0m        \u001b[32m0.4445\u001b[0m                     0.7609        0.4963  0.0090  3.7510\n",
      "      7                     \u001b[36m0.7955\u001b[0m        0.4446                     0.7534        0.5128  0.0085  3.7525\n",
      "      8                     \u001b[36m0.8017\u001b[0m        \u001b[32m0.4357\u001b[0m                     0.7813        0.4649  0.0080  3.7535\n",
      "      9                     0.7984        0.4357                     0.7923        0.4681  0.0075  3.7522\n",
      "     10                     \u001b[36m0.8040\u001b[0m        \u001b[32m0.4313\u001b[0m                     0.7161        0.5804  0.0069  3.7526\n",
      "     11                     \u001b[36m0.8096\u001b[0m        \u001b[32m0.4238\u001b[0m                     0.7716        0.4677  0.0063  3.7526\n",
      "     12                     0.8084        \u001b[32m0.4222\u001b[0m                     0.6523        0.6503  0.0057  3.7521\n",
      "     13                     0.8084        \u001b[32m0.4202\u001b[0m                     0.7016        0.5542  0.0050  3.7548\n",
      "     14                     \u001b[36m0.8112\u001b[0m        \u001b[32m0.4175\u001b[0m                     0.7527        0.5096  0.0043  3.7530\n",
      "     15                     \u001b[36m0.8149\u001b[0m        \u001b[32m0.4092\u001b[0m                     0.5016        1.5385  0.0037  3.7521\n",
      "     16                     0.8141        \u001b[32m0.4066\u001b[0m                     0.7883        0.4521  0.0031  3.7526\n",
      "     17                     \u001b[36m0.8178\u001b[0m        \u001b[32m0.4027\u001b[0m                     0.7229        0.5589  0.0025  3.7533\n",
      "     18                     \u001b[36m0.8188\u001b[0m        \u001b[32m0.3984\u001b[0m                     0.7572        0.4995  0.0020  3.7534\n",
      "     19                     0.8183        \u001b[32m0.3970\u001b[0m                     \u001b[35m0.7969\u001b[0m        \u001b[31m0.4453\u001b[0m  0.0015  3.7512\n",
      "     20                     \u001b[36m0.8255\u001b[0m        \u001b[32m0.3902\u001b[0m                     0.7572        0.5102  0.0010  3.7533\n",
      "     21                     0.8249        \u001b[32m0.3860\u001b[0m                     \u001b[35m0.8026\u001b[0m        \u001b[31m0.4236\u001b[0m  0.0007  3.7524\n",
      "     22                     0.8250        \u001b[32m0.3859\u001b[0m                     \u001b[35m0.8081\u001b[0m        0.4263  0.0004  3.7535\n",
      "     23                     \u001b[36m0.8272\u001b[0m        \u001b[32m0.3834\u001b[0m                     \u001b[35m0.8127\u001b[0m        \u001b[31m0.4157\u001b[0m  0.0002  3.7538\n",
      "     24                     \u001b[36m0.8275\u001b[0m        0.3840                     \u001b[35m0.8134\u001b[0m        \u001b[31m0.4117\u001b[0m  0.0000  3.7518\n",
      "     25                     \u001b[36m0.8277\u001b[0m        0.3847                     \u001b[35m0.8155\u001b[0m        \u001b[31m0.4113\u001b[0m  0.0000  3.7531\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7175\u001b[0m        \u001b[32m0.5567\u001b[0m                     \u001b[35m0.7885\u001b[0m        \u001b[31m0.4570\u001b[0m  0.0100  3.7478\n",
      "      2                     \u001b[36m0.7698\u001b[0m        \u001b[32m0.4818\u001b[0m                     \u001b[35m0.7915\u001b[0m        \u001b[31m0.4460\u001b[0m  0.0100  3.7493\n",
      "      3                     \u001b[36m0.7797\u001b[0m        \u001b[32m0.4726\u001b[0m                     \u001b[35m0.8033\u001b[0m        \u001b[31m0.4387\u001b[0m  0.0098  3.7482\n",
      "      4                     \u001b[36m0.7847\u001b[0m        \u001b[32m0.4656\u001b[0m                     0.7845        0.4598  0.0096  3.7512\n",
      "      5                     \u001b[36m0.7886\u001b[0m        \u001b[32m0.4555\u001b[0m                     0.7780        0.4757  0.0093  3.7499\n",
      "      6                     0.7852        0.4602                     0.8029        \u001b[31m0.4379\u001b[0m  0.0090  3.7500\n",
      "      7                     \u001b[36m0.7962\u001b[0m        \u001b[32m0.4496\u001b[0m                     0.7715        0.4811  0.0085  3.7503\n",
      "      8                     0.7941        \u001b[32m0.4465\u001b[0m                     0.7896        0.4718  0.0080  3.7519\n",
      "      9                     0.7940        \u001b[32m0.4459\u001b[0m                     0.5924        0.6795  0.0075  3.7504\n",
      "     10                     \u001b[36m0.7995\u001b[0m        \u001b[32m0.4368\u001b[0m                     0.7062        0.5407  0.0069  3.7495\n",
      "     11                     0.7972        \u001b[32m0.4357\u001b[0m                     0.7732        0.4974  0.0063  3.7512\n",
      "     12                     \u001b[36m0.8043\u001b[0m        \u001b[32m0.4306\u001b[0m                     0.7900        0.4633  0.0057  3.7604\n",
      "     13                     0.8040        \u001b[32m0.4289\u001b[0m                     0.7024        0.5819  0.0050  3.7513\n",
      "     14                     \u001b[36m0.8127\u001b[0m        \u001b[32m0.4225\u001b[0m                     0.6301        0.6599  0.0043  3.7525\n",
      "     15                     0.8106        \u001b[32m0.4221\u001b[0m                     0.7939        0.4512  0.0037  3.7493\n",
      "     16                     0.8105        \u001b[32m0.4152\u001b[0m                     0.7670        0.5024  0.0031  3.7559\n",
      "     17                     \u001b[36m0.8192\u001b[0m        \u001b[32m0.4101\u001b[0m                     0.7983        \u001b[31m0.4265\u001b[0m  0.0025  3.7525\n",
      "     18                     0.8143        \u001b[32m0.4071\u001b[0m                     0.7515        0.4977  0.0020  3.7524\n",
      "     19                     \u001b[36m0.8207\u001b[0m        \u001b[32m0.4041\u001b[0m                     0.7965        0.4380  0.0015  3.7517\n",
      "     20                     0.8204        \u001b[32m0.4024\u001b[0m                     \u001b[35m0.8098\u001b[0m        \u001b[31m0.4249\u001b[0m  0.0010  3.7532\n",
      "     21                     0.8201        \u001b[32m0.3960\u001b[0m                     \u001b[35m0.8182\u001b[0m        \u001b[31m0.4048\u001b[0m  0.0007  3.7518\n",
      "     22                     \u001b[36m0.8232\u001b[0m        \u001b[32m0.3943\u001b[0m                     \u001b[35m0.8289\u001b[0m        \u001b[31m0.3928\u001b[0m  0.0004  3.7517\n",
      "     23                     \u001b[36m0.8247\u001b[0m        \u001b[32m0.3914\u001b[0m                     0.8277        \u001b[31m0.3921\u001b[0m  0.0002  3.7550\n",
      "     24                     \u001b[36m0.8259\u001b[0m        0.3930                     \u001b[35m0.8299\u001b[0m        \u001b[31m0.3899\u001b[0m  0.0000  3.7521\n",
      "     25                     0.8229        0.3925                     \u001b[35m0.8305\u001b[0m        \u001b[31m0.3898\u001b[0m  0.0000  3.7519\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7191\u001b[0m        \u001b[32m0.5573\u001b[0m                     \u001b[35m0.7802\u001b[0m        \u001b[31m0.4736\u001b[0m  0.0100  3.7470\n",
      "      2                     \u001b[36m0.7760\u001b[0m        \u001b[32m0.4754\u001b[0m                     \u001b[35m0.7805\u001b[0m        \u001b[31m0.4622\u001b[0m  0.0100  3.7506\n",
      "      3                     \u001b[36m0.7844\u001b[0m        \u001b[32m0.4675\u001b[0m                     \u001b[35m0.7838\u001b[0m        \u001b[31m0.4569\u001b[0m  0.0098  3.7508\n",
      "      4                     \u001b[36m0.7920\u001b[0m        \u001b[32m0.4588\u001b[0m                     0.7818        0.4656  0.0096  3.7487\n",
      "      5                     0.7880        \u001b[32m0.4525\u001b[0m                     0.7630        0.4845  0.0093  3.7498\n",
      "      6                     \u001b[36m0.7922\u001b[0m        \u001b[32m0.4508\u001b[0m                     \u001b[35m0.7839\u001b[0m        0.4600  0.0090  3.7499\n",
      "      7                     \u001b[36m0.7934\u001b[0m        \u001b[32m0.4473\u001b[0m                     0.7574        0.5066  0.0085  3.7509\n",
      "      8                     \u001b[36m0.7968\u001b[0m        \u001b[32m0.4415\u001b[0m                     \u001b[35m0.7974\u001b[0m        \u001b[31m0.4481\u001b[0m  0.0080  3.7508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.7975\u001b[0m        \u001b[32m0.4380\u001b[0m                     0.7933        0.4558  0.0075  3.7493\n",
      "     10                     \u001b[36m0.8038\u001b[0m        \u001b[32m0.4356\u001b[0m                     0.7940        0.4527  0.0069  3.7507\n",
      "     11                     \u001b[36m0.8088\u001b[0m        \u001b[32m0.4265\u001b[0m                     0.7822        0.4824  0.0063  3.7528\n",
      "     12                     0.8042        0.4277                     0.7729        0.4927  0.0057  3.7525\n",
      "     13                     \u001b[36m0.8134\u001b[0m        \u001b[32m0.4182\u001b[0m                     0.7519        0.5163  0.0050  3.7513\n",
      "     14                     0.8107        0.4197                     0.6618        0.6546  0.0043  3.7514\n",
      "     15                     \u001b[36m0.8176\u001b[0m        \u001b[32m0.4089\u001b[0m                     0.7450        0.4988  0.0037  3.7524\n",
      "     16                     \u001b[36m0.8201\u001b[0m        \u001b[32m0.4076\u001b[0m                     0.7078        0.5759  0.0031  3.7518\n",
      "     17                     0.8187        \u001b[32m0.4076\u001b[0m                     0.7457        0.5037  0.0025  3.7504\n",
      "     18                     0.8199        \u001b[32m0.4005\u001b[0m                     0.7639        0.4815  0.0020  3.7512\n",
      "     19                     \u001b[36m0.8243\u001b[0m        \u001b[32m0.3972\u001b[0m                     0.7696        0.4787  0.0015  3.7542\n",
      "     20                     0.8228        \u001b[32m0.3942\u001b[0m                     0.7798        0.4687  0.0010  3.7514\n",
      "     21                     0.8242        \u001b[32m0.3930\u001b[0m                     \u001b[35m0.8150\u001b[0m        \u001b[31m0.4032\u001b[0m  0.0007  3.7562\n",
      "     22                     \u001b[36m0.8271\u001b[0m        \u001b[32m0.3863\u001b[0m                     \u001b[35m0.8182\u001b[0m        0.4034  0.0004  3.7520\n",
      "     23                     \u001b[36m0.8306\u001b[0m        0.3867                     0.8181        \u001b[31m0.4019\u001b[0m  0.0002  3.7534\n",
      "     24                     \u001b[36m0.8337\u001b[0m        \u001b[32m0.3813\u001b[0m                     0.8176        \u001b[31m0.3996\u001b[0m  0.0000  3.7540\n",
      "     25                     0.8328        0.3843                     \u001b[35m0.8191\u001b[0m        0.3998  0.0000  3.7540\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.5444\u001b[0m                     \u001b[35m0.7728\u001b[0m        \u001b[31m0.4787\u001b[0m  0.0100  3.7485\n",
      "      2                     \u001b[36m0.7852\u001b[0m        \u001b[32m0.4693\u001b[0m                     \u001b[35m0.7820\u001b[0m        \u001b[31m0.4650\u001b[0m  0.0100  3.7538\n",
      "      3                     \u001b[36m0.7882\u001b[0m        \u001b[32m0.4573\u001b[0m                     0.7174        0.5851  0.0098  3.7496\n",
      "      4                     \u001b[36m0.7938\u001b[0m        \u001b[32m0.4527\u001b[0m                     \u001b[35m0.7861\u001b[0m        \u001b[31m0.4614\u001b[0m  0.0096  3.7508\n",
      "      5                     0.7916        \u001b[32m0.4479\u001b[0m                     \u001b[35m0.7976\u001b[0m        \u001b[31m0.4448\u001b[0m  0.0093  3.7506\n",
      "      6                     \u001b[36m0.7975\u001b[0m        \u001b[32m0.4429\u001b[0m                     0.7217        0.5617  0.0090  3.7511\n",
      "      7                     \u001b[36m0.7990\u001b[0m        \u001b[32m0.4403\u001b[0m                     0.7755        0.4794  0.0085  3.7498\n",
      "      8                     0.7977        0.4405                     0.7476        0.5209  0.0080  3.7504\n",
      "      9                     \u001b[36m0.7998\u001b[0m        \u001b[32m0.4327\u001b[0m                     0.7587        0.4829  0.0075  3.7525\n",
      "     10                     \u001b[36m0.8052\u001b[0m        \u001b[32m0.4323\u001b[0m                     0.6949        0.5661  0.0069  3.7529\n",
      "     11                     \u001b[36m0.8058\u001b[0m        \u001b[32m0.4260\u001b[0m                     0.7752        0.4829  0.0063  3.7517\n",
      "     12                     \u001b[36m0.8090\u001b[0m        \u001b[32m0.4216\u001b[0m                     0.7782        0.4756  0.0057  3.7529\n",
      "     13                     \u001b[36m0.8122\u001b[0m        \u001b[32m0.4193\u001b[0m                     0.6732        0.6377  0.0050  3.7509\n",
      "     14                     0.8109        \u001b[32m0.4165\u001b[0m                     0.7724        0.4830  0.0043  3.7515\n",
      "     15                     \u001b[36m0.8125\u001b[0m        \u001b[32m0.4121\u001b[0m                     \u001b[35m0.8161\u001b[0m        \u001b[31m0.4232\u001b[0m  0.0037  3.7512\n",
      "     16                     \u001b[36m0.8158\u001b[0m        \u001b[32m0.4116\u001b[0m                     0.7462        0.5125  0.0031  3.7510\n",
      "     17                     0.8139        \u001b[32m0.4081\u001b[0m                     0.7294        0.5279  0.0025  3.7525\n",
      "     18                     \u001b[36m0.8182\u001b[0m        \u001b[32m0.3988\u001b[0m                     0.7892        0.4571  0.0020  3.7528\n",
      "     19                     \u001b[36m0.8200\u001b[0m        \u001b[32m0.3979\u001b[0m                     0.7954        0.4367  0.0015  3.7522\n",
      "     20                     \u001b[36m0.8237\u001b[0m        \u001b[32m0.3932\u001b[0m                     0.8073        \u001b[31m0.4225\u001b[0m  0.0010  3.7525\n",
      "     21                     0.8236        \u001b[32m0.3906\u001b[0m                     0.7946        0.4515  0.0007  3.7525\n",
      "     22                     \u001b[36m0.8291\u001b[0m        \u001b[32m0.3872\u001b[0m                     0.8138        \u001b[31m0.4072\u001b[0m  0.0004  3.7515\n",
      "     23                     0.8284        \u001b[32m0.3856\u001b[0m                     \u001b[35m0.8219\u001b[0m        \u001b[31m0.3997\u001b[0m  0.0002  3.7518\n",
      "     24                     0.8257        0.3866                     0.8202        \u001b[31m0.3984\u001b[0m  0.0000  3.7536\n",
      "     25                     0.8278        \u001b[32m0.3850\u001b[0m                     0.8218        \u001b[31m0.3977\u001b[0m  0.0000  3.7521\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.5529\u001b[0m                     \u001b[35m0.7586\u001b[0m        \u001b[31m0.5237\u001b[0m  0.0100  3.7472\n",
      "      2                     \u001b[36m0.7785\u001b[0m        \u001b[32m0.4703\u001b[0m                     \u001b[35m0.7936\u001b[0m        \u001b[31m0.4578\u001b[0m  0.0100  3.7489\n",
      "      3                     \u001b[36m0.7834\u001b[0m        \u001b[32m0.4639\u001b[0m                     0.7855        0.4765  0.0098  3.7542\n",
      "      4                     \u001b[36m0.7897\u001b[0m        \u001b[32m0.4548\u001b[0m                     0.7882        0.4653  0.0096  3.7511\n",
      "      5                     0.7891        \u001b[32m0.4518\u001b[0m                     0.7450        0.5146  0.0093  3.7501\n",
      "      6                     \u001b[36m0.7931\u001b[0m        \u001b[32m0.4470\u001b[0m                     0.7746        0.4772  0.0090  3.7507\n",
      "      7                     0.7899        0.4501                     0.7888        \u001b[31m0.4512\u001b[0m  0.0085  3.7507\n",
      "      8                     \u001b[36m0.7999\u001b[0m        \u001b[32m0.4385\u001b[0m                     0.7807        0.4866  0.0080  3.7530\n",
      "      9                     0.7968        0.4402                     0.7782        0.4793  0.0075  3.7517\n",
      "     10                     \u001b[36m0.8002\u001b[0m        \u001b[32m0.4330\u001b[0m                     0.7932        0.4670  0.0069  3.7508\n",
      "     11                     \u001b[36m0.8052\u001b[0m        \u001b[32m0.4310\u001b[0m                     0.7729        0.4873  0.0063  3.7534\n",
      "     12                     0.8046        \u001b[32m0.4309\u001b[0m                     0.7805        0.4762  0.0057  3.7528\n",
      "     13                     \u001b[36m0.8078\u001b[0m        \u001b[32m0.4257\u001b[0m                     0.7520        0.5201  0.0050  3.7513\n",
      "     14                     \u001b[36m0.8123\u001b[0m        \u001b[32m0.4158\u001b[0m                     0.6493        0.6768  0.0043  3.7515\n",
      "     15                     0.8060        0.4186                     \u001b[35m0.7947\u001b[0m        \u001b[31m0.4429\u001b[0m  0.0037  3.7507\n",
      "     16                     \u001b[36m0.8165\u001b[0m        \u001b[32m0.4130\u001b[0m                     0.7755        0.4804  0.0031  3.7516\n",
      "     17                     \u001b[36m0.8187\u001b[0m        \u001b[32m0.4096\u001b[0m                     \u001b[35m0.7979\u001b[0m        \u001b[31m0.4389\u001b[0m  0.0025  3.7524\n",
      "     18                     0.8167        \u001b[32m0.4073\u001b[0m                     \u001b[35m0.8010\u001b[0m        \u001b[31m0.4365\u001b[0m  0.0020  3.7531\n",
      "     19                     \u001b[36m0.8206\u001b[0m        \u001b[32m0.4004\u001b[0m                     0.7888        0.4520  0.0015  3.7517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                     \u001b[36m0.8215\u001b[0m        \u001b[32m0.4002\u001b[0m                     0.7960        \u001b[31m0.4284\u001b[0m  0.0010  3.7524\n",
      "     21                     \u001b[36m0.8246\u001b[0m        \u001b[32m0.3979\u001b[0m                     \u001b[35m0.8022\u001b[0m        \u001b[31m0.4245\u001b[0m  0.0007  3.7522\n",
      "     22                     0.8236        \u001b[32m0.3934\u001b[0m                     \u001b[35m0.8080\u001b[0m        \u001b[31m0.4161\u001b[0m  0.0004  3.7525\n",
      "     23                     0.8245        \u001b[32m0.3919\u001b[0m                     0.8069        \u001b[31m0.4141\u001b[0m  0.0002  3.7531\n",
      "     24                     0.8244        0.3924                     0.8077        \u001b[31m0.4115\u001b[0m  0.0000  3.7541\n",
      "     25                     \u001b[36m0.8270\u001b[0m        0.3922                     \u001b[35m0.8090\u001b[0m        \u001b[31m0.4113\u001b[0m  0.0000  3.7524\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7083\u001b[0m        \u001b[32m0.5688\u001b[0m                     \u001b[35m0.7523\u001b[0m        \u001b[31m0.5030\u001b[0m  0.0100  3.7475\n",
      "      2                     \u001b[36m0.7722\u001b[0m        \u001b[32m0.4814\u001b[0m                     \u001b[35m0.7867\u001b[0m        \u001b[31m0.4648\u001b[0m  0.0100  3.7501\n",
      "      3                     \u001b[36m0.7831\u001b[0m        \u001b[32m0.4626\u001b[0m                     0.7858        \u001b[31m0.4566\u001b[0m  0.0098  3.7497\n",
      "      4                     \u001b[36m0.7870\u001b[0m        \u001b[32m0.4585\u001b[0m                     0.7154        0.5560  0.0096  3.7488\n",
      "      5                     \u001b[36m0.7890\u001b[0m        \u001b[32m0.4519\u001b[0m                     \u001b[35m0.7993\u001b[0m        \u001b[31m0.4369\u001b[0m  0.0093  3.7495\n",
      "      6                     \u001b[36m0.7924\u001b[0m        \u001b[32m0.4511\u001b[0m                     0.7802        0.4865  0.0090  3.7500\n",
      "      7                     \u001b[36m0.7944\u001b[0m        \u001b[32m0.4456\u001b[0m                     0.6664        0.6148  0.0085  3.7499\n",
      "      8                     \u001b[36m0.7955\u001b[0m        \u001b[32m0.4438\u001b[0m                     0.7904        0.4941  0.0080  3.7498\n",
      "      9                     0.7945        \u001b[32m0.4416\u001b[0m                     0.7736        0.4820  0.0075  3.7540\n",
      "     10                     \u001b[36m0.8032\u001b[0m        \u001b[32m0.4356\u001b[0m                     0.7949        0.4474  0.0069  3.7511\n",
      "     11                     0.8012        \u001b[32m0.4326\u001b[0m                     0.7752        0.4830  0.0063  3.7505\n",
      "     12                     0.8013        \u001b[32m0.4271\u001b[0m                     0.7882        0.4585  0.0057  3.7518\n",
      "     13                     0.8027        0.4284                     0.7573        0.5018  0.0050  3.7521\n",
      "     14                     \u001b[36m0.8069\u001b[0m        \u001b[32m0.4181\u001b[0m                     0.7985        0.4545  0.0043  3.7496\n",
      "     15                     \u001b[36m0.8154\u001b[0m        \u001b[32m0.4134\u001b[0m                     0.7950        0.4629  0.0037  3.7524\n",
      "     16                     0.8131        \u001b[32m0.4133\u001b[0m                     0.7635        0.4942  0.0031  3.7526\n",
      "     17                     \u001b[36m0.8166\u001b[0m        \u001b[32m0.4051\u001b[0m                     0.7247        0.5564  0.0025  3.7505\n",
      "     18                     \u001b[36m0.8213\u001b[0m        \u001b[32m0.4019\u001b[0m                     0.7931        0.4468  0.0020  3.7520\n",
      "     19                     0.8205        \u001b[32m0.3998\u001b[0m                     \u001b[35m0.8095\u001b[0m        \u001b[31m0.4171\u001b[0m  0.0015  3.7523\n",
      "     20                     \u001b[36m0.8233\u001b[0m        \u001b[32m0.3973\u001b[0m                     \u001b[35m0.8191\u001b[0m        \u001b[31m0.4122\u001b[0m  0.0010  3.7513\n",
      "     21                     \u001b[36m0.8248\u001b[0m        \u001b[32m0.3950\u001b[0m                     \u001b[35m0.8245\u001b[0m        \u001b[31m0.4024\u001b[0m  0.0007  3.7517\n",
      "     22                     0.8241        \u001b[32m0.3930\u001b[0m                     0.8223        \u001b[31m0.4005\u001b[0m  0.0004  3.7523\n",
      "     23                     0.8234        0.3931                     0.8205        0.4016  0.0002  3.7528\n",
      "     24                     \u001b[36m0.8261\u001b[0m        \u001b[32m0.3883\u001b[0m                     0.8228        \u001b[31m0.3992\u001b[0m  0.0000  3.7521\n",
      "     25                     0.8253        0.3916                     0.8234        0.3993  0.0000  3.7535\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7197\u001b[0m        \u001b[32m0.5617\u001b[0m                     \u001b[35m0.7705\u001b[0m        \u001b[31m0.4921\u001b[0m  0.0100  3.7474\n",
      "      2                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4690\u001b[0m                     \u001b[35m0.7890\u001b[0m        \u001b[31m0.4582\u001b[0m  0.0100  3.7520\n",
      "      3                     \u001b[36m0.7892\u001b[0m        \u001b[32m0.4563\u001b[0m                     0.7735        0.4817  0.0098  3.7516\n",
      "      4                     \u001b[36m0.7939\u001b[0m        \u001b[32m0.4483\u001b[0m                     0.7821        \u001b[31m0.4581\u001b[0m  0.0096  3.7500\n",
      "      5                     \u001b[36m0.7940\u001b[0m        0.4493                     0.7840        0.4672  0.0093  3.7506\n",
      "      6                     \u001b[36m0.7951\u001b[0m        0.4497                     0.7761        0.4633  0.0090  3.7510\n",
      "      7                     \u001b[36m0.7982\u001b[0m        \u001b[32m0.4399\u001b[0m                     0.7787        0.4707  0.0085  3.7495\n",
      "      8                     \u001b[36m0.7983\u001b[0m        \u001b[32m0.4375\u001b[0m                     0.7482        0.5065  0.0080  3.7506\n",
      "      9                     \u001b[36m0.8055\u001b[0m        \u001b[32m0.4328\u001b[0m                     0.7539        0.5124  0.0075  3.7511\n",
      "     10                     0.8020        \u001b[32m0.4324\u001b[0m                     0.7575        0.4895  0.0069  3.7517\n",
      "     11                     \u001b[36m0.8062\u001b[0m        \u001b[32m0.4281\u001b[0m                     0.7038        0.5579  0.0063  3.7505\n",
      "     12                     \u001b[36m0.8099\u001b[0m        \u001b[32m0.4253\u001b[0m                     0.7372        0.5354  0.0057  3.7498\n",
      "     13                     0.8097        \u001b[32m0.4248\u001b[0m                     0.7602        0.4875  0.0050  3.7507\n",
      "     14                     \u001b[36m0.8146\u001b[0m        \u001b[32m0.4187\u001b[0m                     \u001b[35m0.7965\u001b[0m        \u001b[31m0.4464\u001b[0m  0.0043  3.7513\n",
      "     15                     \u001b[36m0.8162\u001b[0m        \u001b[32m0.4116\u001b[0m                     0.7959        \u001b[31m0.4451\u001b[0m  0.0037  3.7517\n",
      "     16                     0.8158        \u001b[32m0.4103\u001b[0m                     0.7410        0.5120  0.0031  3.7548\n",
      "     17                     \u001b[36m0.8195\u001b[0m        \u001b[32m0.4074\u001b[0m                     0.7942        0.4468  0.0025  3.7528\n",
      "     18                     0.8183        \u001b[32m0.4015\u001b[0m                     0.7929        0.4570  0.0020  3.7517\n",
      "     19                     \u001b[36m0.8240\u001b[0m        \u001b[32m0.3972\u001b[0m                     \u001b[35m0.8008\u001b[0m        \u001b[31m0.4246\u001b[0m  0.0015  3.7513\n",
      "     20                     \u001b[36m0.8240\u001b[0m        \u001b[32m0.3939\u001b[0m                     \u001b[35m0.8135\u001b[0m        \u001b[31m0.4136\u001b[0m  0.0010  3.7517\n",
      "     21                     \u001b[36m0.8268\u001b[0m        \u001b[32m0.3899\u001b[0m                     0.7921        0.4340  0.0007  3.7503\n",
      "     22                     \u001b[36m0.8291\u001b[0m        \u001b[32m0.3875\u001b[0m                     0.7985        0.4290  0.0004  3.7534\n",
      "     23                     0.8241        0.3886                     0.8127        \u001b[31m0.4079\u001b[0m  0.0002  3.7521\n",
      "     24                     0.8259        0.3890                     \u001b[35m0.8139\u001b[0m        \u001b[31m0.4074\u001b[0m  0.0000  3.7522\n",
      "     25                     \u001b[36m0.8306\u001b[0m        0.3886                     0.8138        \u001b[31m0.4072\u001b[0m  0.0000  3.7938\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7221\u001b[0m        \u001b[32m0.5500\u001b[0m                     \u001b[35m0.7686\u001b[0m        \u001b[31m0.5160\u001b[0m  0.0100  3.7743\n",
      "      2                     \u001b[36m0.7779\u001b[0m        \u001b[32m0.4737\u001b[0m                     0.7427        0.5653  0.0100  3.7469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3                     \u001b[36m0.7891\u001b[0m        \u001b[32m0.4628\u001b[0m                     \u001b[35m0.7895\u001b[0m        \u001b[31m0.4592\u001b[0m  0.0098  4.0741\n",
      "      4                     \u001b[36m0.7924\u001b[0m        \u001b[32m0.4509\u001b[0m                     \u001b[35m0.7909\u001b[0m        \u001b[31m0.4570\u001b[0m  0.0096  4.2469\n",
      "      5                     0.7908        0.4539                     0.7663        0.4892  0.0093  4.0429\n",
      "      6                     \u001b[36m0.7925\u001b[0m        \u001b[32m0.4493\u001b[0m                     0.7831        0.4680  0.0090  4.0395\n",
      "      7                     \u001b[36m0.7967\u001b[0m        \u001b[32m0.4434\u001b[0m                     0.7072        0.5883  0.0085  4.0956\n",
      "      8                     \u001b[36m0.7971\u001b[0m        \u001b[32m0.4404\u001b[0m                     \u001b[35m0.8040\u001b[0m        \u001b[31m0.4424\u001b[0m  0.0080  4.0060\n",
      "      9                     \u001b[36m0.7985\u001b[0m        \u001b[32m0.4385\u001b[0m                     0.7417        0.5245  0.0075  3.9925\n",
      "     10                     \u001b[36m0.8024\u001b[0m        \u001b[32m0.4351\u001b[0m                     0.7350        0.5300  0.0069  3.8174\n",
      "     11                     \u001b[36m0.8052\u001b[0m        \u001b[32m0.4287\u001b[0m                     0.7790        0.4688  0.0063  3.8153\n",
      "     12                     0.8050        \u001b[32m0.4276\u001b[0m                     0.7013        0.5792  0.0057  3.8214\n",
      "     13                     \u001b[36m0.8086\u001b[0m        \u001b[32m0.4216\u001b[0m                     0.6730        0.6250  0.0050  3.8148\n",
      "     14                     \u001b[36m0.8140\u001b[0m        \u001b[32m0.4127\u001b[0m                     0.6480        0.6494  0.0043  3.8161\n",
      "     15                     \u001b[36m0.8164\u001b[0m        0.4163                     0.7936        0.4497  0.0037  3.8443\n",
      "     16                     0.8150        \u001b[32m0.4105\u001b[0m                     0.6536        0.6736  0.0031  3.8405\n",
      "     17                     0.8120        0.4117                     0.7353        0.5253  0.0025  3.8164\n",
      "     18                     \u001b[36m0.8175\u001b[0m        \u001b[32m0.4055\u001b[0m                     0.7284        0.5521  0.0020  3.8163\n",
      "     19                     \u001b[36m0.8189\u001b[0m        \u001b[32m0.4002\u001b[0m                     0.7981        \u001b[31m0.4420\u001b[0m  0.0015  3.8151\n",
      "     20                     \u001b[36m0.8224\u001b[0m        \u001b[32m0.3952\u001b[0m                     0.7730        0.4889  0.0010  3.8152\n",
      "     21                     \u001b[36m0.8258\u001b[0m        \u001b[32m0.3941\u001b[0m                     0.7850        0.4489  0.0007  3.8163\n",
      "     22                     0.8240        \u001b[32m0.3932\u001b[0m                     \u001b[35m0.8107\u001b[0m        \u001b[31m0.4214\u001b[0m  0.0004  3.8158\n",
      "     23                     0.8251        \u001b[32m0.3928\u001b[0m                     0.8082        \u001b[31m0.4168\u001b[0m  0.0002  3.8167\n",
      "     24                     0.8251        \u001b[32m0.3909\u001b[0m                     0.8084        \u001b[31m0.4151\u001b[0m  0.0000  3.8164\n",
      "     25                     \u001b[36m0.8272\u001b[0m        \u001b[32m0.3889\u001b[0m                     0.8093        0.4155  0.0000  3.8170\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7016\u001b[0m        \u001b[32m0.5736\u001b[0m                     \u001b[35m0.7740\u001b[0m        \u001b[31m0.4877\u001b[0m  0.0100  3.8104\n",
      "      2                     \u001b[36m0.7743\u001b[0m        \u001b[32m0.4849\u001b[0m                     \u001b[35m0.7892\u001b[0m        \u001b[31m0.4492\u001b[0m  0.0100  3.8164\n",
      "      3                     \u001b[36m0.7805\u001b[0m        \u001b[32m0.4672\u001b[0m                     0.7643        0.4878  0.0098  3.8172\n",
      "      4                     \u001b[36m0.7828\u001b[0m        \u001b[32m0.4653\u001b[0m                     0.7735        0.4722  0.0096  3.8141\n",
      "      5                     \u001b[36m0.7918\u001b[0m        \u001b[32m0.4581\u001b[0m                     0.7612        0.5006  0.0093  3.8156\n",
      "      6                     0.7913        \u001b[32m0.4543\u001b[0m                     \u001b[35m0.7944\u001b[0m        0.4612  0.0090  3.8137\n",
      "      7                     \u001b[36m0.7978\u001b[0m        \u001b[32m0.4440\u001b[0m                     \u001b[35m0.7962\u001b[0m        \u001b[31m0.4431\u001b[0m  0.0085  3.8154\n",
      "      8                     0.7952        0.4450                     0.6824        0.5831  0.0080  3.8138\n",
      "      9                     \u001b[36m0.7997\u001b[0m        \u001b[32m0.4378\u001b[0m                     \u001b[35m0.8001\u001b[0m        0.4452  0.0075  3.8137\n",
      "     10                     \u001b[36m0.8048\u001b[0m        \u001b[32m0.4360\u001b[0m                     0.7412        0.5348  0.0069  3.8154\n",
      "     11                     0.8023        \u001b[32m0.4323\u001b[0m                     0.7601        0.4965  0.0063  3.8165\n",
      "     12                     \u001b[36m0.8066\u001b[0m        \u001b[32m0.4281\u001b[0m                     0.5815        0.7003  0.0057  3.8143\n",
      "     13                     \u001b[36m0.8111\u001b[0m        \u001b[32m0.4212\u001b[0m                     0.6294        0.7021  0.0050  3.8460\n",
      "     14                     \u001b[36m0.8116\u001b[0m        \u001b[32m0.4196\u001b[0m                     0.5371        1.0241  0.0043  3.8157\n",
      "     15                     \u001b[36m0.8156\u001b[0m        \u001b[32m0.4121\u001b[0m                     0.7810        0.4900  0.0037  3.8140\n",
      "     16                     \u001b[36m0.8186\u001b[0m        \u001b[32m0.4114\u001b[0m                     \u001b[35m0.8054\u001b[0m        \u001b[31m0.4325\u001b[0m  0.0031  3.8157\n",
      "     17                     0.8146        \u001b[32m0.4061\u001b[0m                     0.7806        0.4718  0.0025  3.8156\n",
      "     18                     \u001b[36m0.8186\u001b[0m        \u001b[32m0.4043\u001b[0m                     0.7041        0.5903  0.0020  3.8213\n",
      "     19                     \u001b[36m0.8245\u001b[0m        \u001b[32m0.3998\u001b[0m                     0.5998        0.7202  0.0015  3.8165\n",
      "     20                     0.8228        \u001b[32m0.3981\u001b[0m                     0.7978        \u001b[31m0.4246\u001b[0m  0.0010  3.8156\n",
      "     21                     \u001b[36m0.8268\u001b[0m        \u001b[32m0.3952\u001b[0m                     \u001b[35m0.8110\u001b[0m        0.4298  0.0007  3.8316\n",
      "     22                     0.8258        \u001b[32m0.3945\u001b[0m                     0.7999        0.4262  0.0004  3.8391\n",
      "     23                     0.8260        \u001b[32m0.3928\u001b[0m                     \u001b[35m0.8152\u001b[0m        \u001b[31m0.4050\u001b[0m  0.0002  3.8164\n",
      "     24                     \u001b[36m0.8270\u001b[0m        \u001b[32m0.3896\u001b[0m                     0.8143        \u001b[31m0.4028\u001b[0m  0.0000  3.8161\n",
      "     25                     0.8270        \u001b[32m0.3893\u001b[0m                     0.8146        0.4028  0.0000  3.8171\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7217\u001b[0m        \u001b[32m0.5555\u001b[0m                     \u001b[35m0.7625\u001b[0m        \u001b[31m0.5030\u001b[0m  0.0100  3.8111\n",
      "      2                     \u001b[36m0.7644\u001b[0m        \u001b[32m0.4878\u001b[0m                     \u001b[35m0.7855\u001b[0m        \u001b[31m0.4688\u001b[0m  0.0100  3.8138\n",
      "      3                     \u001b[36m0.7786\u001b[0m        \u001b[32m0.4670\u001b[0m                     0.7362        0.5349  0.0098  3.8155\n",
      "      4                     \u001b[36m0.7845\u001b[0m        \u001b[32m0.4589\u001b[0m                     0.7848        \u001b[31m0.4667\u001b[0m  0.0096  3.8134\n",
      "      5                     \u001b[36m0.7917\u001b[0m        \u001b[32m0.4586\u001b[0m                     \u001b[35m0.7907\u001b[0m        0.4727  0.0093  3.8147\n",
      "      6                     \u001b[36m0.7919\u001b[0m        \u001b[32m0.4520\u001b[0m                     0.7427        0.5284  0.0090  3.8132\n",
      "      7                     \u001b[36m0.7957\u001b[0m        \u001b[32m0.4436\u001b[0m                     0.7784        0.4798  0.0085  3.8140\n",
      "      8                     0.7931        \u001b[32m0.4424\u001b[0m                     0.7101        0.5686  0.0080  3.8142\n",
      "      9                     0.7938        \u001b[32m0.4402\u001b[0m                     0.6373        0.6354  0.0075  3.8199\n",
      "     10                     \u001b[36m0.8005\u001b[0m        \u001b[32m0.4369\u001b[0m                     0.7329        0.5495  0.0069  3.8148\n",
      "     11                     \u001b[36m0.8017\u001b[0m        \u001b[32m0.4347\u001b[0m                     0.6654        0.5973  0.0063  3.8144\n",
      "     12                     \u001b[36m0.8044\u001b[0m        \u001b[32m0.4277\u001b[0m                     0.6973        0.5810  0.0057  3.8134\n",
      "     13                     \u001b[36m0.8087\u001b[0m        \u001b[32m0.4214\u001b[0m                     0.6452        0.7358  0.0050  3.8150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14                     \u001b[36m0.8109\u001b[0m        \u001b[32m0.4178\u001b[0m                     0.7441        0.5006  0.0043  3.8147\n",
      "     15                     0.8096        0.4184                     \u001b[35m0.7947\u001b[0m        \u001b[31m0.4531\u001b[0m  0.0037  3.8156\n",
      "     16                     \u001b[36m0.8145\u001b[0m        \u001b[32m0.4123\u001b[0m                     \u001b[35m0.8026\u001b[0m        \u001b[31m0.4450\u001b[0m  0.0031  3.8167\n",
      "     17                     \u001b[36m0.8149\u001b[0m        \u001b[32m0.4076\u001b[0m                     0.7544        0.5247  0.0025  3.8197\n",
      "     18                     \u001b[36m0.8205\u001b[0m        \u001b[32m0.4021\u001b[0m                     \u001b[35m0.8107\u001b[0m        \u001b[31m0.4302\u001b[0m  0.0020  3.8162\n",
      "     19                     \u001b[36m0.8224\u001b[0m        \u001b[32m0.4007\u001b[0m                     0.7603        0.5019  0.0015  3.8155\n",
      "     20                     0.8219        \u001b[32m0.3991\u001b[0m                     0.8017        0.4345  0.0010  3.8148\n",
      "     21                     0.8214        \u001b[32m0.3981\u001b[0m                     0.7990        0.4425  0.0007  3.8167\n",
      "     22                     0.8214        \u001b[32m0.3979\u001b[0m                     \u001b[35m0.8151\u001b[0m        \u001b[31m0.4209\u001b[0m  0.0004  3.8189\n",
      "     23                     \u001b[36m0.8269\u001b[0m        \u001b[32m0.3908\u001b[0m                     \u001b[35m0.8161\u001b[0m        \u001b[31m0.4140\u001b[0m  0.0002  3.8163\n",
      "     24                     0.8240        \u001b[32m0.3908\u001b[0m                     \u001b[35m0.8184\u001b[0m        \u001b[31m0.4090\u001b[0m  0.0000  3.8225\n",
      "     25                     0.8256        \u001b[32m0.3895\u001b[0m                     \u001b[35m0.8199\u001b[0m        \u001b[31m0.4083\u001b[0m  0.0000  3.8181\n"
     ]
    }
   ],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "preprocessing = \"medium\"\n",
    "model_name = \"eegnet\"\n",
    "model_folder = \"ModelComparision\"\n",
    "n_epochs = 25\n",
    "n_splits = 10\n",
    "lr = 0.01\n",
    "\n",
    "for model_name in [\"shallow\", \"deep\", \"eegnet\"]:\n",
    "    for task in [\"N170\", \"N400\", \"P3\", \"N2pc\", \"MMN\", \"ERN\", \"LRP\"]:\n",
    "        df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "        data, labels = DataLoader.create_data_labels(df)\n",
    "        Training.run_exp(data, labels, task, preprocessing, model_folder, model_name, \n",
    "                lr, n_epochs, n_splits, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed78d3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Validation Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N170</td>\n",
       "      <td>eegnet</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.787859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N400</td>\n",
       "      <td>eegnet</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.767197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3</td>\n",
       "      <td>eegnet</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.754733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N2pc</td>\n",
       "      <td>eegnet</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.670033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MMN</td>\n",
       "      <td>eegnet</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.601103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>P3</td>\n",
       "      <td>deep</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.765615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>N2pc</td>\n",
       "      <td>deep</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.683884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MMN</td>\n",
       "      <td>deep</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.604074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ERN</td>\n",
       "      <td>deep</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.906148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LRP</td>\n",
       "      <td>deep</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.812782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Task   Model Preprocessing Validation Balanced Accuracy\n",
       "0   N170  eegnet        medium                     0.787859\n",
       "1   N400  eegnet        medium                     0.767197\n",
       "2     P3  eegnet        medium                     0.754733\n",
       "3   N2pc  eegnet        medium                     0.670033\n",
       "4    MMN  eegnet        medium                     0.601103\n",
       "..   ...     ...           ...                          ...\n",
       "16    P3    deep        medium                     0.765615\n",
       "17  N2pc    deep        medium                     0.683884\n",
       "18   MMN    deep        medium                     0.604074\n",
       "19   ERN    deep        medium                     0.906148\n",
       "20   LRP    deep        medium                     0.812782\n",
       "\n",
       "[105 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Results\n",
    "results = []\n",
    "for model_name in [\"eegnet\",\"shallow\",\"deep\"]:\n",
    "    for preprocessing in [\"medium\"]:\n",
    "        for task in [\"N170\", \"N400\", \"P3\", \"N2pc\", \"MMN\", \"ERN\", \"LRP\"]:\n",
    "            df = Training.load_exp(model_folder, model_name, task, preprocessing, n_splits)\n",
    "            df[\"Model\"] = model_name\n",
    "            df[\"Preprocessing\"] = preprocessing\n",
    "            df[\"Task\"] = task\n",
    "            results.append(df.iloc[-1])\n",
    "df_results = pd.concat(results, axis=1)\n",
    "df_results = df_results.transpose().reset_index()\n",
    "# Put results in correct dataframe for seaborn plot\n",
    "df_task = df_results[[\"Task\", \"Model\", \"Preprocessing\",\"valid_balanced_accuracy\"]]\n",
    "df_task = df_task.rename(columns={\"valid_balanced_accuracy\": \"Validation Balanced Accuracy\"})\n",
    "list = []\n",
    "for i in range(3,8):\n",
    "    list.append(df_task.iloc[:,[0,1,2,i]])\n",
    "df_task = pd.concat(list, axis=0)\n",
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f610ae97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3200x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# N170 plot, no downsampling\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(8, 4), dpi=400)\n",
    "sns.barplot(x=\"Task\", y=\"Validation Balanced Accuracy\", \n",
    "            hue=\"Model\", \n",
    "            data=df_task, ci=\"sd\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421f629",
   "metadata": {},
   "source": [
    "## Equal Sample Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1153c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4893\u001b[0m        \u001b[32m1.4688\u001b[0m                     \u001b[35m0.4907\u001b[0m        \u001b[31m1.1822\u001b[0m  0.0100  1.4990\n",
      "      2                     \u001b[36m0.5201\u001b[0m        \u001b[32m0.9433\u001b[0m                     \u001b[35m0.5168\u001b[0m        \u001b[31m0.8604\u001b[0m  0.0100  0.2175\n",
      "      3                     \u001b[36m0.5444\u001b[0m        \u001b[32m0.8666\u001b[0m                     \u001b[35m0.5336\u001b[0m        \u001b[31m0.7165\u001b[0m  0.0098  0.2156\n",
      "      4                     \u001b[36m0.5645\u001b[0m        \u001b[32m0.7607\u001b[0m                     0.4963        1.1512  0.0096  0.2092\n",
      "      5                     0.5491        0.7836                     \u001b[35m0.5858\u001b[0m        \u001b[31m0.6727\u001b[0m  0.0093  0.2098\n",
      "      6                     \u001b[36m0.5757\u001b[0m        \u001b[32m0.7341\u001b[0m                     0.5746        0.6997  0.0090  0.2104\n",
      "      7                     \u001b[36m0.5790\u001b[0m        0.7600                     0.5858        0.7028  0.0085  0.2094\n",
      "      8                     0.5734        0.8029                     0.5541        0.7517  0.0080  0.2094\n",
      "      9                     0.5556        0.8635                     \u001b[35m0.6306\u001b[0m        0.6904  0.0075  0.2294\n",
      "     10                     \u001b[36m0.6168\u001b[0m        \u001b[32m0.7251\u001b[0m                     0.6138        \u001b[31m0.6494\u001b[0m  0.0069  0.2104\n",
      "     11                     0.6159        \u001b[32m0.7072\u001b[0m                     0.5765        0.7190  0.0063  0.2085\n",
      "     12                     \u001b[36m0.6537\u001b[0m        \u001b[32m0.6586\u001b[0m                     0.6007        0.6907  0.0057  0.2254\n",
      "     13                     0.6248        0.7060                     \u001b[35m0.6549\u001b[0m        \u001b[31m0.6342\u001b[0m  0.0050  0.2135\n",
      "     14                     \u001b[36m0.6570\u001b[0m        \u001b[32m0.6382\u001b[0m                     0.6343        \u001b[31m0.6307\u001b[0m  0.0043  0.2105\n",
      "     15                     \u001b[36m0.6706\u001b[0m        \u001b[32m0.6326\u001b[0m                     \u001b[35m0.6735\u001b[0m        \u001b[31m0.6081\u001b[0m  0.0037  0.2094\n",
      "     16                     \u001b[36m0.6874\u001b[0m        \u001b[32m0.6062\u001b[0m                     0.5989        0.6925  0.0031  0.2095\n",
      "     17                     0.6832        0.6092                     0.6604        0.6101  0.0025  0.2513\n",
      "     18                     \u001b[36m0.7051\u001b[0m        \u001b[32m0.5904\u001b[0m                     0.6511        0.6227  0.0020  0.2099\n",
      "     19                     \u001b[36m0.7065\u001b[0m        \u001b[32m0.5824\u001b[0m                     0.6362        0.6333  0.0015  0.2090\n",
      "     20                     \u001b[36m0.7168\u001b[0m        \u001b[32m0.5676\u001b[0m                     \u001b[35m0.6959\u001b[0m        \u001b[31m0.5882\u001b[0m  0.0010  0.2374\n",
      "     21                     \u001b[36m0.7271\u001b[0m        \u001b[32m0.5509\u001b[0m                     0.6791        0.5887  0.0007  0.2093\n",
      "     22                     0.7243        0.5559                     0.6716        0.5970  0.0004  0.2294\n",
      "     23                     0.7252        \u001b[32m0.5442\u001b[0m                     0.6810        0.5949  0.0002  0.2104\n",
      "     24                     \u001b[36m0.7318\u001b[0m        \u001b[32m0.5373\u001b[0m                     0.6716        0.5968  0.0000  0.2108\n",
      "     25                     0.7238        0.5547                     0.6754        0.5933  0.0000  0.2095\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4902\u001b[0m        \u001b[32m1.2572\u001b[0m                     \u001b[35m0.5373\u001b[0m        \u001b[31m0.7170\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.5262\u001b[0m        \u001b[32m0.9357\u001b[0m                     0.5224        0.7407  0.0100  0.2076\n",
      "      3                     0.5168        1.0024                     \u001b[35m0.5448\u001b[0m        0.7981  0.0098  0.2095\n",
      "      4                     \u001b[36m0.5327\u001b[0m        1.1272                     0.5019        1.0018  0.0096  0.2095\n",
      "      5                     \u001b[36m0.5421\u001b[0m        \u001b[32m0.8318\u001b[0m                     \u001b[35m0.5466\u001b[0m        \u001b[31m0.7067\u001b[0m  0.0093  0.2108\n",
      "      6                     \u001b[36m0.5748\u001b[0m        \u001b[32m0.7314\u001b[0m                     \u001b[35m0.6381\u001b[0m        \u001b[31m0.6502\u001b[0m  0.0090  0.2094\n",
      "      7                     \u001b[36m0.5832\u001b[0m        0.7632                     0.5690        0.8339  0.0085  0.2099\n",
      "      8                     0.5771        0.7434                     0.5821        0.7324  0.0080  0.2324\n",
      "      9                     0.5720        0.7551                     0.5634        0.8058  0.0075  0.2364\n",
      "     10                     \u001b[36m0.6079\u001b[0m        0.7506                     0.6269        \u001b[31m0.6485\u001b[0m  0.0069  0.2224\n",
      "     11                     0.5902        0.7593                     0.5877        0.7345  0.0063  0.2097\n",
      "     12                     \u001b[36m0.6519\u001b[0m        \u001b[32m0.6607\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6255\u001b[0m  0.0057  0.2104\n",
      "     13                     \u001b[36m0.6710\u001b[0m        \u001b[32m0.6233\u001b[0m                     \u001b[35m0.6679\u001b[0m        \u001b[31m0.5990\u001b[0m  0.0050  0.2095\n",
      "     14                     \u001b[36m0.6864\u001b[0m        \u001b[32m0.6168\u001b[0m                     \u001b[35m0.6698\u001b[0m        0.6450  0.0043  0.2274\n",
      "     15                     0.6757        \u001b[32m0.6081\u001b[0m                     0.6567        0.6071  0.0037  0.2114\n",
      "     16                     \u001b[36m0.7140\u001b[0m        \u001b[32m0.5746\u001b[0m                     \u001b[35m0.6903\u001b[0m        \u001b[31m0.5846\u001b[0m  0.0031  0.2094\n",
      "     17                     0.6902        0.5979                     0.6847        0.5863  0.0025  0.2394\n",
      "     18                     0.7084        \u001b[32m0.5701\u001b[0m                     0.6866        \u001b[31m0.5778\u001b[0m  0.0020  0.2322\n",
      "     19                     0.6986        0.5757                     0.6866        \u001b[31m0.5773\u001b[0m  0.0015  0.2099\n",
      "     20                     \u001b[36m0.7145\u001b[0m        \u001b[32m0.5607\u001b[0m                     0.6828        \u001b[31m0.5772\u001b[0m  0.0010  0.2134\n",
      "     21                     \u001b[36m0.7178\u001b[0m        \u001b[32m0.5512\u001b[0m                     0.6810        0.5784  0.0007  0.2104\n",
      "     22                     \u001b[36m0.7248\u001b[0m        0.5598                     0.6791        \u001b[31m0.5754\u001b[0m  0.0004  0.2100\n",
      "     23                     \u001b[36m0.7308\u001b[0m        0.5525                     \u001b[35m0.6959\u001b[0m        \u001b[31m0.5739\u001b[0m  0.0002  0.2404\n",
      "     24                     0.7178        0.5575                     0.6847        0.5740  0.0000  0.2227\n",
      "     25                     \u001b[36m0.7360\u001b[0m        \u001b[32m0.5399\u001b[0m                     0.6791        0.5744  0.0000  0.2235\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4804\u001b[0m        \u001b[32m1.2555\u001b[0m                     \u001b[35m0.5243\u001b[0m        \u001b[31m0.8070\u001b[0m  0.0100  0.2374\n",
      "      2                     \u001b[36m0.5215\u001b[0m        \u001b[32m0.9825\u001b[0m                     \u001b[35m0.5317\u001b[0m        0.8888  0.0100  0.2354\n",
      "      3                     0.5159        1.0690                     0.5149        0.9029  0.0098  0.2214\n",
      "      4                     \u001b[36m0.5561\u001b[0m        \u001b[32m0.8271\u001b[0m                     \u001b[35m0.5485\u001b[0m        \u001b[31m0.7196\u001b[0m  0.0096  0.2195\n",
      "      5                     0.5509        1.0804                     \u001b[35m0.5597\u001b[0m        0.7282  0.0093  0.2092\n",
      "      6                     \u001b[36m0.5579\u001b[0m        0.8589                     \u001b[35m0.5728\u001b[0m        0.7315  0.0090  0.2094\n",
      "      7                     \u001b[36m0.5967\u001b[0m        \u001b[32m0.7530\u001b[0m                     0.5541        0.7618  0.0085  0.2105\n",
      "      8                     \u001b[36m0.6019\u001b[0m        0.8020                     0.5336        0.8487  0.0080  0.2450\n",
      "      9                     0.5916        0.7759                     0.5560        0.7764  0.0075  0.2155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     0.5794        0.7736                     \u001b[35m0.5896\u001b[0m        \u001b[31m0.6694\u001b[0m  0.0069  0.2349\n",
      "     11                     0.5902        \u001b[32m0.6980\u001b[0m                     \u001b[35m0.6455\u001b[0m        \u001b[31m0.6420\u001b[0m  0.0063  0.2097\n",
      "     12                     \u001b[36m0.6322\u001b[0m        \u001b[32m0.6620\u001b[0m                     0.6455        \u001b[31m0.6265\u001b[0m  0.0057  0.2384\n",
      "     13                     \u001b[36m0.6374\u001b[0m        0.7057                     0.5858        0.7299  0.0050  0.2094\n",
      "     14                     \u001b[36m0.6463\u001b[0m        0.6647                     0.6437        0.6480  0.0043  0.2373\n",
      "     15                     \u001b[36m0.6682\u001b[0m        \u001b[32m0.6478\u001b[0m                     \u001b[35m0.6642\u001b[0m        \u001b[31m0.6067\u001b[0m  0.0037  0.2165\n",
      "     16                     \u001b[36m0.6799\u001b[0m        \u001b[32m0.6104\u001b[0m                     \u001b[35m0.6791\u001b[0m        \u001b[31m0.6057\u001b[0m  0.0031  0.2492\n",
      "     17                     \u001b[36m0.6846\u001b[0m        \u001b[32m0.6071\u001b[0m                     0.6772        \u001b[31m0.6051\u001b[0m  0.0025  0.2334\n",
      "     18                     \u001b[36m0.6855\u001b[0m        \u001b[32m0.5951\u001b[0m                     0.6791        \u001b[31m0.5968\u001b[0m  0.0020  0.2340\n",
      "     19                     \u001b[36m0.7168\u001b[0m        \u001b[32m0.5809\u001b[0m                     0.6716        0.6104  0.0015  0.2124\n",
      "     20                     0.7070        \u001b[32m0.5651\u001b[0m                     0.6642        0.6066  0.0010  0.2301\n",
      "     21                     \u001b[36m0.7182\u001b[0m        \u001b[32m0.5586\u001b[0m                     \u001b[35m0.6866\u001b[0m        0.5978  0.0007  0.2373\n",
      "     22                     \u001b[36m0.7201\u001b[0m        \u001b[32m0.5573\u001b[0m                     0.6754        0.6023  0.0004  0.2155\n",
      "     23                     0.7065        0.5719                     0.6866        \u001b[31m0.5951\u001b[0m  0.0002  0.2227\n",
      "     24                     0.7178        0.5609                     0.6828        0.5995  0.0000  0.2380\n",
      "     25                     0.7187        \u001b[32m0.5513\u001b[0m                     0.6791        0.6079  0.0000  0.2264\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5098\u001b[0m        \u001b[32m1.2139\u001b[0m                     \u001b[35m0.4925\u001b[0m        \u001b[31m3.1527\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.5164\u001b[0m        1.3040                     \u001b[35m0.5037\u001b[0m        \u001b[31m0.8267\u001b[0m  0.0100  0.2084\n",
      "      3                     \u001b[36m0.5313\u001b[0m        \u001b[32m0.8748\u001b[0m                     \u001b[35m0.5056\u001b[0m        1.0178  0.0098  0.2069\n",
      "      4                     \u001b[36m0.5664\u001b[0m        \u001b[32m0.7964\u001b[0m                     \u001b[35m0.5205\u001b[0m        \u001b[31m0.8031\u001b[0m  0.0096  0.2312\n",
      "      5                     0.5500        0.9093                     \u001b[35m0.5504\u001b[0m        \u001b[31m0.7215\u001b[0m  0.0093  0.2234\n",
      "      6                     0.5654        \u001b[32m0.7393\u001b[0m                     0.5336        1.0582  0.0090  0.2244\n",
      "      7                     0.5598        0.9740                     \u001b[35m0.5746\u001b[0m        \u001b[31m0.6949\u001b[0m  0.0085  0.2134\n",
      "      8                     \u001b[36m0.5967\u001b[0m        \u001b[32m0.7301\u001b[0m                     0.5728        0.7791  0.0080  0.2085\n",
      "      9                     \u001b[36m0.6042\u001b[0m        \u001b[32m0.7126\u001b[0m                     0.5578        0.7775  0.0075  0.2104\n",
      "     10                     0.5958        0.7571                     \u001b[35m0.5933\u001b[0m        \u001b[31m0.6898\u001b[0m  0.0069  0.2085\n",
      "     11                     \u001b[36m0.6598\u001b[0m        \u001b[32m0.6542\u001b[0m                     \u001b[35m0.5951\u001b[0m        \u001b[31m0.6872\u001b[0m  0.0063  0.2105\n",
      "     12                     0.6542        \u001b[32m0.6330\u001b[0m                     \u001b[35m0.6157\u001b[0m        \u001b[31m0.6576\u001b[0m  0.0057  0.2145\n",
      "     13                     \u001b[36m0.6715\u001b[0m        \u001b[32m0.6167\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6204\u001b[0m  0.0050  0.2134\n",
      "     14                     \u001b[36m0.6832\u001b[0m        \u001b[32m0.6151\u001b[0m                     0.6175        0.6432  0.0043  0.2270\n",
      "     15                     0.6808        0.6194                     0.5951        0.6914  0.0037  0.2184\n",
      "     16                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.5887\u001b[0m                     \u001b[35m0.6549\u001b[0m        \u001b[31m0.6156\u001b[0m  0.0031  0.2094\n",
      "     17                     0.6916        0.5903                     0.6269        0.6295  0.0025  0.2124\n",
      "     18                     \u001b[36m0.7252\u001b[0m        \u001b[32m0.5611\u001b[0m                     0.6437        0.6252  0.0020  0.2095\n",
      "     19                     \u001b[36m0.7313\u001b[0m        \u001b[32m0.5541\u001b[0m                     0.6455        \u001b[31m0.6128\u001b[0m  0.0015  0.2114\n",
      "     20                     \u001b[36m0.7322\u001b[0m        \u001b[32m0.5461\u001b[0m                     0.6343        0.6133  0.0010  0.2085\n",
      "     21                     0.7313        \u001b[32m0.5353\u001b[0m                     0.6511        0.6133  0.0007  0.2115\n",
      "     22                     \u001b[36m0.7364\u001b[0m        \u001b[32m0.5318\u001b[0m                     0.6418        0.6132  0.0004  0.2115\n",
      "     23                     0.7336        0.5329                     0.6381        \u001b[31m0.6124\u001b[0m  0.0002  0.2094\n",
      "     24                     0.7276        0.5417                     0.6343        0.6146  0.0000  0.2114\n",
      "     25                     0.7304        0.5402                     0.6343        0.6130  0.0000  0.2243\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4944\u001b[0m        \u001b[32m1.3628\u001b[0m                     \u001b[35m0.5578\u001b[0m        \u001b[31m0.7219\u001b[0m  0.0100  0.2244\n",
      "      2                     \u001b[36m0.5234\u001b[0m        \u001b[32m1.1441\u001b[0m                     0.5037        1.9035  0.0100  0.2224\n",
      "      3                     0.5215        1.3113                     0.5317        0.8499  0.0098  0.2168\n",
      "      4                     \u001b[36m0.5402\u001b[0m        \u001b[32m0.8461\u001b[0m                     \u001b[35m0.5578\u001b[0m        \u001b[31m0.7153\u001b[0m  0.0096  0.2234\n",
      "      5                     \u001b[36m0.5729\u001b[0m        \u001b[32m0.7371\u001b[0m                     0.5429        \u001b[31m0.6991\u001b[0m  0.0093  0.2184\n",
      "      6                     \u001b[36m0.5832\u001b[0m        \u001b[32m0.7370\u001b[0m                     \u001b[35m0.5765\u001b[0m        0.7422  0.0090  0.2304\n",
      "      7                     \u001b[36m0.6173\u001b[0m        \u001b[32m0.6932\u001b[0m                     \u001b[35m0.6175\u001b[0m        \u001b[31m0.6457\u001b[0m  0.0085  0.2328\n",
      "      8                     0.6019        0.7799                     0.5690        0.7525  0.0080  0.2184\n",
      "      9                     0.6150        0.7091                     0.5280        1.0320  0.0075  0.2094\n",
      "     10                     0.5963        0.7809                     \u001b[35m0.6418\u001b[0m        0.6466  0.0069  0.2101\n",
      "     11                     \u001b[36m0.6341\u001b[0m        \u001b[32m0.6856\u001b[0m                     0.6325        \u001b[31m0.6320\u001b[0m  0.0063  0.2148\n",
      "     12                     \u001b[36m0.6477\u001b[0m        \u001b[32m0.6471\u001b[0m                     0.6343        \u001b[31m0.6315\u001b[0m  0.0057  0.2089\n",
      "     13                     \u001b[36m0.6654\u001b[0m        \u001b[32m0.6314\u001b[0m                     \u001b[35m0.6511\u001b[0m        \u001b[31m0.6066\u001b[0m  0.0050  0.2115\n",
      "     14                     \u001b[36m0.6724\u001b[0m        \u001b[32m0.6212\u001b[0m                     \u001b[35m0.6791\u001b[0m        0.6105  0.0043  0.2094\n",
      "     15                     \u001b[36m0.6743\u001b[0m        0.6387                     0.6119        0.6487  0.0037  0.2108\n",
      "     16                     \u001b[36m0.6836\u001b[0m        \u001b[32m0.6141\u001b[0m                     0.6418        0.6128  0.0031  0.2115\n",
      "     17                     0.6794        \u001b[32m0.6036\u001b[0m                     0.6474        0.6393  0.0025  0.2104\n",
      "     18                     0.6813        0.6124                     0.6679        \u001b[31m0.6040\u001b[0m  0.0020  0.2352\n",
      "     19                     \u001b[36m0.7019\u001b[0m        \u001b[32m0.5779\u001b[0m                     0.6530        0.6044  0.0015  0.2145\n",
      "     20                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.5722\u001b[0m                     0.6604        \u001b[31m0.5975\u001b[0m  0.0010  0.2414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21                     \u001b[36m0.7262\u001b[0m        \u001b[32m0.5620\u001b[0m                     0.6642        \u001b[31m0.5971\u001b[0m  0.0007  0.2105\n",
      "     22                     0.7126        \u001b[32m0.5596\u001b[0m                     0.6623        \u001b[31m0.5939\u001b[0m  0.0004  0.2154\n",
      "     23                     0.7210        \u001b[32m0.5565\u001b[0m                     0.6679        0.5940  0.0002  0.2244\n",
      "     24                     \u001b[36m0.7308\u001b[0m        0.5661                     0.6698        0.5950  0.0000  0.2104\n",
      "     25                     0.7248        0.5614                     0.6642        0.5950  0.0000  0.2184\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5051\u001b[0m        \u001b[32m1.3228\u001b[0m                     \u001b[35m0.4963\u001b[0m        \u001b[31m0.8605\u001b[0m  0.0100  0.2274\n",
      "      2                     \u001b[36m0.5159\u001b[0m        \u001b[32m0.9602\u001b[0m                     \u001b[35m0.5112\u001b[0m        2.1018  0.0100  0.2313\n",
      "      3                     0.5145        \u001b[32m0.9213\u001b[0m                     0.4963        0.9035  0.0098  0.2087\n",
      "      4                     \u001b[36m0.5271\u001b[0m        \u001b[32m0.8557\u001b[0m                     \u001b[35m0.5373\u001b[0m        \u001b[31m0.7652\u001b[0m  0.0096  0.2094\n",
      "      5                     \u001b[36m0.5435\u001b[0m        \u001b[32m0.8465\u001b[0m                     \u001b[35m0.5821\u001b[0m        \u001b[31m0.6967\u001b[0m  0.0093  0.2344\n",
      "      6                     \u001b[36m0.5579\u001b[0m        \u001b[32m0.7910\u001b[0m                     0.5299        0.7881  0.0090  0.2133\n",
      "      7                     \u001b[36m0.5738\u001b[0m        \u001b[32m0.7407\u001b[0m                     \u001b[35m0.5877\u001b[0m        \u001b[31m0.6882\u001b[0m  0.0085  0.2204\n",
      "      8                     \u001b[36m0.5935\u001b[0m        \u001b[32m0.7058\u001b[0m                     0.5746        0.6888  0.0080  0.2160\n",
      "      9                     0.5916        0.7640                     0.4907        1.4337  0.0075  0.2104\n",
      "     10                     0.5902        0.8282                     \u001b[35m0.6007\u001b[0m        0.6988  0.0069  0.2169\n",
      "     11                     \u001b[36m0.6079\u001b[0m        \u001b[32m0.7033\u001b[0m                     \u001b[35m0.6119\u001b[0m        \u001b[31m0.6656\u001b[0m  0.0063  0.2084\n",
      "     12                     \u001b[36m0.6271\u001b[0m        \u001b[32m0.6667\u001b[0m                     \u001b[35m0.6269\u001b[0m        \u001b[31m0.6585\u001b[0m  0.0057  0.2232\n",
      "     13                     \u001b[36m0.6425\u001b[0m        \u001b[32m0.6463\u001b[0m                     0.6269        0.6652  0.0050  0.2144\n",
      "     14                     \u001b[36m0.6458\u001b[0m        0.6525                     \u001b[35m0.6399\u001b[0m        \u001b[31m0.6432\u001b[0m  0.0043  0.2174\n",
      "     15                     \u001b[36m0.6626\u001b[0m        \u001b[32m0.6195\u001b[0m                     0.6381        0.6444  0.0037  0.2093\n",
      "     16                     \u001b[36m0.6827\u001b[0m        \u001b[32m0.6035\u001b[0m                     0.6157        0.6625  0.0031  0.2095\n",
      "     17                     \u001b[36m0.6869\u001b[0m        0.6121                     0.5989        0.6698  0.0025  0.2124\n",
      "     18                     0.6785        0.6206                     0.6045        0.6836  0.0020  0.2284\n",
      "     19                     0.6710        0.6250                     0.6381        \u001b[31m0.6326\u001b[0m  0.0015  0.2144\n",
      "     20                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.5851\u001b[0m                     \u001b[35m0.6511\u001b[0m        \u001b[31m0.6318\u001b[0m  0.0010  0.2144\n",
      "     21                     \u001b[36m0.7131\u001b[0m        \u001b[32m0.5585\u001b[0m                     \u001b[35m0.6530\u001b[0m        \u001b[31m0.6305\u001b[0m  0.0007  0.2214\n",
      "     22                     \u001b[36m0.7145\u001b[0m        0.5690                     0.6474        0.6321  0.0004  0.2164\n",
      "     23                     0.7126        0.5665                     \u001b[35m0.6623\u001b[0m        0.6341  0.0002  0.2131\n",
      "     24                     0.7051        0.5853                     0.6549        0.6337  0.0000  0.2095\n",
      "     25                     \u001b[36m0.7234\u001b[0m        0.5617                     0.6549        0.6337  0.0000  0.2094\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5121\u001b[0m        \u001b[32m1.2982\u001b[0m                     \u001b[35m0.5224\u001b[0m        \u001b[31m0.9199\u001b[0m  0.0100  0.2025\n",
      "      2                     \u001b[36m0.5266\u001b[0m        \u001b[32m1.1220\u001b[0m                     0.4944        4.5950  0.0100  0.2070\n",
      "      3                     0.5215        1.6016                     \u001b[35m0.5429\u001b[0m        \u001b[31m0.8909\u001b[0m  0.0098  0.2085\n",
      "      4                     \u001b[36m0.5514\u001b[0m        \u001b[32m0.8057\u001b[0m                     \u001b[35m0.5840\u001b[0m        \u001b[31m0.6673\u001b[0m  0.0096  0.2164\n",
      "      5                     \u001b[36m0.5640\u001b[0m        \u001b[32m0.7896\u001b[0m                     \u001b[35m0.5877\u001b[0m        \u001b[31m0.6616\u001b[0m  0.0093  0.2283\n",
      "      6                     \u001b[36m0.5762\u001b[0m        0.8094                     0.5690        0.7113  0.0090  0.2339\n",
      "      7                     \u001b[36m0.5981\u001b[0m        \u001b[32m0.7095\u001b[0m                     \u001b[35m0.6269\u001b[0m        \u001b[31m0.6442\u001b[0m  0.0085  0.2186\n",
      "      8                     0.5893        0.7482                     0.5951        0.6980  0.0080  0.2319\n",
      "      9                     \u001b[36m0.6215\u001b[0m        \u001b[32m0.6820\u001b[0m                     \u001b[35m0.6455\u001b[0m        \u001b[31m0.6226\u001b[0m  0.0075  0.2299\n",
      "     10                     \u001b[36m0.6252\u001b[0m        \u001b[32m0.6604\u001b[0m                     \u001b[35m0.6530\u001b[0m        \u001b[31m0.6208\u001b[0m  0.0069  0.2306\n",
      "     11                     \u001b[36m0.6276\u001b[0m        0.6735                     0.6418        0.6321  0.0063  0.2104\n",
      "     12                     \u001b[36m0.6421\u001b[0m        0.6611                     0.6455        0.6425  0.0057  0.2104\n",
      "     13                     \u001b[36m0.6621\u001b[0m        \u001b[32m0.6406\u001b[0m                     \u001b[35m0.6604\u001b[0m        \u001b[31m0.6117\u001b[0m  0.0050  0.2174\n",
      "     14                     \u001b[36m0.6738\u001b[0m        \u001b[32m0.6132\u001b[0m                     \u001b[35m0.6735\u001b[0m        \u001b[31m0.6041\u001b[0m  0.0043  0.2244\n",
      "     15                     0.6696        0.6274                     0.6437        0.6480  0.0037  0.2294\n",
      "     16                     \u001b[36m0.6879\u001b[0m        \u001b[32m0.6130\u001b[0m                     0.6287        0.6721  0.0031  0.2297\n",
      "     17                     \u001b[36m0.7019\u001b[0m        \u001b[32m0.5886\u001b[0m                     \u001b[35m0.7071\u001b[0m        \u001b[31m0.5867\u001b[0m  0.0025  0.2159\n",
      "     18                     \u001b[36m0.7121\u001b[0m        \u001b[32m0.5734\u001b[0m                     0.6866        0.5954  0.0020  0.2134\n",
      "     19                     \u001b[36m0.7262\u001b[0m        \u001b[32m0.5557\u001b[0m                     0.6922        \u001b[31m0.5831\u001b[0m  0.0015  0.2145\n",
      "     20                     0.7084        0.5723                     0.7034        \u001b[31m0.5800\u001b[0m  0.0010  0.2095\n",
      "     21                     \u001b[36m0.7332\u001b[0m        \u001b[32m0.5417\u001b[0m                     0.6828        0.5928  0.0007  0.2284\n",
      "     22                     \u001b[36m0.7364\u001b[0m        0.5438                     0.6772        0.5958  0.0004  0.2277\n",
      "     23                     0.7346        \u001b[32m0.5414\u001b[0m                     0.6698        0.5950  0.0002  0.2129\n",
      "     24                     0.7280        \u001b[32m0.5346\u001b[0m                     0.6884        0.5820  0.0000  0.2109\n",
      "     25                     0.7173        0.5564                     0.6716        0.5959  0.0000  0.2150\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5262\u001b[0m        \u001b[32m1.2741\u001b[0m                     \u001b[35m0.5112\u001b[0m        \u001b[31m1.1969\u001b[0m  0.0100  0.2055\n",
      "      2                     0.5093        \u001b[32m0.9845\u001b[0m                     \u001b[35m0.5448\u001b[0m        \u001b[31m0.7458\u001b[0m  0.0100  0.2087\n",
      "      3                     \u001b[36m0.5402\u001b[0m        \u001b[32m0.8540\u001b[0m                     0.5336        0.9476  0.0098  0.2304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4                     0.5397        0.9645                     0.5000        0.9243  0.0096  0.2214\n",
      "      5                     \u001b[36m0.5472\u001b[0m        \u001b[32m0.8055\u001b[0m                     0.5093        1.1617  0.0093  0.2216\n",
      "      6                     \u001b[36m0.5528\u001b[0m        0.9633                     \u001b[35m0.5448\u001b[0m        0.8259  0.0090  0.2225\n",
      "      7                     \u001b[36m0.5607\u001b[0m        0.8597                     0.5392        0.8487  0.0085  0.2215\n",
      "      8                     \u001b[36m0.5916\u001b[0m        \u001b[32m0.7411\u001b[0m                     \u001b[35m0.5914\u001b[0m        \u001b[31m0.6788\u001b[0m  0.0080  0.2334\n",
      "      9                     0.5822        0.8004                     \u001b[35m0.6138\u001b[0m        \u001b[31m0.6663\u001b[0m  0.0075  0.2324\n",
      "     10                     \u001b[36m0.6037\u001b[0m        \u001b[32m0.7179\u001b[0m                     \u001b[35m0.6194\u001b[0m        \u001b[31m0.6564\u001b[0m  0.0069  0.2144\n",
      "     11                     \u001b[36m0.6388\u001b[0m        \u001b[32m0.6520\u001b[0m                     \u001b[35m0.6306\u001b[0m        \u001b[31m0.6350\u001b[0m  0.0063  0.2095\n",
      "     12                     0.6206        0.6927                     0.5896        0.7073  0.0057  0.2084\n",
      "     13                     \u001b[36m0.6402\u001b[0m        0.6590                     \u001b[35m0.6754\u001b[0m        \u001b[31m0.6037\u001b[0m  0.0050  0.2104\n",
      "     14                     \u001b[36m0.6537\u001b[0m        \u001b[32m0.6429\u001b[0m                     0.6213        0.6772  0.0043  0.2084\n",
      "     15                     \u001b[36m0.6706\u001b[0m        \u001b[32m0.6326\u001b[0m                     \u001b[35m0.6847\u001b[0m        \u001b[31m0.5996\u001b[0m  0.0037  0.2104\n",
      "     16                     \u001b[36m0.6832\u001b[0m        \u001b[32m0.6129\u001b[0m                     0.6754        \u001b[31m0.5913\u001b[0m  0.0031  0.2075\n",
      "     17                     \u001b[36m0.6869\u001b[0m        \u001b[32m0.6061\u001b[0m                     0.6735        0.5976  0.0025  0.2094\n",
      "     18                     \u001b[36m0.6967\u001b[0m        \u001b[32m0.5874\u001b[0m                     0.6791        0.5933  0.0020  0.2094\n",
      "     19                     \u001b[36m0.7037\u001b[0m        \u001b[32m0.5674\u001b[0m                     \u001b[35m0.6866\u001b[0m        0.5928  0.0015  0.2102\n",
      "     20                     \u001b[36m0.7084\u001b[0m        0.5720                     \u001b[35m0.6978\u001b[0m        0.5941  0.0010  0.2120\n",
      "     21                     \u001b[36m0.7201\u001b[0m        \u001b[32m0.5582\u001b[0m                     0.6940        \u001b[31m0.5780\u001b[0m  0.0007  0.2089\n",
      "     22                     0.7136        0.5683                     \u001b[35m0.6996\u001b[0m        0.5843  0.0004  0.2105\n",
      "     23                     0.7201        0.5626                     0.6996        0.5803  0.0002  0.2106\n",
      "     24                     0.7140        \u001b[32m0.5514\u001b[0m                     \u001b[35m0.7034\u001b[0m        \u001b[31m0.5778\u001b[0m  0.0000  0.2095\n",
      "     25                     0.7145        0.5561                     \u001b[35m0.7052\u001b[0m        \u001b[31m0.5776\u001b[0m  0.0000  0.2284\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4897\u001b[0m        \u001b[32m1.7843\u001b[0m                     \u001b[35m0.5112\u001b[0m        \u001b[31m0.8201\u001b[0m  0.0100  0.2114\n",
      "      2                     \u001b[36m0.5341\u001b[0m        \u001b[32m0.9157\u001b[0m                     \u001b[35m0.5821\u001b[0m        \u001b[31m0.7116\u001b[0m  0.0100  0.2334\n",
      "      3                     0.5341        \u001b[32m0.8361\u001b[0m                     \u001b[35m0.5858\u001b[0m        \u001b[31m0.6935\u001b[0m  0.0098  0.2094\n",
      "      4                     \u001b[36m0.5533\u001b[0m        \u001b[32m0.7599\u001b[0m                     0.5728        0.7326  0.0096  0.2236\n",
      "      5                     0.5467        0.7794                     0.5261        0.7329  0.0093  0.2294\n",
      "      6                     \u001b[36m0.5664\u001b[0m        0.8385                     0.5149        0.9417  0.0090  0.2302\n",
      "      7                     0.5579        0.9733                     0.5410        0.7527  0.0085  0.2321\n",
      "      8                     \u001b[36m0.5925\u001b[0m        \u001b[32m0.7409\u001b[0m                     0.5765        0.7050  0.0080  0.2184\n",
      "      9                     0.5860        \u001b[32m0.7266\u001b[0m                     \u001b[35m0.6101\u001b[0m        \u001b[31m0.6578\u001b[0m  0.0075  0.2118\n",
      "     10                     0.5921        \u001b[32m0.7113\u001b[0m                     \u001b[35m0.6119\u001b[0m        0.6914  0.0069  0.2105\n",
      "     11                     0.5925        0.7677                     0.5821        0.6680  0.0063  0.2134\n",
      "     12                     \u001b[36m0.5958\u001b[0m        0.7333                     0.6063        0.6752  0.0057  0.2091\n",
      "     13                     \u001b[36m0.6224\u001b[0m        \u001b[32m0.6731\u001b[0m                     0.6045        \u001b[31m0.6555\u001b[0m  0.0050  0.2094\n",
      "     14                     \u001b[36m0.6379\u001b[0m        \u001b[32m0.6580\u001b[0m                     \u001b[35m0.6343\u001b[0m        \u001b[31m0.6418\u001b[0m  0.0043  0.2114\n",
      "     15                     \u001b[36m0.6416\u001b[0m        0.6623                     0.6194        0.6582  0.0037  0.2097\n",
      "     16                     \u001b[36m0.6528\u001b[0m        \u001b[32m0.6441\u001b[0m                     \u001b[35m0.6735\u001b[0m        \u001b[31m0.6167\u001b[0m  0.0031  0.2116\n",
      "     17                     \u001b[36m0.6593\u001b[0m        \u001b[32m0.6210\u001b[0m                     0.6716        \u001b[31m0.5976\u001b[0m  0.0025  0.2105\n",
      "     18                     \u001b[36m0.6617\u001b[0m        \u001b[32m0.6205\u001b[0m                     \u001b[35m0.6903\u001b[0m        0.6007  0.0020  0.2099\n",
      "     19                     \u001b[36m0.6664\u001b[0m        \u001b[32m0.6040\u001b[0m                     0.6847        0.5992  0.0015  0.2115\n",
      "     20                     \u001b[36m0.6762\u001b[0m        \u001b[32m0.5958\u001b[0m                     0.6847        0.6034  0.0010  0.2105\n",
      "     21                     \u001b[36m0.6799\u001b[0m        0.5964                     \u001b[35m0.7108\u001b[0m        \u001b[31m0.5897\u001b[0m  0.0007  0.2114\n",
      "     22                     \u001b[36m0.7009\u001b[0m        \u001b[32m0.5804\u001b[0m                     0.7090        \u001b[31m0.5889\u001b[0m  0.0004  0.2101\n",
      "     23                     0.6935        0.5844                     0.7108        \u001b[31m0.5878\u001b[0m  0.0002  0.2128\n",
      "     24                     0.6799        0.5956                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5863\u001b[0m  0.0000  0.2105\n",
      "     25                     0.6925        \u001b[32m0.5795\u001b[0m                     0.7108        \u001b[31m0.5852\u001b[0m  0.0000  0.2098\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5318\u001b[0m        \u001b[32m1.2015\u001b[0m                     \u001b[35m0.5000\u001b[0m        \u001b[31m0.7550\u001b[0m  0.0100  0.2055\n",
      "      2                     0.5299        \u001b[32m0.8878\u001b[0m                     \u001b[35m0.5280\u001b[0m        1.2351  0.0100  0.2085\n",
      "      3                     \u001b[36m0.5369\u001b[0m        \u001b[32m0.8606\u001b[0m                     0.5093        \u001b[31m0.7329\u001b[0m  0.0098  0.2114\n",
      "      4                     0.5290        \u001b[32m0.8550\u001b[0m                     0.5131        0.8506  0.0096  0.2094\n",
      "      5                     \u001b[36m0.5495\u001b[0m        \u001b[32m0.8187\u001b[0m                     0.5168        0.8051  0.0093  0.2100\n",
      "      6                     0.5477        0.9178                     0.5243        1.0121  0.0090  0.2115\n",
      "      7                     \u001b[36m0.5766\u001b[0m        \u001b[32m0.7908\u001b[0m                     \u001b[35m0.5914\u001b[0m        \u001b[31m0.6655\u001b[0m  0.0085  0.2094\n",
      "      8                     \u001b[36m0.5995\u001b[0m        \u001b[32m0.7360\u001b[0m                     0.5896        0.7058  0.0080  0.2115\n",
      "      9                     \u001b[36m0.6201\u001b[0m        \u001b[32m0.7261\u001b[0m                     \u001b[35m0.6045\u001b[0m        0.6981  0.0075  0.2084\n",
      "     10                     \u001b[36m0.6598\u001b[0m        \u001b[32m0.6448\u001b[0m                     0.5392        0.9918  0.0069  0.2094\n",
      "     11                     0.6346        0.7118                     \u001b[35m0.6138\u001b[0m        0.6655  0.0063  0.2104\n",
      "     12                     0.6285        0.6990                     \u001b[35m0.6437\u001b[0m        \u001b[31m0.6341\u001b[0m  0.0057  0.2095\n",
      "     13                     \u001b[36m0.6650\u001b[0m        \u001b[32m0.6384\u001b[0m                     0.6269        0.6407  0.0050  0.2105\n",
      "     14                     \u001b[36m0.6935\u001b[0m        \u001b[32m0.6172\u001b[0m                     0.6362        \u001b[31m0.6252\u001b[0m  0.0043  0.2087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15                     0.6864        \u001b[32m0.6125\u001b[0m                     0.6418        0.6440  0.0037  0.2091\n",
      "     16                     0.6771        \u001b[32m0.6116\u001b[0m                     \u001b[35m0.6530\u001b[0m        0.6359  0.0031  0.2095\n",
      "     17                     0.6893        \u001b[32m0.6061\u001b[0m                     0.6493        0.6265  0.0025  0.2104\n",
      "     18                     \u001b[36m0.7070\u001b[0m        \u001b[32m0.5722\u001b[0m                     \u001b[35m0.6679\u001b[0m        \u001b[31m0.6195\u001b[0m  0.0020  0.2114\n",
      "     19                     \u001b[36m0.7215\u001b[0m        \u001b[32m0.5658\u001b[0m                     0.6660        0.6233  0.0015  0.2105\n",
      "     20                     0.7192        \u001b[32m0.5567\u001b[0m                     \u001b[35m0.6754\u001b[0m        0.6195  0.0010  0.2116\n",
      "     21                     0.7201        \u001b[32m0.5547\u001b[0m                     0.6735        0.6209  0.0007  0.2099\n",
      "     22                     \u001b[36m0.7276\u001b[0m        \u001b[32m0.5418\u001b[0m                     0.6679        \u001b[31m0.6187\u001b[0m  0.0004  0.2095\n",
      "     23                     \u001b[36m0.7463\u001b[0m        \u001b[32m0.5338\u001b[0m                     0.6754        \u001b[31m0.6179\u001b[0m  0.0002  0.2115\n",
      "     24                     0.7416        0.5364                     0.6754        0.6183  0.0000  0.2096\n",
      "     25                     0.7234        0.5447                     \u001b[35m0.6772\u001b[0m        0.6183  0.0000  0.2111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5766\u001b[0m        \u001b[32m1.0584\u001b[0m                     \u001b[35m0.6623\u001b[0m        \u001b[31m0.6337\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.6313\u001b[0m        \u001b[32m0.7652\u001b[0m                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.5818\u001b[0m  0.0100  0.2095\n",
      "      3                     \u001b[36m0.6673\u001b[0m        \u001b[32m0.6422\u001b[0m                     0.7090        \u001b[31m0.5652\u001b[0m  0.0098  0.2114\n",
      "      4                     \u001b[36m0.7093\u001b[0m        \u001b[32m0.6097\u001b[0m                     \u001b[35m0.7127\u001b[0m        0.5669  0.0096  0.2095\n",
      "      5                     \u001b[36m0.7271\u001b[0m        \u001b[32m0.5763\u001b[0m                     \u001b[35m0.7388\u001b[0m        0.5833  0.0093  0.2094\n",
      "      6                     \u001b[36m0.7318\u001b[0m        \u001b[32m0.5760\u001b[0m                     0.6996        \u001b[31m0.5611\u001b[0m  0.0090  0.2106\n",
      "      7                     0.7154        0.6034                     0.7164        0.5649  0.0085  0.2104\n",
      "      8                     0.7145        0.6251                     0.7313        0.6041  0.0080  0.2314\n",
      "      9                     \u001b[36m0.7322\u001b[0m        \u001b[32m0.5649\u001b[0m                     \u001b[35m0.7407\u001b[0m        0.5631  0.0075  0.2305\n",
      "     10                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5293\u001b[0m                     0.6996        0.5911  0.0069  0.2327\n",
      "     11                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5248\u001b[0m                     0.7164        \u001b[31m0.5585\u001b[0m  0.0063  0.2291\n",
      "     12                     \u001b[36m0.7720\u001b[0m        \u001b[32m0.4941\u001b[0m                     0.7295        \u001b[31m0.5485\u001b[0m  0.0057  0.2294\n",
      "     13                     0.7706        \u001b[32m0.4835\u001b[0m                     0.7407        0.5622  0.0050  0.2443\n",
      "     14                     \u001b[36m0.7907\u001b[0m        \u001b[32m0.4573\u001b[0m                     \u001b[35m0.7556\u001b[0m        0.5541  0.0043  0.2194\n",
      "     15                     0.7860        \u001b[32m0.4480\u001b[0m                     0.7556        0.5548  0.0037  0.2210\n",
      "     16                     \u001b[36m0.8079\u001b[0m        \u001b[32m0.4245\u001b[0m                     0.7351        \u001b[31m0.5406\u001b[0m  0.0031  0.2104\n",
      "     17                     \u001b[36m0.8126\u001b[0m        \u001b[32m0.4089\u001b[0m                     0.7537        0.5499  0.0025  0.2095\n",
      "     18                     \u001b[36m0.8182\u001b[0m        \u001b[32m0.3919\u001b[0m                     0.7369        0.5455  0.0020  0.2094\n",
      "     19                     \u001b[36m0.8421\u001b[0m        \u001b[32m0.3784\u001b[0m                     0.7463        0.5413  0.0015  0.2094\n",
      "     20                     0.8285        \u001b[32m0.3746\u001b[0m                     0.7500        0.5511  0.0010  0.2105\n",
      "     21                     0.8402        \u001b[32m0.3563\u001b[0m                     0.7519        0.5459  0.0007  0.2094\n",
      "     22                     0.8411        0.3608                     0.7444        0.5483  0.0004  0.2094\n",
      "     23                     \u001b[36m0.8435\u001b[0m        0.3659                     \u001b[35m0.7575\u001b[0m        0.5431  0.0002  0.2105\n",
      "     24                     0.8407        0.3655                     0.7500        0.5467  0.0000  0.2095\n",
      "     25                     \u001b[36m0.8453\u001b[0m        \u001b[32m0.3463\u001b[0m                     0.7556        0.5439  0.0000  0.2095\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5799\u001b[0m        \u001b[32m1.0870\u001b[0m                     \u001b[35m0.6586\u001b[0m        \u001b[31m0.8209\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.6164\u001b[0m        \u001b[32m0.8482\u001b[0m                     0.6586        \u001b[31m0.6822\u001b[0m  0.0100  0.2148\n",
      "      3                     \u001b[36m0.6533\u001b[0m        \u001b[32m0.7163\u001b[0m                     \u001b[35m0.6772\u001b[0m        \u001b[31m0.6346\u001b[0m  0.0098  0.2314\n",
      "      4                     \u001b[36m0.6930\u001b[0m        \u001b[32m0.6137\u001b[0m                     \u001b[35m0.7369\u001b[0m        \u001b[31m0.5290\u001b[0m  0.0096  0.2236\n",
      "      5                     \u001b[36m0.7164\u001b[0m        \u001b[32m0.5942\u001b[0m                     0.7127        0.6278  0.0093  0.2144\n",
      "      6                     0.7047        0.6262                     0.7257        0.5632  0.0090  0.2335\n",
      "      7                     \u001b[36m0.7262\u001b[0m        \u001b[32m0.5761\u001b[0m                     0.7313        0.5545  0.0085  0.2394\n",
      "      8                     \u001b[36m0.7463\u001b[0m        \u001b[32m0.5448\u001b[0m                     \u001b[35m0.7388\u001b[0m        0.5405  0.0080  0.2246\n",
      "      9                     0.7463        \u001b[32m0.5212\u001b[0m                     0.7257        0.5476  0.0075  0.2244\n",
      "     10                     \u001b[36m0.7570\u001b[0m        \u001b[32m0.5101\u001b[0m                     0.7201        0.5943  0.0069  0.2154\n",
      "     11                     \u001b[36m0.7822\u001b[0m        \u001b[32m0.4680\u001b[0m                     0.7220        0.5926  0.0063  0.2094\n",
      "     12                     \u001b[36m0.7850\u001b[0m        0.4890                     0.7351        0.5805  0.0057  0.2088\n",
      "     13                     0.7827        0.4704                     \u001b[35m0.7463\u001b[0m        0.5497  0.0050  0.2091\n",
      "     14                     0.7813        0.4687                     \u001b[35m0.7724\u001b[0m        0.5649  0.0043  0.2095\n",
      "     15                     \u001b[36m0.8117\u001b[0m        \u001b[32m0.4072\u001b[0m                     0.7575        0.5516  0.0037  0.2214\n",
      "     16                     \u001b[36m0.8145\u001b[0m        0.4152                     0.7500        0.5541  0.0031  0.2168\n",
      "     17                     \u001b[36m0.8159\u001b[0m        0.4076                     0.7295        0.5927  0.0025  0.2095\n",
      "     18                     \u001b[36m0.8341\u001b[0m        \u001b[32m0.3810\u001b[0m                     0.7593        0.5647  0.0020  0.2097\n",
      "     19                     0.8252        0.3936                     0.7537        0.5484  0.0015  0.2145\n",
      "     20                     0.8299        \u001b[32m0.3648\u001b[0m                     0.7463        0.5572  0.0010  0.2126\n",
      "     21                     0.8304        0.3816                     0.7556        0.5562  0.0007  0.2155\n",
      "     22                     \u001b[36m0.8425\u001b[0m        \u001b[32m0.3486\u001b[0m                     0.7407        0.5609  0.0004  0.2481\n",
      "     23                     0.8388        0.3637                     0.7425        0.5565  0.0002  0.2266\n",
      "     24                     \u001b[36m0.8486\u001b[0m        \u001b[32m0.3421\u001b[0m                     0.7481        0.5551  0.0000  0.2299\n",
      "     25                     0.8411        0.3636                     0.7500        0.5558  0.0000  0.2252\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5537\u001b[0m        \u001b[32m1.1270\u001b[0m                     \u001b[35m0.6679\u001b[0m        \u001b[31m0.6330\u001b[0m  0.0100  0.2015\n",
      "      2                     \u001b[36m0.6350\u001b[0m        \u001b[32m0.7743\u001b[0m                     \u001b[35m0.7034\u001b[0m        \u001b[31m0.5770\u001b[0m  0.0100  0.2084\n",
      "      3                     \u001b[36m0.6864\u001b[0m        \u001b[32m0.6434\u001b[0m                     0.6810        0.6067  0.0098  0.2093\n",
      "      4                     \u001b[36m0.6972\u001b[0m        \u001b[32m0.6149\u001b[0m                     0.6511        0.7305  0.0096  0.2094\n",
      "      5                     \u001b[36m0.7107\u001b[0m        \u001b[32m0.5842\u001b[0m                     0.7034        0.5875  0.0093  0.2096\n",
      "      6                     \u001b[36m0.7369\u001b[0m        \u001b[32m0.5642\u001b[0m                     0.6959        0.5840  0.0090  0.2100\n",
      "      7                     0.7168        0.5684                     \u001b[35m0.7071\u001b[0m        0.6084  0.0085  0.2103\n",
      "      8                     \u001b[36m0.7486\u001b[0m        \u001b[32m0.5377\u001b[0m                     0.6996        0.6531  0.0080  0.2283\n",
      "      9                     \u001b[36m0.7589\u001b[0m        \u001b[32m0.5294\u001b[0m                     0.6978        0.6031  0.0075  0.2104\n",
      "     10                     0.7533        0.5321                     0.6847        0.6625  0.0069  0.2124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     \u001b[36m0.7724\u001b[0m        \u001b[32m0.4960\u001b[0m                     \u001b[35m0.7276\u001b[0m        0.5834  0.0063  0.2115\n",
      "     12                     \u001b[36m0.7799\u001b[0m        \u001b[32m0.4782\u001b[0m                     \u001b[35m0.7351\u001b[0m        \u001b[31m0.5648\u001b[0m  0.0057  0.2324\n",
      "     13                     \u001b[36m0.7888\u001b[0m        \u001b[32m0.4761\u001b[0m                     \u001b[35m0.7463\u001b[0m        0.5653  0.0050  0.2125\n",
      "     14                     \u001b[36m0.8014\u001b[0m        \u001b[32m0.4523\u001b[0m                     0.7257        \u001b[31m0.5645\u001b[0m  0.0043  0.2319\n",
      "     15                     \u001b[36m0.8047\u001b[0m        \u001b[32m0.4382\u001b[0m                     0.7257        \u001b[31m0.5543\u001b[0m  0.0037  0.2135\n",
      "     16                     \u001b[36m0.8182\u001b[0m        \u001b[32m0.4093\u001b[0m                     \u001b[35m0.7481\u001b[0m        0.5988  0.0031  0.2324\n",
      "     17                     \u001b[36m0.8215\u001b[0m        \u001b[32m0.4058\u001b[0m                     0.7463        0.5760  0.0025  0.2291\n",
      "     18                     0.8117        \u001b[32m0.4001\u001b[0m                     \u001b[35m0.7500\u001b[0m        0.5633  0.0020  0.2214\n",
      "     19                     \u001b[36m0.8308\u001b[0m        \u001b[32m0.3716\u001b[0m                     0.7481        0.5833  0.0015  0.2094\n",
      "     20                     \u001b[36m0.8411\u001b[0m        \u001b[32m0.3653\u001b[0m                     0.7463        0.5653  0.0010  0.2102\n",
      "     21                     0.8402        0.3671                     \u001b[35m0.7537\u001b[0m        0.5661  0.0007  0.2176\n",
      "     22                     \u001b[36m0.8486\u001b[0m        \u001b[32m0.3441\u001b[0m                     0.7444        0.5701  0.0004  0.2119\n",
      "     23                     0.8449        0.3472                     0.7463        0.5817  0.0002  0.2075\n",
      "     24                     0.8435        0.3644                     0.7463        0.5793  0.0000  0.2094\n",
      "     25                     \u001b[36m0.8514\u001b[0m        0.3442                     0.7500        0.5728  0.0000  0.2084\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5612\u001b[0m        \u001b[32m1.1644\u001b[0m                     \u001b[35m0.5858\u001b[0m        \u001b[31m0.8710\u001b[0m  0.0100  0.2025\n",
      "      2                     \u001b[36m0.6215\u001b[0m        \u001b[32m0.7818\u001b[0m                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5557\u001b[0m  0.0100  0.2115\n",
      "      3                     \u001b[36m0.6612\u001b[0m        \u001b[32m0.6666\u001b[0m                     0.7052        0.5682  0.0098  0.2096\n",
      "      4                     \u001b[36m0.6860\u001b[0m        \u001b[32m0.6381\u001b[0m                     \u001b[35m0.7463\u001b[0m        \u001b[31m0.5326\u001b[0m  0.0096  0.2095\n",
      "      5                     \u001b[36m0.6967\u001b[0m        \u001b[32m0.6016\u001b[0m                     0.7276        0.5664  0.0093  0.2124\n",
      "      6                     \u001b[36m0.7215\u001b[0m        \u001b[32m0.5852\u001b[0m                     0.7332        \u001b[31m0.5210\u001b[0m  0.0090  0.2095\n",
      "      7                     \u001b[36m0.7280\u001b[0m        \u001b[32m0.5711\u001b[0m                     \u001b[35m0.7500\u001b[0m        \u001b[31m0.5197\u001b[0m  0.0085  0.2127\n",
      "      8                     \u001b[36m0.7336\u001b[0m        \u001b[32m0.5623\u001b[0m                     0.7332        0.5944  0.0080  0.2130\n",
      "      9                     \u001b[36m0.7374\u001b[0m        \u001b[32m0.5561\u001b[0m                     0.7127        0.5803  0.0075  0.2096\n",
      "     10                     \u001b[36m0.7453\u001b[0m        \u001b[32m0.5427\u001b[0m                     0.7257        0.5592  0.0069  0.2095\n",
      "     11                     \u001b[36m0.7547\u001b[0m        \u001b[32m0.5270\u001b[0m                     \u001b[35m0.7575\u001b[0m        0.5496  0.0063  0.2134\n",
      "     12                     0.7542        \u001b[32m0.5215\u001b[0m                     0.7090        0.5606  0.0057  0.2094\n",
      "     13                     \u001b[36m0.7645\u001b[0m        \u001b[32m0.5016\u001b[0m                     0.7425        0.5384  0.0050  0.2104\n",
      "     14                     \u001b[36m0.7762\u001b[0m        \u001b[32m0.4800\u001b[0m                     \u001b[35m0.7593\u001b[0m        \u001b[31m0.5186\u001b[0m  0.0043  0.2174\n",
      "     15                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.4558\u001b[0m                     \u001b[35m0.7668\u001b[0m        0.5339  0.0037  0.2334\n",
      "     16                     \u001b[36m0.7902\u001b[0m        \u001b[32m0.4484\u001b[0m                     0.7631        0.5297  0.0031  0.2394\n",
      "     17                     \u001b[36m0.8075\u001b[0m        \u001b[32m0.4239\u001b[0m                     0.7425        0.5413  0.0025  0.2268\n",
      "     18                     \u001b[36m0.8173\u001b[0m        \u001b[32m0.3992\u001b[0m                     0.7631        0.5440  0.0020  0.2089\n",
      "     19                     \u001b[36m0.8206\u001b[0m        \u001b[32m0.3912\u001b[0m                     0.7388        0.5544  0.0015  0.2244\n",
      "     20                     0.8154        0.3988                     \u001b[35m0.7687\u001b[0m        0.5369  0.0010  0.2274\n",
      "     21                     \u001b[36m0.8336\u001b[0m        \u001b[32m0.3789\u001b[0m                     0.7537        0.5498  0.0007  0.2195\n",
      "     22                     \u001b[36m0.8360\u001b[0m        0.3821                     0.7556        0.5510  0.0004  0.2324\n",
      "     23                     \u001b[36m0.8425\u001b[0m        \u001b[32m0.3592\u001b[0m                     0.7519        0.5491  0.0002  0.2105\n",
      "     24                     0.8322        0.3702                     0.7612        0.5471  0.0000  0.2178\n",
      "     25                     0.8276        0.3701                     0.7537        0.5478  0.0000  0.2131\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5458\u001b[0m        \u001b[32m1.0755\u001b[0m                     \u001b[35m0.6269\u001b[0m        \u001b[31m0.6719\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6374\u001b[0m        \u001b[32m0.7510\u001b[0m                     \u001b[35m0.6437\u001b[0m        0.8439  0.0100  0.2105\n",
      "      3                     \u001b[36m0.6528\u001b[0m        \u001b[32m0.7324\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.6280\u001b[0m  0.0098  0.2258\n",
      "      4                     \u001b[36m0.6893\u001b[0m        \u001b[32m0.6238\u001b[0m                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.5766\u001b[0m  0.0096  0.2154\n",
      "      5                     \u001b[36m0.7047\u001b[0m        \u001b[32m0.6107\u001b[0m                     0.6847        0.5942  0.0093  0.2099\n",
      "      6                     0.6902        0.6344                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5755\u001b[0m  0.0090  0.2100\n",
      "      7                     \u001b[36m0.7229\u001b[0m        \u001b[32m0.5803\u001b[0m                     0.7071        0.5855  0.0085  0.2095\n",
      "      8                     \u001b[36m0.7407\u001b[0m        \u001b[32m0.5457\u001b[0m                     0.7071        0.6012  0.0080  0.2095\n",
      "      9                     \u001b[36m0.7486\u001b[0m        \u001b[32m0.5269\u001b[0m                     \u001b[35m0.7164\u001b[0m        \u001b[31m0.5716\u001b[0m  0.0075  0.2095\n",
      "     10                     \u001b[36m0.7491\u001b[0m        0.5283                     0.7127        0.6021  0.0069  0.2095\n",
      "     11                     \u001b[36m0.7528\u001b[0m        0.5502                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5461\u001b[0m  0.0063  0.2104\n",
      "     12                     \u001b[36m0.7869\u001b[0m        \u001b[32m0.4770\u001b[0m                     \u001b[35m0.7239\u001b[0m        0.5525  0.0057  0.2095\n",
      "     13                     0.7762        0.4796                     0.7220        0.5718  0.0050  0.2244\n",
      "     14                     0.7864        \u001b[32m0.4605\u001b[0m                     0.7164        0.6061  0.0043  0.2094\n",
      "     15                     \u001b[36m0.7953\u001b[0m        \u001b[32m0.4565\u001b[0m                     0.7239        0.5620  0.0037  0.2244\n",
      "     16                     0.7850        \u001b[32m0.4380\u001b[0m                     \u001b[35m0.7351\u001b[0m        0.5462  0.0031  0.2272\n",
      "     17                     \u001b[36m0.8070\u001b[0m        \u001b[32m0.4142\u001b[0m                     0.7257        0.5560  0.0025  0.2108\n",
      "     18                     \u001b[36m0.8173\u001b[0m        \u001b[32m0.4005\u001b[0m                     0.7351        0.5515  0.0020  0.2104\n",
      "     19                     \u001b[36m0.8238\u001b[0m        \u001b[32m0.3953\u001b[0m                     0.7351        0.5528  0.0015  0.2104\n",
      "     20                     \u001b[36m0.8308\u001b[0m        \u001b[32m0.3825\u001b[0m                     0.7351        \u001b[31m0.5440\u001b[0m  0.0010  0.2114\n",
      "     21                     0.8224        0.3847                     0.7313        \u001b[31m0.5406\u001b[0m  0.0007  0.2154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22                     \u001b[36m0.8467\u001b[0m        \u001b[32m0.3611\u001b[0m                     \u001b[35m0.7481\u001b[0m        0.5435  0.0004  0.2115\n",
      "     23                     0.8299        0.3746                     0.7425        0.5440  0.0002  0.2093\n",
      "     24                     0.8444        0.3645                     0.7425        0.5437  0.0000  0.2103\n",
      "     25                     \u001b[36m0.8477\u001b[0m        0.3616                     0.7407        0.5439  0.0000  0.2094\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5556\u001b[0m        \u001b[32m1.1609\u001b[0m                     \u001b[35m0.6623\u001b[0m        \u001b[31m0.6643\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6477\u001b[0m        \u001b[32m0.7705\u001b[0m                     0.6343        0.7270  0.0100  0.2135\n",
      "      3                     \u001b[36m0.6636\u001b[0m        \u001b[32m0.7120\u001b[0m                     \u001b[35m0.6660\u001b[0m        \u001b[31m0.6037\u001b[0m  0.0098  0.2094\n",
      "      4                     \u001b[36m0.6921\u001b[0m        \u001b[32m0.6137\u001b[0m                     \u001b[35m0.7052\u001b[0m        \u001b[31m0.5677\u001b[0m  0.0096  0.2095\n",
      "      5                     \u001b[36m0.7079\u001b[0m        \u001b[32m0.6014\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5643\u001b[0m  0.0093  0.2105\n",
      "      6                     \u001b[36m0.7234\u001b[0m        \u001b[32m0.5758\u001b[0m                     0.7313        \u001b[31m0.5462\u001b[0m  0.0090  0.2094\n",
      "      7                     0.7196        0.5788                     0.7313        0.5543  0.0085  0.2095\n",
      "      8                     \u001b[36m0.7369\u001b[0m        \u001b[32m0.5488\u001b[0m                     0.7127        0.6108  0.0080  0.2097\n",
      "      9                     0.7280        0.5491                     0.7220        \u001b[31m0.5382\u001b[0m  0.0075  0.2104\n",
      "     10                     \u001b[36m0.7491\u001b[0m        0.5636                     \u001b[35m0.7332\u001b[0m        0.5529  0.0069  0.2106\n",
      "     11                     \u001b[36m0.7603\u001b[0m        \u001b[32m0.5010\u001b[0m                     \u001b[35m0.7425\u001b[0m        \u001b[31m0.5354\u001b[0m  0.0063  0.2084\n",
      "     12                     \u001b[36m0.7766\u001b[0m        \u001b[32m0.4817\u001b[0m                     0.7351        0.5552  0.0057  0.2094\n",
      "     13                     \u001b[36m0.7771\u001b[0m        \u001b[32m0.4807\u001b[0m                     0.7332        0.5478  0.0050  0.2094\n",
      "     14                     \u001b[36m0.8014\u001b[0m        \u001b[32m0.4414\u001b[0m                     \u001b[35m0.7519\u001b[0m        0.5479  0.0043  0.2104\n",
      "     15                     0.7879        0.4462                     0.7500        0.5499  0.0037  0.2088\n",
      "     16                     \u001b[36m0.8103\u001b[0m        \u001b[32m0.4226\u001b[0m                     \u001b[35m0.7556\u001b[0m        0.5469  0.0031  0.2094\n",
      "     17                     \u001b[36m0.8192\u001b[0m        \u001b[32m0.3978\u001b[0m                     0.7481        0.5531  0.0025  0.2128\n",
      "     18                     0.8159        0.4097                     \u001b[35m0.7575\u001b[0m        0.5419  0.0020  0.2085\n",
      "     19                     \u001b[36m0.8290\u001b[0m        \u001b[32m0.3895\u001b[0m                     0.7575        0.5425  0.0015  0.2098\n",
      "     20                     0.8266        \u001b[32m0.3841\u001b[0m                     \u001b[35m0.7705\u001b[0m        0.5527  0.0010  0.2334\n",
      "     21                     \u001b[36m0.8304\u001b[0m        \u001b[32m0.3784\u001b[0m                     0.7612        0.5468  0.0007  0.2154\n",
      "     22                     \u001b[36m0.8360\u001b[0m        \u001b[32m0.3650\u001b[0m                     0.7631        0.5443  0.0004  0.2105\n",
      "     23                     \u001b[36m0.8519\u001b[0m        \u001b[32m0.3498\u001b[0m                     0.7631        0.5426  0.0002  0.2105\n",
      "     24                     0.8322        0.3713                     0.7649        0.5434  0.0000  0.2094\n",
      "     25                     0.8449        0.3534                     0.7631        0.5435  0.0000  0.2117\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5584\u001b[0m        \u001b[32m1.1552\u001b[0m                     \u001b[35m0.6791\u001b[0m        \u001b[31m0.6264\u001b[0m  0.0100  0.2025\n",
      "      2                     \u001b[36m0.6472\u001b[0m        \u001b[32m0.7472\u001b[0m                     \u001b[35m0.7071\u001b[0m        \u001b[31m0.5595\u001b[0m  0.0100  0.2075\n",
      "      3                     \u001b[36m0.6850\u001b[0m        \u001b[32m0.6476\u001b[0m                     0.6959        0.5981  0.0098  0.2080\n",
      "      4                     \u001b[36m0.6860\u001b[0m        \u001b[32m0.6461\u001b[0m                     0.6754        0.6579  0.0096  0.2083\n",
      "      5                     \u001b[36m0.7051\u001b[0m        \u001b[32m0.6418\u001b[0m                     \u001b[35m0.7108\u001b[0m        0.5732  0.0093  0.2094\n",
      "      6                     \u001b[36m0.7229\u001b[0m        \u001b[32m0.5753\u001b[0m                     \u001b[35m0.7593\u001b[0m        \u001b[31m0.5484\u001b[0m  0.0090  0.2096\n",
      "      7                     \u001b[36m0.7294\u001b[0m        \u001b[32m0.5577\u001b[0m                     0.7220        0.5690  0.0085  0.2078\n",
      "      8                     \u001b[36m0.7383\u001b[0m        \u001b[32m0.5486\u001b[0m                     0.7351        0.5688  0.0080  0.2094\n",
      "      9                     \u001b[36m0.7467\u001b[0m        0.5546                     0.7108        0.6205  0.0075  0.2094\n",
      "     10                     \u001b[36m0.7533\u001b[0m        \u001b[32m0.5099\u001b[0m                     0.7220        0.6509  0.0069  0.2114\n",
      "     11                     0.7519        0.5112                     0.7257        0.5820  0.0063  0.2089\n",
      "     12                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4687\u001b[0m                     0.7146        0.6184  0.0057  0.2103\n",
      "     13                     0.7738        0.4838                     0.7444        0.5520  0.0050  0.2144\n",
      "     14                     \u001b[36m0.7935\u001b[0m        \u001b[32m0.4524\u001b[0m                     0.7593        \u001b[31m0.5419\u001b[0m  0.0043  0.2088\n",
      "     15                     \u001b[36m0.7991\u001b[0m        \u001b[32m0.4328\u001b[0m                     0.7593        0.5506  0.0037  0.2105\n",
      "     16                     \u001b[36m0.8145\u001b[0m        \u001b[32m0.4186\u001b[0m                     0.7463        0.5444  0.0031  0.2095\n",
      "     17                     \u001b[36m0.8164\u001b[0m        \u001b[32m0.4027\u001b[0m                     0.7444        0.5722  0.0025  0.2095\n",
      "     18                     \u001b[36m0.8168\u001b[0m        \u001b[32m0.4010\u001b[0m                     0.7556        0.5649  0.0020  0.2102\n",
      "     19                     \u001b[36m0.8196\u001b[0m        0.4025                     0.7519        0.5677  0.0015  0.2108\n",
      "     20                     \u001b[36m0.8299\u001b[0m        \u001b[32m0.3830\u001b[0m                     0.7519        0.5512  0.0010  0.2105\n",
      "     21                     \u001b[36m0.8449\u001b[0m        \u001b[32m0.3626\u001b[0m                     0.7519        0.5539  0.0007  0.2095\n",
      "     22                     \u001b[36m0.8542\u001b[0m        \u001b[32m0.3488\u001b[0m                     0.7575        0.5512  0.0004  0.2125\n",
      "     23                     0.8486        \u001b[32m0.3468\u001b[0m                     0.7575        0.5511  0.0002  0.2095\n",
      "     24                     0.8388        \u001b[32m0.3387\u001b[0m                     0.7481        0.5528  0.0000  0.2095\n",
      "     25                     0.8393        0.3475                     0.7556        0.5522  0.0000  0.2108\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5640\u001b[0m        \u001b[32m1.1643\u001b[0m                     \u001b[35m0.6213\u001b[0m        \u001b[31m0.7263\u001b[0m  0.0100  0.2254\n",
      "      2                     \u001b[36m0.6322\u001b[0m        \u001b[32m0.8120\u001b[0m                     \u001b[35m0.6698\u001b[0m        \u001b[31m0.6045\u001b[0m  0.0100  0.2104\n",
      "      3                     \u001b[36m0.6864\u001b[0m        \u001b[32m0.6647\u001b[0m                     \u001b[35m0.6903\u001b[0m        0.6269  0.0098  0.2101\n",
      "      4                     \u001b[36m0.6981\u001b[0m        \u001b[32m0.6406\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5568\u001b[0m  0.0096  0.2174\n",
      "      5                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.5971\u001b[0m                     \u001b[35m0.7332\u001b[0m        0.5694  0.0093  0.2334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6                     0.7131        \u001b[32m0.5952\u001b[0m                     0.7015        0.6285  0.0090  0.2304\n",
      "      7                     \u001b[36m0.7201\u001b[0m        0.6124                     0.7239        0.5722  0.0085  0.2274\n",
      "      8                     \u001b[36m0.7276\u001b[0m        0.6030                     0.7313        0.5651  0.0080  0.2124\n",
      "      9                     \u001b[36m0.7318\u001b[0m        \u001b[32m0.5479\u001b[0m                     \u001b[35m0.7407\u001b[0m        \u001b[31m0.5379\u001b[0m  0.0075  0.2156\n",
      "     10                     \u001b[36m0.7645\u001b[0m        \u001b[32m0.5133\u001b[0m                     \u001b[35m0.7463\u001b[0m        0.5462  0.0069  0.2164\n",
      "     11                     \u001b[36m0.7654\u001b[0m        \u001b[32m0.5119\u001b[0m                     0.7463        0.5479  0.0063  0.2095\n",
      "     12                     \u001b[36m0.7687\u001b[0m        \u001b[32m0.4990\u001b[0m                     \u001b[35m0.7537\u001b[0m        0.5408  0.0057  0.2096\n",
      "     13                     \u001b[36m0.7710\u001b[0m        \u001b[32m0.4937\u001b[0m                     0.7407        0.5520  0.0050  0.2093\n",
      "     14                     \u001b[36m0.7921\u001b[0m        \u001b[32m0.4396\u001b[0m                     0.7500        \u001b[31m0.5304\u001b[0m  0.0043  0.2349\n",
      "     15                     0.7911        0.4542                     0.7108        0.5858  0.0037  0.2294\n",
      "     16                     \u001b[36m0.8000\u001b[0m        0.4411                     0.7425        0.5370  0.0031  0.2122\n",
      "     17                     \u001b[36m0.8075\u001b[0m        \u001b[32m0.4229\u001b[0m                     0.7388        0.5427  0.0025  0.2086\n",
      "     18                     \u001b[36m0.8112\u001b[0m        \u001b[32m0.4115\u001b[0m                     0.7369        0.5462  0.0020  0.2114\n",
      "     19                     \u001b[36m0.8168\u001b[0m        \u001b[32m0.3978\u001b[0m                     0.7500        0.5374  0.0015  0.2346\n",
      "     20                     0.8140        \u001b[32m0.3883\u001b[0m                     0.7295        0.5524  0.0010  0.2105\n",
      "     21                     \u001b[36m0.8327\u001b[0m        \u001b[32m0.3802\u001b[0m                     \u001b[35m0.7575\u001b[0m        0.5369  0.0007  0.2246\n",
      "     22                     0.8262        0.3810                     0.7500        0.5367  0.0004  0.2351\n",
      "     23                     \u001b[36m0.8369\u001b[0m        \u001b[32m0.3697\u001b[0m                     0.7519        0.5381  0.0002  0.2373\n",
      "     24                     0.8322        0.3775                     0.7537        0.5369  0.0000  0.2453\n",
      "     25                     0.8201        0.3906                     0.7519        0.5364  0.0000  0.2433\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5547\u001b[0m        \u001b[32m1.1653\u001b[0m                     \u001b[35m0.6530\u001b[0m        \u001b[31m0.7037\u001b[0m  0.0100  0.2284\n",
      "      2                     \u001b[36m0.6257\u001b[0m        \u001b[32m0.8171\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.6394\u001b[0m  0.0100  0.2240\n",
      "      3                     \u001b[36m0.6509\u001b[0m        \u001b[32m0.7141\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.6373\u001b[0m  0.0098  0.2304\n",
      "      4                     \u001b[36m0.6995\u001b[0m        \u001b[32m0.6209\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5567\u001b[0m  0.0096  0.2265\n",
      "      5                     0.6799        0.6428                     \u001b[35m0.7388\u001b[0m        \u001b[31m0.5335\u001b[0m  0.0093  0.2274\n",
      "      6                     \u001b[36m0.7121\u001b[0m        \u001b[32m0.5949\u001b[0m                     0.7034        0.5903  0.0090  0.2130\n",
      "      7                     0.7112        \u001b[32m0.5831\u001b[0m                     \u001b[35m0.7407\u001b[0m        0.5680  0.0085  0.2191\n",
      "      8                     \u001b[36m0.7346\u001b[0m        \u001b[32m0.5574\u001b[0m                     \u001b[35m0.7463\u001b[0m        0.5375  0.0080  0.2196\n",
      "      9                     \u001b[36m0.7528\u001b[0m        \u001b[32m0.5299\u001b[0m                     0.7444        0.5534  0.0075  0.2165\n",
      "     10                     0.7444        0.5380                     0.7239        0.6040  0.0069  0.2288\n",
      "     11                     0.7509        \u001b[32m0.5211\u001b[0m                     \u001b[35m0.7481\u001b[0m        0.5637  0.0063  0.2133\n",
      "     12                     \u001b[36m0.7598\u001b[0m        \u001b[32m0.5013\u001b[0m                     0.7201        0.5777  0.0057  0.2224\n",
      "     13                     \u001b[36m0.7752\u001b[0m        \u001b[32m0.4717\u001b[0m                     \u001b[35m0.7556\u001b[0m        0.5563  0.0050  0.2224\n",
      "     14                     \u001b[36m0.7757\u001b[0m        0.4733                     0.7444        0.5569  0.0043  0.2247\n",
      "     15                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4573\u001b[0m                     0.7519        0.5526  0.0037  0.2114\n",
      "     16                     \u001b[36m0.8079\u001b[0m        \u001b[32m0.4241\u001b[0m                     0.7463        0.5552  0.0031  0.2154\n",
      "     17                     0.8042        \u001b[32m0.4225\u001b[0m                     0.7481        0.5480  0.0025  0.2194\n",
      "     18                     \u001b[36m0.8136\u001b[0m        \u001b[32m0.4090\u001b[0m                     0.7407        0.5489  0.0020  0.2348\n",
      "     19                     \u001b[36m0.8210\u001b[0m        \u001b[32m0.3850\u001b[0m                     0.7463        0.5514  0.0015  0.2306\n",
      "     20                     \u001b[36m0.8350\u001b[0m        \u001b[32m0.3744\u001b[0m                     0.7500        0.5469  0.0010  0.2184\n",
      "     21                     0.8262        0.3885                     0.7369        0.5519  0.0007  0.2124\n",
      "     22                     0.8350        \u001b[32m0.3690\u001b[0m                     0.7500        0.5504  0.0004  0.2101\n",
      "     23                     \u001b[36m0.8439\u001b[0m        \u001b[32m0.3622\u001b[0m                     0.7519        0.5468  0.0002  0.2176\n",
      "     24                     0.8430        \u001b[32m0.3503\u001b[0m                     0.7500        0.5480  0.0000  0.2138\n",
      "     25                     0.8360        0.3735                     0.7500        0.5476  0.0000  0.2234\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5738\u001b[0m        \u001b[32m1.0746\u001b[0m                     \u001b[35m0.6362\u001b[0m        \u001b[31m0.6544\u001b[0m  0.0100  0.2144\n",
      "      2                     \u001b[36m0.6421\u001b[0m        \u001b[32m0.7763\u001b[0m                     \u001b[35m0.7071\u001b[0m        \u001b[31m0.5802\u001b[0m  0.0100  0.2344\n",
      "      3                     \u001b[36m0.6822\u001b[0m        \u001b[32m0.6255\u001b[0m                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.5537\u001b[0m  0.0098  0.2138\n",
      "      4                     \u001b[36m0.6869\u001b[0m        0.6270                     0.7090        0.6158  0.0096  0.2119\n",
      "      5                     \u001b[36m0.7019\u001b[0m        0.6341                     0.6903        0.6156  0.0093  0.2324\n",
      "      6                     \u001b[36m0.7089\u001b[0m        \u001b[32m0.5898\u001b[0m                     \u001b[35m0.7146\u001b[0m        0.5882  0.0090  0.2254\n",
      "      7                     0.7047        0.6115                     0.6959        0.5760  0.0085  0.2195\n",
      "      8                     \u001b[36m0.7383\u001b[0m        \u001b[32m0.5674\u001b[0m                     \u001b[35m0.7425\u001b[0m        \u001b[31m0.5498\u001b[0m  0.0080  0.2164\n",
      "      9                     0.7322        \u001b[32m0.5432\u001b[0m                     \u001b[35m0.7463\u001b[0m        \u001b[31m0.5292\u001b[0m  0.0075  0.2113\n",
      "     10                     \u001b[36m0.7439\u001b[0m        \u001b[32m0.5308\u001b[0m                     0.7313        0.5796  0.0069  0.2149\n",
      "     11                     \u001b[36m0.7678\u001b[0m        \u001b[32m0.5051\u001b[0m                     \u001b[35m0.7519\u001b[0m        0.5359  0.0063  0.2163\n",
      "     12                     \u001b[36m0.7785\u001b[0m        \u001b[32m0.4802\u001b[0m                     0.7500        0.5531  0.0057  0.2306\n",
      "     13                     0.7771        \u001b[32m0.4632\u001b[0m                     0.7463        0.5454  0.0050  0.2244\n",
      "     14                     \u001b[36m0.8000\u001b[0m        \u001b[32m0.4483\u001b[0m                     0.7500        0.5533  0.0043  0.2184\n",
      "     15                     \u001b[36m0.8033\u001b[0m        \u001b[32m0.4213\u001b[0m                     \u001b[35m0.7556\u001b[0m        0.5670  0.0037  0.2144\n",
      "     16                     \u001b[36m0.8131\u001b[0m        \u001b[32m0.4156\u001b[0m                     0.7388        0.5411  0.0031  0.2214\n",
      "     17                     \u001b[36m0.8257\u001b[0m        \u001b[32m0.3910\u001b[0m                     \u001b[35m0.7593\u001b[0m        0.5569  0.0025  0.2302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18                     \u001b[36m0.8294\u001b[0m        0.3942                     0.7425        0.5518  0.0020  0.2148\n",
      "     19                     \u001b[36m0.8318\u001b[0m        \u001b[32m0.3725\u001b[0m                     0.7537        0.5432  0.0015  0.2095\n",
      "     20                     \u001b[36m0.8341\u001b[0m        \u001b[32m0.3708\u001b[0m                     0.7407        0.5396  0.0010  0.2194\n",
      "     21                     \u001b[36m0.8486\u001b[0m        \u001b[32m0.3577\u001b[0m                     0.7593        0.5414  0.0007  0.2164\n",
      "     22                     0.8486        \u001b[32m0.3474\u001b[0m                     0.7481        0.5422  0.0004  0.2110\n",
      "     23                     0.8421        0.3605                     0.7593        0.5404  0.0002  0.2225\n",
      "     24                     0.8430        \u001b[32m0.3432\u001b[0m                     0.7481        0.5405  0.0000  0.2334\n",
      "     25                     \u001b[36m0.8500\u001b[0m        \u001b[32m0.3385\u001b[0m                     0.7481        0.5404  0.0000  0.2292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5467\u001b[0m        \u001b[32m1.1941\u001b[0m                     \u001b[35m0.6045\u001b[0m        \u001b[31m0.8348\u001b[0m  0.0100  0.2244\n",
      "      2                     \u001b[36m0.5907\u001b[0m        \u001b[32m0.8799\u001b[0m                     \u001b[35m0.6194\u001b[0m        \u001b[31m0.6598\u001b[0m  0.0100  0.2255\n",
      "      3                     \u001b[36m0.6472\u001b[0m        \u001b[32m0.7019\u001b[0m                     \u001b[35m0.6381\u001b[0m        0.7574  0.0098  0.2164\n",
      "      4                     \u001b[36m0.6752\u001b[0m        \u001b[32m0.6694\u001b[0m                     \u001b[35m0.6978\u001b[0m        \u001b[31m0.6083\u001b[0m  0.0096  0.2168\n",
      "      5                     0.6673        0.6803                     0.6828        0.6335  0.0093  0.2164\n",
      "      6                     0.6617        0.7695                     0.6903        0.6531  0.0090  0.2509\n",
      "      7                     \u001b[36m0.6963\u001b[0m        \u001b[32m0.6405\u001b[0m                     0.6269        0.8655  0.0085  0.2541\n",
      "      8                     \u001b[36m0.7117\u001b[0m        \u001b[32m0.5905\u001b[0m                     0.6791        0.6490  0.0080  0.2274\n",
      "      9                     0.7037        0.6104                     0.6623        0.6492  0.0075  0.2174\n",
      "     10                     \u001b[36m0.7262\u001b[0m        \u001b[32m0.5757\u001b[0m                     0.6791        0.6467  0.0069  0.2144\n",
      "     11                     \u001b[36m0.7299\u001b[0m        \u001b[32m0.5465\u001b[0m                     0.6884        0.6760  0.0063  0.2155\n",
      "     12                     \u001b[36m0.7505\u001b[0m        \u001b[32m0.5270\u001b[0m                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.5999\u001b[0m  0.0057  0.2264\n",
      "     13                     0.7463        \u001b[32m0.5266\u001b[0m                     0.6754        0.6372  0.0050  0.2146\n",
      "     14                     \u001b[36m0.7706\u001b[0m        \u001b[32m0.4832\u001b[0m                     \u001b[35m0.7276\u001b[0m        \u001b[31m0.5723\u001b[0m  0.0043  0.2104\n",
      "     15                     0.7636        0.4856                     \u001b[35m0.7425\u001b[0m        \u001b[31m0.5602\u001b[0m  0.0037  0.2114\n",
      "     16                     \u001b[36m0.7808\u001b[0m        \u001b[32m0.4705\u001b[0m                     \u001b[35m0.7519\u001b[0m        \u001b[31m0.5547\u001b[0m  0.0031  0.2106\n",
      "     17                     0.7794        \u001b[32m0.4615\u001b[0m                     \u001b[35m0.7575\u001b[0m        \u001b[31m0.5498\u001b[0m  0.0025  0.2444\n",
      "     18                     \u001b[36m0.7911\u001b[0m        \u001b[32m0.4503\u001b[0m                     0.7276        0.5709  0.0020  0.2284\n",
      "     19                     0.7888        \u001b[32m0.4489\u001b[0m                     0.7351        0.5543  0.0015  0.2155\n",
      "     20                     \u001b[36m0.7986\u001b[0m        \u001b[32m0.4395\u001b[0m                     0.7519        0.5532  0.0010  0.2234\n",
      "     21                     0.7949        0.4452                     0.7332        0.5680  0.0007  0.2115\n",
      "     22                     \u001b[36m0.8056\u001b[0m        \u001b[32m0.4193\u001b[0m                     0.7369        0.5645  0.0004  0.2349\n",
      "     23                     \u001b[36m0.8079\u001b[0m        0.4243                     0.7388        0.5608  0.0002  0.2385\n",
      "     24                     0.8051        0.4282                     0.7444        0.5598  0.0000  0.2307\n",
      "     25                     0.8019        0.4313                     0.7425        0.5596  0.0000  0.2264\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5640\u001b[0m        \u001b[32m1.1760\u001b[0m                     \u001b[35m0.6250\u001b[0m        \u001b[31m0.6505\u001b[0m  0.0100  0.2244\n",
      "      2                     \u001b[36m0.6065\u001b[0m        \u001b[32m0.7958\u001b[0m                     \u001b[35m0.6549\u001b[0m        0.6576  0.0100  0.2194\n",
      "      3                     \u001b[36m0.6439\u001b[0m        \u001b[32m0.6849\u001b[0m                     \u001b[35m0.6623\u001b[0m        \u001b[31m0.6215\u001b[0m  0.0098  0.2314\n",
      "      4                     \u001b[36m0.6738\u001b[0m        \u001b[32m0.6713\u001b[0m                     0.6455        0.7293  0.0096  0.2268\n",
      "      5                     \u001b[36m0.6949\u001b[0m        \u001b[32m0.6336\u001b[0m                     \u001b[35m0.6828\u001b[0m        0.6252  0.0093  0.2212\n",
      "      6                     \u001b[36m0.6958\u001b[0m        \u001b[32m0.6088\u001b[0m                     0.6660        0.6320  0.0090  0.2204\n",
      "      7                     \u001b[36m0.6967\u001b[0m        0.6234                     \u001b[35m0.6959\u001b[0m        \u001b[31m0.5833\u001b[0m  0.0085  0.2100\n",
      "      8                     \u001b[36m0.7047\u001b[0m        0.6107                     0.6959        0.5897  0.0080  0.2110\n",
      "      9                     \u001b[36m0.7089\u001b[0m        \u001b[32m0.6057\u001b[0m                     0.6194        0.7250  0.0075  0.2293\n",
      "     10                     \u001b[36m0.7210\u001b[0m        \u001b[32m0.5764\u001b[0m                     \u001b[35m0.7239\u001b[0m        0.5872  0.0069  0.2142\n",
      "     11                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5301\u001b[0m                     0.6903        \u001b[31m0.5624\u001b[0m  0.0063  0.2097\n",
      "     12                     \u001b[36m0.7556\u001b[0m        \u001b[32m0.4989\u001b[0m                     0.7071        0.6209  0.0057  0.2105\n",
      "     13                     0.7509        0.5108                     0.7052        0.5749  0.0050  0.2109\n",
      "     14                     \u001b[36m0.7621\u001b[0m        \u001b[32m0.4956\u001b[0m                     0.7108        0.5661  0.0043  0.2095\n",
      "     15                     \u001b[36m0.7907\u001b[0m        \u001b[32m0.4473\u001b[0m                     0.7239        0.5932  0.0037  0.2099\n",
      "     16                     0.7827        0.4645                     0.7052        0.6254  0.0031  0.2104\n",
      "     17                     0.7869        0.4573                     0.7108        0.5703  0.0025  0.2093\n",
      "     18                     \u001b[36m0.7995\u001b[0m        \u001b[32m0.4316\u001b[0m                     0.7052        0.5865  0.0020  0.2092\n",
      "     19                     \u001b[36m0.8117\u001b[0m        \u001b[32m0.4118\u001b[0m                     0.7146        0.5901  0.0015  0.2094\n",
      "     20                     \u001b[36m0.8164\u001b[0m        \u001b[32m0.4090\u001b[0m                     0.7239        0.5883  0.0010  0.2104\n",
      "     21                     0.8033        0.4175                     0.7239        0.5904  0.0007  0.2105\n",
      "     22                     \u001b[36m0.8248\u001b[0m        \u001b[32m0.3992\u001b[0m                     0.7164        0.5882  0.0004  0.2105\n",
      "     23                     \u001b[36m0.8262\u001b[0m        \u001b[32m0.3935\u001b[0m                     0.7164        0.5913  0.0002  0.2096\n",
      "     24                     \u001b[36m0.8262\u001b[0m        0.3957                     0.7146        0.5898  0.0000  0.2099\n",
      "     25                     0.8098        0.4081                     0.7146        0.5899  0.0000  0.2099\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5519\u001b[0m        \u001b[32m1.2348\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6516\u001b[0m  0.0100  0.2015\n",
      "      2                     \u001b[36m0.5776\u001b[0m        \u001b[32m0.8574\u001b[0m                     0.5933        0.8842  0.0100  0.2085\n",
      "      3                     \u001b[36m0.6379\u001b[0m        \u001b[32m0.7132\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.6356\u001b[0m  0.0098  0.2094\n",
      "      4                     \u001b[36m0.6804\u001b[0m        \u001b[32m0.6589\u001b[0m                     \u001b[35m0.7015\u001b[0m        \u001b[31m0.5710\u001b[0m  0.0096  0.2094\n",
      "      5                     \u001b[36m0.7075\u001b[0m        \u001b[32m0.6106\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5506\u001b[0m  0.0093  0.2094\n",
      "      6                     0.6925        0.6243                     0.7034        0.6000  0.0090  0.2095\n",
      "      7                     \u001b[36m0.7079\u001b[0m        \u001b[32m0.5844\u001b[0m                     \u001b[35m0.7407\u001b[0m        \u001b[31m0.5414\u001b[0m  0.0085  0.2114\n",
      "      8                     0.6907        0.6406                     0.6903        0.6175  0.0080  0.2099\n",
      "      9                     \u001b[36m0.7196\u001b[0m        \u001b[32m0.5585\u001b[0m                     \u001b[35m0.7444\u001b[0m        0.5462  0.0075  0.2111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     \u001b[36m0.7374\u001b[0m        \u001b[32m0.5341\u001b[0m                     0.7276        0.5580  0.0069  0.2094\n",
      "     11                     \u001b[36m0.7519\u001b[0m        0.5430                     0.7146        0.5527  0.0063  0.2095\n",
      "     12                     \u001b[36m0.7547\u001b[0m        \u001b[32m0.5088\u001b[0m                     0.7407        0.5543  0.0057  0.2107\n",
      "     13                     \u001b[36m0.7738\u001b[0m        \u001b[32m0.4929\u001b[0m                     0.7444        0.5656  0.0050  0.2085\n",
      "     14                     \u001b[36m0.7748\u001b[0m        \u001b[32m0.4787\u001b[0m                     0.7313        0.5891  0.0043  0.2099\n",
      "     15                     \u001b[36m0.7911\u001b[0m        \u001b[32m0.4523\u001b[0m                     0.7313        0.5738  0.0037  0.2095\n",
      "     16                     \u001b[36m0.8009\u001b[0m        0.4597                     0.7388        0.5447  0.0031  0.2095\n",
      "     17                     0.7762        0.4614                     0.7388        0.5503  0.0025  0.2107\n",
      "     18                     0.7897        \u001b[32m0.4460\u001b[0m                     0.7313        0.5666  0.0020  0.2103\n",
      "     19                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4104\u001b[0m                     0.7295        0.5605  0.0015  0.2113\n",
      "     20                     \u001b[36m0.8126\u001b[0m        0.4122                     0.7369        0.5508  0.0010  0.2324\n",
      "     21                     0.8126        0.4134                     0.7351        0.5507  0.0007  0.2204\n",
      "     22                     \u001b[36m0.8154\u001b[0m        \u001b[32m0.4068\u001b[0m                     0.7425        0.5472  0.0004  0.2149\n",
      "     23                     \u001b[36m0.8266\u001b[0m        \u001b[32m0.3985\u001b[0m                     0.7332        0.5451  0.0002  0.2123\n",
      "     24                     \u001b[36m0.8290\u001b[0m        \u001b[32m0.3834\u001b[0m                     0.7351        0.5476  0.0000  0.2144\n",
      "     25                     0.8215        0.3943                     0.7332        0.5465  0.0000  0.2118\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5832\u001b[0m        \u001b[32m1.0179\u001b[0m                     \u001b[35m0.6399\u001b[0m        \u001b[31m0.6779\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.6136\u001b[0m        \u001b[32m0.8018\u001b[0m                     \u001b[35m0.6884\u001b[0m        \u001b[31m0.6453\u001b[0m  0.0100  0.2095\n",
      "      3                     \u001b[36m0.6477\u001b[0m        \u001b[32m0.7170\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.6072\u001b[0m  0.0098  0.2100\n",
      "      4                     \u001b[36m0.6715\u001b[0m        \u001b[32m0.6897\u001b[0m                     0.6063        0.9030  0.0096  0.2084\n",
      "      5                     0.6621        0.7291                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5645\u001b[0m  0.0093  0.2085\n",
      "      6                     \u001b[36m0.6790\u001b[0m        \u001b[32m0.6449\u001b[0m                     0.7295        \u001b[31m0.5482\u001b[0m  0.0090  0.2085\n",
      "      7                     \u001b[36m0.7070\u001b[0m        \u001b[32m0.5879\u001b[0m                     \u001b[35m0.7369\u001b[0m        \u001b[31m0.5383\u001b[0m  0.0085  0.2085\n",
      "      8                     \u001b[36m0.7271\u001b[0m        \u001b[32m0.5646\u001b[0m                     0.7351        \u001b[31m0.5333\u001b[0m  0.0080  0.2117\n",
      "      9                     \u001b[36m0.7364\u001b[0m        \u001b[32m0.5467\u001b[0m                     \u001b[35m0.7556\u001b[0m        \u001b[31m0.5307\u001b[0m  0.0075  0.2352\n",
      "     10                     0.7234        0.5633                     0.7407        0.5348  0.0069  0.2128\n",
      "     11                     \u001b[36m0.7444\u001b[0m        \u001b[32m0.5250\u001b[0m                     0.7220        0.5840  0.0063  0.2095\n",
      "     12                     \u001b[36m0.7491\u001b[0m        \u001b[32m0.5194\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.5238\u001b[0m  0.0057  0.2095\n",
      "     13                     \u001b[36m0.7631\u001b[0m        \u001b[32m0.4973\u001b[0m                     0.7556        \u001b[31m0.5188\u001b[0m  0.0050  0.2095\n",
      "     14                     \u001b[36m0.7729\u001b[0m        \u001b[32m0.4918\u001b[0m                     0.7425        0.5414  0.0043  0.2094\n",
      "     15                     \u001b[36m0.7855\u001b[0m        \u001b[32m0.4638\u001b[0m                     0.7575        0.5279  0.0037  0.2095\n",
      "     16                     0.7808        0.4687                     0.7649        0.5301  0.0031  0.2094\n",
      "     17                     \u001b[36m0.7944\u001b[0m        \u001b[32m0.4376\u001b[0m                     0.7724        0.5386  0.0025  0.2109\n",
      "     18                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4222\u001b[0m                     0.7612        0.5287  0.0020  0.2097\n",
      "     19                     \u001b[36m0.8136\u001b[0m        \u001b[32m0.4188\u001b[0m                     0.7668        0.5257  0.0015  0.2101\n",
      "     20                     0.8112        \u001b[32m0.4186\u001b[0m                     0.7668        0.5261  0.0010  0.2104\n",
      "     21                     \u001b[36m0.8266\u001b[0m        \u001b[32m0.4056\u001b[0m                     0.7593        0.5319  0.0007  0.2104\n",
      "     22                     0.8098        0.4062                     0.7649        0.5316  0.0004  0.2104\n",
      "     23                     0.8140        0.4113                     0.7649        0.5332  0.0002  0.2114\n",
      "     24                     0.8154        \u001b[32m0.4003\u001b[0m                     0.7649        0.5329  0.0000  0.2107\n",
      "     25                     \u001b[36m0.8313\u001b[0m        \u001b[32m0.3982\u001b[0m                     0.7649        0.5326  0.0000  0.2103\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5565\u001b[0m        \u001b[32m1.2552\u001b[0m                     \u001b[35m0.6381\u001b[0m        \u001b[31m0.6921\u001b[0m  0.0100  0.2025\n",
      "      2                     \u001b[36m0.6364\u001b[0m        \u001b[32m0.8052\u001b[0m                     \u001b[35m0.6735\u001b[0m        \u001b[31m0.6066\u001b[0m  0.0100  0.2085\n",
      "      3                     \u001b[36m0.6514\u001b[0m        \u001b[32m0.7025\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.5935\u001b[0m  0.0098  0.2100\n",
      "      4                     \u001b[36m0.6869\u001b[0m        \u001b[32m0.6620\u001b[0m                     0.6828        \u001b[31m0.5839\u001b[0m  0.0096  0.2095\n",
      "      5                     0.6827        0.6696                     0.6754        0.6295  0.0093  0.2108\n",
      "      6                     \u001b[36m0.6916\u001b[0m        \u001b[32m0.6393\u001b[0m                     \u001b[35m0.6978\u001b[0m        0.6220  0.0090  0.2104\n",
      "      7                     \u001b[36m0.6981\u001b[0m        \u001b[32m0.6368\u001b[0m                     \u001b[35m0.7034\u001b[0m        \u001b[31m0.5698\u001b[0m  0.0085  0.2099\n",
      "      8                     \u001b[36m0.7220\u001b[0m        \u001b[32m0.5706\u001b[0m                     0.7034        \u001b[31m0.5586\u001b[0m  0.0080  0.2103\n",
      "      9                     \u001b[36m0.7243\u001b[0m        \u001b[32m0.5636\u001b[0m                     \u001b[35m0.7052\u001b[0m        0.5816  0.0075  0.2102\n",
      "     10                     \u001b[36m0.7369\u001b[0m        \u001b[32m0.5419\u001b[0m                     0.6866        0.5631  0.0069  0.2094\n",
      "     11                     \u001b[36m0.7579\u001b[0m        \u001b[32m0.5199\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5396\u001b[0m  0.0063  0.2099\n",
      "     12                     0.7477        \u001b[32m0.5156\u001b[0m                     0.7127        0.5593  0.0057  0.2107\n",
      "     13                     0.7486        \u001b[32m0.5140\u001b[0m                     \u001b[35m0.7388\u001b[0m        0.5560  0.0050  0.2090\n",
      "     14                     0.7575        \u001b[32m0.5106\u001b[0m                     \u001b[35m0.7407\u001b[0m        \u001b[31m0.5269\u001b[0m  0.0043  0.2095\n",
      "     15                     \u001b[36m0.7766\u001b[0m        \u001b[32m0.4723\u001b[0m                     0.7146        0.5517  0.0037  0.2100\n",
      "     16                     0.7668        0.4761                     0.7220        0.5514  0.0031  0.2093\n",
      "     17                     \u001b[36m0.7921\u001b[0m        \u001b[32m0.4549\u001b[0m                     0.7332        0.5382  0.0025  0.2104\n",
      "     18                     0.7888        \u001b[32m0.4488\u001b[0m                     0.7295        0.5459  0.0020  0.2096\n",
      "     19                     \u001b[36m0.8056\u001b[0m        \u001b[32m0.4223\u001b[0m                     0.7295        0.5442  0.0015  0.2094\n",
      "     20                     0.7967        0.4244                     0.7239        0.5393  0.0010  0.2105\n",
      "     21                     \u001b[36m0.8173\u001b[0m        \u001b[32m0.4102\u001b[0m                     0.7332        0.5383  0.0007  0.2098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22                     \u001b[36m0.8192\u001b[0m        \u001b[32m0.4051\u001b[0m                     0.7239        0.5393  0.0004  0.2096\n",
      "     23                     0.8028        0.4132                     0.7183        0.5396  0.0002  0.2092\n",
      "     24                     0.8098        0.4072                     0.7201        0.5397  0.0000  0.2082\n",
      "     25                     \u001b[36m0.8220\u001b[0m        0.4059                     0.7201        0.5400  0.0000  0.2105\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5519\u001b[0m        \u001b[32m1.3098\u001b[0m                     \u001b[35m0.6026\u001b[0m        \u001b[31m0.7730\u001b[0m  0.0100  0.2025\n",
      "      2                     \u001b[36m0.5855\u001b[0m        \u001b[32m0.9641\u001b[0m                     \u001b[35m0.6362\u001b[0m        \u001b[31m0.6553\u001b[0m  0.0100  0.2095\n",
      "      3                     \u001b[36m0.6411\u001b[0m        \u001b[32m0.7107\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.6222\u001b[0m  0.0098  0.2094\n",
      "      4                     \u001b[36m0.6430\u001b[0m        0.7682                     \u001b[35m0.6940\u001b[0m        0.6358  0.0096  0.2098\n",
      "      5                     \u001b[36m0.6738\u001b[0m        \u001b[32m0.6647\u001b[0m                     0.6250        0.6680  0.0093  0.2105\n",
      "      6                     \u001b[36m0.6893\u001b[0m        \u001b[32m0.6423\u001b[0m                     \u001b[35m0.7052\u001b[0m        \u001b[31m0.6205\u001b[0m  0.0090  0.2098\n",
      "      7                     \u001b[36m0.7121\u001b[0m        \u001b[32m0.5897\u001b[0m                     0.6884        \u001b[31m0.6080\u001b[0m  0.0085  0.2094\n",
      "      8                     \u001b[36m0.7126\u001b[0m        \u001b[32m0.5808\u001b[0m                     0.7015        \u001b[31m0.5961\u001b[0m  0.0080  0.2089\n",
      "      9                     \u001b[36m0.7131\u001b[0m        0.6017                     0.6735        0.6512  0.0075  0.2088\n",
      "     10                     \u001b[36m0.7341\u001b[0m        \u001b[32m0.5429\u001b[0m                     0.6996        0.6503  0.0069  0.2089\n",
      "     11                     \u001b[36m0.7439\u001b[0m        0.5513                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5682\u001b[0m  0.0063  0.2112\n",
      "     12                     0.7421        \u001b[32m0.5155\u001b[0m                     0.7071        0.6148  0.0057  0.2105\n",
      "     13                     0.7439        0.5291                     0.7220        0.5860  0.0050  0.2103\n",
      "     14                     \u001b[36m0.7692\u001b[0m        \u001b[32m0.5094\u001b[0m                     0.6940        0.5985  0.0043  0.2091\n",
      "     15                     0.7528        0.5097                     0.7015        0.6096  0.0037  0.2083\n",
      "     16                     \u001b[36m0.7790\u001b[0m        \u001b[32m0.4786\u001b[0m                     \u001b[35m0.7425\u001b[0m        \u001b[31m0.5676\u001b[0m  0.0031  0.2104\n",
      "     17                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4560\u001b[0m                     0.7183        0.5886  0.0025  0.2100\n",
      "     18                     0.7701        0.4678                     \u001b[35m0.7463\u001b[0m        0.5741  0.0020  0.2091\n",
      "     19                     \u001b[36m0.7841\u001b[0m        0.4581                     0.7201        0.5792  0.0015  0.2093\n",
      "     20                     \u001b[36m0.7939\u001b[0m        \u001b[32m0.4491\u001b[0m                     0.7332        0.5727  0.0010  0.2096\n",
      "     21                     0.7860        \u001b[32m0.4429\u001b[0m                     0.7425        0.5751  0.0007  0.2105\n",
      "     22                     \u001b[36m0.7972\u001b[0m        \u001b[32m0.4324\u001b[0m                     0.7369        0.5784  0.0004  0.2091\n",
      "     23                     0.7972        \u001b[32m0.4240\u001b[0m                     0.7332        0.5775  0.0002  0.2111\n",
      "     24                     \u001b[36m0.8042\u001b[0m        0.4268                     0.7388        0.5763  0.0000  0.2088\n",
      "     25                     0.8009        0.4347                     0.7332        0.5783  0.0000  0.2094\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5500\u001b[0m        \u001b[32m1.3654\u001b[0m                     \u001b[35m0.5616\u001b[0m        \u001b[31m0.8892\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.5925\u001b[0m        \u001b[32m1.0233\u001b[0m                     \u001b[35m0.6455\u001b[0m        \u001b[31m0.6603\u001b[0m  0.0100  0.2079\n",
      "      3                     \u001b[36m0.6523\u001b[0m        \u001b[32m0.7279\u001b[0m                     0.6455        0.6892  0.0098  0.2085\n",
      "      4                     \u001b[36m0.6626\u001b[0m        \u001b[32m0.6633\u001b[0m                     \u001b[35m0.6530\u001b[0m        \u001b[31m0.6421\u001b[0m  0.0096  0.2108\n",
      "      5                     \u001b[36m0.6958\u001b[0m        \u001b[32m0.6195\u001b[0m                     \u001b[35m0.6940\u001b[0m        \u001b[31m0.5825\u001b[0m  0.0093  0.2111\n",
      "      6                     \u001b[36m0.7145\u001b[0m        \u001b[32m0.5865\u001b[0m                     0.6604        0.7160  0.0090  0.2095\n",
      "      7                     0.7098        0.6267                     0.6474        0.6441  0.0085  0.2096\n",
      "      8                     0.7084        0.5900                     \u001b[35m0.7052\u001b[0m        \u001b[31m0.5682\u001b[0m  0.0080  0.2085\n",
      "      9                     \u001b[36m0.7290\u001b[0m        \u001b[32m0.5426\u001b[0m                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5613\u001b[0m  0.0075  0.2112\n",
      "     10                     0.7187        0.5667                     0.6940        0.6368  0.0069  0.2101\n",
      "     11                     \u001b[36m0.7449\u001b[0m        \u001b[32m0.5253\u001b[0m                     0.7034        0.5694  0.0063  0.2102\n",
      "     12                     \u001b[36m0.7463\u001b[0m        \u001b[32m0.5191\u001b[0m                     \u001b[35m0.7201\u001b[0m        0.5768  0.0057  0.2108\n",
      "     13                     0.7439        0.5397                     0.6940        0.6002  0.0050  0.2093\n",
      "     14                     \u001b[36m0.7654\u001b[0m        \u001b[32m0.4931\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5390\u001b[0m  0.0043  0.2095\n",
      "     15                     0.7612        \u001b[32m0.4928\u001b[0m                     0.6828        0.5971  0.0037  0.2113\n",
      "     16                     \u001b[36m0.7729\u001b[0m        \u001b[32m0.4776\u001b[0m                     0.7332        0.5544  0.0031  0.2085\n",
      "     17                     \u001b[36m0.7785\u001b[0m        \u001b[32m0.4737\u001b[0m                     0.7052        0.5631  0.0025  0.2111\n",
      "     18                     \u001b[36m0.7925\u001b[0m        \u001b[32m0.4505\u001b[0m                     0.7295        0.5625  0.0020  0.2096\n",
      "     19                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4382\u001b[0m                     0.6996        0.5747  0.0015  0.2100\n",
      "     20                     \u001b[36m0.8070\u001b[0m        \u001b[32m0.4273\u001b[0m                     0.7108        0.5597  0.0010  0.2101\n",
      "     21                     0.8005        \u001b[32m0.4212\u001b[0m                     0.7257        0.5625  0.0007  0.2109\n",
      "     22                     \u001b[36m0.8159\u001b[0m        \u001b[32m0.4145\u001b[0m                     0.7164        0.5607  0.0004  0.2104\n",
      "     23                     0.7986        0.4290                     0.7257        0.5635  0.0002  0.2105\n",
      "     24                     0.8103        0.4157                     0.7239        0.5633  0.0000  0.2104\n",
      "     25                     \u001b[36m0.8164\u001b[0m        \u001b[32m0.4038\u001b[0m                     0.7239        0.5630  0.0000  0.2100\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5687\u001b[0m        \u001b[32m1.0989\u001b[0m                     \u001b[35m0.6642\u001b[0m        \u001b[31m0.6196\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.6262\u001b[0m        \u001b[32m0.8111\u001b[0m                     0.6250        0.8626  0.0100  0.2075\n",
      "      3                     \u001b[36m0.6664\u001b[0m        \u001b[32m0.7367\u001b[0m                     0.6623        0.6718  0.0098  0.2095\n",
      "      4                     \u001b[36m0.6860\u001b[0m        \u001b[32m0.6406\u001b[0m                     \u001b[35m0.6772\u001b[0m        0.6390  0.0096  0.2102\n",
      "      5                     0.6794        \u001b[32m0.6393\u001b[0m                     \u001b[35m0.6791\u001b[0m        0.6518  0.0093  0.2106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6                     \u001b[36m0.7079\u001b[0m        \u001b[32m0.6045\u001b[0m                     \u001b[35m0.6884\u001b[0m        0.6464  0.0090  0.2105\n",
      "      7                     0.7028        \u001b[32m0.5940\u001b[0m                     0.6828        0.6603  0.0085  0.2104\n",
      "      8                     0.7019        0.6022                     \u001b[35m0.6903\u001b[0m        0.6447  0.0080  0.2096\n",
      "      9                     \u001b[36m0.7210\u001b[0m        \u001b[32m0.5639\u001b[0m                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5858\u001b[0m  0.0075  0.2098\n",
      "     10                     \u001b[36m0.7505\u001b[0m        \u001b[32m0.5331\u001b[0m                     0.7071        0.5984  0.0069  0.2094\n",
      "     11                     0.7369        0.5460                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.5753\u001b[0m  0.0063  0.2105\n",
      "     12                     \u001b[36m0.7528\u001b[0m        \u001b[32m0.5257\u001b[0m                     0.7127        \u001b[31m0.5727\u001b[0m  0.0057  0.2098\n",
      "     13                     \u001b[36m0.7621\u001b[0m        \u001b[32m0.5040\u001b[0m                     0.7146        \u001b[31m0.5622\u001b[0m  0.0050  0.2095\n",
      "     14                     \u001b[36m0.7701\u001b[0m        0.5096                     0.6940        0.6236  0.0043  0.2095\n",
      "     15                     0.7696        \u001b[32m0.4928\u001b[0m                     0.7164        0.5822  0.0037  0.2106\n",
      "     16                     \u001b[36m0.7762\u001b[0m        \u001b[32m0.4752\u001b[0m                     0.7239        \u001b[31m0.5521\u001b[0m  0.0031  0.2094\n",
      "     17                     \u001b[36m0.7846\u001b[0m        \u001b[32m0.4626\u001b[0m                     0.7090        0.5609  0.0025  0.2096\n",
      "     18                     \u001b[36m0.7953\u001b[0m        \u001b[32m0.4454\u001b[0m                     0.7220        \u001b[31m0.5480\u001b[0m  0.0020  0.2097\n",
      "     19                     0.7921        0.4495                     0.7183        \u001b[31m0.5429\u001b[0m  0.0015  0.2100\n",
      "     20                     \u001b[36m0.8019\u001b[0m        \u001b[32m0.4351\u001b[0m                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5383\u001b[0m  0.0010  0.2095\n",
      "     21                     0.7921        0.4367                     0.7295        0.5480  0.0007  0.2093\n",
      "     22                     0.8014        \u001b[32m0.4311\u001b[0m                     0.7220        0.5465  0.0004  0.2111\n",
      "     23                     \u001b[36m0.8023\u001b[0m        \u001b[32m0.4296\u001b[0m                     0.7220        0.5476  0.0002  0.2117\n",
      "     24                     \u001b[36m0.8136\u001b[0m        \u001b[32m0.4048\u001b[0m                     0.7276        0.5447  0.0000  0.2110\n",
      "     25                     \u001b[36m0.8224\u001b[0m        \u001b[32m0.4045\u001b[0m                     0.7201        0.5486  0.0000  0.2103\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5051\u001b[0m        \u001b[32m1.6408\u001b[0m                     \u001b[35m0.6082\u001b[0m        \u001b[31m0.7981\u001b[0m  0.0100  0.2184\n",
      "      2                     \u001b[36m0.5706\u001b[0m        \u001b[32m0.9835\u001b[0m                     \u001b[35m0.6250\u001b[0m        \u001b[31m0.6996\u001b[0m  0.0100  0.2304\n",
      "      3                     \u001b[36m0.6439\u001b[0m        \u001b[32m0.7313\u001b[0m                     \u001b[35m0.6866\u001b[0m        \u001b[31m0.6133\u001b[0m  0.0098  0.2194\n",
      "      4                     \u001b[36m0.6668\u001b[0m        \u001b[32m0.6739\u001b[0m                     0.6772        0.6307  0.0096  0.2161\n",
      "      5                     \u001b[36m0.6841\u001b[0m        \u001b[32m0.6483\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.5840\u001b[0m  0.0093  0.2085\n",
      "      6                     0.6841        \u001b[32m0.6327\u001b[0m                     0.6735        0.6013  0.0090  0.2104\n",
      "      7                     \u001b[36m0.7089\u001b[0m        \u001b[32m0.5940\u001b[0m                     0.6698        0.6090  0.0085  0.2154\n",
      "      8                     \u001b[36m0.7332\u001b[0m        \u001b[32m0.5662\u001b[0m                     0.6679        \u001b[31m0.5820\u001b[0m  0.0080  0.2147\n",
      "      9                     0.7084        0.5948                     \u001b[35m0.7052\u001b[0m        \u001b[31m0.5772\u001b[0m  0.0075  0.2105\n",
      "     10                     0.7154        0.5856                     0.7034        \u001b[31m0.5535\u001b[0m  0.0069  0.2280\n",
      "     11                     \u001b[36m0.7350\u001b[0m        \u001b[32m0.5447\u001b[0m                     0.7015        0.5570  0.0063  0.2098\n",
      "     12                     \u001b[36m0.7528\u001b[0m        \u001b[32m0.5078\u001b[0m                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5449\u001b[0m  0.0057  0.2157\n",
      "     13                     0.7481        0.5124                     0.7034        0.5696  0.0050  0.2096\n",
      "     14                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4886\u001b[0m                     0.6940        0.5932  0.0043  0.2243\n",
      "     15                     0.7589        0.4928                     \u001b[35m0.7257\u001b[0m        0.5606  0.0037  0.2324\n",
      "     16                     \u001b[36m0.7762\u001b[0m        0.4901                     0.6996        0.5604  0.0031  0.2324\n",
      "     17                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.4562\u001b[0m                     0.7071        0.5499  0.0025  0.2122\n",
      "     18                     \u001b[36m0.7902\u001b[0m        0.4563                     0.7201        0.5501  0.0020  0.2103\n",
      "     19                     0.7888        \u001b[32m0.4524\u001b[0m                     0.7090        0.5593  0.0015  0.2155\n",
      "     20                     0.7864        0.4537                     0.6903        0.5621  0.0010  0.2214\n",
      "     21                     \u001b[36m0.7981\u001b[0m        \u001b[32m0.4434\u001b[0m                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5444\u001b[0m  0.0007  0.2152\n",
      "     22                     \u001b[36m0.8084\u001b[0m        \u001b[32m0.4231\u001b[0m                     0.7276        0.5473  0.0004  0.2115\n",
      "     23                     \u001b[36m0.8126\u001b[0m        0.4273                     0.7239        0.5456  0.0002  0.2109\n",
      "     24                     0.8079        \u001b[32m0.4167\u001b[0m                     0.7276        0.5453  0.0000  0.2109\n",
      "     25                     0.8051        0.4171                     \u001b[35m0.7332\u001b[0m        0.5455  0.0000  0.2105\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5556\u001b[0m        \u001b[32m1.4724\u001b[0m                     \u001b[35m0.6418\u001b[0m        \u001b[31m0.6589\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.6061\u001b[0m        \u001b[32m0.8259\u001b[0m                     0.6362        0.6781  0.0100  0.2101\n",
      "      3                     \u001b[36m0.6477\u001b[0m        \u001b[32m0.7236\u001b[0m                     0.6157        0.6693  0.0098  0.2105\n",
      "      4                     \u001b[36m0.6607\u001b[0m        \u001b[32m0.6749\u001b[0m                     \u001b[35m0.6567\u001b[0m        \u001b[31m0.6516\u001b[0m  0.0096  0.2100\n",
      "      5                     \u001b[36m0.6748\u001b[0m        \u001b[32m0.6548\u001b[0m                     \u001b[35m0.7052\u001b[0m        \u001b[31m0.5792\u001b[0m  0.0093  0.2114\n",
      "      6                     \u001b[36m0.6888\u001b[0m        \u001b[32m0.6140\u001b[0m                     0.6623        0.6545  0.0090  0.2405\n",
      "      7                     \u001b[36m0.6972\u001b[0m        \u001b[32m0.6132\u001b[0m                     0.6437        0.6910  0.0085  0.2274\n",
      "      8                     \u001b[36m0.6995\u001b[0m        \u001b[32m0.6109\u001b[0m                     0.6978        0.5837  0.0080  0.2239\n",
      "      9                     0.6822        0.6670                     0.7034        \u001b[31m0.5703\u001b[0m  0.0075  0.2224\n",
      "     10                     \u001b[36m0.7103\u001b[0m        0.6538                     \u001b[35m0.7146\u001b[0m        0.5958  0.0069  0.2332\n",
      "     11                     \u001b[36m0.7201\u001b[0m        \u001b[32m0.5740\u001b[0m                     \u001b[35m0.7332\u001b[0m        0.5730  0.0063  0.2184\n",
      "     12                     \u001b[36m0.7435\u001b[0m        \u001b[32m0.5374\u001b[0m                     0.7201        \u001b[31m0.5679\u001b[0m  0.0057  0.2204\n",
      "     13                     \u001b[36m0.7542\u001b[0m        \u001b[32m0.5082\u001b[0m                     0.7276        \u001b[31m0.5583\u001b[0m  0.0050  0.2244\n",
      "     14                     \u001b[36m0.7626\u001b[0m        0.5131                     0.7257        0.5606  0.0043  0.2104\n",
      "     15                     0.7603        \u001b[32m0.4835\u001b[0m                     0.7183        0.5728  0.0037  0.2125\n",
      "     16                     \u001b[36m0.7743\u001b[0m        \u001b[32m0.4791\u001b[0m                     0.6884        0.6086  0.0031  0.2088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17                     0.7692        \u001b[32m0.4790\u001b[0m                     0.7183        0.5603  0.0025  0.2105\n",
      "     18                     0.7710        \u001b[32m0.4691\u001b[0m                     0.7164        0.5747  0.0020  0.2105\n",
      "     19                     \u001b[36m0.7836\u001b[0m        \u001b[32m0.4471\u001b[0m                     \u001b[35m0.7463\u001b[0m        \u001b[31m0.5388\u001b[0m  0.0015  0.2104\n",
      "     20                     \u001b[36m0.7888\u001b[0m        0.4539                     0.7351        0.5468  0.0010  0.2332\n",
      "     21                     \u001b[36m0.7995\u001b[0m        \u001b[32m0.4384\u001b[0m                     0.7183        0.5482  0.0007  0.2105\n",
      "     22                     \u001b[36m0.8033\u001b[0m        \u001b[32m0.4205\u001b[0m                     0.7351        0.5435  0.0004  0.2184\n",
      "     23                     0.7939        0.4423                     \u001b[35m0.7500\u001b[0m        0.5392  0.0002  0.2384\n",
      "     24                     0.7925        0.4406                     0.7388        0.5423  0.0000  0.2144\n",
      "     25                     0.8005        0.4318                     0.7388        0.5400  0.0000  0.2105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5009\u001b[0m        \u001b[32m1.2871\u001b[0m                     \u001b[35m0.5243\u001b[0m        \u001b[31m0.7675\u001b[0m  0.0100  0.2184\n",
      "      2                     \u001b[36m0.5467\u001b[0m        \u001b[32m0.8455\u001b[0m                     \u001b[35m0.5280\u001b[0m        \u001b[31m0.7448\u001b[0m  0.0100  0.2125\n",
      "      3                     \u001b[36m0.5505\u001b[0m        \u001b[32m0.7795\u001b[0m                     \u001b[35m0.5802\u001b[0m        \u001b[31m0.6889\u001b[0m  0.0098  0.2484\n",
      "      4                     \u001b[36m0.5785\u001b[0m        \u001b[32m0.7221\u001b[0m                     0.5392        0.7786  0.0096  0.2108\n",
      "      5                     \u001b[36m0.6098\u001b[0m        \u001b[32m0.7070\u001b[0m                     0.5672        0.7062  0.0093  0.2135\n",
      "      6                     \u001b[36m0.6159\u001b[0m        \u001b[32m0.6839\u001b[0m                     0.5709        0.7435  0.0090  0.2114\n",
      "      7                     0.6037        0.7009                     0.5709        \u001b[31m0.6812\u001b[0m  0.0085  0.2166\n",
      "      8                     \u001b[36m0.6458\u001b[0m        \u001b[32m0.6749\u001b[0m                     \u001b[35m0.5970\u001b[0m        0.7000  0.0080  0.2360\n",
      "      9                     \u001b[36m0.6509\u001b[0m        \u001b[32m0.6554\u001b[0m                     \u001b[35m0.6026\u001b[0m        \u001b[31m0.6775\u001b[0m  0.0075  0.2094\n",
      "     10                     \u001b[36m0.6827\u001b[0m        \u001b[32m0.6120\u001b[0m                     \u001b[35m0.6157\u001b[0m        \u001b[31m0.6603\u001b[0m  0.0069  0.2101\n",
      "     11                     0.6589        0.6390                     0.5951        0.7626  0.0063  0.2125\n",
      "     12                     0.6752        0.6284                     0.5653        0.8284  0.0057  0.2105\n",
      "     13                     \u001b[36m0.6879\u001b[0m        0.6352                     0.6119        0.6831  0.0050  0.2132\n",
      "     14                     \u001b[36m0.6916\u001b[0m        \u001b[32m0.5968\u001b[0m                     0.5840        0.7152  0.0043  0.2085\n",
      "     15                     \u001b[36m0.6981\u001b[0m        \u001b[32m0.5870\u001b[0m                     0.6007        0.7210  0.0037  0.2098\n",
      "     16                     \u001b[36m0.7000\u001b[0m        \u001b[32m0.5770\u001b[0m                     0.6119        0.7039  0.0031  0.2126\n",
      "     17                     \u001b[36m0.7103\u001b[0m        \u001b[32m0.5522\u001b[0m                     0.5858        0.7085  0.0025  0.2099\n",
      "     18                     \u001b[36m0.7248\u001b[0m        \u001b[32m0.5470\u001b[0m                     0.5989        0.7260  0.0020  0.2125\n",
      "     19                     \u001b[36m0.7453\u001b[0m        \u001b[32m0.5185\u001b[0m                     0.6045        0.7304  0.0015  0.2091\n",
      "     20                     \u001b[36m0.7589\u001b[0m        \u001b[32m0.5020\u001b[0m                     0.6026        0.7149  0.0010  0.2111\n",
      "     21                     0.7584        0.5099                     0.5989        0.7134  0.0007  0.2104\n",
      "     22                     \u001b[36m0.7607\u001b[0m        \u001b[32m0.4979\u001b[0m                     0.5933        0.7226  0.0004  0.2094\n",
      "     23                     \u001b[36m0.7701\u001b[0m        \u001b[32m0.4911\u001b[0m                     0.5989        0.7180  0.0002  0.2104\n",
      "     24                     0.7589        0.5005                     0.5970        0.7187  0.0000  0.2095\n",
      "     25                     \u001b[36m0.7734\u001b[0m        \u001b[32m0.4868\u001b[0m                     0.5970        0.7188  0.0000  0.2114\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5145\u001b[0m        \u001b[32m1.2018\u001b[0m                     \u001b[35m0.4888\u001b[0m        \u001b[31m0.7570\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.5196\u001b[0m        \u001b[32m0.8636\u001b[0m                     \u001b[35m0.5392\u001b[0m        \u001b[31m0.7136\u001b[0m  0.0100  0.2095\n",
      "      3                     \u001b[36m0.5290\u001b[0m        \u001b[32m0.8340\u001b[0m                     \u001b[35m0.5672\u001b[0m        \u001b[31m0.7006\u001b[0m  0.0098  0.2115\n",
      "      4                     \u001b[36m0.5575\u001b[0m        \u001b[32m0.7648\u001b[0m                     0.5336        0.7211  0.0096  0.2094\n",
      "      5                     \u001b[36m0.5799\u001b[0m        \u001b[32m0.7477\u001b[0m                     0.5205        0.7958  0.0093  0.2104\n",
      "      6                     \u001b[36m0.5977\u001b[0m        0.7553                     \u001b[35m0.5746\u001b[0m        0.7302  0.0090  0.2094\n",
      "      7                     \u001b[36m0.6005\u001b[0m        \u001b[32m0.7304\u001b[0m                     \u001b[35m0.6082\u001b[0m        \u001b[31m0.6844\u001b[0m  0.0085  0.2095\n",
      "      8                     \u001b[36m0.6117\u001b[0m        \u001b[32m0.7180\u001b[0m                     \u001b[35m0.6213\u001b[0m        0.6948  0.0080  0.2107\n",
      "      9                     \u001b[36m0.6458\u001b[0m        \u001b[32m0.6672\u001b[0m                     0.5933        0.6894  0.0075  0.2095\n",
      "     10                     \u001b[36m0.6500\u001b[0m        \u001b[32m0.6488\u001b[0m                     0.6157        \u001b[31m0.6808\u001b[0m  0.0069  0.2115\n",
      "     11                     0.6481        0.6643                     0.5877        0.7835  0.0063  0.2104\n",
      "     12                     \u001b[36m0.6561\u001b[0m        0.6813                     \u001b[35m0.6343\u001b[0m        \u001b[31m0.6761\u001b[0m  0.0057  0.2114\n",
      "     13                     \u001b[36m0.6818\u001b[0m        \u001b[32m0.6266\u001b[0m                     0.6194        0.6920  0.0050  0.2104\n",
      "     14                     \u001b[36m0.6907\u001b[0m        \u001b[32m0.6118\u001b[0m                     \u001b[35m0.6381\u001b[0m        \u001b[31m0.6647\u001b[0m  0.0043  0.2095\n",
      "     15                     \u001b[36m0.7005\u001b[0m        \u001b[32m0.5805\u001b[0m                     0.6138        0.6925  0.0037  0.2114\n",
      "     16                     \u001b[36m0.7061\u001b[0m        0.5910                     0.6138        0.7048  0.0031  0.2101\n",
      "     17                     \u001b[36m0.7164\u001b[0m        \u001b[32m0.5690\u001b[0m                     \u001b[35m0.6399\u001b[0m        0.6846  0.0025  0.2124\n",
      "     18                     0.7112        \u001b[32m0.5523\u001b[0m                     0.6343        0.6910  0.0020  0.2103\n",
      "     19                     \u001b[36m0.7243\u001b[0m        \u001b[32m0.5405\u001b[0m                     0.6325        0.6848  0.0015  0.2109\n",
      "     20                     \u001b[36m0.7276\u001b[0m        \u001b[32m0.5357\u001b[0m                     0.6325        0.6874  0.0010  0.2115\n",
      "     21                     \u001b[36m0.7346\u001b[0m        \u001b[32m0.5204\u001b[0m                     0.6381        0.6871  0.0007  0.2105\n",
      "     22                     \u001b[36m0.7551\u001b[0m        \u001b[32m0.5073\u001b[0m                     \u001b[35m0.6474\u001b[0m        0.6853  0.0004  0.2115\n",
      "     23                     0.7341        0.5266                     0.6418        0.6861  0.0002  0.2094\n",
      "     24                     0.7523        0.5213                     0.6437        0.6869  0.0000  0.2105\n",
      "     25                     0.7449        0.5238                     0.6455        0.6867  0.0000  0.2126\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5196\u001b[0m        \u001b[32m1.2924\u001b[0m                     \u001b[35m0.5168\u001b[0m        \u001b[31m0.7937\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.5327\u001b[0m        \u001b[32m0.8522\u001b[0m                     0.5149        \u001b[31m0.7840\u001b[0m  0.0100  0.2115\n",
      "      3                     \u001b[36m0.5584\u001b[0m        0.8589                     \u001b[35m0.5560\u001b[0m        \u001b[31m0.6997\u001b[0m  0.0098  0.2094\n",
      "      4                     \u001b[36m0.5584\u001b[0m        \u001b[32m0.7671\u001b[0m                     \u001b[35m0.5690\u001b[0m        0.7643  0.0096  0.2125\n",
      "      5                     \u001b[36m0.5864\u001b[0m        \u001b[32m0.7333\u001b[0m                     \u001b[35m0.5746\u001b[0m        0.7034  0.0093  0.2104\n",
      "      6                     \u001b[36m0.5939\u001b[0m        0.7371                     0.5541        \u001b[31m0.6914\u001b[0m  0.0090  0.2106\n",
      "      7                     \u001b[36m0.6257\u001b[0m        \u001b[32m0.7000\u001b[0m                     \u001b[35m0.6157\u001b[0m        \u001b[31m0.6868\u001b[0m  0.0085  0.2105\n",
      "      8                     \u001b[36m0.6439\u001b[0m        \u001b[32m0.6708\u001b[0m                     0.6119        \u001b[31m0.6688\u001b[0m  0.0080  0.2105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.6486\u001b[0m        0.6802                     0.5634        0.7123  0.0075  0.2124\n",
      "     10                     0.6444        0.6799                     0.5933        0.6958  0.0069  0.2095\n",
      "     11                     \u001b[36m0.6500\u001b[0m        \u001b[32m0.6632\u001b[0m                     \u001b[35m0.6194\u001b[0m        0.6800  0.0063  0.2104\n",
      "     12                     \u001b[36m0.6617\u001b[0m        \u001b[32m0.6453\u001b[0m                     \u001b[35m0.6213\u001b[0m        \u001b[31m0.6666\u001b[0m  0.0057  0.2116\n",
      "     13                     \u001b[36m0.6864\u001b[0m        \u001b[32m0.6081\u001b[0m                     0.6138        0.6702  0.0050  0.2108\n",
      "     14                     0.6818        \u001b[32m0.5961\u001b[0m                     \u001b[35m0.6399\u001b[0m        \u001b[31m0.6513\u001b[0m  0.0043  0.2124\n",
      "     15                     \u001b[36m0.6963\u001b[0m        0.5997                     0.6231        0.6748  0.0037  0.2094\n",
      "     16                     \u001b[36m0.7033\u001b[0m        \u001b[32m0.5745\u001b[0m                     0.6119        0.6729  0.0031  0.2114\n",
      "     17                     \u001b[36m0.7192\u001b[0m        \u001b[32m0.5494\u001b[0m                     0.6194        0.6655  0.0025  0.2167\n",
      "     18                     \u001b[36m0.7379\u001b[0m        \u001b[32m0.5267\u001b[0m                     0.6287        0.6693  0.0020  0.2167\n",
      "     19                     \u001b[36m0.7388\u001b[0m        0.5284                     0.6138        0.6744  0.0015  0.2147\n",
      "     20                     \u001b[36m0.7537\u001b[0m        \u001b[32m0.5060\u001b[0m                     \u001b[35m0.6437\u001b[0m        0.6639  0.0010  0.2104\n",
      "     21                     0.7519        0.5172                     0.6306        0.6662  0.0007  0.2104\n",
      "     22                     0.7463        \u001b[32m0.5019\u001b[0m                     0.6175        0.6672  0.0004  0.2115\n",
      "     23                     0.7523        \u001b[32m0.4993\u001b[0m                     0.6250        0.6658  0.0002  0.2111\n",
      "     24                     \u001b[36m0.7612\u001b[0m        \u001b[32m0.4927\u001b[0m                     0.6269        0.6663  0.0000  0.2117\n",
      "     25                     \u001b[36m0.7645\u001b[0m        \u001b[32m0.4917\u001b[0m                     0.6287        0.6661  0.0000  0.2115\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5131\u001b[0m        \u001b[32m1.4279\u001b[0m                     \u001b[35m0.5485\u001b[0m        \u001b[31m0.9583\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5425\u001b[0m        \u001b[32m0.9099\u001b[0m                     0.5187        \u001b[31m0.8063\u001b[0m  0.0100  0.2105\n",
      "      3                     \u001b[36m0.5519\u001b[0m        \u001b[32m0.7898\u001b[0m                     0.5354        \u001b[31m0.7098\u001b[0m  0.0098  0.2084\n",
      "      4                     \u001b[36m0.5794\u001b[0m        \u001b[32m0.7676\u001b[0m                     0.5056        0.8310  0.0096  0.2084\n",
      "      5                     0.5785        \u001b[32m0.7530\u001b[0m                     0.5429        0.7109  0.0093  0.2094\n",
      "      6                     \u001b[36m0.6150\u001b[0m        \u001b[32m0.7067\u001b[0m                     \u001b[35m0.5597\u001b[0m        \u001b[31m0.7033\u001b[0m  0.0090  0.2094\n",
      "      7                     \u001b[36m0.6294\u001b[0m        \u001b[32m0.6871\u001b[0m                     \u001b[35m0.6063\u001b[0m        \u001b[31m0.6799\u001b[0m  0.0085  0.2095\n",
      "      8                     \u001b[36m0.6355\u001b[0m        \u001b[32m0.6751\u001b[0m                     0.6045        \u001b[31m0.6755\u001b[0m  0.0080  0.2104\n",
      "      9                     \u001b[36m0.6463\u001b[0m        \u001b[32m0.6627\u001b[0m                     0.5951        0.6903  0.0075  0.2094\n",
      "     10                     0.6327        0.6818                     \u001b[35m0.6101\u001b[0m        \u001b[31m0.6752\u001b[0m  0.0069  0.2115\n",
      "     11                     \u001b[36m0.6664\u001b[0m        \u001b[32m0.6434\u001b[0m                     \u001b[35m0.6250\u001b[0m        \u001b[31m0.6751\u001b[0m  0.0063  0.2095\n",
      "     12                     \u001b[36m0.6790\u001b[0m        \u001b[32m0.6158\u001b[0m                     0.6213        0.6809  0.0057  0.2114\n",
      "     13                     \u001b[36m0.6907\u001b[0m        0.6226                     0.5970        \u001b[31m0.6744\u001b[0m  0.0050  0.2093\n",
      "     14                     \u001b[36m0.6949\u001b[0m        \u001b[32m0.6071\u001b[0m                     0.5914        0.6810  0.0043  0.2104\n",
      "     15                     \u001b[36m0.6967\u001b[0m        \u001b[32m0.5893\u001b[0m                     0.5914        0.6998  0.0037  0.2105\n",
      "     16                     \u001b[36m0.7084\u001b[0m        \u001b[32m0.5599\u001b[0m                     0.5821        0.7301  0.0031  0.2104\n",
      "     17                     \u001b[36m0.7243\u001b[0m        \u001b[32m0.5506\u001b[0m                     0.6157        0.6950  0.0025  0.2108\n",
      "     18                     \u001b[36m0.7346\u001b[0m        \u001b[32m0.5418\u001b[0m                     0.5951        0.6936  0.0020  0.2095\n",
      "     19                     \u001b[36m0.7355\u001b[0m        \u001b[32m0.5320\u001b[0m                     0.5896        0.6917  0.0015  0.2115\n",
      "     20                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5140\u001b[0m                     0.6119        0.7026  0.0010  0.2104\n",
      "     21                     \u001b[36m0.7509\u001b[0m        0.5147                     0.6157        0.6937  0.0007  0.2114\n",
      "     22                     \u001b[36m0.7612\u001b[0m        \u001b[32m0.5097\u001b[0m                     0.6101        0.6958  0.0004  0.2114\n",
      "     23                     0.7607        \u001b[32m0.4988\u001b[0m                     0.5989        0.6998  0.0002  0.2105\n",
      "     24                     0.7579        \u001b[32m0.4965\u001b[0m                     0.6007        0.6969  0.0000  0.2104\n",
      "     25                     0.7472        0.5046                     0.5951        0.6969  0.0000  0.2105\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5112\u001b[0m        \u001b[32m1.1653\u001b[0m                     \u001b[35m0.4627\u001b[0m        \u001b[31m0.8347\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.5318\u001b[0m        \u001b[32m0.8493\u001b[0m                     \u001b[35m0.5560\u001b[0m        \u001b[31m0.7203\u001b[0m  0.0100  0.2095\n",
      "      3                     \u001b[36m0.5463\u001b[0m        \u001b[32m0.8348\u001b[0m                     \u001b[35m0.5634\u001b[0m        0.7239  0.0098  0.2104\n",
      "      4                     \u001b[36m0.5836\u001b[0m        \u001b[32m0.7329\u001b[0m                     0.5634        \u001b[31m0.7001\u001b[0m  0.0096  0.2104\n",
      "      5                     \u001b[36m0.6014\u001b[0m        \u001b[32m0.7267\u001b[0m                     \u001b[35m0.5802\u001b[0m        0.7179  0.0093  0.2205\n",
      "      6                     \u001b[36m0.6051\u001b[0m        \u001b[32m0.7257\u001b[0m                     \u001b[35m0.5840\u001b[0m        \u001b[31m0.6873\u001b[0m  0.0090  0.2334\n",
      "      7                     \u001b[36m0.6070\u001b[0m        \u001b[32m0.7143\u001b[0m                     0.5709        0.7342  0.0085  0.2354\n",
      "      8                     \u001b[36m0.6285\u001b[0m        \u001b[32m0.7031\u001b[0m                     0.5690        0.7017  0.0080  0.2294\n",
      "      9                     0.6159        \u001b[32m0.7013\u001b[0m                     0.5672        0.7500  0.0075  0.2132\n",
      "     10                     \u001b[36m0.6495\u001b[0m        \u001b[32m0.6759\u001b[0m                     \u001b[35m0.6082\u001b[0m        0.6915  0.0069  0.2164\n",
      "     11                     \u001b[36m0.6561\u001b[0m        \u001b[32m0.6422\u001b[0m                     0.5951        \u001b[31m0.6803\u001b[0m  0.0063  0.2344\n",
      "     12                     \u001b[36m0.6813\u001b[0m        \u001b[32m0.6089\u001b[0m                     \u001b[35m0.6213\u001b[0m        \u001b[31m0.6779\u001b[0m  0.0057  0.2319\n",
      "     13                     \u001b[36m0.7005\u001b[0m        \u001b[32m0.6056\u001b[0m                     0.5541        0.7653  0.0050  0.2254\n",
      "     14                     0.6808        0.6063                     0.5840        0.7218  0.0043  0.2324\n",
      "     15                     \u001b[36m0.7131\u001b[0m        \u001b[32m0.5944\u001b[0m                     0.6138        0.6887  0.0037  0.2374\n",
      "     16                     \u001b[36m0.7196\u001b[0m        \u001b[32m0.5505\u001b[0m                     0.6213        0.6954  0.0031  0.2329\n",
      "     17                     \u001b[36m0.7388\u001b[0m        \u001b[32m0.5360\u001b[0m                     0.6045        0.7139  0.0025  0.2462\n",
      "     18                     0.7350        \u001b[32m0.5302\u001b[0m                     0.6119        0.6964  0.0020  0.2144\n",
      "     19                     \u001b[36m0.7505\u001b[0m        \u001b[32m0.5210\u001b[0m                     0.6157        0.6912  0.0015  0.2265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                     \u001b[36m0.7640\u001b[0m        \u001b[32m0.4994\u001b[0m                     0.5989        0.7128  0.0010  0.2290\n",
      "     21                     0.7631        \u001b[32m0.4979\u001b[0m                     0.5989        0.7111  0.0007  0.2129\n",
      "     22                     \u001b[36m0.7650\u001b[0m        0.5000                     0.6045        0.6976  0.0004  0.2104\n",
      "     23                     0.7645        \u001b[32m0.4888\u001b[0m                     0.6063        0.7000  0.0002  0.2118\n",
      "     24                     \u001b[36m0.7673\u001b[0m        0.4935                     0.6007        0.7008  0.0000  0.2102\n",
      "     25                     \u001b[36m0.7687\u001b[0m        \u001b[32m0.4853\u001b[0m                     0.6063        0.7001  0.0000  0.2100\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5070\u001b[0m        \u001b[32m1.1135\u001b[0m                     \u001b[35m0.5168\u001b[0m        \u001b[31m0.8127\u001b[0m  0.0100  0.2055\n",
      "      2                     \u001b[36m0.5458\u001b[0m        \u001b[32m0.8237\u001b[0m                     \u001b[35m0.5187\u001b[0m        0.8695  0.0100  0.2091\n",
      "      3                     \u001b[36m0.5463\u001b[0m        0.8836                     \u001b[35m0.5299\u001b[0m        0.9062  0.0098  0.2124\n",
      "      4                     \u001b[36m0.5869\u001b[0m        \u001b[32m0.7703\u001b[0m                     \u001b[35m0.5392\u001b[0m        \u001b[31m0.7717\u001b[0m  0.0096  0.2127\n",
      "      5                     \u001b[36m0.5907\u001b[0m        \u001b[32m0.7663\u001b[0m                     0.5149        0.8095  0.0093  0.2323\n",
      "      6                     0.5846        0.7816                     \u001b[35m0.5597\u001b[0m        \u001b[31m0.7286\u001b[0m  0.0090  0.2285\n",
      "      7                     \u001b[36m0.5967\u001b[0m        \u001b[32m0.7229\u001b[0m                     \u001b[35m0.5784\u001b[0m        \u001b[31m0.6955\u001b[0m  0.0085  0.2234\n",
      "      8                     \u001b[36m0.6304\u001b[0m        \u001b[32m0.6825\u001b[0m                     \u001b[35m0.6119\u001b[0m        \u001b[31m0.6928\u001b[0m  0.0080  0.2294\n",
      "      9                     \u001b[36m0.6495\u001b[0m        \u001b[32m0.6457\u001b[0m                     0.6082        0.6932  0.0075  0.2324\n",
      "     10                     \u001b[36m0.6505\u001b[0m        0.6563                     0.5840        0.7450  0.0069  0.2330\n",
      "     11                     \u001b[36m0.6668\u001b[0m        \u001b[32m0.6332\u001b[0m                     0.5858        0.7309  0.0063  0.2324\n",
      "     12                     \u001b[36m0.6710\u001b[0m        0.6424                     \u001b[35m0.6175\u001b[0m        0.7211  0.0057  0.2175\n",
      "     13                     \u001b[36m0.6846\u001b[0m        \u001b[32m0.6064\u001b[0m                     0.6157        \u001b[31m0.6905\u001b[0m  0.0050  0.2324\n",
      "     14                     \u001b[36m0.6855\u001b[0m        0.6184                     0.6119        0.7135  0.0043  0.2324\n",
      "     15                     \u001b[36m0.7061\u001b[0m        \u001b[32m0.5831\u001b[0m                     0.6007        0.7306  0.0037  0.2323\n",
      "     16                     \u001b[36m0.7234\u001b[0m        \u001b[32m0.5610\u001b[0m                     0.6026        0.7353  0.0031  0.2183\n",
      "     17                     \u001b[36m0.7313\u001b[0m        \u001b[32m0.5365\u001b[0m                     0.5989        0.7300  0.0025  0.2174\n",
      "     18                     \u001b[36m0.7416\u001b[0m        0.5370                     0.6082        0.7346  0.0020  0.2105\n",
      "     19                     \u001b[36m0.7542\u001b[0m        \u001b[32m0.5169\u001b[0m                     0.5858        0.7296  0.0015  0.2274\n",
      "     20                     0.7393        \u001b[32m0.5133\u001b[0m                     0.5989        0.7366  0.0010  0.2413\n",
      "     21                     \u001b[36m0.7645\u001b[0m        \u001b[32m0.4941\u001b[0m                     0.6138        0.7294  0.0007  0.2325\n",
      "     22                     \u001b[36m0.7696\u001b[0m        0.4987                     0.6101        0.7311  0.0004  0.2168\n",
      "     23                     0.7603        \u001b[32m0.4876\u001b[0m                     0.6101        0.7329  0.0002  0.2297\n",
      "     24                     0.7533        0.5023                     0.6119        0.7329  0.0000  0.2137\n",
      "     25                     \u001b[36m0.7720\u001b[0m        \u001b[32m0.4781\u001b[0m                     0.6119        0.7324  0.0000  0.2285\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4916\u001b[0m        \u001b[32m1.1336\u001b[0m                     \u001b[35m0.5336\u001b[0m        \u001b[31m0.7868\u001b[0m  0.0100  0.2304\n",
      "      2                     \u001b[36m0.5280\u001b[0m        \u001b[32m0.8404\u001b[0m                     0.4869        0.8538  0.0100  0.2295\n",
      "      3                     \u001b[36m0.5500\u001b[0m        \u001b[32m0.7787\u001b[0m                     \u001b[35m0.5690\u001b[0m        0.8203  0.0098  0.2259\n",
      "      4                     \u001b[36m0.5682\u001b[0m        \u001b[32m0.7711\u001b[0m                     0.5578        \u001b[31m0.7706\u001b[0m  0.0096  0.2131\n",
      "      5                     0.5612        \u001b[32m0.7446\u001b[0m                     \u001b[35m0.5784\u001b[0m        \u001b[31m0.7076\u001b[0m  0.0093  0.2223\n",
      "      6                     \u001b[36m0.5949\u001b[0m        \u001b[32m0.7267\u001b[0m                     \u001b[35m0.5989\u001b[0m        0.7098  0.0090  0.2124\n",
      "      7                     \u001b[36m0.6093\u001b[0m        \u001b[32m0.6977\u001b[0m                     0.5896        \u001b[31m0.6967\u001b[0m  0.0085  0.2094\n",
      "      8                     \u001b[36m0.6271\u001b[0m        \u001b[32m0.6868\u001b[0m                     0.5840        0.6974  0.0080  0.2314\n",
      "      9                     \u001b[36m0.6285\u001b[0m        0.6978                     \u001b[35m0.6063\u001b[0m        0.7446  0.0075  0.2105\n",
      "     10                     \u001b[36m0.6444\u001b[0m        \u001b[32m0.6722\u001b[0m                     0.5709        0.7122  0.0069  0.2104\n",
      "     11                     \u001b[36m0.6579\u001b[0m        \u001b[32m0.6475\u001b[0m                     \u001b[35m0.6082\u001b[0m        0.7136  0.0063  0.2119\n",
      "     12                     \u001b[36m0.6617\u001b[0m        \u001b[32m0.6356\u001b[0m                     \u001b[35m0.6175\u001b[0m        \u001b[31m0.6958\u001b[0m  0.0057  0.2154\n",
      "     13                     \u001b[36m0.6804\u001b[0m        \u001b[32m0.6109\u001b[0m                     0.5951        \u001b[31m0.6939\u001b[0m  0.0050  0.2363\n",
      "     14                     \u001b[36m0.7056\u001b[0m        \u001b[32m0.5919\u001b[0m                     0.6119        \u001b[31m0.6937\u001b[0m  0.0043  0.2423\n",
      "     15                     0.6977        \u001b[32m0.5874\u001b[0m                     \u001b[35m0.6194\u001b[0m        \u001b[31m0.6838\u001b[0m  0.0037  0.2441\n",
      "     16                     \u001b[36m0.7164\u001b[0m        \u001b[32m0.5572\u001b[0m                     \u001b[35m0.6269\u001b[0m        0.6866  0.0031  0.2261\n",
      "     17                     \u001b[36m0.7402\u001b[0m        \u001b[32m0.5381\u001b[0m                     \u001b[35m0.6362\u001b[0m        \u001b[31m0.6809\u001b[0m  0.0025  0.2230\n",
      "     18                     0.7299        \u001b[32m0.5361\u001b[0m                     0.6362        0.6971  0.0020  0.2340\n",
      "     19                     0.7336        0.5375                     0.6194        0.6944  0.0015  0.2432\n",
      "     20                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5157\u001b[0m                     0.6157        0.6979  0.0010  0.2351\n",
      "     21                     \u001b[36m0.7481\u001b[0m        \u001b[32m0.5072\u001b[0m                     0.6007        0.7022  0.0007  0.2259\n",
      "     22                     \u001b[36m0.7528\u001b[0m        \u001b[32m0.4978\u001b[0m                     0.6063        0.7014  0.0004  0.2314\n",
      "     23                     \u001b[36m0.7561\u001b[0m        0.5004                     0.6082        0.7019  0.0002  0.2205\n",
      "     24                     \u001b[36m0.7673\u001b[0m        \u001b[32m0.4871\u001b[0m                     0.6157        0.7025  0.0000  0.2273\n",
      "     25                     \u001b[36m0.7696\u001b[0m        \u001b[32m0.4846\u001b[0m                     0.6119        0.7025  0.0000  0.2298\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5023\u001b[0m        \u001b[32m1.1972\u001b[0m                     \u001b[35m0.5634\u001b[0m        \u001b[31m0.7244\u001b[0m  0.0100  0.2114\n",
      "      2                     \u001b[36m0.5355\u001b[0m        \u001b[32m0.8389\u001b[0m                     0.5354        0.7453  0.0100  0.2085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3                     \u001b[36m0.5561\u001b[0m        \u001b[32m0.7944\u001b[0m                     \u001b[35m0.5746\u001b[0m        \u001b[31m0.6907\u001b[0m  0.0098  0.2184\n",
      "      4                     \u001b[36m0.5850\u001b[0m        \u001b[32m0.7584\u001b[0m                     \u001b[35m0.5951\u001b[0m        \u001b[31m0.6896\u001b[0m  0.0096  0.2125\n",
      "      5                     \u001b[36m0.6065\u001b[0m        \u001b[32m0.7017\u001b[0m                     \u001b[35m0.6381\u001b[0m        \u001b[31m0.6431\u001b[0m  0.0093  0.2115\n",
      "      6                     \u001b[36m0.6084\u001b[0m        0.7227                     0.6045        0.7266  0.0090  0.2278\n",
      "      7                     \u001b[36m0.6164\u001b[0m        0.7041                     0.5597        0.7866  0.0085  0.2324\n",
      "      8                     \u001b[36m0.6369\u001b[0m        \u001b[32m0.6745\u001b[0m                     0.6213        0.6724  0.0080  0.2184\n",
      "      9                     0.6332        0.6758                     0.6325        \u001b[31m0.6391\u001b[0m  0.0075  0.2329\n",
      "     10                     \u001b[36m0.6411\u001b[0m        0.6766                     0.5951        0.7089  0.0069  0.2319\n",
      "     11                     \u001b[36m0.6659\u001b[0m        \u001b[32m0.6437\u001b[0m                     0.6343        0.6474  0.0063  0.2284\n",
      "     12                     \u001b[36m0.6696\u001b[0m        \u001b[32m0.6299\u001b[0m                     0.6250        0.6441  0.0057  0.2324\n",
      "     13                     \u001b[36m0.6883\u001b[0m        \u001b[32m0.6063\u001b[0m                     0.6231        0.6602  0.0050  0.2256\n",
      "     14                     0.6790        \u001b[32m0.5931\u001b[0m                     \u001b[35m0.6493\u001b[0m        \u001b[31m0.6281\u001b[0m  0.0043  0.2144\n",
      "     15                     \u001b[36m0.6981\u001b[0m        \u001b[32m0.5926\u001b[0m                     \u001b[35m0.6604\u001b[0m        0.6616  0.0037  0.2334\n",
      "     16                     \u001b[36m0.7107\u001b[0m        \u001b[32m0.5664\u001b[0m                     0.6381        0.6606  0.0031  0.2277\n",
      "     17                     \u001b[36m0.7168\u001b[0m        \u001b[32m0.5616\u001b[0m                     0.6325        0.6640  0.0025  0.2112\n",
      "     18                     \u001b[36m0.7304\u001b[0m        \u001b[32m0.5338\u001b[0m                     0.6399        0.6509  0.0020  0.2098\n",
      "     19                     \u001b[36m0.7449\u001b[0m        \u001b[32m0.5213\u001b[0m                     0.6381        0.6470  0.0015  0.2214\n",
      "     20                     \u001b[36m0.7523\u001b[0m        \u001b[32m0.5183\u001b[0m                     0.6343        0.6554  0.0010  0.2327\n",
      "     21                     \u001b[36m0.7528\u001b[0m        \u001b[32m0.5133\u001b[0m                     0.6455        0.6561  0.0007  0.2154\n",
      "     22                     0.7509        \u001b[32m0.5022\u001b[0m                     0.6381        0.6537  0.0004  0.2134\n",
      "     23                     \u001b[36m0.7631\u001b[0m        \u001b[32m0.4927\u001b[0m                     0.6362        0.6545  0.0002  0.2115\n",
      "     24                     \u001b[36m0.7636\u001b[0m        0.4971                     0.6381        0.6553  0.0000  0.2124\n",
      "     25                     0.7612        0.5064                     0.6362        0.6553  0.0000  0.2094\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5112\u001b[0m        \u001b[32m1.2056\u001b[0m                     \u001b[35m0.5243\u001b[0m        \u001b[31m1.1063\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.5276\u001b[0m        \u001b[32m0.8520\u001b[0m                     \u001b[35m0.5354\u001b[0m        \u001b[31m0.7297\u001b[0m  0.0100  0.2105\n",
      "      3                     \u001b[36m0.5790\u001b[0m        \u001b[32m0.7355\u001b[0m                     \u001b[35m0.5410\u001b[0m        \u001b[31m0.6980\u001b[0m  0.0098  0.2104\n",
      "      4                     \u001b[36m0.5804\u001b[0m        \u001b[32m0.7315\u001b[0m                     \u001b[35m0.6101\u001b[0m        \u001b[31m0.6912\u001b[0m  0.0096  0.2121\n",
      "      5                     \u001b[36m0.5963\u001b[0m        \u001b[32m0.7248\u001b[0m                     0.5858        \u001b[31m0.6835\u001b[0m  0.0093  0.2095\n",
      "      6                     \u001b[36m0.6140\u001b[0m        \u001b[32m0.7027\u001b[0m                     0.5896        0.6899  0.0090  0.2095\n",
      "      7                     \u001b[36m0.6369\u001b[0m        \u001b[32m0.6722\u001b[0m                     0.5728        0.7601  0.0085  0.2114\n",
      "      8                     \u001b[36m0.6411\u001b[0m        0.6772                     \u001b[35m0.6250\u001b[0m        0.6919  0.0080  0.2115\n",
      "      9                     \u001b[36m0.6598\u001b[0m        \u001b[32m0.6440\u001b[0m                     0.5616        0.8115  0.0075  0.2126\n",
      "     10                     0.6421        0.6761                     0.5933        0.7744  0.0069  0.2102\n",
      "     11                     0.6491        0.6601                     0.6175        0.7025  0.0063  0.2174\n",
      "     12                     0.6584        0.6638                     0.5802        0.7264  0.0057  0.2164\n",
      "     13                     \u001b[36m0.6939\u001b[0m        \u001b[32m0.6010\u001b[0m                     0.6045        0.7356  0.0050  0.2174\n",
      "     14                     0.6916        \u001b[32m0.5709\u001b[0m                     \u001b[35m0.6306\u001b[0m        0.6896  0.0043  0.2234\n",
      "     15                     \u001b[36m0.7238\u001b[0m        \u001b[32m0.5627\u001b[0m                     0.6287        \u001b[31m0.6759\u001b[0m  0.0037  0.2095\n",
      "     16                     0.7192        \u001b[32m0.5441\u001b[0m                     0.6157        0.7119  0.0031  0.2341\n",
      "     17                     \u001b[36m0.7294\u001b[0m        \u001b[32m0.5393\u001b[0m                     0.6082        0.6926  0.0025  0.2304\n",
      "     18                     \u001b[36m0.7341\u001b[0m        \u001b[32m0.5289\u001b[0m                     \u001b[35m0.6418\u001b[0m        0.7040  0.0020  0.2113\n",
      "     19                     \u001b[36m0.7435\u001b[0m        \u001b[32m0.5182\u001b[0m                     0.6007        0.7060  0.0015  0.2274\n",
      "     20                     \u001b[36m0.7570\u001b[0m        \u001b[32m0.5005\u001b[0m                     0.6119        0.7202  0.0010  0.2344\n",
      "     21                     0.7523        \u001b[32m0.4915\u001b[0m                     0.6231        0.6975  0.0007  0.2332\n",
      "     22                     \u001b[36m0.7607\u001b[0m        \u001b[32m0.4838\u001b[0m                     0.6157        0.7032  0.0004  0.2254\n",
      "     23                     \u001b[36m0.7776\u001b[0m        \u001b[32m0.4738\u001b[0m                     0.6175        0.7041  0.0002  0.2204\n",
      "     24                     \u001b[36m0.7799\u001b[0m        0.4773                     0.6175        0.7045  0.0000  0.2325\n",
      "     25                     0.7636        0.4846                     0.6194        0.7039  0.0000  0.2354\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5056\u001b[0m        \u001b[32m1.3037\u001b[0m                     \u001b[35m0.5410\u001b[0m        \u001b[31m0.7882\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5341\u001b[0m        \u001b[32m0.8524\u001b[0m                     0.5410        \u001b[31m0.7686\u001b[0m  0.0100  0.2095\n",
      "      3                     \u001b[36m0.5706\u001b[0m        \u001b[32m0.7685\u001b[0m                     \u001b[35m0.5448\u001b[0m        0.8361  0.0098  0.2155\n",
      "      4                     0.5486        0.8003                     \u001b[35m0.5840\u001b[0m        \u001b[31m0.6969\u001b[0m  0.0096  0.2196\n",
      "      5                     0.5696        \u001b[32m0.7562\u001b[0m                     0.5709        0.7029  0.0093  0.2227\n",
      "      6                     0.5701        0.7892                     0.5728        0.7530  0.0090  0.2184\n",
      "      7                     \u001b[36m0.5930\u001b[0m        \u001b[32m0.7524\u001b[0m                     \u001b[35m0.6175\u001b[0m        \u001b[31m0.6724\u001b[0m  0.0085  0.2284\n",
      "      8                     \u001b[36m0.6266\u001b[0m        \u001b[32m0.6805\u001b[0m                     0.6082        0.6871  0.0080  0.2104\n",
      "      9                     0.6257        0.6810                     \u001b[35m0.6287\u001b[0m        \u001b[31m0.6551\u001b[0m  0.0075  0.2100\n",
      "     10                     \u001b[36m0.6444\u001b[0m        \u001b[32m0.6593\u001b[0m                     0.6138        0.6642  0.0069  0.2095\n",
      "     11                     \u001b[36m0.6467\u001b[0m        0.6653                     0.6082        0.6949  0.0063  0.2094\n",
      "     12                     \u001b[36m0.6547\u001b[0m        0.6702                     0.6213        0.7117  0.0057  0.2125\n",
      "     13                     \u001b[36m0.6790\u001b[0m        \u001b[32m0.6222\u001b[0m                     0.6138        0.6941  0.0050  0.2093\n",
      "     14                     \u001b[36m0.6813\u001b[0m        \u001b[32m0.6208\u001b[0m                     0.5914        0.6965  0.0043  0.2121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.5972\u001b[0m                     0.5933        0.7013  0.0037  0.2344\n",
      "     16                     0.6981        \u001b[32m0.5876\u001b[0m                     \u001b[35m0.6306\u001b[0m        0.6728  0.0031  0.2344\n",
      "     17                     \u001b[36m0.7131\u001b[0m        \u001b[32m0.5644\u001b[0m                     \u001b[35m0.6325\u001b[0m        0.6716  0.0025  0.2343\n",
      "     18                     \u001b[36m0.7224\u001b[0m        \u001b[32m0.5450\u001b[0m                     0.6082        0.6889  0.0020  0.2434\n",
      "     19                     \u001b[36m0.7439\u001b[0m        \u001b[32m0.5258\u001b[0m                     0.6119        0.6788  0.0015  0.2314\n",
      "     20                     \u001b[36m0.7551\u001b[0m        \u001b[32m0.5083\u001b[0m                     0.6213        0.6759  0.0010  0.2164\n",
      "     21                     0.7435        0.5190                     0.6213        0.6842  0.0007  0.2374\n",
      "     22                     \u001b[36m0.7626\u001b[0m        \u001b[32m0.5040\u001b[0m                     0.6082        0.6759  0.0004  0.2234\n",
      "     23                     0.7570        0.5075                     0.6101        0.6818  0.0002  0.2403\n",
      "     24                     0.7505        0.5102                     0.6138        0.6778  0.0000  0.2114\n",
      "     25                     \u001b[36m0.7631\u001b[0m        \u001b[32m0.4985\u001b[0m                     0.6119        0.6792  0.0000  0.2319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5075\u001b[0m        \u001b[32m1.2984\u001b[0m                     \u001b[35m0.5056\u001b[0m        \u001b[31m0.8195\u001b[0m  0.0100  0.2573\n",
      "      2                     0.5037        \u001b[32m0.8999\u001b[0m                     0.4832        0.8885  0.0100  0.2219\n",
      "      3                     0.5047        \u001b[32m0.8771\u001b[0m                     \u001b[35m0.5373\u001b[0m        1.0368  0.0098  0.2235\n",
      "      4                     0.4986        0.9517                     0.5056        \u001b[31m0.7983\u001b[0m  0.0096  0.2254\n",
      "      5                     \u001b[36m0.5117\u001b[0m        \u001b[32m0.8607\u001b[0m                     0.5093        \u001b[31m0.7433\u001b[0m  0.0093  0.2164\n",
      "      6                     0.5107        \u001b[32m0.8215\u001b[0m                     0.5187        \u001b[31m0.7119\u001b[0m  0.0090  0.2134\n",
      "      7                     \u001b[36m0.5229\u001b[0m        \u001b[32m0.7514\u001b[0m                     0.4944        0.7569  0.0085  0.2124\n",
      "      8                     \u001b[36m0.5407\u001b[0m        \u001b[32m0.7493\u001b[0m                     0.5280        \u001b[31m0.6956\u001b[0m  0.0080  0.2174\n",
      "      9                     \u001b[36m0.5514\u001b[0m        \u001b[32m0.7188\u001b[0m                     0.4888        0.7473  0.0075  0.2374\n",
      "     10                     0.5318        0.7732                     0.5187        0.7940  0.0069  0.2354\n",
      "     11                     0.5453        0.7448                     0.5336        0.7370  0.0063  0.2114\n",
      "     12                     \u001b[36m0.5766\u001b[0m        \u001b[32m0.6988\u001b[0m                     \u001b[35m0.5746\u001b[0m        \u001b[31m0.6878\u001b[0m  0.0057  0.2144\n",
      "     13                     0.5626        0.7143                     0.5075        0.7304  0.0050  0.2284\n",
      "     14                     \u001b[36m0.5790\u001b[0m        0.6996                     0.5746        0.6902  0.0043  0.2383\n",
      "     15                     \u001b[36m0.6005\u001b[0m        \u001b[32m0.6831\u001b[0m                     0.5616        \u001b[31m0.6868\u001b[0m  0.0037  0.2284\n",
      "     16                     \u001b[36m0.6028\u001b[0m        \u001b[32m0.6727\u001b[0m                     0.5728        \u001b[31m0.6826\u001b[0m  0.0031  0.2434\n",
      "     17                     \u001b[36m0.6117\u001b[0m        \u001b[32m0.6654\u001b[0m                     0.5709        0.6965  0.0025  0.2184\n",
      "     18                     \u001b[36m0.6234\u001b[0m        \u001b[32m0.6538\u001b[0m                     \u001b[35m0.5802\u001b[0m        0.6875  0.0020  0.2274\n",
      "     19                     \u001b[36m0.6374\u001b[0m        \u001b[32m0.6470\u001b[0m                     0.5597        0.6911  0.0015  0.2085\n",
      "     20                     \u001b[36m0.6393\u001b[0m        \u001b[32m0.6455\u001b[0m                     0.5616        0.6878  0.0010  0.2084\n",
      "     21                     0.6299        \u001b[32m0.6422\u001b[0m                     0.5709        0.6893  0.0007  0.2104\n",
      "     22                     \u001b[36m0.6486\u001b[0m        \u001b[32m0.6328\u001b[0m                     0.5802        0.6907  0.0004  0.2254\n",
      "     23                     \u001b[36m0.6565\u001b[0m        \u001b[32m0.6265\u001b[0m                     0.5728        0.6888  0.0002  0.2494\n",
      "     24                     0.6486        0.6349                     0.5765        0.6895  0.0000  0.2372\n",
      "     25                     0.6519        0.6374                     0.5728        0.6889  0.0000  0.2244\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4893\u001b[0m        \u001b[32m1.3859\u001b[0m                     \u001b[35m0.4925\u001b[0m        \u001b[31m0.9414\u001b[0m  0.0100  0.2025\n",
      "      2                     \u001b[36m0.5126\u001b[0m        \u001b[32m0.9785\u001b[0m                     0.4832        \u001b[31m0.7661\u001b[0m  0.0100  0.2085\n",
      "      3                     \u001b[36m0.5192\u001b[0m        \u001b[32m0.8277\u001b[0m                     \u001b[35m0.5168\u001b[0m        1.0098  0.0098  0.2084\n",
      "      4                     0.5098        \u001b[32m0.8193\u001b[0m                     0.4944        \u001b[31m0.7352\u001b[0m  0.0096  0.2094\n",
      "      5                     \u001b[36m0.5234\u001b[0m        \u001b[32m0.7755\u001b[0m                     0.4944        0.8850  0.0093  0.2113\n",
      "      6                     \u001b[36m0.5332\u001b[0m        0.8046                     0.4664        0.7602  0.0090  0.2206\n",
      "      7                     0.5145        \u001b[32m0.7596\u001b[0m                     \u001b[35m0.5299\u001b[0m        \u001b[31m0.6994\u001b[0m  0.0085  0.2155\n",
      "      8                     \u001b[36m0.5336\u001b[0m        0.7732                     0.4981        0.7399  0.0080  0.2184\n",
      "      9                     0.5257        0.7754                     \u001b[35m0.5522\u001b[0m        \u001b[31m0.6812\u001b[0m  0.0075  0.2098\n",
      "     10                     0.5285        \u001b[32m0.7337\u001b[0m                     0.5280        0.7142  0.0069  0.2215\n",
      "     11                     \u001b[36m0.5463\u001b[0m        \u001b[32m0.7277\u001b[0m                     0.5336        0.7210  0.0063  0.2334\n",
      "     12                     \u001b[36m0.5682\u001b[0m        \u001b[32m0.7158\u001b[0m                     0.5299        0.6927  0.0057  0.2104\n",
      "     13                     0.5551        0.7174                     0.5075        0.7299  0.0050  0.2284\n",
      "     14                     \u001b[36m0.5706\u001b[0m        \u001b[32m0.7051\u001b[0m                     0.5149        0.7163  0.0043  0.2204\n",
      "     15                     \u001b[36m0.5911\u001b[0m        \u001b[32m0.6869\u001b[0m                     0.5410        0.6989  0.0037  0.2234\n",
      "     16                     \u001b[36m0.6014\u001b[0m        \u001b[32m0.6747\u001b[0m                     0.5168        0.7088  0.0031  0.2315\n",
      "     17                     0.5939        0.6843                     0.5299        0.7023  0.0025  0.2354\n",
      "     18                     \u001b[36m0.6042\u001b[0m        \u001b[32m0.6743\u001b[0m                     0.5448        0.6977  0.0020  0.2603\n",
      "     19                     0.5991        \u001b[32m0.6734\u001b[0m                     0.5261        0.7054  0.0015  0.2124\n",
      "     20                     \u001b[36m0.6178\u001b[0m        \u001b[32m0.6687\u001b[0m                     0.5224        0.7072  0.0010  0.2260\n",
      "     21                     0.6112        \u001b[32m0.6670\u001b[0m                     0.5410        0.6950  0.0007  0.2389\n",
      "     22                     \u001b[36m0.6229\u001b[0m        \u001b[32m0.6591\u001b[0m                     0.5354        0.6969  0.0004  0.2114\n",
      "     23                     \u001b[36m0.6243\u001b[0m        \u001b[32m0.6470\u001b[0m                     0.5373        0.6966  0.0002  0.2266\n",
      "     24                     0.6210        \u001b[32m0.6440\u001b[0m                     0.5429        0.6964  0.0000  0.2175\n",
      "     25                     0.6220        0.6494                     0.5485        0.6968  0.0000  0.2174\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4967\u001b[0m        \u001b[32m1.5180\u001b[0m                     \u001b[35m0.5019\u001b[0m        \u001b[31m0.7999\u001b[0m  0.0100  0.2264\n",
      "      2                     \u001b[36m0.5154\u001b[0m        \u001b[32m0.9126\u001b[0m                     \u001b[35m0.5112\u001b[0m        \u001b[31m0.7918\u001b[0m  0.0100  0.2165\n",
      "      3                     0.5150        0.9187                     0.4832        \u001b[31m0.7046\u001b[0m  0.0098  0.2095\n",
      "      4                     \u001b[36m0.5327\u001b[0m        \u001b[32m0.7975\u001b[0m                     0.4944        0.7090  0.0096  0.2199\n",
      "      5                     0.5136        \u001b[32m0.7839\u001b[0m                     0.5093        0.7503  0.0093  0.2301\n",
      "      6                     \u001b[36m0.5341\u001b[0m        \u001b[32m0.7572\u001b[0m                     0.4944        0.7145  0.0090  0.2194\n",
      "      7                     0.5262        \u001b[32m0.7349\u001b[0m                     0.5037        0.7552  0.0085  0.2304\n",
      "      8                     \u001b[36m0.5463\u001b[0m        0.7848                     \u001b[35m0.5149\u001b[0m        0.7337  0.0080  0.2292\n",
      "      9                     0.5430        0.7462                     0.5093        0.7109  0.0075  0.2141\n",
      "     10                     \u001b[36m0.5556\u001b[0m        0.7574                     0.5037        0.7529  0.0069  0.2111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     0.5364        0.7552                     \u001b[35m0.5187\u001b[0m        0.7178  0.0063  0.2177\n",
      "     12                     \u001b[36m0.5715\u001b[0m        \u001b[32m0.7152\u001b[0m                     0.4851        0.7805  0.0057  0.2334\n",
      "     13                     \u001b[36m0.5808\u001b[0m        \u001b[32m0.7042\u001b[0m                     \u001b[35m0.5261\u001b[0m        0.7164  0.0050  0.2294\n",
      "     14                     0.5738        0.7051                     0.5112        0.7572  0.0043  0.2214\n",
      "     15                     0.5673        0.7122                     \u001b[35m0.5336\u001b[0m        0.7230  0.0037  0.2105\n",
      "     16                     0.5659        \u001b[32m0.6968\u001b[0m                     \u001b[35m0.5373\u001b[0m        0.7057  0.0031  0.2107\n",
      "     17                     \u001b[36m0.5986\u001b[0m        \u001b[32m0.6804\u001b[0m                     0.5336        \u001b[31m0.7018\u001b[0m  0.0025  0.2155\n",
      "     18                     0.5902        \u001b[32m0.6788\u001b[0m                     \u001b[35m0.5504\u001b[0m        \u001b[31m0.7007\u001b[0m  0.0020  0.2104\n",
      "     19                     \u001b[36m0.6061\u001b[0m        \u001b[32m0.6663\u001b[0m                     \u001b[35m0.5522\u001b[0m        0.7018  0.0015  0.2124\n",
      "     20                     0.5944        \u001b[32m0.6628\u001b[0m                     0.5299        0.7131  0.0010  0.2104\n",
      "     21                     \u001b[36m0.6150\u001b[0m        \u001b[32m0.6591\u001b[0m                     0.5168        0.7128  0.0007  0.2124\n",
      "     22                     \u001b[36m0.6187\u001b[0m        \u001b[32m0.6529\u001b[0m                     0.5280        0.7083  0.0004  0.2094\n",
      "     23                     0.6117        0.6581                     0.5317        0.7076  0.0002  0.2115\n",
      "     24                     0.6159        0.6591                     0.5299        0.7073  0.0000  0.2127\n",
      "     25                     \u001b[36m0.6290\u001b[0m        \u001b[32m0.6503\u001b[0m                     0.5373        0.7073  0.0000  0.2154\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5079\u001b[0m        \u001b[32m1.7585\u001b[0m                     \u001b[35m0.5037\u001b[0m        \u001b[31m0.9444\u001b[0m  0.0100  0.2214\n",
      "      2                     \u001b[36m0.5215\u001b[0m        \u001b[32m1.5693\u001b[0m                     0.5019        1.2251  0.0100  0.2279\n",
      "      3                     \u001b[36m0.5388\u001b[0m        \u001b[32m0.9646\u001b[0m                     0.4832        \u001b[31m0.9082\u001b[0m  0.0098  0.2145\n",
      "      4                     0.5285        \u001b[32m0.8487\u001b[0m                     \u001b[35m0.5075\u001b[0m        \u001b[31m0.7350\u001b[0m  0.0096  0.2184\n",
      "      5                     0.5388        \u001b[32m0.7587\u001b[0m                     \u001b[35m0.5168\u001b[0m        \u001b[31m0.7118\u001b[0m  0.0093  0.2124\n",
      "      6                     0.5290        \u001b[32m0.7468\u001b[0m                     \u001b[35m0.5280\u001b[0m        0.7148  0.0090  0.2304\n",
      "      7                     \u001b[36m0.5575\u001b[0m        \u001b[32m0.7321\u001b[0m                     0.4776        0.7357  0.0085  0.2214\n",
      "      8                     \u001b[36m0.5631\u001b[0m        0.7371                     0.4981        0.7409  0.0080  0.2324\n",
      "      9                     0.5537        0.7337                     0.5112        0.7530  0.0075  0.2264\n",
      "     10                     \u001b[36m0.5687\u001b[0m        \u001b[32m0.7123\u001b[0m                     0.5075        0.7238  0.0069  0.2152\n",
      "     11                     \u001b[36m0.5743\u001b[0m        0.7213                     0.5037        0.7283  0.0063  0.2334\n",
      "     12                     0.5706        \u001b[32m0.6959\u001b[0m                     0.5187        0.7211  0.0057  0.2224\n",
      "     13                     0.5495        0.7291                     0.5037        0.7364  0.0050  0.2114\n",
      "     14                     \u001b[36m0.5832\u001b[0m        \u001b[32m0.6899\u001b[0m                     0.5224        0.7150  0.0043  0.2135\n",
      "     15                     \u001b[36m0.6000\u001b[0m        \u001b[32m0.6682\u001b[0m                     0.4925        0.7505  0.0037  0.2292\n",
      "     16                     \u001b[36m0.6028\u001b[0m        0.6835                     \u001b[35m0.5317\u001b[0m        0.7271  0.0031  0.2310\n",
      "     17                     \u001b[36m0.6079\u001b[0m        0.6752                     0.4925        0.7336  0.0025  0.2314\n",
      "     18                     0.5949        \u001b[32m0.6595\u001b[0m                     0.4963        0.7265  0.0020  0.2185\n",
      "     19                     \u001b[36m0.6336\u001b[0m        \u001b[32m0.6502\u001b[0m                     0.5224        0.7214  0.0015  0.2144\n",
      "     20                     0.6327        0.6567                     0.5205        0.7319  0.0010  0.2155\n",
      "     21                     0.6308        \u001b[32m0.6425\u001b[0m                     0.4981        0.7265  0.0007  0.2275\n",
      "     22                     \u001b[36m0.6393\u001b[0m        \u001b[32m0.6320\u001b[0m                     0.5168        0.7268  0.0004  0.2255\n",
      "     23                     0.6346        0.6421                     0.5075        0.7263  0.0002  0.2274\n",
      "     24                     0.6379        0.6392                     0.5112        0.7263  0.0000  0.2134\n",
      "     25                     0.6322        0.6451                     0.5093        0.7262  0.0000  0.2234\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4944\u001b[0m        \u001b[32m1.6151\u001b[0m                     \u001b[35m0.4888\u001b[0m        \u001b[31m1.0104\u001b[0m  0.0100  0.2055\n",
      "      2                     \u001b[36m0.5103\u001b[0m        \u001b[32m0.9975\u001b[0m                     \u001b[35m0.5280\u001b[0m        \u001b[31m0.9408\u001b[0m  0.0100  0.2194\n",
      "      3                     0.5028        \u001b[32m0.8471\u001b[0m                     \u001b[35m0.5448\u001b[0m        \u001b[31m0.7050\u001b[0m  0.0098  0.2113\n",
      "      4                     \u001b[36m0.5187\u001b[0m        \u001b[32m0.7862\u001b[0m                     0.5149        0.8139  0.0096  0.2287\n",
      "      5                     \u001b[36m0.5327\u001b[0m        0.7981                     0.5205        0.7325  0.0093  0.2105\n",
      "      6                     \u001b[36m0.5360\u001b[0m        0.8218                     0.5280        0.8904  0.0090  0.2135\n",
      "      7                     0.5332        0.8738                     0.4981        0.7396  0.0085  0.2115\n",
      "      8                     0.5294        \u001b[32m0.7623\u001b[0m                     0.4925        0.8429  0.0080  0.2295\n",
      "      9                     \u001b[36m0.5364\u001b[0m        0.7726                     0.5093        0.7844  0.0075  0.2214\n",
      "     10                     \u001b[36m0.5449\u001b[0m        \u001b[32m0.7447\u001b[0m                     0.5205        \u001b[31m0.7010\u001b[0m  0.0069  0.2105\n",
      "     11                     \u001b[36m0.5738\u001b[0m        \u001b[32m0.7075\u001b[0m                     0.4851        0.7323  0.0063  0.2104\n",
      "     12                     0.5636        \u001b[32m0.7070\u001b[0m                     0.5149        0.7251  0.0057  0.2105\n",
      "     13                     \u001b[36m0.5864\u001b[0m        \u001b[32m0.7014\u001b[0m                     0.4869        0.7243  0.0050  0.2104\n",
      "     14                     0.5832        \u001b[32m0.6983\u001b[0m                     0.5093        0.7220  0.0043  0.2194\n",
      "     15                     0.5804        \u001b[32m0.6910\u001b[0m                     0.5354        0.7095  0.0037  0.2144\n",
      "     16                     \u001b[36m0.6121\u001b[0m        \u001b[32m0.6605\u001b[0m                     0.5299        0.7122  0.0031  0.2104\n",
      "     17                     0.6079        0.6672                     0.5299        0.7239  0.0025  0.2095\n",
      "     18                     0.6028        0.6657                     0.5280        0.7130  0.0020  0.2104\n",
      "     19                     \u001b[36m0.6201\u001b[0m        \u001b[32m0.6514\u001b[0m                     0.5299        0.7120  0.0015  0.2104\n",
      "     20                     \u001b[36m0.6262\u001b[0m        \u001b[32m0.6459\u001b[0m                     0.5317        0.7123  0.0010  0.2267\n",
      "     21                     \u001b[36m0.6318\u001b[0m        0.6515                     0.5410        0.7191  0.0007  0.2294\n",
      "     22                     0.6206        0.6495                     0.5243        0.7188  0.0004  0.2165\n",
      "     23                     0.6280        \u001b[32m0.6434\u001b[0m                     0.5317        0.7170  0.0002  0.2344\n",
      "     24                     \u001b[36m0.6411\u001b[0m        \u001b[32m0.6377\u001b[0m                     0.5410        0.7164  0.0000  0.2336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     25                     0.6355        0.6473                     0.5354        0.7164  0.0000  0.2234\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4953\u001b[0m        \u001b[32m1.5701\u001b[0m                     \u001b[35m0.4888\u001b[0m        \u001b[31m0.8606\u001b[0m  0.0100  0.2124\n",
      "      2                     \u001b[36m0.5224\u001b[0m        \u001b[32m0.9276\u001b[0m                     0.4590        \u001b[31m0.7724\u001b[0m  0.0100  0.2224\n",
      "      3                     0.5014        \u001b[32m0.8340\u001b[0m                     \u001b[35m0.5000\u001b[0m        \u001b[31m0.7155\u001b[0m  0.0098  0.2224\n",
      "      4                     \u001b[36m0.5238\u001b[0m        \u001b[32m0.7801\u001b[0m                     0.4981        0.8615  0.0096  0.2115\n",
      "      5                     \u001b[36m0.5243\u001b[0m        \u001b[32m0.7750\u001b[0m                     0.4832        0.7956  0.0093  0.2298\n",
      "      6                     \u001b[36m0.5294\u001b[0m        0.8356                     0.5000        0.8396  0.0090  0.2242\n",
      "      7                     0.5206        0.8495                     \u001b[35m0.5149\u001b[0m        0.7329  0.0085  0.2294\n",
      "      8                     \u001b[36m0.5463\u001b[0m        \u001b[32m0.7575\u001b[0m                     0.5056        0.7646  0.0080  0.2294\n",
      "      9                     0.5332        0.7679                     0.5075        0.7382  0.0075  0.2124\n",
      "     10                     0.5444        \u001b[32m0.7274\u001b[0m                     \u001b[35m0.5243\u001b[0m        0.7190  0.0069  0.2393\n",
      "     11                     \u001b[36m0.5500\u001b[0m        0.7303                     0.4963        0.7181  0.0063  0.2353\n",
      "     12                     \u001b[36m0.5706\u001b[0m        \u001b[32m0.7121\u001b[0m                     0.4832        0.7746  0.0057  0.2184\n",
      "     13                     \u001b[36m0.5888\u001b[0m        \u001b[32m0.6864\u001b[0m                     0.5112        0.7258  0.0050  0.2274\n",
      "     14                     \u001b[36m0.6051\u001b[0m        0.6874                     0.4963        0.7338  0.0043  0.2184\n",
      "     15                     0.5706        0.7291                     0.5093        0.7314  0.0037  0.2326\n",
      "     16                     \u001b[36m0.6084\u001b[0m        \u001b[32m0.6679\u001b[0m                     0.5149        0.7337  0.0031  0.2314\n",
      "     17                     0.6051        0.6785                     0.5187        0.7226  0.0025  0.2269\n",
      "     18                     0.6051        \u001b[32m0.6608\u001b[0m                     0.5131        0.7183  0.0020  0.2121\n",
      "     19                     \u001b[36m0.6210\u001b[0m        \u001b[32m0.6579\u001b[0m                     \u001b[35m0.5261\u001b[0m        0.7302  0.0015  0.2155\n",
      "     20                     \u001b[36m0.6257\u001b[0m        \u001b[32m0.6508\u001b[0m                     0.5112        0.7232  0.0010  0.2124\n",
      "     21                     \u001b[36m0.6523\u001b[0m        \u001b[32m0.6303\u001b[0m                     0.5168        0.7215  0.0007  0.2125\n",
      "     22                     0.6397        0.6342                     0.5224        0.7222  0.0004  0.2184\n",
      "     23                     0.6421        0.6362                     0.5261        0.7223  0.0002  0.2144\n",
      "     24                     0.6467        \u001b[32m0.6231\u001b[0m                     0.5224        0.7220  0.0000  0.2253\n",
      "     25                     0.6500        0.6322                     \u001b[35m0.5299\u001b[0m        0.7223  0.0000  0.2095\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5014\u001b[0m        \u001b[32m1.1843\u001b[0m                     \u001b[35m0.5224\u001b[0m        \u001b[31m0.7339\u001b[0m  0.0100  0.2314\n",
      "      2                     \u001b[36m0.5084\u001b[0m        1.1990                     0.4534        0.8371  0.0100  0.2174\n",
      "      3                     \u001b[36m0.5168\u001b[0m        \u001b[32m0.8708\u001b[0m                     0.5149        \u001b[31m0.7117\u001b[0m  0.0098  0.2284\n",
      "      4                     \u001b[36m0.5332\u001b[0m        \u001b[32m0.7712\u001b[0m                     0.5037        \u001b[31m0.7001\u001b[0m  0.0096  0.2254\n",
      "      5                     0.5248        \u001b[32m0.7558\u001b[0m                     0.5149        0.7170  0.0093  0.2215\n",
      "      6                     \u001b[36m0.5514\u001b[0m        \u001b[32m0.7225\u001b[0m                     \u001b[35m0.5354\u001b[0m        0.7149  0.0090  0.2204\n",
      "      7                     0.5280        0.7470                     0.5093        0.7238  0.0085  0.2135\n",
      "      8                     \u001b[36m0.5542\u001b[0m        \u001b[32m0.7178\u001b[0m                     0.5224        0.7132  0.0080  0.2106\n",
      "      9                     0.5500        0.7346                     0.5131        0.7215  0.0075  0.2117\n",
      "     10                     0.5322        0.7509                     0.4739        0.7471  0.0069  0.2095\n",
      "     11                     \u001b[36m0.5645\u001b[0m        0.7304                     0.4832        0.9120  0.0063  0.2124\n",
      "     12                     0.5547        0.7562                     0.4963        0.7322  0.0057  0.2104\n",
      "     13                     \u001b[36m0.5654\u001b[0m        0.7208                     \u001b[35m0.5522\u001b[0m        0.7850  0.0050  0.2105\n",
      "     14                     \u001b[36m0.5766\u001b[0m        \u001b[32m0.7137\u001b[0m                     0.5037        0.7297  0.0043  0.2124\n",
      "     15                     0.5729        0.7215                     0.5037        0.7789  0.0037  0.2105\n",
      "     16                     \u001b[36m0.5879\u001b[0m        \u001b[32m0.7094\u001b[0m                     0.5019        0.7518  0.0031  0.2132\n",
      "     17                     \u001b[36m0.6089\u001b[0m        \u001b[32m0.6688\u001b[0m                     0.5075        0.7223  0.0025  0.2105\n",
      "     18                     \u001b[36m0.6393\u001b[0m        \u001b[32m0.6481\u001b[0m                     0.4963        0.7205  0.0020  0.2125\n",
      "     19                     0.6220        \u001b[32m0.6478\u001b[0m                     0.5093        0.7277  0.0015  0.2107\n",
      "     20                     \u001b[36m0.6575\u001b[0m        \u001b[32m0.6338\u001b[0m                     0.5093        0.7233  0.0010  0.2116\n",
      "     21                     \u001b[36m0.6593\u001b[0m        \u001b[32m0.6215\u001b[0m                     0.5261        0.7267  0.0007  0.2174\n",
      "     22                     0.6551        0.6327                     0.5112        0.7331  0.0004  0.2109\n",
      "     23                     0.6584        \u001b[32m0.6209\u001b[0m                     0.5149        0.7277  0.0002  0.2149\n",
      "     24                     0.6491        0.6235                     0.5131        0.7279  0.0000  0.2103\n",
      "     25                     \u001b[36m0.6757\u001b[0m        \u001b[32m0.6107\u001b[0m                     0.5149        0.7283  0.0000  0.2184\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5098\u001b[0m        \u001b[32m1.8032\u001b[0m                     \u001b[35m0.5354\u001b[0m        \u001b[31m1.0287\u001b[0m  0.0100  0.2094\n",
      "      2                     \u001b[36m0.5229\u001b[0m        \u001b[32m1.1444\u001b[0m                     0.5317        \u001b[31m0.7655\u001b[0m  0.0100  0.2224\n",
      "      3                     \u001b[36m0.5252\u001b[0m        \u001b[32m0.8622\u001b[0m                     0.5261        \u001b[31m0.7047\u001b[0m  0.0098  0.2234\n",
      "      4                     0.5126        \u001b[32m0.8209\u001b[0m                     0.4963        0.7195  0.0096  0.2164\n",
      "      5                     0.5164        \u001b[32m0.7700\u001b[0m                     0.5243        0.7514  0.0093  0.2124\n",
      "      6                     0.5168        0.7704                     0.5112        0.7600  0.0090  0.2103\n",
      "      7                     0.5103        0.7759                     0.4944        0.7320  0.0085  0.2123\n",
      "      8                     \u001b[36m0.5374\u001b[0m        \u001b[32m0.7553\u001b[0m                     0.5205        \u001b[31m0.6971\u001b[0m  0.0080  0.2100\n",
      "      9                     \u001b[36m0.5411\u001b[0m        \u001b[32m0.7481\u001b[0m                     0.5299        0.7285  0.0075  0.2105\n",
      "     10                     \u001b[36m0.5430\u001b[0m        0.7585                     0.5280        0.7006  0.0069  0.2104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     \u001b[36m0.5673\u001b[0m        \u001b[32m0.7085\u001b[0m                     0.5168        0.7442  0.0063  0.2104\n",
      "     12                     \u001b[36m0.5696\u001b[0m        0.7194                     0.5224        0.8826  0.0057  0.2125\n",
      "     13                     \u001b[36m0.5762\u001b[0m        0.7789                     \u001b[35m0.5485\u001b[0m        \u001b[31m0.6949\u001b[0m  0.0050  0.2174\n",
      "     14                     0.5729        0.7287                     0.5131        0.7154  0.0043  0.2443\n",
      "     15                     \u001b[36m0.5911\u001b[0m        \u001b[32m0.6991\u001b[0m                     0.5187        0.7139  0.0037  0.2179\n",
      "     16                     \u001b[36m0.6075\u001b[0m        \u001b[32m0.6889\u001b[0m                     0.5354        0.7600  0.0031  0.2179\n",
      "     17                     0.5939        0.6911                     0.5168        0.7042  0.0025  0.2109\n",
      "     18                     \u001b[36m0.6084\u001b[0m        \u001b[32m0.6747\u001b[0m                     0.5205        0.7057  0.0020  0.2125\n",
      "     19                     \u001b[36m0.6276\u001b[0m        \u001b[32m0.6519\u001b[0m                     0.5187        0.7163  0.0015  0.2105\n",
      "     20                     0.6238        0.6659                     0.5168        0.7024  0.0010  0.2104\n",
      "     21                     0.6271        \u001b[32m0.6456\u001b[0m                     0.5112        0.7032  0.0007  0.2125\n",
      "     22                     \u001b[36m0.6322\u001b[0m        \u001b[32m0.6437\u001b[0m                     0.5224        0.7034  0.0004  0.2134\n",
      "     23                     \u001b[36m0.6430\u001b[0m        \u001b[32m0.6391\u001b[0m                     0.5149        0.7029  0.0002  0.2095\n",
      "     24                     \u001b[36m0.6537\u001b[0m        \u001b[32m0.6299\u001b[0m                     0.5224        0.7031  0.0000  0.2111\n",
      "     25                     \u001b[36m0.6584\u001b[0m        \u001b[32m0.6229\u001b[0m                     0.5205        0.7031  0.0000  0.2103\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5028\u001b[0m        \u001b[32m1.7836\u001b[0m                     \u001b[35m0.4795\u001b[0m        \u001b[31m0.9791\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.5121\u001b[0m        \u001b[32m1.1312\u001b[0m                     \u001b[35m0.4851\u001b[0m        \u001b[31m0.7442\u001b[0m  0.0100  0.2204\n",
      "      3                     0.4911        \u001b[32m0.8841\u001b[0m                     0.4851        0.7529  0.0098  0.2141\n",
      "      4                     \u001b[36m0.5210\u001b[0m        0.8952                     \u001b[35m0.4944\u001b[0m        \u001b[31m0.7375\u001b[0m  0.0096  0.2174\n",
      "      5                     \u001b[36m0.5313\u001b[0m        \u001b[32m0.7831\u001b[0m                     0.4944        \u001b[31m0.7322\u001b[0m  0.0093  0.2390\n",
      "      6                     \u001b[36m0.5355\u001b[0m        \u001b[32m0.7635\u001b[0m                     0.4664        0.7893  0.0090  0.2343\n",
      "      7                     \u001b[36m0.5439\u001b[0m        \u001b[32m0.7546\u001b[0m                     0.4552        0.9286  0.0085  0.2304\n",
      "      8                     0.5350        0.7674                     0.4813        0.8100  0.0080  0.2104\n",
      "      9                     0.5336        \u001b[32m0.7526\u001b[0m                     0.4925        \u001b[31m0.7246\u001b[0m  0.0075  0.2184\n",
      "     10                     \u001b[36m0.5537\u001b[0m        \u001b[32m0.7290\u001b[0m                     \u001b[35m0.5168\u001b[0m        \u001b[31m0.7122\u001b[0m  0.0069  0.2171\n",
      "     11                     \u001b[36m0.5729\u001b[0m        \u001b[32m0.7222\u001b[0m                     0.5075        0.7195  0.0063  0.2164\n",
      "     12                     0.5664        0.7279                     0.4925        0.7380  0.0057  0.2414\n",
      "     13                     0.5715        \u001b[32m0.7065\u001b[0m                     0.4963        0.7346  0.0050  0.2224\n",
      "     14                     \u001b[36m0.5827\u001b[0m        \u001b[32m0.6836\u001b[0m                     \u001b[35m0.5429\u001b[0m        0.7294  0.0043  0.2293\n",
      "     15                     \u001b[36m0.6000\u001b[0m        0.6920                     0.5224        0.7597  0.0037  0.2257\n",
      "     16                     \u001b[36m0.6182\u001b[0m        \u001b[32m0.6808\u001b[0m                     0.5224        0.7512  0.0031  0.2204\n",
      "     17                     0.5953        \u001b[32m0.6801\u001b[0m                     0.5112        0.7231  0.0025  0.2324\n",
      "     18                     \u001b[36m0.6234\u001b[0m        \u001b[32m0.6558\u001b[0m                     0.5299        0.7336  0.0020  0.2115\n",
      "     19                     \u001b[36m0.6402\u001b[0m        \u001b[32m0.6515\u001b[0m                     0.5410        0.7242  0.0015  0.2234\n",
      "     20                     0.6308        \u001b[32m0.6515\u001b[0m                     0.5299        0.7188  0.0010  0.2324\n",
      "     21                     0.6299        \u001b[32m0.6469\u001b[0m                     0.5354        0.7228  0.0007  0.2194\n",
      "     22                     \u001b[36m0.6593\u001b[0m        \u001b[32m0.6257\u001b[0m                     \u001b[35m0.5466\u001b[0m        0.7225  0.0004  0.2222\n",
      "     23                     0.6551        0.6327                     0.5410        0.7217  0.0002  0.2166\n",
      "     24                     0.6570        \u001b[32m0.6231\u001b[0m                     0.5392        0.7219  0.0000  0.2423\n",
      "     25                     0.6463        0.6338                     0.5429        0.7220  0.0000  0.2336\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5098\u001b[0m        \u001b[32m1.6231\u001b[0m                     \u001b[35m0.4963\u001b[0m        \u001b[31m0.8576\u001b[0m  0.0100  0.2194\n",
      "      2                     \u001b[36m0.5136\u001b[0m        \u001b[32m1.1716\u001b[0m                     0.4869        \u001b[31m0.7867\u001b[0m  0.0100  0.2115\n",
      "      3                     \u001b[36m0.5248\u001b[0m        \u001b[32m0.8471\u001b[0m                     0.4925        \u001b[31m0.7407\u001b[0m  0.0098  0.2104\n",
      "      4                     0.5145        \u001b[32m0.7892\u001b[0m                     0.4832        \u001b[31m0.7088\u001b[0m  0.0096  0.2123\n",
      "      5                     0.5121        \u001b[32m0.7683\u001b[0m                     \u001b[35m0.5093\u001b[0m        0.7385  0.0093  0.2114\n",
      "      6                     0.5210        \u001b[32m0.7665\u001b[0m                     \u001b[35m0.5280\u001b[0m        0.7148  0.0090  0.2114\n",
      "      7                     \u001b[36m0.5304\u001b[0m        0.7936                     \u001b[35m0.5299\u001b[0m        1.0340  0.0085  0.2105\n",
      "      8                     0.5238        0.8202                     \u001b[35m0.5354\u001b[0m        \u001b[31m0.7082\u001b[0m  0.0080  0.2255\n",
      "      9                     \u001b[36m0.5402\u001b[0m        \u001b[32m0.7456\u001b[0m                     0.5093        0.7196  0.0075  0.2300\n",
      "     10                     \u001b[36m0.5617\u001b[0m        \u001b[32m0.7317\u001b[0m                     0.5093        0.7507  0.0069  0.2144\n",
      "     11                     \u001b[36m0.5682\u001b[0m        \u001b[32m0.7124\u001b[0m                     0.5093        0.7251  0.0063  0.2094\n",
      "     12                     0.5556        0.7219                     0.5187        0.7566  0.0057  0.2126\n",
      "     13                     \u001b[36m0.5813\u001b[0m        \u001b[32m0.7088\u001b[0m                     0.5131        0.7624  0.0050  0.2105\n",
      "     14                     \u001b[36m0.5916\u001b[0m        \u001b[32m0.6896\u001b[0m                     0.4981        0.7328  0.0043  0.2105\n",
      "     15                     0.5911        \u001b[32m0.6889\u001b[0m                     0.5187        0.7151  0.0037  0.2125\n",
      "     16                     \u001b[36m0.5953\u001b[0m        \u001b[32m0.6769\u001b[0m                     0.5149        0.7188  0.0031  0.2104\n",
      "     17                     \u001b[36m0.6047\u001b[0m        \u001b[32m0.6739\u001b[0m                     0.5093        0.7309  0.0025  0.2124\n",
      "     18                     \u001b[36m0.6154\u001b[0m        \u001b[32m0.6698\u001b[0m                     0.5112        0.7237  0.0020  0.2104\n",
      "     19                     0.6084        \u001b[32m0.6600\u001b[0m                     0.5019        0.7313  0.0015  0.2218\n",
      "     20                     \u001b[36m0.6449\u001b[0m        \u001b[32m0.6504\u001b[0m                     0.5093        0.7305  0.0010  0.2254\n",
      "     21                     0.6416        \u001b[32m0.6483\u001b[0m                     0.5056        0.7312  0.0007  0.2234\n",
      "     22                     \u001b[36m0.6495\u001b[0m        \u001b[32m0.6291\u001b[0m                     0.5075        0.7306  0.0004  0.2104\n",
      "     23                     0.6393        0.6364                     0.5112        0.7304  0.0002  0.2116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24                     0.6458        0.6357                     0.5075        0.7308  0.0000  0.2117\n",
      "     25                     0.6439        0.6309                     0.5075        0.7314  0.0000  0.2104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5879\u001b[0m        \u001b[32m1.0431\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5495\u001b[0m  0.0100  0.2094\n",
      "      2                     \u001b[36m0.7346\u001b[0m        \u001b[32m0.5880\u001b[0m                     \u001b[35m0.8470\u001b[0m        \u001b[31m0.3708\u001b[0m  0.0100  0.2095\n",
      "      3                     \u001b[36m0.7981\u001b[0m        \u001b[32m0.4571\u001b[0m                     \u001b[35m0.8582\u001b[0m        \u001b[31m0.3672\u001b[0m  0.0098  0.2104\n",
      "      4                     \u001b[36m0.8201\u001b[0m        \u001b[32m0.4159\u001b[0m                     0.8377        0.3828  0.0096  0.2105\n",
      "      5                     \u001b[36m0.8257\u001b[0m        0.4386                     \u001b[35m0.8713\u001b[0m        \u001b[31m0.3361\u001b[0m  0.0093  0.2125\n",
      "      6                     \u001b[36m0.8313\u001b[0m        \u001b[32m0.3917\u001b[0m                     0.8713        \u001b[31m0.3341\u001b[0m  0.0090  0.2124\n",
      "      7                     \u001b[36m0.8481\u001b[0m        \u001b[32m0.3702\u001b[0m                     0.8675        0.3400  0.0085  0.2091\n",
      "      8                     0.8421        0.3730                     0.8694        0.3477  0.0080  0.2094\n",
      "      9                     \u001b[36m0.8514\u001b[0m        \u001b[32m0.3605\u001b[0m                     \u001b[35m0.8806\u001b[0m        \u001b[31m0.3095\u001b[0m  0.0075  0.2094\n",
      "     10                     \u001b[36m0.8607\u001b[0m        \u001b[32m0.3464\u001b[0m                     0.8750        0.3356  0.0069  0.2094\n",
      "     11                     \u001b[36m0.8794\u001b[0m        \u001b[32m0.2967\u001b[0m                     0.8731        0.3422  0.0063  0.2134\n",
      "     12                     0.8780        \u001b[32m0.2910\u001b[0m                     0.8694        0.3319  0.0057  0.2174\n",
      "     13                     0.8794        \u001b[32m0.2906\u001b[0m                     \u001b[35m0.8862\u001b[0m        0.3110  0.0050  0.2136\n",
      "     14                     \u001b[36m0.8860\u001b[0m        \u001b[32m0.2722\u001b[0m                     0.8787        \u001b[31m0.3032\u001b[0m  0.0043  0.2125\n",
      "     15                     \u001b[36m0.9019\u001b[0m        \u001b[32m0.2446\u001b[0m                     \u001b[35m0.8937\u001b[0m        \u001b[31m0.2989\u001b[0m  0.0037  0.2095\n",
      "     16                     0.8981        \u001b[32m0.2418\u001b[0m                     0.8843        0.3036  0.0031  0.2105\n",
      "     17                     \u001b[36m0.9079\u001b[0m        \u001b[32m0.2305\u001b[0m                     \u001b[35m0.9011\u001b[0m        0.2992  0.0025  0.2095\n",
      "     18                     \u001b[36m0.9154\u001b[0m        \u001b[32m0.2191\u001b[0m                     0.8918        \u001b[31m0.2943\u001b[0m  0.0020  0.2104\n",
      "     19                     0.9126        0.2200                     0.8862        0.3078  0.0015  0.2094\n",
      "     20                     \u001b[36m0.9224\u001b[0m        \u001b[32m0.2043\u001b[0m                     0.8937        0.2972  0.0010  0.2095\n",
      "     21                     0.9224        \u001b[32m0.1981\u001b[0m                     0.8881        0.2962  0.0007  0.2119\n",
      "     22                     \u001b[36m0.9248\u001b[0m        \u001b[32m0.1958\u001b[0m                     0.8918        0.2965  0.0004  0.2104\n",
      "     23                     0.9178        0.1996                     0.8918        0.2966  0.0002  0.2124\n",
      "     24                     0.9229        \u001b[32m0.1947\u001b[0m                     0.8955        0.2961  0.0000  0.2095\n",
      "     25                     0.9229        0.2030                     0.8937        0.2962  0.0000  0.2094\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5991\u001b[0m        \u001b[32m1.0274\u001b[0m                     \u001b[35m0.7631\u001b[0m        \u001b[31m0.5055\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.7173\u001b[0m        \u001b[32m0.6446\u001b[0m                     \u001b[35m0.8302\u001b[0m        \u001b[31m0.3827\u001b[0m  0.0100  0.2085\n",
      "      3                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.5056\u001b[0m                     \u001b[35m0.8433\u001b[0m        \u001b[31m0.3523\u001b[0m  0.0098  0.2124\n",
      "      4                     \u001b[36m0.8126\u001b[0m        \u001b[32m0.4579\u001b[0m                     \u001b[35m0.8526\u001b[0m        \u001b[31m0.3337\u001b[0m  0.0096  0.2095\n",
      "      5                     \u001b[36m0.8150\u001b[0m        \u001b[32m0.4308\u001b[0m                     \u001b[35m0.8731\u001b[0m        \u001b[31m0.3039\u001b[0m  0.0093  0.2145\n",
      "      6                     \u001b[36m0.8318\u001b[0m        \u001b[32m0.3978\u001b[0m                     0.8582        0.3108  0.0090  0.2095\n",
      "      7                     \u001b[36m0.8505\u001b[0m        \u001b[32m0.3553\u001b[0m                     \u001b[35m0.8787\u001b[0m        \u001b[31m0.2852\u001b[0m  0.0085  0.2105\n",
      "      8                     \u001b[36m0.8542\u001b[0m        \u001b[32m0.3452\u001b[0m                     0.8713        0.3166  0.0080  0.2164\n",
      "      9                     \u001b[36m0.8692\u001b[0m        \u001b[32m0.3045\u001b[0m                     0.8657        0.3165  0.0075  0.2155\n",
      "     10                     0.8682        0.3368                     \u001b[35m0.8843\u001b[0m        0.3005  0.0069  0.2245\n",
      "     11                     \u001b[36m0.8762\u001b[0m        0.3054                     0.8731        0.3161  0.0063  0.2102\n",
      "     12                     \u001b[36m0.8785\u001b[0m        \u001b[32m0.3042\u001b[0m                     0.8843        \u001b[31m0.2831\u001b[0m  0.0057  0.2124\n",
      "     13                     \u001b[36m0.8869\u001b[0m        \u001b[32m0.2795\u001b[0m                     0.8843        0.3041  0.0050  0.2113\n",
      "     14                     \u001b[36m0.8902\u001b[0m        0.2866                     0.8769        0.3164  0.0043  0.2088\n",
      "     15                     \u001b[36m0.8916\u001b[0m        \u001b[32m0.2785\u001b[0m                     \u001b[35m0.8918\u001b[0m        \u001b[31m0.2819\u001b[0m  0.0037  0.2134\n",
      "     16                     \u001b[36m0.9033\u001b[0m        \u001b[32m0.2627\u001b[0m                     0.8750        0.3013  0.0031  0.2097\n",
      "     17                     \u001b[36m0.9056\u001b[0m        \u001b[32m0.2371\u001b[0m                     0.8918        \u001b[31m0.2693\u001b[0m  0.0025  0.2114\n",
      "     18                     \u001b[36m0.9168\u001b[0m        \u001b[32m0.2263\u001b[0m                     0.8899        0.2728  0.0020  0.2104\n",
      "     19                     \u001b[36m0.9257\u001b[0m        \u001b[32m0.2079\u001b[0m                     0.8862        0.2747  0.0015  0.2104\n",
      "     20                     0.9215        0.2085                     0.8899        \u001b[31m0.2662\u001b[0m  0.0010  0.2125\n",
      "     21                     \u001b[36m0.9262\u001b[0m        \u001b[32m0.1912\u001b[0m                     \u001b[35m0.8955\u001b[0m        0.2670  0.0007  0.2104\n",
      "     22                     0.9182        0.2052                     0.8955        \u001b[31m0.2661\u001b[0m  0.0004  0.2244\n",
      "     23                     \u001b[36m0.9299\u001b[0m        0.1926                     \u001b[35m0.8974\u001b[0m        \u001b[31m0.2656\u001b[0m  0.0002  0.2324\n",
      "     24                     \u001b[36m0.9318\u001b[0m        0.1947                     0.8974        0.2663  0.0000  0.2353\n",
      "     25                     0.9201        0.2001                     0.8974        0.2662  0.0000  0.2316\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5776\u001b[0m        \u001b[32m1.0805\u001b[0m                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5462\u001b[0m  0.0100  0.2284\n",
      "      2                     \u001b[36m0.7178\u001b[0m        \u001b[32m0.6569\u001b[0m                     \u001b[35m0.8284\u001b[0m        \u001b[31m0.3911\u001b[0m  0.0100  0.2386\n",
      "      3                     \u001b[36m0.7977\u001b[0m        \u001b[32m0.4709\u001b[0m                     \u001b[35m0.8433\u001b[0m        \u001b[31m0.3635\u001b[0m  0.0098  0.2341\n",
      "      4                     \u001b[36m0.8164\u001b[0m        \u001b[32m0.4166\u001b[0m                     \u001b[35m0.8563\u001b[0m        \u001b[31m0.3427\u001b[0m  0.0096  0.2124\n",
      "      5                     \u001b[36m0.8224\u001b[0m        \u001b[32m0.4132\u001b[0m                     0.8507        \u001b[31m0.3246\u001b[0m  0.0093  0.2310\n",
      "      6                     \u001b[36m0.8435\u001b[0m        \u001b[32m0.3736\u001b[0m                     \u001b[35m0.8825\u001b[0m        \u001b[31m0.2922\u001b[0m  0.0090  0.2343\n",
      "      7                     \u001b[36m0.8439\u001b[0m        \u001b[32m0.3580\u001b[0m                     0.8731        0.3058  0.0085  0.2253\n",
      "      8                     \u001b[36m0.8603\u001b[0m        \u001b[32m0.3404\u001b[0m                     0.8638        0.3064  0.0080  0.2134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.8668\u001b[0m        \u001b[32m0.3221\u001b[0m                     0.8825        \u001b[31m0.2764\u001b[0m  0.0075  0.2256\n",
      "     10                     \u001b[36m0.8692\u001b[0m        \u001b[32m0.3076\u001b[0m                     \u001b[35m0.8993\u001b[0m        \u001b[31m0.2754\u001b[0m  0.0069  0.2324\n",
      "     11                     0.8692        0.3205                     0.8825        0.2903  0.0063  0.2326\n",
      "     12                     0.8687        0.3156                     0.8843        0.2941  0.0057  0.2279\n",
      "     13                     \u001b[36m0.8710\u001b[0m        \u001b[32m0.2969\u001b[0m                     0.8993        \u001b[31m0.2538\u001b[0m  0.0050  0.2276\n",
      "     14                     \u001b[36m0.8939\u001b[0m        \u001b[32m0.2727\u001b[0m                     0.8974        0.2538  0.0043  0.2195\n",
      "     15                     \u001b[36m0.8981\u001b[0m        \u001b[32m0.2600\u001b[0m                     0.8843        0.2723  0.0037  0.2214\n",
      "     16                     \u001b[36m0.9061\u001b[0m        \u001b[32m0.2449\u001b[0m                     0.8937        0.2548  0.0031  0.2130\n",
      "     17                     0.8977        0.2612                     0.8899        \u001b[31m0.2509\u001b[0m  0.0025  0.2314\n",
      "     18                     0.9028        \u001b[32m0.2345\u001b[0m                     0.8955        \u001b[31m0.2463\u001b[0m  0.0020  0.2304\n",
      "     19                     \u001b[36m0.9070\u001b[0m        \u001b[32m0.2297\u001b[0m                     0.8881        0.2463  0.0015  0.2207\n",
      "     20                     \u001b[36m0.9145\u001b[0m        \u001b[32m0.2166\u001b[0m                     0.8881        0.2488  0.0010  0.2207\n",
      "     21                     \u001b[36m0.9215\u001b[0m        \u001b[32m0.2037\u001b[0m                     0.8937        0.2484  0.0007  0.2116\n",
      "     22                     \u001b[36m0.9271\u001b[0m        \u001b[32m0.1972\u001b[0m                     0.8937        0.2470  0.0004  0.2213\n",
      "     23                     0.9224        \u001b[32m0.1957\u001b[0m                     0.8899        0.2479  0.0002  0.2266\n",
      "     24                     0.9229        0.1996                     0.8881        0.2476  0.0000  0.2268\n",
      "     25                     0.9187        0.1998                     0.8881        0.2472  0.0000  0.2103\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6023\u001b[0m        \u001b[32m1.0026\u001b[0m                     \u001b[35m0.7425\u001b[0m        \u001b[31m0.5219\u001b[0m  0.0100  0.2264\n",
      "      2                     \u001b[36m0.7178\u001b[0m        \u001b[32m0.6336\u001b[0m                     \u001b[35m0.8284\u001b[0m        \u001b[31m0.3734\u001b[0m  0.0100  0.2314\n",
      "      3                     \u001b[36m0.7799\u001b[0m        \u001b[32m0.5091\u001b[0m                     0.8190        0.3988  0.0098  0.2294\n",
      "      4                     \u001b[36m0.8107\u001b[0m        \u001b[32m0.4677\u001b[0m                     \u001b[35m0.8619\u001b[0m        \u001b[31m0.3314\u001b[0m  0.0096  0.2228\n",
      "      5                     \u001b[36m0.8374\u001b[0m        \u001b[32m0.3840\u001b[0m                     0.8545        0.3364  0.0093  0.2334\n",
      "      6                     0.8355        \u001b[32m0.3831\u001b[0m                     \u001b[35m0.8657\u001b[0m        \u001b[31m0.3011\u001b[0m  0.0090  0.2303\n",
      "      7                     0.8374        0.3894                     0.8657        0.3220  0.0085  0.2156\n",
      "      8                     \u001b[36m0.8617\u001b[0m        \u001b[32m0.3509\u001b[0m                     \u001b[35m0.8899\u001b[0m        \u001b[31m0.2740\u001b[0m  0.0080  0.2232\n",
      "      9                     0.8607        \u001b[32m0.3273\u001b[0m                     0.8787        0.3020  0.0075  0.2114\n",
      "     10                     0.8495        0.3326                     0.8713        0.3133  0.0069  0.2294\n",
      "     11                     \u001b[36m0.8738\u001b[0m        \u001b[32m0.3117\u001b[0m                     0.8843        0.2777  0.0063  0.2294\n",
      "     12                     \u001b[36m0.8836\u001b[0m        \u001b[32m0.2901\u001b[0m                     0.8694        0.2814  0.0057  0.2104\n",
      "     13                     \u001b[36m0.8883\u001b[0m        \u001b[32m0.2798\u001b[0m                     0.8881        0.2781  0.0050  0.2207\n",
      "     14                     \u001b[36m0.8907\u001b[0m        \u001b[32m0.2753\u001b[0m                     0.8881        0.2774  0.0043  0.2183\n",
      "     15                     \u001b[36m0.9014\u001b[0m        \u001b[32m0.2437\u001b[0m                     0.8881        0.2783  0.0037  0.2098\n",
      "     16                     \u001b[36m0.9056\u001b[0m        \u001b[32m0.2331\u001b[0m                     0.8862        0.2928  0.0031  0.2118\n",
      "     17                     \u001b[36m0.9093\u001b[0m        \u001b[32m0.2248\u001b[0m                     0.8862        0.2755  0.0025  0.2254\n",
      "     18                     \u001b[36m0.9145\u001b[0m        \u001b[32m0.2209\u001b[0m                     \u001b[35m0.8918\u001b[0m        0.2806  0.0020  0.2223\n",
      "     19                     0.9051        0.2290                     \u001b[35m0.8993\u001b[0m        \u001b[31m0.2640\u001b[0m  0.0015  0.2324\n",
      "     20                     \u001b[36m0.9154\u001b[0m        0.2223                     0.8993        0.2658  0.0010  0.2301\n",
      "     21                     \u001b[36m0.9192\u001b[0m        \u001b[32m0.1997\u001b[0m                     \u001b[35m0.9011\u001b[0m        0.2662  0.0007  0.2241\n",
      "     22                     \u001b[36m0.9215\u001b[0m        0.2087                     \u001b[35m0.9030\u001b[0m        0.2683  0.0004  0.2343\n",
      "     23                     \u001b[36m0.9252\u001b[0m        \u001b[32m0.1907\u001b[0m                     0.9011        0.2723  0.0002  0.2117\n",
      "     24                     0.9252        0.1974                     0.8993        0.2693  0.0000  0.2114\n",
      "     25                     0.9168        0.1966                     0.9030        0.2702  0.0000  0.2145\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5804\u001b[0m        \u001b[32m1.0701\u001b[0m                     \u001b[35m0.7108\u001b[0m        \u001b[31m0.6612\u001b[0m  0.0100  0.2194\n",
      "      2                     \u001b[36m0.7598\u001b[0m        \u001b[32m0.5864\u001b[0m                     \u001b[35m0.8526\u001b[0m        \u001b[31m0.3627\u001b[0m  0.0100  0.2105\n",
      "      3                     \u001b[36m0.8084\u001b[0m        \u001b[32m0.4576\u001b[0m                     \u001b[35m0.8619\u001b[0m        \u001b[31m0.3461\u001b[0m  0.0098  0.2213\n",
      "      4                     \u001b[36m0.8220\u001b[0m        \u001b[32m0.4226\u001b[0m                     0.8601        \u001b[31m0.3179\u001b[0m  0.0096  0.2164\n",
      "      5                     \u001b[36m0.8355\u001b[0m        \u001b[32m0.3940\u001b[0m                     0.8545        0.3359  0.0093  0.2144\n",
      "      6                     0.8341        0.4119                     0.8526        0.3464  0.0090  0.2234\n",
      "      7                     \u001b[36m0.8444\u001b[0m        \u001b[32m0.3769\u001b[0m                     0.8358        0.3933  0.0085  0.2104\n",
      "      8                     \u001b[36m0.8500\u001b[0m        \u001b[32m0.3631\u001b[0m                     0.8619        0.3718  0.0080  0.2251\n",
      "      9                     \u001b[36m0.8710\u001b[0m        \u001b[32m0.3093\u001b[0m                     \u001b[35m0.8750\u001b[0m        \u001b[31m0.3046\u001b[0m  0.0075  0.2324\n",
      "     10                     0.8650        0.3395                     \u001b[35m0.8899\u001b[0m        0.3079  0.0069  0.2173\n",
      "     11                     0.8696        0.3192                     0.8881        \u001b[31m0.2889\u001b[0m  0.0063  0.2204\n",
      "     12                     \u001b[36m0.8766\u001b[0m        \u001b[32m0.2865\u001b[0m                     0.8881        \u001b[31m0.2771\u001b[0m  0.0057  0.2176\n",
      "     13                     \u001b[36m0.8790\u001b[0m        0.2975                     0.8657        0.3210  0.0050  0.2194\n",
      "     14                     \u001b[36m0.8860\u001b[0m        \u001b[32m0.2659\u001b[0m                     0.8713        0.2873  0.0043  0.2204\n",
      "     15                     \u001b[36m0.9051\u001b[0m        \u001b[32m0.2365\u001b[0m                     0.8694        0.3092  0.0037  0.2122\n",
      "     16                     0.9028        0.2386                     0.8750        0.2917  0.0031  0.2104\n",
      "     17                     0.8949        0.2447                     0.8843        \u001b[31m0.2739\u001b[0m  0.0025  0.2307\n",
      "     18                     \u001b[36m0.9126\u001b[0m        \u001b[32m0.2199\u001b[0m                     0.8862        0.2784  0.0020  0.2341\n",
      "     19                     \u001b[36m0.9215\u001b[0m        \u001b[32m0.2123\u001b[0m                     0.8862        0.2779  0.0015  0.2326\n",
      "     20                     0.9121        0.2182                     \u001b[35m0.8937\u001b[0m        0.2762  0.0010  0.2325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21                     0.9173        \u001b[32m0.2040\u001b[0m                     0.8918        \u001b[31m0.2722\u001b[0m  0.0007  0.2360\n",
      "     22                     \u001b[36m0.9271\u001b[0m        \u001b[32m0.1901\u001b[0m                     0.8899        \u001b[31m0.2713\u001b[0m  0.0004  0.2335\n",
      "     23                     \u001b[36m0.9285\u001b[0m        \u001b[32m0.1898\u001b[0m                     \u001b[35m0.8993\u001b[0m        \u001b[31m0.2703\u001b[0m  0.0002  0.2115\n",
      "     24                     0.9220        0.1952                     0.8993        \u001b[31m0.2696\u001b[0m  0.0000  0.2145\n",
      "     25                     0.9271        \u001b[32m0.1876\u001b[0m                     0.8993        0.2699  0.0000  0.2104\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6411\u001b[0m        \u001b[32m0.8592\u001b[0m                     \u001b[35m0.7743\u001b[0m        \u001b[31m0.4655\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.7617\u001b[0m        \u001b[32m0.5749\u001b[0m                     \u001b[35m0.7929\u001b[0m        0.5562  0.0100  0.2105\n",
      "      3                     \u001b[36m0.8065\u001b[0m        \u001b[32m0.4931\u001b[0m                     \u001b[35m0.8414\u001b[0m        \u001b[31m0.3758\u001b[0m  0.0098  0.2104\n",
      "      4                     \u001b[36m0.8271\u001b[0m        \u001b[32m0.4400\u001b[0m                     0.8321        0.4062  0.0096  0.2107\n",
      "      5                     \u001b[36m0.8355\u001b[0m        \u001b[32m0.4125\u001b[0m                     0.8321        0.3923  0.0093  0.2105\n",
      "      6                     0.8346        \u001b[32m0.3810\u001b[0m                     \u001b[35m0.8489\u001b[0m        \u001b[31m0.3666\u001b[0m  0.0090  0.2085\n",
      "      7                     \u001b[36m0.8692\u001b[0m        \u001b[32m0.3212\u001b[0m                     \u001b[35m0.8563\u001b[0m        0.3761  0.0085  0.2094\n",
      "      8                     \u001b[36m0.8743\u001b[0m        \u001b[32m0.3171\u001b[0m                     \u001b[35m0.8657\u001b[0m        \u001b[31m0.3234\u001b[0m  0.0080  0.2113\n",
      "      9                     \u001b[36m0.8879\u001b[0m        \u001b[32m0.2994\u001b[0m                     \u001b[35m0.8675\u001b[0m        0.3284  0.0075  0.2104\n",
      "     10                     0.8860        \u001b[32m0.2875\u001b[0m                     0.8545        0.3670  0.0069  0.2104\n",
      "     11                     \u001b[36m0.8888\u001b[0m        \u001b[32m0.2837\u001b[0m                     \u001b[35m0.8694\u001b[0m        0.3344  0.0063  0.2104\n",
      "     12                     \u001b[36m0.9009\u001b[0m        \u001b[32m0.2657\u001b[0m                     0.8638        0.3565  0.0057  0.2094\n",
      "     13                     \u001b[36m0.9051\u001b[0m        \u001b[32m0.2464\u001b[0m                     0.8675        0.3702  0.0050  0.2095\n",
      "     14                     0.8944        0.2554                     0.8619        0.3358  0.0043  0.2104\n",
      "     15                     \u001b[36m0.9140\u001b[0m        \u001b[32m0.2157\u001b[0m                     \u001b[35m0.8787\u001b[0m        0.3576  0.0037  0.2098\n",
      "     16                     \u001b[36m0.9150\u001b[0m        \u001b[32m0.2121\u001b[0m                     0.8769        0.3362  0.0031  0.2095\n",
      "     17                     \u001b[36m0.9168\u001b[0m        \u001b[32m0.2027\u001b[0m                     \u001b[35m0.8806\u001b[0m        0.3488  0.0025  0.2114\n",
      "     18                     \u001b[36m0.9234\u001b[0m        \u001b[32m0.1942\u001b[0m                     \u001b[35m0.8825\u001b[0m        0.3389  0.0020  0.2095\n",
      "     19                     \u001b[36m0.9294\u001b[0m        \u001b[32m0.1882\u001b[0m                     0.8787        0.3426  0.0015  0.2114\n",
      "     20                     \u001b[36m0.9313\u001b[0m        \u001b[32m0.1784\u001b[0m                     0.8806        0.3436  0.0010  0.2094\n",
      "     21                     \u001b[36m0.9336\u001b[0m        \u001b[32m0.1704\u001b[0m                     \u001b[35m0.8881\u001b[0m        0.3461  0.0007  0.2105\n",
      "     22                     \u001b[36m0.9355\u001b[0m        \u001b[32m0.1637\u001b[0m                     0.8825        0.3485  0.0004  0.2095\n",
      "     23                     \u001b[36m0.9374\u001b[0m        0.1687                     0.8843        0.3479  0.0002  0.2108\n",
      "     24                     \u001b[36m0.9416\u001b[0m        \u001b[32m0.1558\u001b[0m                     0.8862        0.3472  0.0000  0.2114\n",
      "     25                     0.9374        0.1597                     0.8862        0.3476  0.0000  0.2104\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5832\u001b[0m        \u001b[32m1.0529\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5115\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.7402\u001b[0m        \u001b[32m0.6044\u001b[0m                     \u001b[35m0.8451\u001b[0m        \u001b[31m0.3686\u001b[0m  0.0100  0.2095\n",
      "      3                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4860\u001b[0m                     \u001b[35m0.8545\u001b[0m        \u001b[31m0.3659\u001b[0m  0.0098  0.2104\n",
      "      4                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4602\u001b[0m                     0.8340        0.4012  0.0096  0.2114\n",
      "      5                     \u001b[36m0.8327\u001b[0m        \u001b[32m0.4009\u001b[0m                     0.8470        \u001b[31m0.3423\u001b[0m  0.0093  0.2105\n",
      "      6                     \u001b[36m0.8495\u001b[0m        \u001b[32m0.3663\u001b[0m                     \u001b[35m0.8563\u001b[0m        0.3452  0.0090  0.2105\n",
      "      7                     0.8439        0.3758                     \u001b[35m0.8750\u001b[0m        \u001b[31m0.3134\u001b[0m  0.0085  0.2463\n",
      "      8                     \u001b[36m0.8509\u001b[0m        \u001b[32m0.3445\u001b[0m                     0.8657        0.3360  0.0080  0.2112\n",
      "      9                     0.8500        0.3548                     0.8582        0.3399  0.0075  0.2113\n",
      "     10                     \u001b[36m0.8659\u001b[0m        \u001b[32m0.3368\u001b[0m                     0.8750        0.3153  0.0069  0.2106\n",
      "     11                     \u001b[36m0.8734\u001b[0m        \u001b[32m0.3062\u001b[0m                     \u001b[35m0.8862\u001b[0m        \u001b[31m0.3132\u001b[0m  0.0063  0.2114\n",
      "     12                     0.8710        \u001b[32m0.3034\u001b[0m                     0.8750        0.3272  0.0057  0.2104\n",
      "     13                     \u001b[36m0.8897\u001b[0m        \u001b[32m0.2759\u001b[0m                     0.8825        0.3163  0.0050  0.2114\n",
      "     14                     \u001b[36m0.8963\u001b[0m        \u001b[32m0.2637\u001b[0m                     0.8489        0.3767  0.0043  0.2115\n",
      "     15                     \u001b[36m0.9023\u001b[0m        \u001b[32m0.2393\u001b[0m                     0.8769        0.3409  0.0037  0.2104\n",
      "     16                     0.8972        0.2496                     0.8657        0.3357  0.0031  0.2105\n",
      "     17                     \u001b[36m0.9121\u001b[0m        \u001b[32m0.2291\u001b[0m                     0.8713        0.3155  0.0025  0.2113\n",
      "     18                     \u001b[36m0.9168\u001b[0m        \u001b[32m0.2127\u001b[0m                     0.8787        0.3321  0.0020  0.2111\n",
      "     19                     \u001b[36m0.9187\u001b[0m        \u001b[32m0.2012\u001b[0m                     0.8694        0.3364  0.0015  0.2104\n",
      "     20                     \u001b[36m0.9229\u001b[0m        \u001b[32m0.1970\u001b[0m                     0.8675        0.3310  0.0010  0.2104\n",
      "     21                     0.9192        0.1981                     0.8713        0.3383  0.0007  0.2095\n",
      "     22                     \u001b[36m0.9276\u001b[0m        \u001b[32m0.1797\u001b[0m                     0.8713        0.3288  0.0004  0.2113\n",
      "     23                     0.9243        0.1894                     0.8731        0.3283  0.0002  0.2102\n",
      "     24                     0.9229        0.1920                     0.8731        0.3290  0.0000  0.2117\n",
      "     25                     0.9248        0.1885                     0.8750        0.3285  0.0000  0.2111\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5939\u001b[0m        \u001b[32m1.0228\u001b[0m                     \u001b[35m0.7612\u001b[0m        \u001b[31m0.5164\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.7266\u001b[0m        \u001b[32m0.6688\u001b[0m                     \u001b[35m0.7873\u001b[0m        \u001b[31m0.4686\u001b[0m  0.0100  0.2106\n",
      "      3                     \u001b[36m0.7949\u001b[0m        \u001b[32m0.5005\u001b[0m                     \u001b[35m0.7929\u001b[0m        \u001b[31m0.4500\u001b[0m  0.0098  0.2104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4                     \u001b[36m0.8154\u001b[0m        \u001b[32m0.4332\u001b[0m                     \u001b[35m0.8302\u001b[0m        \u001b[31m0.3843\u001b[0m  0.0096  0.2096\n",
      "      5                     \u001b[36m0.8360\u001b[0m        \u001b[32m0.3801\u001b[0m                     0.8228        0.3938  0.0093  0.2104\n",
      "      6                     \u001b[36m0.8439\u001b[0m        \u001b[32m0.3688\u001b[0m                     \u001b[35m0.8377\u001b[0m        \u001b[31m0.3642\u001b[0m  0.0090  0.2100\n",
      "      7                     \u001b[36m0.8598\u001b[0m        \u001b[32m0.3553\u001b[0m                     \u001b[35m0.8489\u001b[0m        \u001b[31m0.3578\u001b[0m  0.0085  0.2104\n",
      "      8                     \u001b[36m0.8668\u001b[0m        \u001b[32m0.3341\u001b[0m                     \u001b[35m0.8638\u001b[0m        \u001b[31m0.3507\u001b[0m  0.0080  0.2115\n",
      "      9                     \u001b[36m0.8762\u001b[0m        \u001b[32m0.3107\u001b[0m                     0.8321        0.3957  0.0075  0.2105\n",
      "     10                     0.8729        0.3241                     0.8507        \u001b[31m0.3289\u001b[0m  0.0069  0.2095\n",
      "     11                     \u001b[36m0.8813\u001b[0m        \u001b[32m0.2833\u001b[0m                     0.8601        0.3321  0.0063  0.2094\n",
      "     12                     \u001b[36m0.8911\u001b[0m        \u001b[32m0.2777\u001b[0m                     0.8582        0.3393  0.0057  0.2095\n",
      "     13                     \u001b[36m0.8935\u001b[0m        \u001b[32m0.2660\u001b[0m                     \u001b[35m0.8750\u001b[0m        0.3291  0.0050  0.2104\n",
      "     14                     \u001b[36m0.9000\u001b[0m        \u001b[32m0.2497\u001b[0m                     0.8507        0.3382  0.0043  0.2095\n",
      "     15                     \u001b[36m0.9065\u001b[0m        0.2542                     0.8638        0.3440  0.0037  0.2103\n",
      "     16                     0.9056        \u001b[32m0.2447\u001b[0m                     0.8638        0.3357  0.0031  0.2102\n",
      "     17                     \u001b[36m0.9140\u001b[0m        \u001b[32m0.2267\u001b[0m                     0.8657        \u001b[31m0.3208\u001b[0m  0.0025  0.2100\n",
      "     18                     \u001b[36m0.9243\u001b[0m        \u001b[32m0.2021\u001b[0m                     0.8619        0.3312  0.0020  0.2106\n",
      "     19                     0.9220        0.2122                     0.8713        \u001b[31m0.3134\u001b[0m  0.0015  0.2105\n",
      "     20                     \u001b[36m0.9248\u001b[0m        \u001b[32m0.2018\u001b[0m                     \u001b[35m0.8769\u001b[0m        \u001b[31m0.3128\u001b[0m  0.0010  0.2094\n",
      "     21                     0.9220        0.2079                     0.8769        0.3175  0.0007  0.2092\n",
      "     22                     \u001b[36m0.9285\u001b[0m        \u001b[32m0.1907\u001b[0m                     \u001b[35m0.8843\u001b[0m        0.3151  0.0004  0.2100\n",
      "     23                     0.9271        0.2005                     0.8787        0.3157  0.0002  0.2091\n",
      "     24                     \u001b[36m0.9299\u001b[0m        \u001b[32m0.1833\u001b[0m                     \u001b[35m0.8862\u001b[0m        0.3172  0.0000  0.2099\n",
      "     25                     \u001b[36m0.9308\u001b[0m        0.1859                     0.8806        0.3163  0.0000  0.2104\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5995\u001b[0m        \u001b[32m1.0776\u001b[0m                     \u001b[35m0.7705\u001b[0m        \u001b[31m0.4817\u001b[0m  0.0100  0.2025\n",
      "      2                     \u001b[36m0.7393\u001b[0m        \u001b[32m0.5711\u001b[0m                     \u001b[35m0.8246\u001b[0m        \u001b[31m0.3833\u001b[0m  0.0100  0.2105\n",
      "      3                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4717\u001b[0m                     \u001b[35m0.8414\u001b[0m        \u001b[31m0.3797\u001b[0m  0.0098  0.2104\n",
      "      4                     \u001b[36m0.8182\u001b[0m        \u001b[32m0.4695\u001b[0m                     \u001b[35m0.8489\u001b[0m        \u001b[31m0.3496\u001b[0m  0.0096  0.2104\n",
      "      5                     \u001b[36m0.8379\u001b[0m        \u001b[32m0.3839\u001b[0m                     0.8489        \u001b[31m0.3147\u001b[0m  0.0093  0.2114\n",
      "      6                     \u001b[36m0.8537\u001b[0m        \u001b[32m0.3575\u001b[0m                     \u001b[35m0.8713\u001b[0m        \u001b[31m0.2953\u001b[0m  0.0090  0.2094\n",
      "      7                     0.8514        0.3598                     0.8526        0.3025  0.0085  0.2104\n",
      "      8                     \u001b[36m0.8650\u001b[0m        0.3663                     0.8638        0.3347  0.0080  0.2105\n",
      "      9                     \u001b[36m0.8724\u001b[0m        \u001b[32m0.3205\u001b[0m                     0.8601        0.3232  0.0075  0.2104\n",
      "     10                     0.8696        0.3249                     0.8638        0.3329  0.0069  0.2114\n",
      "     11                     0.8692        \u001b[32m0.3098\u001b[0m                     0.8507        0.3264  0.0063  0.2114\n",
      "     12                     \u001b[36m0.8888\u001b[0m        \u001b[32m0.2685\u001b[0m                     \u001b[35m0.8769\u001b[0m        0.3315  0.0057  0.2105\n",
      "     13                     \u001b[36m0.8949\u001b[0m        \u001b[32m0.2538\u001b[0m                     \u001b[35m0.8787\u001b[0m        \u001b[31m0.2864\u001b[0m  0.0050  0.2105\n",
      "     14                     \u001b[36m0.9028\u001b[0m        \u001b[32m0.2497\u001b[0m                     0.8731        0.3115  0.0043  0.2100\n",
      "     15                     \u001b[36m0.9107\u001b[0m        \u001b[32m0.2408\u001b[0m                     0.8713        0.3046  0.0037  0.2099\n",
      "     16                     \u001b[36m0.9140\u001b[0m        \u001b[32m0.2244\u001b[0m                     0.8713        0.3078  0.0031  0.2114\n",
      "     17                     \u001b[36m0.9145\u001b[0m        0.2325                     0.8787        0.3258  0.0025  0.2124\n",
      "     18                     \u001b[36m0.9150\u001b[0m        0.2270                     0.8713        0.3091  0.0020  0.2104\n",
      "     19                     \u001b[36m0.9206\u001b[0m        \u001b[32m0.2129\u001b[0m                     0.8750        0.3095  0.0015  0.2104\n",
      "     20                     \u001b[36m0.9294\u001b[0m        \u001b[32m0.1972\u001b[0m                     0.8787        0.3045  0.0010  0.2105\n",
      "     21                     0.9266        \u001b[32m0.1962\u001b[0m                     \u001b[35m0.8806\u001b[0m        0.3088  0.0007  0.2101\n",
      "     22                     0.9220        \u001b[32m0.1904\u001b[0m                     0.8787        0.3084  0.0004  0.2106\n",
      "     23                     0.9262        0.1931                     0.8769        0.3093  0.0002  0.2105\n",
      "     24                     0.9238        0.2007                     0.8750        0.3094  0.0000  0.2104\n",
      "     25                     \u001b[36m0.9318\u001b[0m        0.1947                     0.8769        0.3096  0.0000  0.2114\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6107\u001b[0m        \u001b[32m1.0499\u001b[0m                     \u001b[35m0.7407\u001b[0m        \u001b[31m0.5646\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.7565\u001b[0m        \u001b[32m0.5703\u001b[0m                     \u001b[35m0.8284\u001b[0m        \u001b[31m0.3668\u001b[0m  0.0100  0.2105\n",
      "      3                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4934\u001b[0m                     0.8190        0.3860  0.0098  0.2104\n",
      "      4                     \u001b[36m0.8290\u001b[0m        \u001b[32m0.4236\u001b[0m                     \u001b[35m0.8302\u001b[0m        0.4055  0.0096  0.2104\n",
      "      5                     \u001b[36m0.8430\u001b[0m        \u001b[32m0.3828\u001b[0m                     0.8172        0.3903  0.0093  0.2104\n",
      "      6                     \u001b[36m0.8514\u001b[0m        \u001b[32m0.3562\u001b[0m                     \u001b[35m0.8377\u001b[0m        \u001b[31m0.3471\u001b[0m  0.0090  0.2099\n",
      "      7                     \u001b[36m0.8584\u001b[0m        \u001b[32m0.3535\u001b[0m                     0.8265        0.3692  0.0085  0.2104\n",
      "      8                     0.8561        \u001b[32m0.3531\u001b[0m                     0.8190        0.4643  0.0080  0.2109\n",
      "      9                     \u001b[36m0.8706\u001b[0m        \u001b[32m0.3385\u001b[0m                     \u001b[35m0.8619\u001b[0m        \u001b[31m0.3131\u001b[0m  0.0075  0.2096\n",
      "     10                     \u001b[36m0.8841\u001b[0m        \u001b[32m0.3001\u001b[0m                     0.8507        0.3292  0.0069  0.2104\n",
      "     11                     \u001b[36m0.8855\u001b[0m        \u001b[32m0.2940\u001b[0m                     0.8489        0.3487  0.0063  0.2095\n",
      "     12                     0.8846        \u001b[32m0.2741\u001b[0m                     \u001b[35m0.8787\u001b[0m        \u001b[31m0.2994\u001b[0m  0.0057  0.2100\n",
      "     13                     \u001b[36m0.8907\u001b[0m        0.2755                     0.8619        0.3072  0.0050  0.2104\n",
      "     14                     \u001b[36m0.8977\u001b[0m        \u001b[32m0.2520\u001b[0m                     0.8694        0.3031  0.0043  0.2095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15                     0.8977        0.2582                     0.8675        0.3186  0.0037  0.2095\n",
      "     16                     \u001b[36m0.9033\u001b[0m        \u001b[32m0.2393\u001b[0m                     0.8750        0.2995  0.0031  0.2105\n",
      "     17                     \u001b[36m0.9089\u001b[0m        \u001b[32m0.2231\u001b[0m                     0.8657        0.3154  0.0025  0.2104\n",
      "     18                     \u001b[36m0.9150\u001b[0m        \u001b[32m0.2188\u001b[0m                     \u001b[35m0.8862\u001b[0m        \u001b[31m0.2921\u001b[0m  0.0020  0.2416\n",
      "     19                     \u001b[36m0.9187\u001b[0m        \u001b[32m0.1979\u001b[0m                     0.8825        \u001b[31m0.2884\u001b[0m  0.0015  0.2104\n",
      "     20                     \u001b[36m0.9229\u001b[0m        0.2078                     0.8843        \u001b[31m0.2860\u001b[0m  0.0010  0.2104\n",
      "     21                     0.9182        0.2015                     0.8862        0.2899  0.0007  0.2104\n",
      "     22                     \u001b[36m0.9271\u001b[0m        \u001b[32m0.1881\u001b[0m                     0.8806        0.2946  0.0004  0.2105\n",
      "     23                     \u001b[36m0.9276\u001b[0m        \u001b[32m0.1806\u001b[0m                     \u001b[35m0.8881\u001b[0m        0.2912  0.0002  0.2106\n",
      "     24                     0.9215        0.1966                     0.8787        0.2932  0.0000  0.2104\n",
      "     25                     0.9276        0.1815                     0.8787        0.2931  0.0000  0.2104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4944\u001b[0m        \u001b[32m1.3649\u001b[0m                     \u001b[35m0.5205\u001b[0m        \u001b[31m0.9678\u001b[0m  0.0100  0.2064\n",
      "      2                     \u001b[36m0.5107\u001b[0m        \u001b[32m1.0343\u001b[0m                     0.5187        \u001b[31m0.8021\u001b[0m  0.0100  0.2111\n",
      "      3                     \u001b[36m0.5603\u001b[0m        \u001b[32m0.7970\u001b[0m                     \u001b[35m0.5280\u001b[0m        \u001b[31m0.7245\u001b[0m  0.0098  0.2105\n",
      "      4                     \u001b[36m0.5673\u001b[0m        \u001b[32m0.7531\u001b[0m                     \u001b[35m0.5858\u001b[0m        \u001b[31m0.6901\u001b[0m  0.0096  0.2104\n",
      "      5                     \u001b[36m0.6215\u001b[0m        \u001b[32m0.6844\u001b[0m                     \u001b[35m0.6007\u001b[0m        0.6993  0.0093  0.2125\n",
      "      6                     \u001b[36m0.6411\u001b[0m        0.6855                     \u001b[35m0.6306\u001b[0m        0.7025  0.0090  0.2112\n",
      "      7                     \u001b[36m0.6682\u001b[0m        \u001b[32m0.6568\u001b[0m                     \u001b[35m0.6866\u001b[0m        \u001b[31m0.6060\u001b[0m  0.0085  0.2113\n",
      "      8                     \u001b[36m0.6869\u001b[0m        \u001b[32m0.6207\u001b[0m                     0.6716        0.6255  0.0080  0.2125\n",
      "      9                     \u001b[36m0.7009\u001b[0m        \u001b[32m0.5988\u001b[0m                     0.6343        0.7405  0.0075  0.2101\n",
      "     10                     \u001b[36m0.7061\u001b[0m        \u001b[32m0.5798\u001b[0m                     0.6269        0.6736  0.0069  0.2105\n",
      "     11                     \u001b[36m0.7131\u001b[0m        0.5849                     \u001b[35m0.6978\u001b[0m        \u001b[31m0.5928\u001b[0m  0.0063  0.2094\n",
      "     12                     \u001b[36m0.7248\u001b[0m        \u001b[32m0.5486\u001b[0m                     \u001b[35m0.7034\u001b[0m        \u001b[31m0.5827\u001b[0m  0.0057  0.2094\n",
      "     13                     \u001b[36m0.7369\u001b[0m        \u001b[32m0.5331\u001b[0m                     \u001b[35m0.7108\u001b[0m        \u001b[31m0.5688\u001b[0m  0.0050  0.2111\n",
      "     14                     \u001b[36m0.7486\u001b[0m        \u001b[32m0.5122\u001b[0m                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.5574\u001b[0m  0.0043  0.2113\n",
      "     15                     \u001b[36m0.7556\u001b[0m        \u001b[32m0.5067\u001b[0m                     \u001b[35m0.7276\u001b[0m        0.5595  0.0037  0.2097\n",
      "     16                     \u001b[36m0.7664\u001b[0m        \u001b[32m0.4833\u001b[0m                     0.7183        0.5707  0.0031  0.2095\n",
      "     17                     \u001b[36m0.7673\u001b[0m        \u001b[32m0.4822\u001b[0m                     0.7239        0.5729  0.0025  0.2104\n",
      "     18                     \u001b[36m0.7776\u001b[0m        \u001b[32m0.4570\u001b[0m                     0.7257        0.5627  0.0020  0.2095\n",
      "     19                     \u001b[36m0.8019\u001b[0m        \u001b[32m0.4302\u001b[0m                     0.7276        0.5644  0.0015  0.2104\n",
      "     20                     0.8014        \u001b[32m0.4302\u001b[0m                     0.7239        0.5698  0.0010  0.2104\n",
      "     21                     0.7981        \u001b[32m0.4223\u001b[0m                     0.7276        0.5619  0.0007  0.2104\n",
      "     22                     \u001b[36m0.8079\u001b[0m        0.4240                     0.7276        0.5600  0.0004  0.2089\n",
      "     23                     \u001b[36m0.8164\u001b[0m        \u001b[32m0.4180\u001b[0m                     0.7276        0.5603  0.0002  0.2114\n",
      "     24                     0.8121        0.4183                     \u001b[35m0.7332\u001b[0m        0.5620  0.0000  0.2104\n",
      "     25                     0.8145        \u001b[32m0.4009\u001b[0m                     0.7295        0.5611  0.0000  0.2105\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5084\u001b[0m        \u001b[32m1.2690\u001b[0m                     \u001b[35m0.5056\u001b[0m        \u001b[31m0.8527\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.5495\u001b[0m        \u001b[32m0.8530\u001b[0m                     \u001b[35m0.5485\u001b[0m        \u001b[31m0.7182\u001b[0m  0.0100  0.2085\n",
      "      3                     \u001b[36m0.5729\u001b[0m        \u001b[32m0.8038\u001b[0m                     0.5392        0.8493  0.0098  0.2094\n",
      "      4                     \u001b[36m0.6033\u001b[0m        \u001b[32m0.7265\u001b[0m                     \u001b[35m0.6082\u001b[0m        \u001b[31m0.6845\u001b[0m  0.0096  0.2104\n",
      "      5                     \u001b[36m0.6421\u001b[0m        \u001b[32m0.6899\u001b[0m                     0.6045        0.7148  0.0093  0.2115\n",
      "      6                     \u001b[36m0.6561\u001b[0m        \u001b[32m0.6457\u001b[0m                     \u001b[35m0.6455\u001b[0m        \u001b[31m0.6228\u001b[0m  0.0090  0.2106\n",
      "      7                     \u001b[36m0.6762\u001b[0m        0.6514                     \u001b[35m0.6660\u001b[0m        0.6240  0.0085  0.2104\n",
      "      8                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.6014\u001b[0m                     \u001b[35m0.6660\u001b[0m        0.6337  0.0080  0.2095\n",
      "      9                     \u001b[36m0.7089\u001b[0m        \u001b[32m0.5693\u001b[0m                     0.6175        0.6739  0.0075  0.2118\n",
      "     10                     \u001b[36m0.7136\u001b[0m        0.5723                     0.6586        0.6422  0.0069  0.2104\n",
      "     11                     \u001b[36m0.7421\u001b[0m        \u001b[32m0.5292\u001b[0m                     \u001b[35m0.6679\u001b[0m        0.6362  0.0063  0.2105\n",
      "     12                     \u001b[36m0.7500\u001b[0m        \u001b[32m0.5257\u001b[0m                     \u001b[35m0.6716\u001b[0m        0.6325  0.0057  0.2106\n",
      "     13                     \u001b[36m0.7528\u001b[0m        \u001b[32m0.5175\u001b[0m                     \u001b[35m0.6735\u001b[0m        0.6366  0.0050  0.2111\n",
      "     14                     \u001b[36m0.7556\u001b[0m        \u001b[32m0.5037\u001b[0m                     0.6698        0.6713  0.0043  0.2108\n",
      "     15                     \u001b[36m0.7748\u001b[0m        \u001b[32m0.4967\u001b[0m                     0.6604        0.6481  0.0037  0.2104\n",
      "     16                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4617\u001b[0m                     0.6381        0.7036  0.0031  0.2105\n",
      "     17                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4495\u001b[0m                     0.6362        0.6952  0.0025  0.2105\n",
      "     18                     0.7939        \u001b[32m0.4423\u001b[0m                     0.6660        0.6582  0.0020  0.2104\n",
      "     19                     \u001b[36m0.8065\u001b[0m        \u001b[32m0.4153\u001b[0m                     \u001b[35m0.6847\u001b[0m        0.6482  0.0015  0.2114\n",
      "     20                     0.7977        0.4217                     \u001b[35m0.6884\u001b[0m        0.6376  0.0010  0.2105\n",
      "     21                     \u001b[36m0.8173\u001b[0m        \u001b[32m0.4018\u001b[0m                     0.6810        0.6464  0.0007  0.2114\n",
      "     22                     0.8154        0.4072                     0.6791        0.6518  0.0004  0.2105\n",
      "     23                     \u001b[36m0.8229\u001b[0m        \u001b[32m0.3992\u001b[0m                     0.6866        0.6549  0.0002  0.2104\n",
      "     24                     0.8117        0.4030                     0.6847        0.6553  0.0000  0.2114\n",
      "     25                     0.8112        0.4003                     0.6866        0.6553  0.0000  0.2104\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5051\u001b[0m        \u001b[32m1.2561\u001b[0m                     \u001b[35m0.5168\u001b[0m        \u001b[31m0.9105\u001b[0m  0.0100  0.2025\n",
      "      2                     \u001b[36m0.5458\u001b[0m        \u001b[32m0.8695\u001b[0m                     \u001b[35m0.5877\u001b[0m        \u001b[31m0.6790\u001b[0m  0.0100  0.2095\n",
      "      3                     \u001b[36m0.5776\u001b[0m        \u001b[32m0.7530\u001b[0m                     0.5840        0.7185  0.0098  0.2094\n",
      "      4                     \u001b[36m0.5925\u001b[0m        0.7575                     \u001b[35m0.6101\u001b[0m        \u001b[31m0.6436\u001b[0m  0.0096  0.2104\n",
      "      5                     \u001b[36m0.6472\u001b[0m        \u001b[32m0.6845\u001b[0m                     \u001b[35m0.6437\u001b[0m        0.6532  0.0093  0.2106\n",
      "      6                     0.6411        0.6877                     0.5784        0.7525  0.0090  0.2094\n",
      "      7                     \u001b[36m0.6692\u001b[0m        \u001b[32m0.6374\u001b[0m                     \u001b[35m0.6754\u001b[0m        \u001b[31m0.6280\u001b[0m  0.0085  0.2103\n",
      "      8                     \u001b[36m0.6879\u001b[0m        \u001b[32m0.6141\u001b[0m                     0.6679        \u001b[31m0.6279\u001b[0m  0.0080  0.2107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.6063\u001b[0m                     0.6623        0.6614  0.0075  0.2114\n",
      "     10                     \u001b[36m0.7229\u001b[0m        \u001b[32m0.5632\u001b[0m                     \u001b[35m0.6772\u001b[0m        0.6378  0.0069  0.2114\n",
      "     11                     \u001b[36m0.7299\u001b[0m        \u001b[32m0.5497\u001b[0m                     \u001b[35m0.6866\u001b[0m        \u001b[31m0.6241\u001b[0m  0.0063  0.2093\n",
      "     12                     \u001b[36m0.7421\u001b[0m        \u001b[32m0.5384\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.5937\u001b[0m  0.0057  0.2099\n",
      "     13                     \u001b[36m0.7500\u001b[0m        \u001b[32m0.5124\u001b[0m                     \u001b[35m0.6959\u001b[0m        0.6203  0.0050  0.2094\n",
      "     14                     \u001b[36m0.7636\u001b[0m        \u001b[32m0.4996\u001b[0m                     0.6791        0.6217  0.0043  0.2095\n",
      "     15                     \u001b[36m0.7720\u001b[0m        \u001b[32m0.4725\u001b[0m                     0.6903        0.6222  0.0037  0.2104\n",
      "     16                     \u001b[36m0.7785\u001b[0m        \u001b[32m0.4700\u001b[0m                     \u001b[35m0.6978\u001b[0m        0.6053  0.0031  0.2104\n",
      "     17                     \u001b[36m0.7860\u001b[0m        \u001b[32m0.4555\u001b[0m                     0.6940        0.6162  0.0025  0.2097\n",
      "     18                     \u001b[36m0.7995\u001b[0m        \u001b[32m0.4267\u001b[0m                     0.6903        0.6290  0.0020  0.2104\n",
      "     19                     \u001b[36m0.8107\u001b[0m        \u001b[32m0.4218\u001b[0m                     0.6940        0.6364  0.0015  0.2096\n",
      "     20                     0.8103        \u001b[32m0.4143\u001b[0m                     \u001b[35m0.7034\u001b[0m        0.6255  0.0010  0.2095\n",
      "     21                     \u001b[36m0.8220\u001b[0m        \u001b[32m0.4070\u001b[0m                     \u001b[35m0.7108\u001b[0m        0.6188  0.0007  0.2104\n",
      "     22                     0.8220        \u001b[32m0.3940\u001b[0m                     0.7052        0.6238  0.0004  0.2104\n",
      "     23                     0.8150        \u001b[32m0.3932\u001b[0m                     \u001b[35m0.7164\u001b[0m        0.6248  0.0002  0.2114\n",
      "     24                     0.8192        \u001b[32m0.3882\u001b[0m                     0.7108        0.6261  0.0000  0.2094\n",
      "     25                     0.8150        0.4021                     0.7108        0.6250  0.0000  0.2091\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5079\u001b[0m        \u001b[32m1.1208\u001b[0m                     \u001b[35m0.5317\u001b[0m        \u001b[31m0.7379\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.5262\u001b[0m        \u001b[32m0.8841\u001b[0m                     0.5131        0.8054  0.0100  0.2085\n",
      "      3                     \u001b[36m0.5477\u001b[0m        \u001b[32m0.7898\u001b[0m                     \u001b[35m0.5672\u001b[0m        \u001b[31m0.6857\u001b[0m  0.0098  0.2104\n",
      "      4                     \u001b[36m0.5827\u001b[0m        \u001b[32m0.7321\u001b[0m                     \u001b[35m0.5672\u001b[0m        0.7702  0.0096  0.2095\n",
      "      5                     \u001b[36m0.6047\u001b[0m        0.7576                     \u001b[35m0.6269\u001b[0m        0.6862  0.0093  0.2104\n",
      "      6                     \u001b[36m0.6369\u001b[0m        \u001b[32m0.7071\u001b[0m                     \u001b[35m0.6306\u001b[0m        0.7259  0.0090  0.2099\n",
      "      7                     \u001b[36m0.6710\u001b[0m        \u001b[32m0.6458\u001b[0m                     \u001b[35m0.6735\u001b[0m        \u001b[31m0.6361\u001b[0m  0.0085  0.2104\n",
      "      8                     \u001b[36m0.6963\u001b[0m        \u001b[32m0.5991\u001b[0m                     0.6735        \u001b[31m0.6293\u001b[0m  0.0080  0.2095\n",
      "      9                     0.6864        0.6042                     \u001b[35m0.6754\u001b[0m        \u001b[31m0.6288\u001b[0m  0.0075  0.2108\n",
      "     10                     \u001b[36m0.7070\u001b[0m        \u001b[32m0.5790\u001b[0m                     0.6418        0.6716  0.0069  0.2114\n",
      "     11                     0.7061        0.6111                     0.6437        0.7613  0.0063  0.2094\n",
      "     12                     \u001b[36m0.7126\u001b[0m        0.5840                     0.6511        0.6710  0.0057  0.2104\n",
      "     13                     \u001b[36m0.7397\u001b[0m        \u001b[32m0.5264\u001b[0m                     \u001b[35m0.6828\u001b[0m        \u001b[31m0.6014\u001b[0m  0.0050  0.2104\n",
      "     14                     \u001b[36m0.7421\u001b[0m        0.5353                     0.6716        0.6093  0.0043  0.2111\n",
      "     15                     \u001b[36m0.7561\u001b[0m        \u001b[32m0.5108\u001b[0m                     \u001b[35m0.6903\u001b[0m        0.6026  0.0037  0.2105\n",
      "     16                     \u001b[36m0.7579\u001b[0m        \u001b[32m0.4869\u001b[0m                     0.6847        0.6246  0.0031  0.2105\n",
      "     17                     \u001b[36m0.7757\u001b[0m        \u001b[32m0.4621\u001b[0m                     \u001b[35m0.6922\u001b[0m        0.6050  0.0025  0.2108\n",
      "     18                     \u001b[36m0.7836\u001b[0m        0.4693                     0.6716        0.6123  0.0020  0.2102\n",
      "     19                     \u001b[36m0.7986\u001b[0m        \u001b[32m0.4532\u001b[0m                     0.6810        \u001b[31m0.6003\u001b[0m  0.0015  0.2105\n",
      "     20                     0.7930        \u001b[32m0.4417\u001b[0m                     0.6791        0.6024  0.0010  0.2105\n",
      "     21                     \u001b[36m0.8005\u001b[0m        \u001b[32m0.4335\u001b[0m                     0.6716        0.6036  0.0007  0.2104\n",
      "     22                     \u001b[36m0.8042\u001b[0m        \u001b[32m0.4184\u001b[0m                     0.6866        0.6026  0.0004  0.2106\n",
      "     23                     \u001b[36m0.8154\u001b[0m        \u001b[32m0.4037\u001b[0m                     0.6922        0.6042  0.0002  0.2104\n",
      "     24                     0.8089        0.4163                     \u001b[35m0.6959\u001b[0m        0.6042  0.0000  0.2108\n",
      "     25                     0.8084        0.4131                     0.6940        0.6041  0.0000  0.2121\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5131\u001b[0m        \u001b[32m1.2471\u001b[0m                     \u001b[35m0.5093\u001b[0m        \u001b[31m0.8048\u001b[0m  0.0100  0.2045\n",
      "      2                     \u001b[36m0.5173\u001b[0m        \u001b[32m0.8825\u001b[0m                     0.4963        \u001b[31m0.7774\u001b[0m  0.0100  0.2106\n",
      "      3                     \u001b[36m0.5350\u001b[0m        \u001b[32m0.8750\u001b[0m                     \u001b[35m0.5466\u001b[0m        0.8326  0.0098  0.2105\n",
      "      4                     \u001b[36m0.5752\u001b[0m        \u001b[32m0.7893\u001b[0m                     \u001b[35m0.6399\u001b[0m        \u001b[31m0.6548\u001b[0m  0.0096  0.2105\n",
      "      5                     \u001b[36m0.5888\u001b[0m        \u001b[32m0.7213\u001b[0m                     \u001b[35m0.6437\u001b[0m        \u001b[31m0.6270\u001b[0m  0.0093  0.2124\n",
      "      6                     \u001b[36m0.6196\u001b[0m        \u001b[32m0.6902\u001b[0m                     \u001b[35m0.6604\u001b[0m        \u001b[31m0.6088\u001b[0m  0.0090  0.2095\n",
      "      7                     \u001b[36m0.6407\u001b[0m        \u001b[32m0.6487\u001b[0m                     0.6343        0.6643  0.0085  0.2108\n",
      "      8                     \u001b[36m0.6607\u001b[0m        0.6682                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5587\u001b[0m  0.0080  0.2093\n",
      "      9                     \u001b[36m0.6864\u001b[0m        \u001b[32m0.6126\u001b[0m                     0.6922        0.5631  0.0075  0.2114\n",
      "     10                     \u001b[36m0.6907\u001b[0m        0.6127                     0.6996        0.5676  0.0069  0.2104\n",
      "     11                     \u001b[36m0.7136\u001b[0m        \u001b[32m0.5605\u001b[0m                     0.6959        0.5686  0.0063  0.2114\n",
      "     12                     0.7117        \u001b[32m0.5511\u001b[0m                     0.7015        0.5665  0.0057  0.2095\n",
      "     13                     \u001b[36m0.7481\u001b[0m        \u001b[32m0.5222\u001b[0m                     0.6828        0.5880  0.0050  0.2114\n",
      "     14                     0.7458        0.5250                     0.6828        0.6150  0.0043  0.2095\n",
      "     15                     \u001b[36m0.7621\u001b[0m        \u001b[32m0.5028\u001b[0m                     0.7090        0.5725  0.0037  0.2104\n",
      "     16                     \u001b[36m0.7706\u001b[0m        \u001b[32m0.4791\u001b[0m                     0.7034        0.5668  0.0031  0.2105\n",
      "     17                     \u001b[36m0.7790\u001b[0m        \u001b[32m0.4680\u001b[0m                     0.6940        0.5687  0.0025  0.2095\n",
      "     18                     \u001b[36m0.7813\u001b[0m        0.4743                     0.6922        0.5728  0.0020  0.2100\n",
      "     19                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4361\u001b[0m                     0.7015        0.5734  0.0015  0.2095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                     \u001b[36m0.8005\u001b[0m        \u001b[32m0.4359\u001b[0m                     0.7052        0.5767  0.0010  0.2095\n",
      "     21                     0.7930        \u001b[32m0.4283\u001b[0m                     0.6903        0.5683  0.0007  0.2118\n",
      "     22                     \u001b[36m0.8098\u001b[0m        \u001b[32m0.4207\u001b[0m                     0.7015        0.5708  0.0004  0.2110\n",
      "     23                     0.8051        \u001b[32m0.4169\u001b[0m                     0.7015        0.5657  0.0002  0.2101\n",
      "     24                     \u001b[36m0.8140\u001b[0m        \u001b[32m0.4062\u001b[0m                     0.6940        0.5664  0.0000  0.2098\n",
      "     25                     \u001b[36m0.8210\u001b[0m        0.4097                     0.6996        0.5658  0.0000  0.2105\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.4967\u001b[0m        \u001b[32m1.3061\u001b[0m                     \u001b[35m0.5093\u001b[0m        \u001b[31m0.7878\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.5379\u001b[0m        \u001b[32m0.8659\u001b[0m                     \u001b[35m0.5690\u001b[0m        \u001b[31m0.6976\u001b[0m  0.0100  0.2115\n",
      "      3                     \u001b[36m0.5850\u001b[0m        \u001b[32m0.7547\u001b[0m                     \u001b[35m0.5914\u001b[0m        \u001b[31m0.6797\u001b[0m  0.0098  0.2102\n",
      "      4                     \u001b[36m0.6089\u001b[0m        \u001b[32m0.6995\u001b[0m                     0.5840        \u001b[31m0.6757\u001b[0m  0.0096  0.2098\n",
      "      5                     \u001b[36m0.6313\u001b[0m        \u001b[32m0.6904\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6292\u001b[0m  0.0093  0.2094\n",
      "      6                     \u001b[36m0.6523\u001b[0m        \u001b[32m0.6606\u001b[0m                     0.6269        0.6522  0.0090  0.2094\n",
      "      7                     0.6416        0.6704                     \u001b[35m0.6828\u001b[0m        \u001b[31m0.6100\u001b[0m  0.0085  0.2095\n",
      "      8                     \u001b[36m0.6860\u001b[0m        \u001b[32m0.6136\u001b[0m                     0.6735        0.6188  0.0080  0.2095\n",
      "      9                     \u001b[36m0.7065\u001b[0m        \u001b[32m0.5903\u001b[0m                     0.6474        0.6573  0.0075  0.2104\n",
      "     10                     \u001b[36m0.7107\u001b[0m        \u001b[32m0.5883\u001b[0m                     \u001b[35m0.6884\u001b[0m        \u001b[31m0.5697\u001b[0m  0.0069  0.2104\n",
      "     11                     0.7065        \u001b[32m0.5763\u001b[0m                     0.6772        0.6063  0.0063  0.2104\n",
      "     12                     0.7098        \u001b[32m0.5528\u001b[0m                     \u001b[35m0.6978\u001b[0m        0.5763  0.0057  0.2092\n",
      "     13                     \u001b[36m0.7388\u001b[0m        \u001b[32m0.5250\u001b[0m                     \u001b[35m0.7146\u001b[0m        \u001b[31m0.5498\u001b[0m  0.0050  0.2095\n",
      "     14                     \u001b[36m0.7425\u001b[0m        0.5277                     0.7090        0.5639  0.0043  0.2098\n",
      "     15                     \u001b[36m0.7593\u001b[0m        \u001b[32m0.5025\u001b[0m                     0.7127        \u001b[31m0.5419\u001b[0m  0.0037  0.2116\n",
      "     16                     \u001b[36m0.7734\u001b[0m        \u001b[32m0.4754\u001b[0m                     0.6903        0.5877  0.0031  0.2085\n",
      "     17                     \u001b[36m0.7738\u001b[0m        0.4759                     \u001b[35m0.7164\u001b[0m        0.5460  0.0025  0.2095\n",
      "     18                     \u001b[36m0.8000\u001b[0m        \u001b[32m0.4442\u001b[0m                     0.7146        0.5478  0.0020  0.2105\n",
      "     19                     0.7879        0.4541                     \u001b[35m0.7276\u001b[0m        0.5477  0.0015  0.2104\n",
      "     20                     0.7991        \u001b[32m0.4232\u001b[0m                     0.7201        0.5601  0.0010  0.2105\n",
      "     21                     \u001b[36m0.8098\u001b[0m        0.4262                     0.7239        0.5517  0.0007  0.2104\n",
      "     22                     \u001b[36m0.8173\u001b[0m        \u001b[32m0.4165\u001b[0m                     0.7108        0.5478  0.0004  0.2114\n",
      "     23                     0.8168        \u001b[32m0.4077\u001b[0m                     0.7090        0.5471  0.0002  0.2115\n",
      "     24                     \u001b[36m0.8257\u001b[0m        \u001b[32m0.4009\u001b[0m                     0.7127        0.5481  0.0000  0.2104\n",
      "     25                     0.8238        \u001b[32m0.3948\u001b[0m                     0.7090        0.5484  0.0000  0.2105\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5234\u001b[0m        \u001b[32m1.0845\u001b[0m                     \u001b[35m0.5205\u001b[0m        \u001b[31m0.7485\u001b[0m  0.0100  0.2124\n",
      "      2                     \u001b[36m0.5477\u001b[0m        \u001b[32m0.8265\u001b[0m                     \u001b[35m0.5522\u001b[0m        0.7786  0.0100  0.2334\n",
      "      3                     \u001b[36m0.5804\u001b[0m        \u001b[32m0.7434\u001b[0m                     \u001b[35m0.6007\u001b[0m        \u001b[31m0.6470\u001b[0m  0.0098  0.2307\n",
      "      4                     \u001b[36m0.6224\u001b[0m        \u001b[32m0.7028\u001b[0m                     \u001b[35m0.6306\u001b[0m        \u001b[31m0.6425\u001b[0m  0.0096  0.2319\n",
      "      5                     \u001b[36m0.6393\u001b[0m        \u001b[32m0.6678\u001b[0m                     0.6045        0.6725  0.0093  0.2392\n",
      "      6                     \u001b[36m0.6631\u001b[0m        \u001b[32m0.6645\u001b[0m                     \u001b[35m0.6325\u001b[0m        0.6434  0.0090  0.2334\n",
      "      7                     \u001b[36m0.6883\u001b[0m        \u001b[32m0.6242\u001b[0m                     \u001b[35m0.6343\u001b[0m        0.6582  0.0085  0.2324\n",
      "      8                     \u001b[36m0.6893\u001b[0m        \u001b[32m0.6112\u001b[0m                     \u001b[35m0.6549\u001b[0m        0.6634  0.0080  0.2354\n",
      "      9                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.5881\u001b[0m                     \u001b[35m0.6735\u001b[0m        \u001b[31m0.6319\u001b[0m  0.0075  0.2280\n",
      "     10                     0.6953        0.6041                     \u001b[35m0.6810\u001b[0m        0.6420  0.0069  0.2129\n",
      "     11                     \u001b[36m0.7112\u001b[0m        \u001b[32m0.5683\u001b[0m                     0.6660        \u001b[31m0.6182\u001b[0m  0.0063  0.2124\n",
      "     12                     \u001b[36m0.7364\u001b[0m        \u001b[32m0.5332\u001b[0m                     0.6810        \u001b[31m0.6131\u001b[0m  0.0057  0.2258\n",
      "     13                     0.7290        0.5346                     \u001b[35m0.6884\u001b[0m        \u001b[31m0.6038\u001b[0m  0.0050  0.2244\n",
      "     14                     \u001b[36m0.7729\u001b[0m        \u001b[32m0.4960\u001b[0m                     \u001b[35m0.6903\u001b[0m        \u001b[31m0.5917\u001b[0m  0.0043  0.2309\n",
      "     15                     0.7692        \u001b[32m0.4899\u001b[0m                     \u001b[35m0.6978\u001b[0m        0.5987  0.0037  0.2327\n",
      "     16                     \u001b[36m0.7846\u001b[0m        \u001b[32m0.4710\u001b[0m                     0.6903        0.6274  0.0031  0.2154\n",
      "     17                     0.7808        \u001b[32m0.4582\u001b[0m                     0.6922        0.5944  0.0025  0.2139\n",
      "     18                     \u001b[36m0.7925\u001b[0m        \u001b[32m0.4394\u001b[0m                     \u001b[35m0.7052\u001b[0m        \u001b[31m0.5912\u001b[0m  0.0020  0.2211\n",
      "     19                     \u001b[36m0.8028\u001b[0m        \u001b[32m0.4263\u001b[0m                     0.6959        0.5956  0.0015  0.2304\n",
      "     20                     \u001b[36m0.8107\u001b[0m        \u001b[32m0.4138\u001b[0m                     0.6940        0.5945  0.0010  0.2307\n",
      "     21                     \u001b[36m0.8117\u001b[0m        \u001b[32m0.4102\u001b[0m                     0.6828        0.5922  0.0007  0.2104\n",
      "     22                     \u001b[36m0.8192\u001b[0m        \u001b[32m0.3990\u001b[0m                     0.7034        0.5977  0.0004  0.2110\n",
      "     23                     \u001b[36m0.8322\u001b[0m        \u001b[32m0.3957\u001b[0m                     0.6959        0.5982  0.0002  0.2130\n",
      "     24                     0.8294        \u001b[32m0.3854\u001b[0m                     0.6922        0.5984  0.0000  0.2214\n",
      "     25                     0.8280        0.3865                     0.6978        0.5992  0.0000  0.2284\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5220\u001b[0m        \u001b[32m1.1452\u001b[0m                     \u001b[35m0.5187\u001b[0m        \u001b[31m0.7581\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.5533\u001b[0m        \u001b[32m0.8506\u001b[0m                     \u001b[35m0.5336\u001b[0m        \u001b[31m0.7298\u001b[0m  0.0100  0.2172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3                     \u001b[36m0.5743\u001b[0m        \u001b[32m0.7671\u001b[0m                     \u001b[35m0.5560\u001b[0m        \u001b[31m0.7170\u001b[0m  0.0098  0.2135\n",
      "      4                     \u001b[36m0.6023\u001b[0m        \u001b[32m0.7092\u001b[0m                     \u001b[35m0.6231\u001b[0m        \u001b[31m0.6657\u001b[0m  0.0096  0.2111\n",
      "      5                     \u001b[36m0.6271\u001b[0m        \u001b[32m0.6672\u001b[0m                     \u001b[35m0.6418\u001b[0m        \u001b[31m0.6395\u001b[0m  0.0093  0.2106\n",
      "      6                     \u001b[36m0.6416\u001b[0m        \u001b[32m0.6634\u001b[0m                     \u001b[35m0.6567\u001b[0m        0.6463  0.0090  0.2135\n",
      "      7                     \u001b[36m0.6505\u001b[0m        0.6802                     0.6399        0.6635  0.0085  0.2228\n",
      "      8                     \u001b[36m0.6687\u001b[0m        \u001b[32m0.6257\u001b[0m                     \u001b[35m0.6847\u001b[0m        \u001b[31m0.6166\u001b[0m  0.0080  0.2174\n",
      "      9                     \u001b[36m0.6967\u001b[0m        \u001b[32m0.6012\u001b[0m                     0.6343        0.6746  0.0075  0.2214\n",
      "     10                     \u001b[36m0.7154\u001b[0m        \u001b[32m0.5671\u001b[0m                     \u001b[35m0.6884\u001b[0m        \u001b[31m0.5918\u001b[0m  0.0069  0.2194\n",
      "     11                     \u001b[36m0.7327\u001b[0m        \u001b[32m0.5442\u001b[0m                     0.6604        0.6497  0.0063  0.2216\n",
      "     12                     \u001b[36m0.7364\u001b[0m        \u001b[32m0.5233\u001b[0m                     \u001b[35m0.6996\u001b[0m        \u001b[31m0.5870\u001b[0m  0.0057  0.2315\n",
      "     13                     \u001b[36m0.7500\u001b[0m        0.5240                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.5764\u001b[0m  0.0050  0.2229\n",
      "     14                     \u001b[36m0.7607\u001b[0m        \u001b[32m0.5048\u001b[0m                     0.6903        0.6150  0.0043  0.2187\n",
      "     15                     \u001b[36m0.7631\u001b[0m        \u001b[32m0.4828\u001b[0m                     0.6847        0.5976  0.0037  0.2257\n",
      "     16                     \u001b[36m0.7738\u001b[0m        0.4829                     0.6978        0.5999  0.0031  0.2325\n",
      "     17                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4430\u001b[0m                     0.6978        0.5805  0.0025  0.2271\n",
      "     18                     \u001b[36m0.8005\u001b[0m        \u001b[32m0.4398\u001b[0m                     0.6978        0.5990  0.0020  0.2204\n",
      "     19                     \u001b[36m0.8126\u001b[0m        \u001b[32m0.4241\u001b[0m                     \u001b[35m0.7108\u001b[0m        \u001b[31m0.5738\u001b[0m  0.0015  0.2234\n",
      "     20                     0.8084        \u001b[32m0.4144\u001b[0m                     0.6996        0.5826  0.0010  0.2113\n",
      "     21                     \u001b[36m0.8201\u001b[0m        \u001b[32m0.4019\u001b[0m                     0.7071        0.5944  0.0007  0.2224\n",
      "     22                     0.8182        0.4033                     \u001b[35m0.7164\u001b[0m        0.5866  0.0004  0.2142\n",
      "     23                     \u001b[36m0.8262\u001b[0m        \u001b[32m0.3797\u001b[0m                     0.7164        0.5820  0.0002  0.2224\n",
      "     24                     0.8159        0.3983                     0.7127        0.5860  0.0000  0.2248\n",
      "     25                     0.8164        0.3825                     \u001b[35m0.7183\u001b[0m        0.5852  0.0000  0.2278\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5019\u001b[0m        \u001b[32m1.3340\u001b[0m                     \u001b[35m0.5280\u001b[0m        \u001b[31m0.7426\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.5126\u001b[0m        \u001b[32m0.9220\u001b[0m                     \u001b[35m0.5429\u001b[0m        \u001b[31m0.7095\u001b[0m  0.0100  0.2111\n",
      "      3                     \u001b[36m0.5696\u001b[0m        \u001b[32m0.8297\u001b[0m                     \u001b[35m0.5728\u001b[0m        \u001b[31m0.6665\u001b[0m  0.0098  0.2107\n",
      "      4                     \u001b[36m0.6093\u001b[0m        \u001b[32m0.7125\u001b[0m                     \u001b[35m0.6325\u001b[0m        \u001b[31m0.6384\u001b[0m  0.0096  0.2115\n",
      "      5                     \u001b[36m0.6435\u001b[0m        \u001b[32m0.6711\u001b[0m                     \u001b[35m0.6511\u001b[0m        0.6591  0.0093  0.2106\n",
      "      6                     \u001b[36m0.6458\u001b[0m        0.6841                     0.6455        \u001b[31m0.6263\u001b[0m  0.0090  0.2109\n",
      "      7                     \u001b[36m0.6692\u001b[0m        \u001b[32m0.6252\u001b[0m                     \u001b[35m0.6604\u001b[0m        \u001b[31m0.6096\u001b[0m  0.0085  0.2104\n",
      "      8                     0.6593        0.6516                     0.6455        0.6659  0.0080  0.2107\n",
      "      9                     \u001b[36m0.6883\u001b[0m        0.6358                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.5940\u001b[0m  0.0075  0.2109\n",
      "     10                     \u001b[36m0.7075\u001b[0m        \u001b[32m0.6004\u001b[0m                     0.6903        \u001b[31m0.5824\u001b[0m  0.0069  0.2095\n",
      "     11                     \u001b[36m0.7150\u001b[0m        \u001b[32m0.5826\u001b[0m                     \u001b[35m0.6996\u001b[0m        0.5891  0.0063  0.2100\n",
      "     12                     \u001b[36m0.7248\u001b[0m        \u001b[32m0.5608\u001b[0m                     0.6959        0.5970  0.0057  0.2108\n",
      "     13                     \u001b[36m0.7486\u001b[0m        \u001b[32m0.5298\u001b[0m                     0.6959        \u001b[31m0.5819\u001b[0m  0.0050  0.2105\n",
      "     14                     0.7383        \u001b[32m0.5214\u001b[0m                     \u001b[35m0.7146\u001b[0m        \u001b[31m0.5580\u001b[0m  0.0043  0.2111\n",
      "     15                     \u001b[36m0.7603\u001b[0m        \u001b[32m0.5035\u001b[0m                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5410\u001b[0m  0.0037  0.2113\n",
      "     16                     \u001b[36m0.7650\u001b[0m        \u001b[32m0.4856\u001b[0m                     0.7146        0.5580  0.0031  0.2114\n",
      "     17                     \u001b[36m0.7762\u001b[0m        \u001b[32m0.4728\u001b[0m                     0.7146        0.5461  0.0025  0.2100\n",
      "     18                     \u001b[36m0.7790\u001b[0m        \u001b[32m0.4585\u001b[0m                     0.7257        0.5513  0.0020  0.2112\n",
      "     19                     \u001b[36m0.7827\u001b[0m        \u001b[32m0.4542\u001b[0m                     0.7090        0.5571  0.0015  0.2103\n",
      "     20                     \u001b[36m0.8042\u001b[0m        \u001b[32m0.4442\u001b[0m                     0.7090        0.5552  0.0010  0.2105\n",
      "     21                     0.7995        \u001b[32m0.4286\u001b[0m                     0.7201        0.5528  0.0007  0.2124\n",
      "     22                     0.8019        0.4361                     0.7108        0.5578  0.0004  0.2105\n",
      "     23                     0.8042        \u001b[32m0.4221\u001b[0m                     0.7146        0.5560  0.0002  0.2104\n",
      "     24                     \u001b[36m0.8117\u001b[0m        \u001b[32m0.4083\u001b[0m                     0.7146        0.5562  0.0000  0.2104\n",
      "     25                     0.7963        0.4219                     0.7127        0.5568  0.0000  0.2101\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5154\u001b[0m        \u001b[32m1.2889\u001b[0m                     \u001b[35m0.5205\u001b[0m        \u001b[31m0.7236\u001b[0m  0.0100  0.2035\n",
      "      2                     \u001b[36m0.5547\u001b[0m        \u001b[32m0.8708\u001b[0m                     \u001b[35m0.5784\u001b[0m        \u001b[31m0.6809\u001b[0m  0.0100  0.2095\n",
      "      3                     \u001b[36m0.5935\u001b[0m        \u001b[32m0.7451\u001b[0m                     \u001b[35m0.5840\u001b[0m        0.7441  0.0098  0.2095\n",
      "      4                     \u001b[36m0.6173\u001b[0m        \u001b[32m0.6980\u001b[0m                     0.5821        0.7330  0.0096  0.2104\n",
      "      5                     0.6140        0.7020                     \u001b[35m0.6101\u001b[0m        0.6821  0.0093  0.2104\n",
      "      6                     \u001b[36m0.6542\u001b[0m        \u001b[32m0.6467\u001b[0m                     \u001b[35m0.6586\u001b[0m        \u001b[31m0.6372\u001b[0m  0.0090  0.2105\n",
      "      7                     \u001b[36m0.6785\u001b[0m        \u001b[32m0.6182\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.6230\u001b[0m  0.0085  0.2105\n",
      "      8                     \u001b[36m0.6836\u001b[0m        0.6410                     \u001b[35m0.6828\u001b[0m        \u001b[31m0.5954\u001b[0m  0.0080  0.2086\n",
      "      9                     \u001b[36m0.6949\u001b[0m        \u001b[32m0.6167\u001b[0m                     0.6791        0.6205  0.0075  0.2094\n",
      "     10                     \u001b[36m0.7014\u001b[0m        \u001b[32m0.5837\u001b[0m                     0.6735        0.6285  0.0069  0.2101\n",
      "     11                     \u001b[36m0.7168\u001b[0m        \u001b[32m0.5631\u001b[0m                     0.6437        0.6877  0.0063  0.2110\n",
      "     12                     \u001b[36m0.7336\u001b[0m        \u001b[32m0.5479\u001b[0m                     \u001b[35m0.7034\u001b[0m        \u001b[31m0.5896\u001b[0m  0.0057  0.2094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13                     \u001b[36m0.7444\u001b[0m        \u001b[32m0.5346\u001b[0m                     0.6903        \u001b[31m0.5670\u001b[0m  0.0050  0.2105\n",
      "     14                     \u001b[36m0.7537\u001b[0m        \u001b[32m0.5084\u001b[0m                     \u001b[35m0.7108\u001b[0m        \u001b[31m0.5562\u001b[0m  0.0043  0.2094\n",
      "     15                     \u001b[36m0.7776\u001b[0m        \u001b[32m0.4738\u001b[0m                     0.6922        0.5916  0.0037  0.2114\n",
      "     16                     0.7762        0.4870                     0.7052        0.5810  0.0031  0.2104\n",
      "     17                     0.7701        \u001b[32m0.4700\u001b[0m                     \u001b[35m0.7127\u001b[0m        0.5685  0.0025  0.2287\n",
      "     18                     0.7771        \u001b[32m0.4531\u001b[0m                     0.6791        0.5984  0.0020  0.2319\n",
      "     19                     \u001b[36m0.7921\u001b[0m        \u001b[32m0.4351\u001b[0m                     0.6959        0.5814  0.0015  0.2332\n",
      "     20                     \u001b[36m0.8009\u001b[0m        0.4353                     0.6847        0.5899  0.0010  0.2332\n",
      "     21                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4261\u001b[0m                     0.6866        0.6004  0.0007  0.2339\n",
      "     22                     \u001b[36m0.8061\u001b[0m        0.4298                     0.6884        0.5951  0.0004  0.2198\n",
      "     23                     \u001b[36m0.8196\u001b[0m        \u001b[32m0.4174\u001b[0m                     0.6884        0.5928  0.0002  0.2275\n",
      "     24                     0.8117        \u001b[32m0.4035\u001b[0m                     0.6884        0.5944  0.0000  0.2324\n",
      "     25                     0.8126        0.4074                     0.6922        0.5935  0.0000  0.2264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5416\u001b[0m        \u001b[32m1.5521\u001b[0m                     \u001b[35m0.5336\u001b[0m        \u001b[31m1.3430\u001b[0m  0.0100  0.8097\n",
      "      2                     \u001b[36m0.6238\u001b[0m        \u001b[32m0.7654\u001b[0m                     \u001b[35m0.5392\u001b[0m        \u001b[31m0.7664\u001b[0m  0.0100  0.2332\n",
      "      3                     \u001b[36m0.6257\u001b[0m        \u001b[32m0.7550\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6480\u001b[0m  0.0098  0.2267\n",
      "      4                     \u001b[36m0.6425\u001b[0m        0.7646                     0.5672        0.8261  0.0096  0.2281\n",
      "      5                     \u001b[36m0.6607\u001b[0m        \u001b[32m0.6607\u001b[0m                     0.6381        0.6530  0.0093  0.2245\n",
      "      6                     \u001b[36m0.6855\u001b[0m        \u001b[32m0.6023\u001b[0m                     0.6474        0.6547  0.0090  0.2284\n",
      "      7                     \u001b[36m0.6977\u001b[0m        \u001b[32m0.5969\u001b[0m                     \u001b[35m0.6772\u001b[0m        \u001b[31m0.6230\u001b[0m  0.0085  0.2204\n",
      "      8                     0.6879        0.6368                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5709\u001b[0m  0.0080  0.2209\n",
      "      9                     \u001b[36m0.7173\u001b[0m        \u001b[32m0.5630\u001b[0m                     0.6940        0.6271  0.0075  0.2362\n",
      "     10                     \u001b[36m0.7411\u001b[0m        \u001b[32m0.5282\u001b[0m                     0.6026        0.8993  0.0069  0.2136\n",
      "     11                     0.7393        0.5620                     0.6959        0.6198  0.0063  0.2136\n",
      "     12                     \u001b[36m0.7486\u001b[0m        \u001b[32m0.5115\u001b[0m                     0.7071        0.6523  0.0057  0.2164\n",
      "     13                     \u001b[36m0.7547\u001b[0m        \u001b[32m0.5038\u001b[0m                     \u001b[35m0.7276\u001b[0m        \u001b[31m0.5625\u001b[0m  0.0050  0.2324\n",
      "     14                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4709\u001b[0m                     0.7108        0.5938  0.0043  0.2344\n",
      "     15                     \u001b[36m0.7846\u001b[0m        \u001b[32m0.4688\u001b[0m                     \u001b[35m0.7425\u001b[0m        \u001b[31m0.5551\u001b[0m  0.0037  0.2319\n",
      "     16                     0.7827        \u001b[32m0.4557\u001b[0m                     \u001b[35m0.7463\u001b[0m        \u001b[31m0.5285\u001b[0m  0.0031  0.2201\n",
      "     17                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4371\u001b[0m                     \u001b[35m0.7612\u001b[0m        0.5337  0.0025  0.2176\n",
      "     18                     \u001b[36m0.7981\u001b[0m        0.4399                     0.7500        0.5466  0.0020  0.2254\n",
      "     19                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4309\u001b[0m                     0.7332        0.5473  0.0015  0.2164\n",
      "     20                     0.8033        \u001b[32m0.4254\u001b[0m                     0.7537        \u001b[31m0.5264\u001b[0m  0.0010  0.2254\n",
      "     21                     \u001b[36m0.8164\u001b[0m        \u001b[32m0.4148\u001b[0m                     0.7407        0.5371  0.0007  0.2210\n",
      "     22                     0.8136        \u001b[32m0.4126\u001b[0m                     0.7369        0.5452  0.0004  0.2151\n",
      "     23                     \u001b[36m0.8192\u001b[0m        \u001b[32m0.3944\u001b[0m                     0.7556        0.5283  0.0002  0.2214\n",
      "     24                     \u001b[36m0.8224\u001b[0m        \u001b[32m0.3944\u001b[0m                     0.7593        0.5270  0.0000  0.2204\n",
      "     25                     \u001b[36m0.8248\u001b[0m        0.4000                     0.7519        0.5313  0.0000  0.2159\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5280\u001b[0m        \u001b[32m1.4619\u001b[0m                     \u001b[35m0.5765\u001b[0m        \u001b[31m0.7099\u001b[0m  0.0100  0.2274\n",
      "      2                     \u001b[36m0.5808\u001b[0m        \u001b[32m0.7987\u001b[0m                     \u001b[35m0.5914\u001b[0m        0.7346  0.0100  0.2184\n",
      "      3                     \u001b[36m0.6126\u001b[0m        \u001b[32m0.7795\u001b[0m                     0.5802        0.8467  0.0098  0.2123\n",
      "      4                     \u001b[36m0.6327\u001b[0m        \u001b[32m0.7063\u001b[0m                     \u001b[35m0.6642\u001b[0m        \u001b[31m0.6141\u001b[0m  0.0096  0.2142\n",
      "      5                     0.6304        0.7149                     \u001b[35m0.6791\u001b[0m        \u001b[31m0.5954\u001b[0m  0.0093  0.2125\n",
      "      6                     \u001b[36m0.6715\u001b[0m        \u001b[32m0.6504\u001b[0m                     0.5634        0.7873  0.0090  0.2155\n",
      "      7                     0.6626        \u001b[32m0.6437\u001b[0m                     0.6772        \u001b[31m0.5940\u001b[0m  0.0085  0.2164\n",
      "      8                     \u001b[36m0.6902\u001b[0m        \u001b[32m0.5982\u001b[0m                     0.6437        0.6568  0.0080  0.2292\n",
      "      9                     0.6850        0.6427                     \u001b[35m0.6810\u001b[0m        0.6891  0.0075  0.2227\n",
      "     10                     \u001b[36m0.7042\u001b[0m        \u001b[32m0.5820\u001b[0m                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5419\u001b[0m  0.0069  0.2140\n",
      "     11                     \u001b[36m0.7224\u001b[0m        \u001b[32m0.5694\u001b[0m                     0.6903        0.5909  0.0063  0.2130\n",
      "     12                     \u001b[36m0.7229\u001b[0m        \u001b[32m0.5522\u001b[0m                     \u001b[35m0.7332\u001b[0m        0.5888  0.0057  0.2133\n",
      "     13                     \u001b[36m0.7500\u001b[0m        \u001b[32m0.5119\u001b[0m                     \u001b[35m0.7500\u001b[0m        0.5503  0.0050  0.2138\n",
      "     14                     0.7467        \u001b[32m0.5112\u001b[0m                     \u001b[35m0.7743\u001b[0m        \u001b[31m0.5137\u001b[0m  0.0043  0.2128\n",
      "     15                     \u001b[36m0.7603\u001b[0m        \u001b[32m0.4899\u001b[0m                     0.7500        0.5350  0.0037  0.2121\n",
      "     16                     \u001b[36m0.7743\u001b[0m        \u001b[32m0.4786\u001b[0m                     0.7537        0.5428  0.0031  0.2136\n",
      "     17                     0.7734        \u001b[32m0.4680\u001b[0m                     0.7743        0.5263  0.0025  0.2129\n",
      "     18                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.4556\u001b[0m                     0.7649        0.5373  0.0020  0.2139\n",
      "     19                     \u001b[36m0.7953\u001b[0m        \u001b[32m0.4396\u001b[0m                     0.7687        0.5345  0.0015  0.2134\n",
      "     20                     \u001b[36m0.7972\u001b[0m        \u001b[32m0.4356\u001b[0m                     0.7631        0.5418  0.0010  0.2136\n",
      "     21                     0.7902        0.4481                     0.7705        0.5316  0.0007  0.2143\n",
      "     22                     \u001b[36m0.8023\u001b[0m        \u001b[32m0.4269\u001b[0m                     \u001b[35m0.7761\u001b[0m        0.5355  0.0004  0.2127\n",
      "     23                     \u001b[36m0.8145\u001b[0m        \u001b[32m0.4167\u001b[0m                     0.7761        0.5320  0.0002  0.2125\n",
      "     24                     0.8056        \u001b[32m0.4139\u001b[0m                     0.7743        0.5310  0.0000  0.2136\n",
      "     25                     0.8075        0.4196                     0.7761        0.5321  0.0000  0.2144\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5444\u001b[0m        \u001b[32m1.5009\u001b[0m                     \u001b[35m0.5448\u001b[0m        \u001b[31m0.9190\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5626\u001b[0m        \u001b[32m0.9856\u001b[0m                     \u001b[35m0.5784\u001b[0m        0.9438  0.0100  0.2151\n",
      "      3                     \u001b[36m0.6042\u001b[0m        \u001b[32m0.7526\u001b[0m                     \u001b[35m0.6511\u001b[0m        \u001b[31m0.6518\u001b[0m  0.0098  0.2135\n",
      "      4                     \u001b[36m0.6500\u001b[0m        \u001b[32m0.6954\u001b[0m                     0.6101        0.7185  0.0096  0.2145\n",
      "      5                     0.6383        \u001b[32m0.6887\u001b[0m                     0.6269        0.6972  0.0093  0.2123\n",
      "      6                     \u001b[36m0.6561\u001b[0m        \u001b[32m0.6407\u001b[0m                     \u001b[35m0.6735\u001b[0m        \u001b[31m0.5912\u001b[0m  0.0090  0.2138\n",
      "      7                     \u001b[36m0.6678\u001b[0m        \u001b[32m0.6394\u001b[0m                     \u001b[35m0.6810\u001b[0m        0.6115  0.0085  0.2141\n",
      "      8                     0.6668        \u001b[32m0.6385\u001b[0m                     \u001b[35m0.7257\u001b[0m        \u001b[31m0.5484\u001b[0m  0.0080  0.2146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.7121\u001b[0m        \u001b[32m0.5650\u001b[0m                     0.6754        0.6039  0.0075  0.2144\n",
      "     10                     \u001b[36m0.7266\u001b[0m        \u001b[32m0.5623\u001b[0m                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5371\u001b[0m  0.0069  0.2135\n",
      "     11                     0.7201        \u001b[32m0.5598\u001b[0m                     0.7164        \u001b[31m0.5362\u001b[0m  0.0063  0.2139\n",
      "     12                     0.7056        0.5658                     \u001b[35m0.7388\u001b[0m        \u001b[31m0.5146\u001b[0m  0.0057  0.2151\n",
      "     13                     \u001b[36m0.7397\u001b[0m        \u001b[32m0.5240\u001b[0m                     0.7388        \u001b[31m0.5139\u001b[0m  0.0050  0.2129\n",
      "     14                     \u001b[36m0.7425\u001b[0m        \u001b[32m0.5094\u001b[0m                     \u001b[35m0.7463\u001b[0m        0.5231  0.0043  0.2141\n",
      "     15                     \u001b[36m0.7631\u001b[0m        \u001b[32m0.4907\u001b[0m                     0.7295        0.5242  0.0037  0.2144\n",
      "     16                     \u001b[36m0.7724\u001b[0m        \u001b[32m0.4885\u001b[0m                     0.7369        0.5289  0.0031  0.2135\n",
      "     17                     \u001b[36m0.7780\u001b[0m        \u001b[32m0.4738\u001b[0m                     0.7332        \u001b[31m0.5099\u001b[0m  0.0025  0.2149\n",
      "     18                     0.7748        \u001b[32m0.4693\u001b[0m                     0.7388        \u001b[31m0.4923\u001b[0m  0.0020  0.2135\n",
      "     19                     \u001b[36m0.7804\u001b[0m        \u001b[32m0.4538\u001b[0m                     0.7201        0.5313  0.0015  0.2117\n",
      "     20                     \u001b[36m0.7911\u001b[0m        \u001b[32m0.4454\u001b[0m                     \u001b[35m0.7556\u001b[0m        \u001b[31m0.4793\u001b[0m  0.0010  0.2147\n",
      "     21                     \u001b[36m0.7986\u001b[0m        \u001b[32m0.4323\u001b[0m                     0.7519        0.4832  0.0007  0.2151\n",
      "     22                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4120\u001b[0m                     0.7481        0.4963  0.0004  0.2139\n",
      "     23                     0.8028        0.4202                     0.7463        0.4878  0.0002  0.2125\n",
      "     24                     \u001b[36m0.8089\u001b[0m        0.4126                     0.7500        0.4868  0.0000  0.2134\n",
      "     25                     \u001b[36m0.8093\u001b[0m        0.4127                     0.7444        0.4794  0.0000  0.2136\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5626\u001b[0m        \u001b[32m1.3910\u001b[0m                     \u001b[35m0.5429\u001b[0m        \u001b[31m0.9490\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.5757\u001b[0m        \u001b[32m0.8054\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.6135\u001b[0m  0.0100  0.2162\n",
      "      3                     \u001b[36m0.6280\u001b[0m        \u001b[32m0.7096\u001b[0m                     0.5504        1.2053  0.0098  0.2128\n",
      "      4                     \u001b[36m0.6402\u001b[0m        \u001b[32m0.6926\u001b[0m                     0.6343        0.6557  0.0096  0.2133\n",
      "      5                     \u001b[36m0.6575\u001b[0m        \u001b[32m0.6588\u001b[0m                     0.6343        0.6957  0.0093  0.2134\n",
      "      6                     \u001b[36m0.6687\u001b[0m        0.6858                     0.6269        0.7887  0.0090  0.2135\n",
      "      7                     \u001b[36m0.6911\u001b[0m        0.6786                     0.6381        0.6412  0.0085  0.2147\n",
      "      8                     \u001b[36m0.7140\u001b[0m        \u001b[32m0.5867\u001b[0m                     0.6698        \u001b[31m0.6091\u001b[0m  0.0080  0.2135\n",
      "      9                     \u001b[36m0.7201\u001b[0m        \u001b[32m0.5719\u001b[0m                     \u001b[35m0.6884\u001b[0m        \u001b[31m0.5732\u001b[0m  0.0075  0.2138\n",
      "     10                     \u001b[36m0.7252\u001b[0m        0.5754                     \u001b[35m0.6903\u001b[0m        0.6212  0.0069  0.2141\n",
      "     11                     \u001b[36m0.7271\u001b[0m        \u001b[32m0.5520\u001b[0m                     \u001b[35m0.6959\u001b[0m        0.5751  0.0063  0.2133\n",
      "     12                     \u001b[36m0.7598\u001b[0m        \u001b[32m0.5209\u001b[0m                     \u001b[35m0.7015\u001b[0m        \u001b[31m0.5473\u001b[0m  0.0057  0.2140\n",
      "     13                     \u001b[36m0.7720\u001b[0m        \u001b[32m0.4894\u001b[0m                     \u001b[35m0.7090\u001b[0m        0.6199  0.0050  0.2134\n",
      "     14                     \u001b[36m0.7743\u001b[0m        0.4919                     0.6996        0.5780  0.0043  0.2144\n",
      "     15                     \u001b[36m0.7799\u001b[0m        \u001b[32m0.4739\u001b[0m                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.5334\u001b[0m  0.0037  0.2134\n",
      "     16                     \u001b[36m0.7893\u001b[0m        \u001b[32m0.4592\u001b[0m                     0.7220        0.5415  0.0031  0.2139\n",
      "     17                     \u001b[36m0.7907\u001b[0m        0.4677                     0.7239        \u001b[31m0.5294\u001b[0m  0.0025  0.2131\n",
      "     18                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4390\u001b[0m                     0.7220        0.5316  0.0020  0.2132\n",
      "     19                     \u001b[36m0.8126\u001b[0m        \u001b[32m0.4249\u001b[0m                     0.7201        \u001b[31m0.5197\u001b[0m  0.0015  0.2140\n",
      "     20                     0.8103        \u001b[32m0.4180\u001b[0m                     0.7220        0.5378  0.0010  0.2151\n",
      "     21                     0.8084        0.4214                     0.7220        0.5221  0.0007  0.2138\n",
      "     22                     0.8075        0.4257                     0.7220        0.5224  0.0004  0.2150\n",
      "     23                     \u001b[36m0.8168\u001b[0m        \u001b[32m0.4122\u001b[0m                     0.7220        0.5260  0.0002  0.2134\n",
      "     24                     0.8033        0.4161                     \u001b[35m0.7257\u001b[0m        0.5246  0.0000  0.2145\n",
      "     25                     \u001b[36m0.8234\u001b[0m        \u001b[32m0.3984\u001b[0m                     0.7239        0.5222  0.0000  0.2154\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5505\u001b[0m        \u001b[32m1.3144\u001b[0m                     \u001b[35m0.5634\u001b[0m        \u001b[31m1.0110\u001b[0m  0.0100  0.2064\n",
      "      2                     \u001b[36m0.5794\u001b[0m        \u001b[32m0.8387\u001b[0m                     \u001b[35m0.6418\u001b[0m        \u001b[31m0.7301\u001b[0m  0.0100  0.2155\n",
      "      3                     \u001b[36m0.6215\u001b[0m        \u001b[32m0.6859\u001b[0m                     \u001b[35m0.6623\u001b[0m        \u001b[31m0.6306\u001b[0m  0.0098  0.2154\n",
      "      4                     \u001b[36m0.6547\u001b[0m        \u001b[32m0.6643\u001b[0m                     \u001b[35m0.6791\u001b[0m        \u001b[31m0.5932\u001b[0m  0.0096  0.2144\n",
      "      5                     \u001b[36m0.6762\u001b[0m        \u001b[32m0.6226\u001b[0m                     \u001b[35m0.6996\u001b[0m        0.6104  0.0093  0.2160\n",
      "      6                     0.6748        0.6490                     \u001b[35m0.7276\u001b[0m        \u001b[31m0.5726\u001b[0m  0.0090  0.2147\n",
      "      7                     \u001b[36m0.6967\u001b[0m        \u001b[32m0.6088\u001b[0m                     0.7015        0.6230  0.0085  0.2150\n",
      "      8                     \u001b[36m0.7019\u001b[0m        0.6146                     0.7183        0.5905  0.0080  0.2138\n",
      "      9                     \u001b[36m0.7243\u001b[0m        \u001b[32m0.5574\u001b[0m                     0.7276        0.5822  0.0075  0.2152\n",
      "     10                     \u001b[36m0.7439\u001b[0m        \u001b[32m0.5245\u001b[0m                     0.6996        0.6019  0.0069  0.2143\n",
      "     11                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.5091\u001b[0m                     0.7201        \u001b[31m0.5657\u001b[0m  0.0063  0.2144\n",
      "     12                     0.7481        \u001b[32m0.5050\u001b[0m                     \u001b[35m0.7295\u001b[0m        0.5711  0.0057  0.2146\n",
      "     13                     0.7542        0.5094                     0.7276        0.5811  0.0050  0.2142\n",
      "     14                     0.7509        0.5201                     0.6996        0.5801  0.0043  0.2139\n",
      "     15                     \u001b[36m0.7757\u001b[0m        \u001b[32m0.4750\u001b[0m                     0.7201        \u001b[31m0.5456\u001b[0m  0.0037  0.2143\n",
      "     16                     \u001b[36m0.7846\u001b[0m        \u001b[32m0.4542\u001b[0m                     \u001b[35m0.7556\u001b[0m        \u001b[31m0.5272\u001b[0m  0.0031  0.2134\n",
      "     17                     \u001b[36m0.7949\u001b[0m        \u001b[32m0.4293\u001b[0m                     0.7444        \u001b[31m0.5266\u001b[0m  0.0025  0.2147\n",
      "     18                     0.7930        0.4328                     \u001b[35m0.7649\u001b[0m        \u001b[31m0.5230\u001b[0m  0.0020  0.2107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19                     \u001b[36m0.8224\u001b[0m        \u001b[32m0.4126\u001b[0m                     0.7649        0.5254  0.0015  0.2140\n",
      "     20                     0.8112        \u001b[32m0.4105\u001b[0m                     0.7575        0.5242  0.0010  0.2145\n",
      "     21                     0.8187        \u001b[32m0.3940\u001b[0m                     0.7519        0.5267  0.0007  0.2112\n",
      "     22                     \u001b[36m0.8234\u001b[0m        0.3960                     0.7593        0.5246  0.0004  0.2145\n",
      "     23                     0.8196        \u001b[32m0.3915\u001b[0m                     0.7575        0.5267  0.0002  0.2142\n",
      "     24                     0.8206        0.3968                     0.7575        0.5265  0.0000  0.2150\n",
      "     25                     0.8192        0.3925                     0.7593        0.5255  0.0000  0.2155\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5374\u001b[0m        \u001b[32m1.3573\u001b[0m                     \u001b[35m0.5821\u001b[0m        \u001b[31m0.8125\u001b[0m  0.0100  0.2064\n",
      "      2                     \u001b[36m0.5930\u001b[0m        \u001b[32m0.7699\u001b[0m                     \u001b[35m0.6007\u001b[0m        \u001b[31m0.7159\u001b[0m  0.0100  0.2152\n",
      "      3                     \u001b[36m0.6294\u001b[0m        \u001b[32m0.7052\u001b[0m                     \u001b[35m0.6418\u001b[0m        \u001b[31m0.6226\u001b[0m  0.0098  0.2144\n",
      "      4                     \u001b[36m0.6350\u001b[0m        \u001b[32m0.6958\u001b[0m                     \u001b[35m0.6791\u001b[0m        \u001b[31m0.6100\u001b[0m  0.0096  0.2127\n",
      "      5                     \u001b[36m0.6636\u001b[0m        \u001b[32m0.6622\u001b[0m                     0.6530        0.6973  0.0093  0.2134\n",
      "      6                     \u001b[36m0.6682\u001b[0m        0.6818                     \u001b[35m0.6940\u001b[0m        \u001b[31m0.5919\u001b[0m  0.0090  0.2138\n",
      "      7                     \u001b[36m0.6972\u001b[0m        \u001b[32m0.5983\u001b[0m                     0.6698        0.6007  0.0085  0.2141\n",
      "      8                     \u001b[36m0.7075\u001b[0m        0.6008                     \u001b[35m0.6959\u001b[0m        \u001b[31m0.5885\u001b[0m  0.0080  0.2129\n",
      "      9                     0.6897        0.6323                     \u001b[35m0.7164\u001b[0m        \u001b[31m0.5671\u001b[0m  0.0075  0.2146\n",
      "     10                     \u001b[36m0.7425\u001b[0m        \u001b[32m0.5425\u001b[0m                     \u001b[35m0.7481\u001b[0m        \u001b[31m0.5085\u001b[0m  0.0069  0.2141\n",
      "     11                     0.7393        \u001b[32m0.5217\u001b[0m                     \u001b[35m0.7705\u001b[0m        \u001b[31m0.5052\u001b[0m  0.0063  0.2135\n",
      "     12                     \u001b[36m0.7453\u001b[0m        \u001b[32m0.5205\u001b[0m                     0.7351        0.5357  0.0057  0.2134\n",
      "     13                     0.7430        0.5328                     0.7444        0.5057  0.0050  0.2135\n",
      "     14                     \u001b[36m0.7701\u001b[0m        \u001b[32m0.4721\u001b[0m                     0.7575        \u001b[31m0.5035\u001b[0m  0.0043  0.2135\n",
      "     15                     0.7636        0.4886                     0.7388        \u001b[31m0.5030\u001b[0m  0.0037  0.2131\n",
      "     16                     \u001b[36m0.7836\u001b[0m        \u001b[32m0.4530\u001b[0m                     0.7575        0.5302  0.0031  0.2135\n",
      "     17                     \u001b[36m0.7949\u001b[0m        \u001b[32m0.4339\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.4882\u001b[0m  0.0025  0.2136\n",
      "     18                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4270\u001b[0m                     0.7593        0.5054  0.0020  0.2129\n",
      "     19                     \u001b[36m0.8070\u001b[0m        \u001b[32m0.4250\u001b[0m                     \u001b[35m0.7780\u001b[0m        \u001b[31m0.4811\u001b[0m  0.0015  0.2144\n",
      "     20                     \u001b[36m0.8103\u001b[0m        \u001b[32m0.4098\u001b[0m                     0.7705        \u001b[31m0.4769\u001b[0m  0.0010  0.2131\n",
      "     21                     \u001b[36m0.8192\u001b[0m        \u001b[32m0.4013\u001b[0m                     0.7556        0.4960  0.0007  0.2132\n",
      "     22                     \u001b[36m0.8210\u001b[0m        \u001b[32m0.3969\u001b[0m                     0.7705        0.4785  0.0004  0.2138\n",
      "     23                     \u001b[36m0.8252\u001b[0m        \u001b[32m0.3955\u001b[0m                     0.7743        0.4775  0.0002  0.2145\n",
      "     24                     0.8117        0.4002                     0.7724        \u001b[31m0.4757\u001b[0m  0.0000  0.2144\n",
      "     25                     0.8210        \u001b[32m0.3857\u001b[0m                     0.7705        \u001b[31m0.4754\u001b[0m  0.0000  0.2147\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5533\u001b[0m        \u001b[32m1.3624\u001b[0m                     \u001b[35m0.6231\u001b[0m        \u001b[31m0.7358\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.5790\u001b[0m        \u001b[32m0.8041\u001b[0m                     \u001b[35m0.6455\u001b[0m        \u001b[31m0.6306\u001b[0m  0.0100  0.2145\n",
      "      3                     \u001b[36m0.6248\u001b[0m        \u001b[32m0.7076\u001b[0m                     \u001b[35m0.6567\u001b[0m        \u001b[31m0.6278\u001b[0m  0.0098  0.2136\n",
      "      4                     \u001b[36m0.6290\u001b[0m        \u001b[32m0.7009\u001b[0m                     0.6138        0.7432  0.0096  0.2135\n",
      "      5                     \u001b[36m0.6439\u001b[0m        0.7310                     \u001b[35m0.6660\u001b[0m        \u001b[31m0.6102\u001b[0m  0.0093  0.2145\n",
      "      6                     \u001b[36m0.6561\u001b[0m        \u001b[32m0.6738\u001b[0m                     \u001b[35m0.6698\u001b[0m        0.6865  0.0090  0.2141\n",
      "      7                     \u001b[36m0.6850\u001b[0m        \u001b[32m0.6263\u001b[0m                     0.6474        0.6409  0.0085  0.2137\n",
      "      8                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.5947\u001b[0m                     \u001b[35m0.7500\u001b[0m        \u001b[31m0.5210\u001b[0m  0.0080  0.2142\n",
      "      9                     \u001b[36m0.7164\u001b[0m        \u001b[32m0.5590\u001b[0m                     0.7295        0.5280  0.0075  0.2135\n",
      "     10                     \u001b[36m0.7285\u001b[0m        \u001b[32m0.5519\u001b[0m                     0.7127        0.5595  0.0069  0.2141\n",
      "     11                     \u001b[36m0.7360\u001b[0m        \u001b[32m0.5389\u001b[0m                     0.7332        \u001b[31m0.5148\u001b[0m  0.0063  0.2138\n",
      "     12                     \u001b[36m0.7603\u001b[0m        \u001b[32m0.5050\u001b[0m                     \u001b[35m0.7612\u001b[0m        0.5225  0.0057  0.2172\n",
      "     13                     0.7579        0.5143                     0.7313        0.5358  0.0050  0.2145\n",
      "     14                     \u001b[36m0.7626\u001b[0m        \u001b[32m0.4921\u001b[0m                     0.7519        0.5158  0.0043  0.2138\n",
      "     15                     \u001b[36m0.7724\u001b[0m        \u001b[32m0.4740\u001b[0m                     \u001b[35m0.7705\u001b[0m        \u001b[31m0.4965\u001b[0m  0.0037  0.2144\n",
      "     16                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4599\u001b[0m                     0.7649        0.5171  0.0031  0.2125\n",
      "     17                     0.7930        \u001b[32m0.4541\u001b[0m                     \u001b[35m0.7780\u001b[0m        0.5024  0.0025  0.2123\n",
      "     18                     \u001b[36m0.8033\u001b[0m        \u001b[32m0.4390\u001b[0m                     0.7407        0.5298  0.0020  0.2145\n",
      "     19                     \u001b[36m0.8103\u001b[0m        \u001b[32m0.4278\u001b[0m                     0.7724        \u001b[31m0.4813\u001b[0m  0.0015  0.2148\n",
      "     20                     0.8098        0.4331                     \u001b[35m0.7854\u001b[0m        0.4930  0.0010  0.2171\n",
      "     21                     0.8084        \u001b[32m0.4080\u001b[0m                     0.7668        0.4884  0.0007  0.2159\n",
      "     22                     \u001b[36m0.8313\u001b[0m        \u001b[32m0.3885\u001b[0m                     0.7817        0.4839  0.0004  0.2170\n",
      "     23                     0.8178        0.4083                     0.7854        0.4841  0.0002  0.2144\n",
      "     24                     0.8121        0.4000                     0.7799        0.4861  0.0000  0.2143\n",
      "     25                     0.8187        0.3969                     0.7780        0.4832  0.0000  0.2144\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5397\u001b[0m        \u001b[32m1.5945\u001b[0m                     \u001b[35m0.5261\u001b[0m        \u001b[31m1.2578\u001b[0m  0.0100  0.2094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                     \u001b[36m0.5953\u001b[0m        \u001b[32m0.7590\u001b[0m                     \u001b[35m0.6250\u001b[0m        \u001b[31m0.6530\u001b[0m  0.0100  0.2165\n",
      "      3                     \u001b[36m0.6084\u001b[0m        \u001b[32m0.7541\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6525\u001b[0m  0.0098  0.2147\n",
      "      4                     \u001b[36m0.6294\u001b[0m        \u001b[32m0.6789\u001b[0m                     0.6362        \u001b[31m0.6467\u001b[0m  0.0096  0.2141\n",
      "      5                     \u001b[36m0.6472\u001b[0m        \u001b[32m0.6574\u001b[0m                     0.6138        0.7460  0.0093  0.2149\n",
      "      6                     0.6402        0.7522                     \u001b[35m0.6884\u001b[0m        \u001b[31m0.5905\u001b[0m  0.0090  0.2158\n",
      "      7                     \u001b[36m0.6556\u001b[0m        0.6601                     0.6884        0.6044  0.0085  0.2135\n",
      "      8                     \u001b[36m0.7000\u001b[0m        \u001b[32m0.6138\u001b[0m                     \u001b[35m0.7015\u001b[0m        \u001b[31m0.5676\u001b[0m  0.0080  0.2135\n",
      "      9                     0.6995        \u001b[32m0.5854\u001b[0m                     0.6586        0.7047  0.0075  0.2135\n",
      "     10                     \u001b[36m0.7056\u001b[0m        0.6063                     \u001b[35m0.7108\u001b[0m        0.5688  0.0069  0.2145\n",
      "     11                     \u001b[36m0.7285\u001b[0m        \u001b[32m0.5436\u001b[0m                     0.6940        0.5916  0.0063  0.2131\n",
      "     12                     \u001b[36m0.7355\u001b[0m        \u001b[32m0.5388\u001b[0m                     \u001b[35m0.7463\u001b[0m        \u001b[31m0.5501\u001b[0m  0.0057  0.2137\n",
      "     13                     \u001b[36m0.7664\u001b[0m        \u001b[32m0.5027\u001b[0m                     \u001b[35m0.7780\u001b[0m        \u001b[31m0.5030\u001b[0m  0.0050  0.2140\n",
      "     14                     \u001b[36m0.7682\u001b[0m        \u001b[32m0.4922\u001b[0m                     0.7146        0.5687  0.0043  0.2123\n",
      "     15                     0.7682        \u001b[32m0.4813\u001b[0m                     0.7201        0.5910  0.0037  0.2135\n",
      "     16                     \u001b[36m0.7766\u001b[0m        0.4891                     0.7593        \u001b[31m0.4960\u001b[0m  0.0031  0.2117\n",
      "     17                     \u001b[36m0.7930\u001b[0m        \u001b[32m0.4551\u001b[0m                     0.7388        0.5196  0.0025  0.2129\n",
      "     18                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4447\u001b[0m                     0.7537        0.5086  0.0020  0.2147\n",
      "     19                     \u001b[36m0.8023\u001b[0m        \u001b[32m0.4346\u001b[0m                     0.7687        \u001b[31m0.4948\u001b[0m  0.0015  0.2130\n",
      "     20                     \u001b[36m0.8028\u001b[0m        \u001b[32m0.4272\u001b[0m                     0.7724        \u001b[31m0.4912\u001b[0m  0.0010  0.2134\n",
      "     21                     \u001b[36m0.8182\u001b[0m        \u001b[32m0.4132\u001b[0m                     \u001b[35m0.7836\u001b[0m        0.4917  0.0007  0.2145\n",
      "     22                     \u001b[36m0.8220\u001b[0m        \u001b[32m0.4060\u001b[0m                     0.7705        0.4940  0.0004  0.2144\n",
      "     23                     0.8131        \u001b[32m0.4040\u001b[0m                     0.7780        \u001b[31m0.4902\u001b[0m  0.0002  0.2134\n",
      "     24                     0.8210        \u001b[32m0.3940\u001b[0m                     0.7761        0.4905  0.0000  0.2144\n",
      "     25                     0.8164        0.4073                     0.7743        \u001b[31m0.4901\u001b[0m  0.0000  0.2134\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5047\u001b[0m        \u001b[32m1.6883\u001b[0m                     \u001b[35m0.5485\u001b[0m        \u001b[31m1.6172\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.5813\u001b[0m        \u001b[32m0.9523\u001b[0m                     \u001b[35m0.6325\u001b[0m        \u001b[31m0.6530\u001b[0m  0.0100  0.2164\n",
      "      3                     \u001b[36m0.6023\u001b[0m        \u001b[32m0.7012\u001b[0m                     0.6213        0.6821  0.0098  0.2144\n",
      "      4                     \u001b[36m0.6463\u001b[0m        \u001b[32m0.6871\u001b[0m                     \u001b[35m0.6381\u001b[0m        0.6608  0.0096  0.2134\n",
      "      5                     \u001b[36m0.6547\u001b[0m        \u001b[32m0.6474\u001b[0m                     0.5877        0.6840  0.0093  0.2145\n",
      "      6                     \u001b[36m0.6752\u001b[0m        \u001b[32m0.6443\u001b[0m                     \u001b[35m0.6903\u001b[0m        \u001b[31m0.6189\u001b[0m  0.0090  0.2140\n",
      "      7                     \u001b[36m0.6836\u001b[0m        \u001b[32m0.6156\u001b[0m                     0.6772        \u001b[31m0.6095\u001b[0m  0.0085  0.2131\n",
      "      8                     \u001b[36m0.7042\u001b[0m        \u001b[32m0.5913\u001b[0m                     0.6791        0.6246  0.0080  0.2137\n",
      "      9                     \u001b[36m0.7079\u001b[0m        \u001b[32m0.5781\u001b[0m                     \u001b[35m0.6940\u001b[0m        \u001b[31m0.6079\u001b[0m  0.0075  0.2137\n",
      "     10                     \u001b[36m0.7121\u001b[0m        \u001b[32m0.5624\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5576\u001b[0m  0.0069  0.2136\n",
      "     11                     \u001b[36m0.7322\u001b[0m        \u001b[32m0.5486\u001b[0m                     \u001b[35m0.7444\u001b[0m        \u001b[31m0.5198\u001b[0m  0.0063  0.2145\n",
      "     12                     \u001b[36m0.7505\u001b[0m        \u001b[32m0.5125\u001b[0m                     0.7090        0.5379  0.0057  0.2147\n",
      "     13                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.5049\u001b[0m                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.5141\u001b[0m  0.0050  0.2145\n",
      "     14                     \u001b[36m0.7621\u001b[0m        \u001b[32m0.5017\u001b[0m                     0.6679        0.6025  0.0043  0.2145\n",
      "     15                     \u001b[36m0.7710\u001b[0m        \u001b[32m0.4820\u001b[0m                     0.7519        0.5307  0.0037  0.2142\n",
      "     16                     \u001b[36m0.7893\u001b[0m        \u001b[32m0.4642\u001b[0m                     0.7593        \u001b[31m0.5085\u001b[0m  0.0031  0.2138\n",
      "     17                     0.7832        0.4653                     0.7556        0.5125  0.0025  0.2146\n",
      "     18                     \u001b[36m0.7935\u001b[0m        \u001b[32m0.4412\u001b[0m                     0.7575        \u001b[31m0.5013\u001b[0m  0.0020  0.2145\n",
      "     19                     \u001b[36m0.7986\u001b[0m        \u001b[32m0.4351\u001b[0m                     0.7575        0.5065  0.0015  0.2144\n",
      "     20                     \u001b[36m0.8056\u001b[0m        \u001b[32m0.4200\u001b[0m                     0.7500        0.5115  0.0010  0.2144\n",
      "     21                     \u001b[36m0.8154\u001b[0m        \u001b[32m0.4197\u001b[0m                     0.7575        \u001b[31m0.5011\u001b[0m  0.0007  0.2150\n",
      "     22                     \u001b[36m0.8196\u001b[0m        \u001b[32m0.4039\u001b[0m                     \u001b[35m0.7743\u001b[0m        \u001b[31m0.4954\u001b[0m  0.0004  0.2143\n",
      "     23                     \u001b[36m0.8243\u001b[0m        0.4056                     0.7705        0.5002  0.0002  0.2148\n",
      "     24                     0.8182        \u001b[32m0.3993\u001b[0m                     0.7593        \u001b[31m0.4919\u001b[0m  0.0000  0.2150\n",
      "     25                     0.8159        0.4028                     0.7687        0.4970  0.0000  0.2147\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5523\u001b[0m        \u001b[32m1.5971\u001b[0m                     \u001b[35m0.5429\u001b[0m        \u001b[31m1.3648\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5766\u001b[0m        \u001b[32m0.9466\u001b[0m                     \u001b[35m0.5746\u001b[0m        \u001b[31m0.8530\u001b[0m  0.0100  0.2134\n",
      "      3                     \u001b[36m0.6449\u001b[0m        \u001b[32m0.6965\u001b[0m                     \u001b[35m0.5896\u001b[0m        \u001b[31m0.6901\u001b[0m  0.0098  0.2135\n",
      "      4                     0.6252        0.7068                     0.5709        0.7489  0.0096  0.2124\n",
      "      5                     0.6229        0.7445                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6315\u001b[0m  0.0093  0.2133\n",
      "      6                     \u001b[36m0.6607\u001b[0m        \u001b[32m0.6509\u001b[0m                     \u001b[35m0.7164\u001b[0m        \u001b[31m0.5620\u001b[0m  0.0090  0.2137\n",
      "      7                     \u001b[36m0.6963\u001b[0m        \u001b[32m0.6154\u001b[0m                     0.6940        0.6387  0.0085  0.2130\n",
      "      8                     \u001b[36m0.7084\u001b[0m        \u001b[32m0.5781\u001b[0m                     0.6791        0.6261  0.0080  0.2127\n",
      "      9                     0.6846        0.6252                     0.6828        0.5916  0.0075  0.2145\n",
      "     10                     0.7037        \u001b[32m0.5742\u001b[0m                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.5516\u001b[0m  0.0069  0.2130\n",
      "     11                     \u001b[36m0.7313\u001b[0m        \u001b[32m0.5295\u001b[0m                     0.7183        \u001b[31m0.5356\u001b[0m  0.0063  0.2135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     12                     \u001b[36m0.7402\u001b[0m        \u001b[32m0.5122\u001b[0m                     \u001b[35m0.7425\u001b[0m        \u001b[31m0.5274\u001b[0m  0.0057  0.2137\n",
      "     13                     \u001b[36m0.7453\u001b[0m        \u001b[32m0.4997\u001b[0m                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.5115\u001b[0m  0.0050  0.2142\n",
      "     14                     \u001b[36m0.7706\u001b[0m        \u001b[32m0.4884\u001b[0m                     0.7369        0.5230  0.0043  0.2135\n",
      "     15                     0.7668        \u001b[32m0.4882\u001b[0m                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.5092\u001b[0m  0.0037  0.2124\n",
      "     16                     \u001b[36m0.7822\u001b[0m        \u001b[32m0.4702\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.4954\u001b[0m  0.0031  0.2134\n",
      "     17                     0.7818        \u001b[32m0.4660\u001b[0m                     0.7612        0.5039  0.0025  0.2139\n",
      "     18                     \u001b[36m0.7883\u001b[0m        \u001b[32m0.4463\u001b[0m                     0.7705        \u001b[31m0.4873\u001b[0m  0.0020  0.2145\n",
      "     19                     \u001b[36m0.7986\u001b[0m        \u001b[32m0.4460\u001b[0m                     0.7631        0.4908  0.0015  0.2102\n",
      "     20                     \u001b[36m0.8051\u001b[0m        \u001b[32m0.4190\u001b[0m                     0.7687        0.4954  0.0010  0.2135\n",
      "     21                     \u001b[36m0.8089\u001b[0m        0.4192                     0.7649        \u001b[31m0.4862\u001b[0m  0.0007  0.2134\n",
      "     22                     \u001b[36m0.8093\u001b[0m        \u001b[32m0.4179\u001b[0m                     0.7556        0.4973  0.0004  0.2135\n",
      "     23                     0.8056        0.4190                     0.7724        \u001b[31m0.4837\u001b[0m  0.0002  0.2135\n",
      "     24                     \u001b[36m0.8131\u001b[0m        \u001b[32m0.4108\u001b[0m                     0.7631        0.4843  0.0000  0.2143\n",
      "     25                     \u001b[36m0.8150\u001b[0m        \u001b[32m0.4017\u001b[0m                     0.7687        \u001b[31m0.4834\u001b[0m  0.0000  0.2156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6234\u001b[0m        \u001b[32m1.4809\u001b[0m                     \u001b[35m0.6884\u001b[0m        \u001b[31m0.8665\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.6621\u001b[0m        \u001b[32m0.7405\u001b[0m                     0.6772        0.9417  0.0100  0.2145\n",
      "      3                     \u001b[36m0.6729\u001b[0m        \u001b[32m0.7028\u001b[0m                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.6589\u001b[0m  0.0098  0.2144\n",
      "      4                     \u001b[36m0.7201\u001b[0m        \u001b[32m0.5929\u001b[0m                     \u001b[35m0.7164\u001b[0m        \u001b[31m0.5912\u001b[0m  0.0096  0.2135\n",
      "      5                     0.7159        0.5993                     \u001b[35m0.7593\u001b[0m        \u001b[31m0.4951\u001b[0m  0.0093  0.2126\n",
      "      6                     \u001b[36m0.7220\u001b[0m        \u001b[32m0.5681\u001b[0m                     0.7313        0.5449  0.0090  0.2140\n",
      "      7                     \u001b[36m0.7379\u001b[0m        \u001b[32m0.5274\u001b[0m                     0.7295        0.5750  0.0085  0.2130\n",
      "      8                     0.7252        0.5659                     0.7481        0.5312  0.0080  0.2147\n",
      "      9                     \u001b[36m0.7463\u001b[0m        \u001b[32m0.5201\u001b[0m                     0.7220        0.5642  0.0075  0.2138\n",
      "     10                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5104\u001b[0m                     0.6978        0.5721  0.0069  0.2134\n",
      "     11                     \u001b[36m0.7491\u001b[0m        \u001b[32m0.5050\u001b[0m                     0.7444        0.5553  0.0063  0.2129\n",
      "     12                     \u001b[36m0.7565\u001b[0m        \u001b[32m0.4959\u001b[0m                     0.7425        0.5018  0.0057  0.2095\n",
      "     13                     \u001b[36m0.7855\u001b[0m        \u001b[32m0.4697\u001b[0m                     0.7425        0.5302  0.0050  0.2127\n",
      "     14                     0.7678        0.4725                     \u001b[35m0.7724\u001b[0m        0.4975  0.0043  0.2135\n",
      "     15                     0.7771        0.4748                     0.7537        0.4963  0.0037  0.2135\n",
      "     16                     0.7752        \u001b[32m0.4648\u001b[0m                     0.7724        0.4992  0.0031  0.2135\n",
      "     17                     0.7818        \u001b[32m0.4597\u001b[0m                     0.7593        \u001b[31m0.4878\u001b[0m  0.0025  0.2219\n",
      "     18                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.4475\u001b[0m                     0.7444        \u001b[31m0.4838\u001b[0m  0.0020  0.2294\n",
      "     19                     \u001b[36m0.7949\u001b[0m        \u001b[32m0.4298\u001b[0m                     0.7575        0.4850  0.0015  0.2122\n",
      "     20                     \u001b[36m0.7972\u001b[0m        \u001b[32m0.4230\u001b[0m                     0.7556        0.4883  0.0010  0.2139\n",
      "     21                     \u001b[36m0.8005\u001b[0m        0.4246                     0.7612        \u001b[31m0.4801\u001b[0m  0.0007  0.2134\n",
      "     22                     0.7963        \u001b[32m0.4229\u001b[0m                     0.7575        \u001b[31m0.4793\u001b[0m  0.0004  0.2154\n",
      "     23                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4202\u001b[0m                     0.7612        \u001b[31m0.4782\u001b[0m  0.0002  0.2129\n",
      "     24                     0.8000        \u001b[32m0.4184\u001b[0m                     0.7593        \u001b[31m0.4780\u001b[0m  0.0000  0.2154\n",
      "     25                     \u001b[36m0.8047\u001b[0m        0.4218                     0.7631        0.4786  0.0000  0.2133\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6206\u001b[0m        \u001b[32m1.3324\u001b[0m                     \u001b[35m0.6698\u001b[0m        \u001b[31m0.8245\u001b[0m  0.0100  0.2154\n",
      "      2                     \u001b[36m0.6855\u001b[0m        \u001b[32m0.6587\u001b[0m                     0.6698        \u001b[31m0.6835\u001b[0m  0.0100  0.2173\n",
      "      3                     \u001b[36m0.7000\u001b[0m        \u001b[32m0.6585\u001b[0m                     \u001b[35m0.6847\u001b[0m        0.8457  0.0098  0.2133\n",
      "      4                     \u001b[36m0.7075\u001b[0m        \u001b[32m0.6337\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5872\u001b[0m  0.0096  0.2118\n",
      "      5                     \u001b[36m0.7201\u001b[0m        \u001b[32m0.5832\u001b[0m                     0.6586        0.8181  0.0093  0.2128\n",
      "      6                     \u001b[36m0.7336\u001b[0m        \u001b[32m0.5594\u001b[0m                     \u001b[35m0.7575\u001b[0m        \u001b[31m0.5287\u001b[0m  0.0090  0.2134\n",
      "      7                     \u001b[36m0.7463\u001b[0m        \u001b[32m0.5384\u001b[0m                     0.7146        0.5567  0.0085  0.2135\n",
      "      8                     0.7248        0.5807                     0.7295        0.6834  0.0080  0.2135\n",
      "      9                     \u001b[36m0.7547\u001b[0m        0.5390                     0.7537        0.5482  0.0075  0.2123\n",
      "     10                     \u001b[36m0.7636\u001b[0m        \u001b[32m0.4944\u001b[0m                     0.7444        0.6018  0.0069  0.2134\n",
      "     11                     0.7603        0.4972                     0.7146        0.6154  0.0063  0.2145\n",
      "     12                     \u001b[36m0.7776\u001b[0m        \u001b[32m0.4727\u001b[0m                     0.7425        0.5658  0.0057  0.2134\n",
      "     13                     \u001b[36m0.7822\u001b[0m        \u001b[32m0.4622\u001b[0m                     0.7257        0.5630  0.0050  0.2139\n",
      "     14                     0.7729        0.4635                     0.7295        0.5495  0.0043  0.2144\n",
      "     15                     \u001b[36m0.7874\u001b[0m        \u001b[32m0.4412\u001b[0m                     0.7276        0.5678  0.0037  0.2135\n",
      "     16                     \u001b[36m0.7981\u001b[0m        0.4422                     0.7351        0.5528  0.0031  0.2144\n",
      "     17                     \u001b[36m0.8093\u001b[0m        \u001b[32m0.4262\u001b[0m                     \u001b[35m0.7668\u001b[0m        0.5627  0.0025  0.2135\n",
      "     18                     \u001b[36m0.8159\u001b[0m        \u001b[32m0.4241\u001b[0m                     0.7369        0.5648  0.0020  0.2144\n",
      "     19                     0.8019        \u001b[32m0.4171\u001b[0m                     0.7537        0.5562  0.0015  0.2145\n",
      "     20                     0.8117        0.4201                     0.7519        0.5517  0.0010  0.2141\n",
      "     21                     \u001b[36m0.8164\u001b[0m        \u001b[32m0.4101\u001b[0m                     0.7481        0.5414  0.0007  0.2140\n",
      "     22                     \u001b[36m0.8206\u001b[0m        \u001b[32m0.4052\u001b[0m                     0.7500        0.5435  0.0004  0.2133\n",
      "     23                     0.8089        0.4130                     0.7537        0.5382  0.0002  0.2129\n",
      "     24                     0.8173        \u001b[32m0.4041\u001b[0m                     0.7537        0.5399  0.0000  0.2095\n",
      "     25                     \u001b[36m0.8220\u001b[0m        \u001b[32m0.3984\u001b[0m                     0.7537        0.5431  0.0000  0.2162\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6355\u001b[0m        \u001b[32m1.1860\u001b[0m                     \u001b[35m0.7015\u001b[0m        \u001b[31m0.6347\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6790\u001b[0m        \u001b[32m0.6805\u001b[0m                     0.6679        0.8548  0.0100  0.2145\n",
      "      3                     \u001b[36m0.7051\u001b[0m        \u001b[32m0.6079\u001b[0m                     0.6660        0.8174  0.0098  0.2134\n",
      "      4                     \u001b[36m0.7210\u001b[0m        \u001b[32m0.5998\u001b[0m                     0.6455        0.6536  0.0096  0.2143\n",
      "      5                     0.7131        0.6190                     \u001b[35m0.7052\u001b[0m        \u001b[31m0.5481\u001b[0m  0.0093  0.2135\n",
      "      6                     0.6977        0.6341                     \u001b[35m0.7127\u001b[0m        0.5986  0.0090  0.2144\n",
      "      7                     0.7154        0.6085                     0.7071        0.5746  0.0085  0.2131\n",
      "      8                     \u001b[36m0.7327\u001b[0m        \u001b[32m0.5686\u001b[0m                     0.7127        0.5712  0.0080  0.2154\n",
      "      9                     \u001b[36m0.7383\u001b[0m        \u001b[32m0.5308\u001b[0m                     0.6791        0.6124  0.0075  0.2136\n",
      "     10                     \u001b[36m0.7542\u001b[0m        \u001b[32m0.5123\u001b[0m                     \u001b[35m0.7146\u001b[0m        0.5557  0.0069  0.2146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     \u001b[36m0.7598\u001b[0m        \u001b[32m0.4855\u001b[0m                     \u001b[35m0.7220\u001b[0m        0.5516  0.0063  0.2138\n",
      "     12                     0.7593        0.4966                     0.7108        0.5631  0.0057  0.2107\n",
      "     13                     0.7584        0.4890                     0.7146        \u001b[31m0.5426\u001b[0m  0.0050  0.2137\n",
      "     14                     \u001b[36m0.7799\u001b[0m        \u001b[32m0.4623\u001b[0m                     0.7183        \u001b[31m0.5298\u001b[0m  0.0043  0.2139\n",
      "     15                     0.7603        0.4890                     \u001b[35m0.7369\u001b[0m        \u001b[31m0.5243\u001b[0m  0.0037  0.2145\n",
      "     16                     \u001b[36m0.7846\u001b[0m        \u001b[32m0.4621\u001b[0m                     0.7201        \u001b[31m0.5204\u001b[0m  0.0031  0.2134\n",
      "     17                     \u001b[36m0.7911\u001b[0m        \u001b[32m0.4426\u001b[0m                     0.7295        0.5451  0.0025  0.2141\n",
      "     18                     \u001b[36m0.7930\u001b[0m        \u001b[32m0.4314\u001b[0m                     0.7239        0.5380  0.0020  0.2128\n",
      "     19                     0.7855        0.4405                     0.7351        0.5327  0.0015  0.2141\n",
      "     20                     \u001b[36m0.7977\u001b[0m        \u001b[32m0.4263\u001b[0m                     0.7369        0.5267  0.0010  0.2142\n",
      "     21                     \u001b[36m0.7991\u001b[0m        0.4359                     0.7257        \u001b[31m0.5168\u001b[0m  0.0007  0.2152\n",
      "     22                     0.7935        0.4340                     0.7276        0.5220  0.0004  0.2144\n",
      "     23                     \u001b[36m0.8009\u001b[0m        \u001b[32m0.4242\u001b[0m                     0.7257        0.5228  0.0002  0.2124\n",
      "     24                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4223\u001b[0m                     0.7220        0.5190  0.0000  0.2116\n",
      "     25                     0.8005        0.4251                     0.7257        0.5202  0.0000  0.2140\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5766\u001b[0m        \u001b[32m1.4527\u001b[0m                     \u001b[35m0.6549\u001b[0m        \u001b[31m0.7289\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.6626\u001b[0m        \u001b[32m0.7262\u001b[0m                     0.6101        0.7371  0.0100  0.2145\n",
      "      3                     \u001b[36m0.6864\u001b[0m        \u001b[32m0.6388\u001b[0m                     \u001b[35m0.7407\u001b[0m        \u001b[31m0.5385\u001b[0m  0.0098  0.2136\n",
      "      4                     \u001b[36m0.7224\u001b[0m        \u001b[32m0.5928\u001b[0m                     0.7257        \u001b[31m0.5354\u001b[0m  0.0096  0.2092\n",
      "      5                     0.7126        \u001b[32m0.5699\u001b[0m                     0.7015        0.6420  0.0093  0.2135\n",
      "      6                     0.7192        0.5915                     0.6325        0.7895  0.0090  0.2131\n",
      "      7                     0.7196        0.6004                     0.6772        0.6748  0.0085  0.2132\n",
      "      8                     0.7173        \u001b[32m0.5631\u001b[0m                     0.7183        0.5653  0.0080  0.2127\n",
      "      9                     \u001b[36m0.7336\u001b[0m        \u001b[32m0.5274\u001b[0m                     \u001b[35m0.7761\u001b[0m        \u001b[31m0.4912\u001b[0m  0.0075  0.2090\n",
      "     10                     \u001b[36m0.7491\u001b[0m        \u001b[32m0.5220\u001b[0m                     0.7649        \u001b[31m0.4793\u001b[0m  0.0069  0.2125\n",
      "     11                     0.7453        \u001b[32m0.5178\u001b[0m                     0.7761        0.5012  0.0063  0.2125\n",
      "     12                     \u001b[36m0.7603\u001b[0m        \u001b[32m0.5138\u001b[0m                     0.7388        0.5360  0.0057  0.2126\n",
      "     13                     0.7579        \u001b[32m0.4985\u001b[0m                     0.7724        0.4933  0.0050  0.2136\n",
      "     14                     \u001b[36m0.7650\u001b[0m        0.5054                     0.7593        0.4987  0.0043  0.2141\n",
      "     15                     \u001b[36m0.7706\u001b[0m        \u001b[32m0.4775\u001b[0m                     0.7724        0.4839  0.0037  0.2135\n",
      "     16                     0.7687        \u001b[32m0.4708\u001b[0m                     \u001b[35m0.7799\u001b[0m        \u001b[31m0.4731\u001b[0m  0.0031  0.2135\n",
      "     17                     \u001b[36m0.7766\u001b[0m        \u001b[32m0.4624\u001b[0m                     0.7668        0.4925  0.0025  0.2134\n",
      "     18                     \u001b[36m0.7939\u001b[0m        \u001b[32m0.4455\u001b[0m                     0.7612        0.4775  0.0020  0.2133\n",
      "     19                     \u001b[36m0.7967\u001b[0m        \u001b[32m0.4348\u001b[0m                     0.7649        0.4780  0.0015  0.2127\n",
      "     20                     0.7944        0.4353                     0.7612        0.4744  0.0010  0.2135\n",
      "     21                     \u001b[36m0.7995\u001b[0m        \u001b[32m0.4305\u001b[0m                     0.7425        0.4955  0.0007  0.2145\n",
      "     22                     \u001b[36m0.8047\u001b[0m        \u001b[32m0.4282\u001b[0m                     0.7556        \u001b[31m0.4691\u001b[0m  0.0004  0.2135\n",
      "     23                     0.7991        \u001b[32m0.4207\u001b[0m                     0.7593        0.4709  0.0002  0.2134\n",
      "     24                     \u001b[36m0.8051\u001b[0m        \u001b[32m0.4132\u001b[0m                     0.7556        0.4705  0.0000  0.2134\n",
      "     25                     \u001b[36m0.8131\u001b[0m        0.4157                     0.7556        0.4706  0.0000  0.2135\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6257\u001b[0m        \u001b[32m1.1896\u001b[0m                     \u001b[35m0.6903\u001b[0m        \u001b[31m0.7989\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6659\u001b[0m        \u001b[32m0.7217\u001b[0m                     \u001b[35m0.6959\u001b[0m        \u001b[31m0.6136\u001b[0m  0.0100  0.2145\n",
      "      3                     \u001b[36m0.6972\u001b[0m        \u001b[32m0.6275\u001b[0m                     0.6287        0.7576  0.0098  0.2121\n",
      "      4                     \u001b[36m0.7332\u001b[0m        \u001b[32m0.5598\u001b[0m                     \u001b[35m0.7388\u001b[0m        \u001b[31m0.6135\u001b[0m  0.0096  0.2129\n",
      "      5                     0.7257        \u001b[32m0.5519\u001b[0m                     0.7127        0.6883  0.0093  0.2154\n",
      "      6                     0.7299        0.5604                     0.7164        0.6448  0.0090  0.2132\n",
      "      7                     \u001b[36m0.7341\u001b[0m        \u001b[32m0.5300\u001b[0m                     0.7313        \u001b[31m0.6090\u001b[0m  0.0085  0.2125\n",
      "      8                     0.7266        0.5650                     \u001b[35m0.7444\u001b[0m        0.7371  0.0080  0.2134\n",
      "      9                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5255\u001b[0m                     \u001b[35m0.7537\u001b[0m        \u001b[31m0.5429\u001b[0m  0.0075  0.2134\n",
      "     10                     0.7458        \u001b[32m0.5049\u001b[0m                     0.7537        0.5773  0.0069  0.2135\n",
      "     11                     0.7463        0.5073                     0.7425        0.5590  0.0063  0.2184\n",
      "     12                     \u001b[36m0.7780\u001b[0m        \u001b[32m0.4720\u001b[0m                     0.7425        0.5604  0.0057  0.2138\n",
      "     13                     0.7734        \u001b[32m0.4633\u001b[0m                     0.7463        0.5535  0.0050  0.2178\n",
      "     14                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4578\u001b[0m                     \u001b[35m0.7631\u001b[0m        0.5516  0.0043  0.2184\n",
      "     15                     0.7701        \u001b[32m0.4568\u001b[0m                     0.7388        0.6181  0.0037  0.2115\n",
      "     16                     \u001b[36m0.8005\u001b[0m        \u001b[32m0.4356\u001b[0m                     \u001b[35m0.7649\u001b[0m        0.5524  0.0031  0.2135\n",
      "     17                     0.7930        0.4357                     0.7556        0.5891  0.0025  0.2143\n",
      "     18                     0.7963        \u001b[32m0.4316\u001b[0m                     0.7481        \u001b[31m0.5369\u001b[0m  0.0020  0.2151\n",
      "     19                     \u001b[36m0.8014\u001b[0m        \u001b[32m0.4152\u001b[0m                     0.7575        0.5383  0.0015  0.2141\n",
      "     20                     \u001b[36m0.8019\u001b[0m        \u001b[32m0.4108\u001b[0m                     0.7519        \u001b[31m0.5303\u001b[0m  0.0010  0.2147\n",
      "     21                     \u001b[36m0.8187\u001b[0m        \u001b[32m0.3952\u001b[0m                     0.7556        0.5345  0.0007  0.2137\n",
      "     22                     0.8187        0.3995                     0.7612        0.5386  0.0004  0.2135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23                     0.8121        0.4026                     0.7612        0.5331  0.0002  0.2146\n",
      "     24                     0.8145        \u001b[32m0.3888\u001b[0m                     \u001b[35m0.7668\u001b[0m        0.5311  0.0000  0.2125\n",
      "     25                     0.8145        0.3889                     0.7631        0.5323  0.0000  0.2148\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6126\u001b[0m        \u001b[32m1.3836\u001b[0m                     \u001b[35m0.5802\u001b[0m        \u001b[31m1.3909\u001b[0m  0.0100  0.2064\n",
      "      2                     \u001b[36m0.6790\u001b[0m        \u001b[32m0.7236\u001b[0m                     \u001b[35m0.6213\u001b[0m        \u001b[31m0.6674\u001b[0m  0.0100  0.2174\n",
      "      3                     0.6776        \u001b[32m0.6560\u001b[0m                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.6048\u001b[0m  0.0098  0.2144\n",
      "      4                     \u001b[36m0.6846\u001b[0m        0.6815                     0.7127        \u001b[31m0.5665\u001b[0m  0.0096  0.2102\n",
      "      5                     \u001b[36m0.7168\u001b[0m        \u001b[32m0.5692\u001b[0m                     0.7220        0.5994  0.0093  0.2143\n",
      "      6                     \u001b[36m0.7304\u001b[0m        \u001b[32m0.5382\u001b[0m                     \u001b[35m0.7631\u001b[0m        \u001b[31m0.5364\u001b[0m  0.0090  0.2124\n",
      "      7                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5111\u001b[0m                     0.7463        \u001b[31m0.5352\u001b[0m  0.0085  0.2126\n",
      "      8                     0.7360        0.5223                     0.7369        \u001b[31m0.5229\u001b[0m  0.0080  0.2089\n",
      "      9                     0.7393        0.5342                     0.7631        0.5238  0.0075  0.2138\n",
      "     10                     0.7425        0.5439                     0.7090        0.5612  0.0069  0.2139\n",
      "     11                     0.7393        0.5423                     0.7463        \u001b[31m0.5122\u001b[0m  0.0063  0.2139\n",
      "     12                     \u001b[36m0.7659\u001b[0m        \u001b[32m0.4991\u001b[0m                     0.7369        0.5219  0.0057  0.2104\n",
      "     13                     \u001b[36m0.7710\u001b[0m        \u001b[32m0.4804\u001b[0m                     \u001b[35m0.7649\u001b[0m        \u001b[31m0.5018\u001b[0m  0.0050  0.2143\n",
      "     14                     0.7640        \u001b[32m0.4778\u001b[0m                     0.7463        0.5199  0.0043  0.2134\n",
      "     15                     \u001b[36m0.7771\u001b[0m        \u001b[32m0.4666\u001b[0m                     \u001b[35m0.7668\u001b[0m        0.5130  0.0037  0.2095\n",
      "     16                     0.7752        0.4770                     \u001b[35m0.7668\u001b[0m        0.5095  0.0031  0.2126\n",
      "     17                     \u001b[36m0.7827\u001b[0m        \u001b[32m0.4554\u001b[0m                     0.7463        0.5098  0.0025  0.2129\n",
      "     18                     0.7818        \u001b[32m0.4497\u001b[0m                     0.7556        0.5085  0.0020  0.2135\n",
      "     19                     \u001b[36m0.8023\u001b[0m        \u001b[32m0.4278\u001b[0m                     0.7575        0.5056  0.0015  0.2137\n",
      "     20                     0.8019        \u001b[32m0.4200\u001b[0m                     0.7631        \u001b[31m0.4998\u001b[0m  0.0010  0.2125\n",
      "     21                     0.7972        0.4371                     0.7556        0.5020  0.0007  0.2125\n",
      "     22                     0.8005        \u001b[32m0.4196\u001b[0m                     0.7593        \u001b[31m0.4997\u001b[0m  0.0004  0.2131\n",
      "     23                     \u001b[36m0.8047\u001b[0m        0.4207                     0.7631        \u001b[31m0.4986\u001b[0m  0.0002  0.2143\n",
      "     24                     0.8005        \u001b[32m0.4142\u001b[0m                     0.7575        0.4998  0.0000  0.2132\n",
      "     25                     0.7949        0.4199                     0.7612        0.4987  0.0000  0.2137\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5944\u001b[0m        \u001b[32m1.5214\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m1.0146\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.6636\u001b[0m        \u001b[32m0.7630\u001b[0m                     \u001b[35m0.6604\u001b[0m        \u001b[31m0.7100\u001b[0m  0.0100  0.2154\n",
      "      3                     \u001b[36m0.6827\u001b[0m        \u001b[32m0.6370\u001b[0m                     \u001b[35m0.7015\u001b[0m        \u001b[31m0.5786\u001b[0m  0.0098  0.2133\n",
      "      4                     \u001b[36m0.7234\u001b[0m        \u001b[32m0.5671\u001b[0m                     0.6660        0.6037  0.0096  0.2135\n",
      "      5                     \u001b[36m0.7308\u001b[0m        0.5740                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.5433\u001b[0m  0.0093  0.2154\n",
      "      6                     \u001b[36m0.7318\u001b[0m        \u001b[32m0.5512\u001b[0m                     0.7052        0.6496  0.0090  0.2128\n",
      "      7                     \u001b[36m0.7374\u001b[0m        0.5717                     0.7146        0.5843  0.0085  0.2126\n",
      "      8                     \u001b[36m0.7379\u001b[0m        \u001b[32m0.5434\u001b[0m                     0.7052        0.5578  0.0080  0.2127\n",
      "      9                     0.7280        0.5585                     0.7220        0.5537  0.0075  0.2138\n",
      "     10                     \u001b[36m0.7547\u001b[0m        \u001b[32m0.5150\u001b[0m                     0.6940        0.5883  0.0069  0.2134\n",
      "     11                     \u001b[36m0.7593\u001b[0m        \u001b[32m0.4893\u001b[0m                     \u001b[35m0.7425\u001b[0m        \u001b[31m0.5255\u001b[0m  0.0063  0.2133\n",
      "     12                     0.7486        \u001b[32m0.4866\u001b[0m                     0.7407        0.5276  0.0057  0.2107\n",
      "     13                     \u001b[36m0.7692\u001b[0m        \u001b[32m0.4811\u001b[0m                     0.7276        0.5563  0.0050  0.2136\n",
      "     14                     0.7617        0.4881                     0.7239        0.5334  0.0043  0.2139\n",
      "     15                     \u001b[36m0.7720\u001b[0m        \u001b[32m0.4650\u001b[0m                     0.7332        0.5407  0.0037  0.2129\n",
      "     16                     \u001b[36m0.7874\u001b[0m        \u001b[32m0.4465\u001b[0m                     \u001b[35m0.7444\u001b[0m        0.5383  0.0031  0.2135\n",
      "     17                     \u001b[36m0.7888\u001b[0m        0.4496                     0.7407        \u001b[31m0.5211\u001b[0m  0.0025  0.2144\n",
      "     18                     \u001b[36m0.7911\u001b[0m        0.4510                     \u001b[35m0.7463\u001b[0m        \u001b[31m0.5177\u001b[0m  0.0020  0.2134\n",
      "     19                     \u001b[36m0.8033\u001b[0m        \u001b[32m0.4305\u001b[0m                     0.7351        0.5237  0.0015  0.2139\n",
      "     20                     0.7939        \u001b[32m0.4277\u001b[0m                     0.7425        0.5262  0.0010  0.2136\n",
      "     21                     0.7935        \u001b[32m0.4269\u001b[0m                     0.7425        0.5197  0.0007  0.2118\n",
      "     22                     0.7981        \u001b[32m0.4182\u001b[0m                     0.7369        0.5178  0.0004  0.2137\n",
      "     23                     0.8014        \u001b[32m0.4111\u001b[0m                     \u001b[35m0.7500\u001b[0m        0.5204  0.0002  0.2145\n",
      "     24                     \u001b[36m0.8065\u001b[0m        0.4127                     0.7463        \u001b[31m0.5143\u001b[0m  0.0000  0.2147\n",
      "     25                     0.7977        \u001b[32m0.4099\u001b[0m                     0.7425        0.5173  0.0000  0.2150\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6061\u001b[0m        \u001b[32m1.5035\u001b[0m                     \u001b[35m0.6418\u001b[0m        \u001b[31m0.9904\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.6570\u001b[0m        \u001b[32m0.7698\u001b[0m                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.7196\u001b[0m  0.0100  0.2119\n",
      "      3                     \u001b[36m0.6762\u001b[0m        \u001b[32m0.7290\u001b[0m                     0.6959        \u001b[31m0.6014\u001b[0m  0.0098  0.2138\n",
      "      4                     \u001b[36m0.7210\u001b[0m        \u001b[32m0.5511\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5523\u001b[0m  0.0096  0.2144\n",
      "      5                     \u001b[36m0.7262\u001b[0m        0.5713                     0.7183        0.6519  0.0093  0.2110\n",
      "      6                     \u001b[36m0.7341\u001b[0m        \u001b[32m0.5299\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5395\u001b[0m  0.0090  0.2138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7                     \u001b[36m0.7407\u001b[0m        0.5458                     0.6903        0.6518  0.0085  0.2126\n",
      "      8                     0.7313        0.5679                     0.7127        0.6913  0.0080  0.2145\n",
      "      9                     0.7393        0.5313                     0.7295        0.5550  0.0075  0.2144\n",
      "     10                     0.7308        0.5713                     0.7108        0.6062  0.0069  0.2139\n",
      "     11                     0.7313        0.5454                     \u001b[35m0.7463\u001b[0m        \u001b[31m0.5255\u001b[0m  0.0063  0.2137\n",
      "     12                     \u001b[36m0.7636\u001b[0m        \u001b[32m0.4848\u001b[0m                     0.7332        0.5376  0.0057  0.2139\n",
      "     13                     \u001b[36m0.7720\u001b[0m        \u001b[32m0.4796\u001b[0m                     0.7425        \u001b[31m0.5182\u001b[0m  0.0050  0.2164\n",
      "     14                     0.7682        \u001b[32m0.4692\u001b[0m                     0.7351        \u001b[31m0.5135\u001b[0m  0.0043  0.2154\n",
      "     15                     \u001b[36m0.7748\u001b[0m        \u001b[32m0.4663\u001b[0m                     0.7276        \u001b[31m0.5091\u001b[0m  0.0037  0.2159\n",
      "     16                     \u001b[36m0.7860\u001b[0m        \u001b[32m0.4531\u001b[0m                     0.7388        0.5124  0.0031  0.2161\n",
      "     17                     0.7832        \u001b[32m0.4456\u001b[0m                     0.7407        0.5253  0.0025  0.2130\n",
      "     18                     \u001b[36m0.7921\u001b[0m        \u001b[32m0.4398\u001b[0m                     0.7425        0.5153  0.0020  0.2134\n",
      "     19                     0.7836        \u001b[32m0.4362\u001b[0m                     0.7407        0.5109  0.0015  0.2134\n",
      "     20                     \u001b[36m0.7925\u001b[0m        \u001b[32m0.4285\u001b[0m                     0.7444        0.5295  0.0010  0.2127\n",
      "     21                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4281\u001b[0m                     \u001b[35m0.7500\u001b[0m        0.5156  0.0007  0.2135\n",
      "     22                     0.7944        \u001b[32m0.4197\u001b[0m                     0.7500        \u001b[31m0.5089\u001b[0m  0.0004  0.2136\n",
      "     23                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4112\u001b[0m                     0.7407        0.5097  0.0002  0.2108\n",
      "     24                     \u001b[36m0.8023\u001b[0m        0.4193                     0.7407        0.5112  0.0000  0.2128\n",
      "     25                     0.8023        \u001b[32m0.4038\u001b[0m                     0.7500        0.5095  0.0000  0.2143\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6103\u001b[0m        \u001b[32m1.4463\u001b[0m                     \u001b[35m0.7108\u001b[0m        \u001b[31m0.6574\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6813\u001b[0m        \u001b[32m0.6599\u001b[0m                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5483\u001b[0m  0.0100  0.2157\n",
      "      3                     \u001b[36m0.6916\u001b[0m        \u001b[32m0.6316\u001b[0m                     0.6716        0.7076  0.0098  0.2127\n",
      "      4                     \u001b[36m0.6963\u001b[0m        \u001b[32m0.6265\u001b[0m                     0.6959        0.5855  0.0096  0.2144\n",
      "      5                     \u001b[36m0.7341\u001b[0m        \u001b[32m0.5444\u001b[0m                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5245\u001b[0m  0.0093  0.2142\n",
      "      6                     0.7280        0.5701                     0.7052        0.5950  0.0090  0.2135\n",
      "      7                     0.7318        0.5452                     0.7164        0.5284  0.0085  0.2137\n",
      "      8                     0.7304        0.5878                     0.6250        0.7866  0.0080  0.2119\n",
      "      9                     \u001b[36m0.7364\u001b[0m        0.5454                     \u001b[35m0.7407\u001b[0m        0.5444  0.0075  0.2121\n",
      "     10                     0.7285        0.5736                     0.7257        0.5450  0.0069  0.2132\n",
      "     11                     \u001b[36m0.7421\u001b[0m        0.5449                     0.6866        0.6985  0.0063  0.2137\n",
      "     12                     \u001b[36m0.7551\u001b[0m        \u001b[32m0.5241\u001b[0m                     0.7407        \u001b[31m0.5161\u001b[0m  0.0057  0.2129\n",
      "     13                     \u001b[36m0.7589\u001b[0m        \u001b[32m0.5005\u001b[0m                     \u001b[35m0.7463\u001b[0m        \u001b[31m0.5069\u001b[0m  0.0050  0.2141\n",
      "     14                     \u001b[36m0.7785\u001b[0m        \u001b[32m0.4807\u001b[0m                     0.7369        \u001b[31m0.5044\u001b[0m  0.0043  0.2133\n",
      "     15                     0.7776        \u001b[32m0.4633\u001b[0m                     \u001b[35m0.7500\u001b[0m        \u001b[31m0.5027\u001b[0m  0.0037  0.2127\n",
      "     16                     \u001b[36m0.7804\u001b[0m        \u001b[32m0.4625\u001b[0m                     \u001b[35m0.7593\u001b[0m        0.5074  0.0031  0.2135\n",
      "     17                     0.7771        \u001b[32m0.4562\u001b[0m                     \u001b[35m0.7687\u001b[0m        \u001b[31m0.5010\u001b[0m  0.0025  0.2135\n",
      "     18                     \u001b[36m0.7944\u001b[0m        \u001b[32m0.4403\u001b[0m                     0.7668        \u001b[31m0.4906\u001b[0m  0.0020  0.2144\n",
      "     19                     0.7902        \u001b[32m0.4367\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.4890\u001b[0m  0.0015  0.2144\n",
      "     20                     \u001b[36m0.8056\u001b[0m        \u001b[32m0.4213\u001b[0m                     0.7519        0.4980  0.0010  0.2139\n",
      "     21                     0.8042        0.4222                     0.7705        \u001b[31m0.4821\u001b[0m  0.0007  0.2123\n",
      "     22                     0.8019        \u001b[32m0.4174\u001b[0m                     0.7556        0.4944  0.0004  0.2132\n",
      "     23                     0.7949        0.4204                     0.7631        0.4882  0.0002  0.2135\n",
      "     24                     \u001b[36m0.8075\u001b[0m        \u001b[32m0.4151\u001b[0m                     0.7668        0.4871  0.0000  0.2132\n",
      "     25                     0.8028        0.4168                     0.7575        0.4912  0.0000  0.2142\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6159\u001b[0m        \u001b[32m1.4610\u001b[0m                     \u001b[35m0.7164\u001b[0m        \u001b[31m0.6147\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.7009\u001b[0m        \u001b[32m0.6070\u001b[0m                     0.7034        \u001b[31m0.5914\u001b[0m  0.0100  0.2279\n",
      "      3                     \u001b[36m0.7220\u001b[0m        \u001b[32m0.5777\u001b[0m                     0.7127        \u001b[31m0.5830\u001b[0m  0.0098  0.2129\n",
      "      4                     0.7051        0.6311                     \u001b[35m0.7183\u001b[0m        0.6712  0.0096  0.2137\n",
      "      5                     0.7140        0.6046                     0.6754        0.6545  0.0093  0.2144\n",
      "      6                     0.7159        0.6213                     0.6828        \u001b[31m0.5815\u001b[0m  0.0090  0.2135\n",
      "      7                     \u001b[36m0.7304\u001b[0m        \u001b[32m0.5286\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5256\u001b[0m  0.0085  0.2136\n",
      "      8                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5069\u001b[0m                     0.7295        0.5375  0.0080  0.2135\n",
      "      9                     0.7472        0.5189                     0.7201        0.5746  0.0075  0.2147\n",
      "     10                     \u001b[36m0.7481\u001b[0m        \u001b[32m0.5062\u001b[0m                     0.7071        0.5692  0.0069  0.2192\n",
      "     11                     \u001b[36m0.7607\u001b[0m        \u001b[32m0.4961\u001b[0m                     0.7276        0.5454  0.0063  0.2146\n",
      "     12                     \u001b[36m0.7617\u001b[0m        \u001b[32m0.4820\u001b[0m                     \u001b[35m0.7481\u001b[0m        0.5313  0.0057  0.2142\n",
      "     13                     \u001b[36m0.7668\u001b[0m        0.4820                     0.7481        0.5315  0.0050  0.2130\n",
      "     14                     \u001b[36m0.7678\u001b[0m        \u001b[32m0.4689\u001b[0m                     0.7239        0.5542  0.0043  0.2115\n",
      "     15                     \u001b[36m0.7752\u001b[0m        \u001b[32m0.4650\u001b[0m                     0.7332        0.5404  0.0037  0.2150\n",
      "     16                     0.7715        0.4697                     0.7239        0.5649  0.0031  0.2135\n",
      "     17                     \u001b[36m0.7780\u001b[0m        \u001b[32m0.4519\u001b[0m                     0.7313        0.5349  0.0025  0.2147\n",
      "     18                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4451\u001b[0m                     0.7407        \u001b[31m0.5231\u001b[0m  0.0020  0.2135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19                     \u001b[36m0.7939\u001b[0m        \u001b[32m0.4364\u001b[0m                     0.7201        0.5556  0.0015  0.2144\n",
      "     20                     \u001b[36m0.7967\u001b[0m        \u001b[32m0.4252\u001b[0m                     0.7463        0.5527  0.0010  0.2144\n",
      "     21                     \u001b[36m0.7972\u001b[0m        \u001b[32m0.4209\u001b[0m                     \u001b[35m0.7519\u001b[0m        0.5288  0.0007  0.2139\n",
      "     22                     \u001b[36m0.8051\u001b[0m        \u001b[32m0.4104\u001b[0m                     0.7407        0.5234  0.0004  0.2105\n",
      "     23                     \u001b[36m0.8112\u001b[0m        \u001b[32m0.4098\u001b[0m                     0.7444        \u001b[31m0.5230\u001b[0m  0.0002  0.2134\n",
      "     24                     0.8107        \u001b[32m0.4073\u001b[0m                     0.7444        0.5232  0.0000  0.2144\n",
      "     25                     0.8033        0.4150                     0.7444        \u001b[31m0.5228\u001b[0m  0.0000  0.2152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5930\u001b[0m        \u001b[32m1.3772\u001b[0m                     \u001b[35m0.6213\u001b[0m        \u001b[31m0.7770\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.6463\u001b[0m        \u001b[32m0.6942\u001b[0m                     0.6157        \u001b[31m0.7497\u001b[0m  0.0100  0.2174\n",
      "      3                     0.6360        0.7725                     0.5914        0.8087  0.0098  0.2134\n",
      "      4                     \u001b[36m0.6659\u001b[0m        \u001b[32m0.6709\u001b[0m                     0.5093        0.8163  0.0096  0.2135\n",
      "      5                     0.6617        0.7343                     \u001b[35m0.6810\u001b[0m        \u001b[31m0.6286\u001b[0m  0.0093  0.2137\n",
      "      6                     \u001b[36m0.6860\u001b[0m        \u001b[32m0.6025\u001b[0m                     0.6455        0.7409  0.0090  0.2139\n",
      "      7                     \u001b[36m0.7023\u001b[0m        0.6075                     0.6567        0.7171  0.0085  0.2155\n",
      "      8                     \u001b[36m0.7145\u001b[0m        \u001b[32m0.5753\u001b[0m                     \u001b[35m0.6884\u001b[0m        0.6380  0.0080  0.2124\n",
      "      9                     0.7145        0.5882                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.6081\u001b[0m  0.0075  0.2105\n",
      "     10                     \u001b[36m0.7304\u001b[0m        \u001b[32m0.5458\u001b[0m                     0.6437        0.6634  0.0069  0.2136\n",
      "     11                     \u001b[36m0.7430\u001b[0m        \u001b[32m0.5183\u001b[0m                     \u001b[35m0.6940\u001b[0m        \u001b[31m0.5920\u001b[0m  0.0063  0.2120\n",
      "     12                     \u001b[36m0.7650\u001b[0m        \u001b[32m0.5081\u001b[0m                     \u001b[35m0.7146\u001b[0m        0.5964  0.0057  0.2144\n",
      "     13                     0.7621        \u001b[32m0.5059\u001b[0m                     0.6903        \u001b[31m0.5790\u001b[0m  0.0050  0.2136\n",
      "     14                     \u001b[36m0.7701\u001b[0m        \u001b[32m0.4811\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5623\u001b[0m  0.0043  0.2128\n",
      "     15                     0.7495        0.5039                     0.6866        0.6103  0.0037  0.2111\n",
      "     16                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4697\u001b[0m                     0.7108        0.5670  0.0031  0.2136\n",
      "     17                     0.7678        0.4700                     0.7127        0.5772  0.0025  0.2132\n",
      "     18                     0.7813        \u001b[32m0.4538\u001b[0m                     \u001b[35m0.7239\u001b[0m        0.5734  0.0020  0.2145\n",
      "     19                     \u001b[36m0.7953\u001b[0m        0.4655                     0.6884        0.5845  0.0015  0.2135\n",
      "     20                     \u001b[36m0.8042\u001b[0m        \u001b[32m0.4377\u001b[0m                     \u001b[35m0.7388\u001b[0m        \u001b[31m0.5544\u001b[0m  0.0010  0.2137\n",
      "     21                     0.7995        \u001b[32m0.4292\u001b[0m                     0.7313        0.5643  0.0007  0.2125\n",
      "     22                     0.7977        0.4303                     0.7276        0.5610  0.0004  0.2134\n",
      "     23                     0.7995        0.4345                     0.7295        0.5566  0.0002  0.2121\n",
      "     24                     \u001b[36m0.8047\u001b[0m        \u001b[32m0.4280\u001b[0m                     0.7276        0.5561  0.0000  0.2128\n",
      "     25                     0.8009        \u001b[32m0.4270\u001b[0m                     0.7313        \u001b[31m0.5505\u001b[0m  0.0000  0.2095\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5804\u001b[0m        \u001b[32m1.4852\u001b[0m                     \u001b[35m0.5784\u001b[0m        \u001b[31m1.0811\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6164\u001b[0m        \u001b[32m0.8180\u001b[0m                     \u001b[35m0.5802\u001b[0m        \u001b[31m0.7650\u001b[0m  0.0100  0.2144\n",
      "      3                     \u001b[36m0.6486\u001b[0m        \u001b[32m0.7280\u001b[0m                     \u001b[35m0.6381\u001b[0m        \u001b[31m0.6601\u001b[0m  0.0098  0.2125\n",
      "      4                     \u001b[36m0.6687\u001b[0m        \u001b[32m0.6430\u001b[0m                     \u001b[35m0.6903\u001b[0m        \u001b[31m0.6313\u001b[0m  0.0096  0.2124\n",
      "      5                     \u001b[36m0.7107\u001b[0m        \u001b[32m0.5745\u001b[0m                     0.6119        0.7047  0.0093  0.2136\n",
      "      6                     0.7037        0.5927                     0.6399        0.7177  0.0090  0.2164\n",
      "      7                     0.7061        0.5781                     \u001b[35m0.7052\u001b[0m        \u001b[31m0.6287\u001b[0m  0.0085  0.2145\n",
      "      8                     0.7093        0.5964                     \u001b[35m0.7164\u001b[0m        0.7075  0.0080  0.2134\n",
      "      9                     0.6995        0.5972                     0.6884        0.6486  0.0075  0.2159\n",
      "     10                     \u001b[36m0.7178\u001b[0m        \u001b[32m0.5580\u001b[0m                     0.6978        0.6480  0.0069  0.2132\n",
      "     11                     \u001b[36m0.7425\u001b[0m        \u001b[32m0.5361\u001b[0m                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5918\u001b[0m  0.0063  0.2136\n",
      "     12                     \u001b[36m0.7500\u001b[0m        \u001b[32m0.5130\u001b[0m                     0.7090        0.6255  0.0057  0.2126\n",
      "     13                     0.7402        0.5245                     0.7034        0.6309  0.0050  0.2135\n",
      "     14                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5087\u001b[0m                     0.7146        0.6151  0.0043  0.2130\n",
      "     15                     \u001b[36m0.7692\u001b[0m        \u001b[32m0.4866\u001b[0m                     0.7276        \u001b[31m0.5804\u001b[0m  0.0037  0.2130\n",
      "     16                     \u001b[36m0.7724\u001b[0m        \u001b[32m0.4699\u001b[0m                     \u001b[35m0.7351\u001b[0m        0.6005  0.0031  0.2121\n",
      "     17                     \u001b[36m0.7771\u001b[0m        \u001b[32m0.4653\u001b[0m                     0.7257        0.6325  0.0025  0.2139\n",
      "     18                     \u001b[36m0.7850\u001b[0m        \u001b[32m0.4576\u001b[0m                     0.7313        0.6171  0.0020  0.2135\n",
      "     19                     0.7743        \u001b[32m0.4566\u001b[0m                     \u001b[35m0.7463\u001b[0m        0.6046  0.0015  0.2125\n",
      "     20                     0.7846        \u001b[32m0.4430\u001b[0m                     0.7276        0.5984  0.0010  0.2131\n",
      "     21                     \u001b[36m0.7850\u001b[0m        0.4564                     0.7332        0.6117  0.0007  0.2133\n",
      "     22                     \u001b[36m0.7925\u001b[0m        \u001b[32m0.4333\u001b[0m                     0.7351        0.6012  0.0004  0.2131\n",
      "     23                     \u001b[36m0.7981\u001b[0m        \u001b[32m0.4324\u001b[0m                     0.7388        0.6065  0.0002  0.2125\n",
      "     24                     0.7949        \u001b[32m0.4291\u001b[0m                     0.7351        0.6031  0.0000  0.2150\n",
      "     25                     0.7944        0.4343                     0.7369        0.6038  0.0000  0.2134\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5972\u001b[0m        \u001b[32m1.2378\u001b[0m                     \u001b[35m0.6437\u001b[0m        \u001b[31m0.6817\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6668\u001b[0m        \u001b[32m0.6558\u001b[0m                     \u001b[35m0.6810\u001b[0m        \u001b[31m0.6147\u001b[0m  0.0100  0.2161\n",
      "      3                     \u001b[36m0.6818\u001b[0m        \u001b[32m0.6342\u001b[0m                     0.6735        0.7194  0.0098  0.2143\n",
      "      4                     0.6692        0.6744                     0.6269        0.8115  0.0096  0.2134\n",
      "      5                     \u001b[36m0.6850\u001b[0m        0.6480                     0.6306        0.7077  0.0093  0.2143\n",
      "      6                     \u001b[36m0.6864\u001b[0m        \u001b[32m0.6229\u001b[0m                     0.6679        0.6826  0.0090  0.2130\n",
      "      7                     \u001b[36m0.7005\u001b[0m        \u001b[32m0.5997\u001b[0m                     0.6231        0.7756  0.0085  0.2140\n",
      "      8                     0.6925        0.6151                     \u001b[35m0.6940\u001b[0m        \u001b[31m0.5947\u001b[0m  0.0080  0.2134\n",
      "      9                     \u001b[36m0.7150\u001b[0m        \u001b[32m0.5722\u001b[0m                     \u001b[35m0.7071\u001b[0m        \u001b[31m0.5829\u001b[0m  0.0075  0.2139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     0.7093        0.5815                     \u001b[35m0.7239\u001b[0m        0.5849  0.0069  0.2135\n",
      "     11                     \u001b[36m0.7224\u001b[0m        \u001b[32m0.5335\u001b[0m                     0.6940        0.6528  0.0063  0.2138\n",
      "     12                     \u001b[36m0.7313\u001b[0m        0.5417                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5768\u001b[0m  0.0057  0.2139\n",
      "     13                     \u001b[36m0.7523\u001b[0m        \u001b[32m0.5106\u001b[0m                     \u001b[35m0.7575\u001b[0m        \u001b[31m0.5577\u001b[0m  0.0050  0.2129\n",
      "     14                     0.7444        0.5235                     0.7537        0.5699  0.0043  0.2134\n",
      "     15                     \u001b[36m0.7598\u001b[0m        \u001b[32m0.4993\u001b[0m                     0.7388        0.5581  0.0037  0.2134\n",
      "     16                     \u001b[36m0.7664\u001b[0m        \u001b[32m0.4775\u001b[0m                     0.7519        \u001b[31m0.5408\u001b[0m  0.0031  0.2130\n",
      "     17                     0.7612        0.4881                     0.7239        0.5655  0.0025  0.2092\n",
      "     18                     0.7589        0.4811                     0.7481        0.5420  0.0020  0.2133\n",
      "     19                     \u001b[36m0.7687\u001b[0m        \u001b[32m0.4676\u001b[0m                     0.7332        0.5722  0.0015  0.2125\n",
      "     20                     \u001b[36m0.7780\u001b[0m        \u001b[32m0.4579\u001b[0m                     0.7537        \u001b[31m0.5300\u001b[0m  0.0010  0.2137\n",
      "     21                     \u001b[36m0.7808\u001b[0m        \u001b[32m0.4522\u001b[0m                     \u001b[35m0.7612\u001b[0m        0.5373  0.0007  0.2135\n",
      "     22                     \u001b[36m0.7921\u001b[0m        \u001b[32m0.4452\u001b[0m                     0.7575        0.5358  0.0004  0.2105\n",
      "     23                     0.7874        \u001b[32m0.4419\u001b[0m                     \u001b[35m0.7649\u001b[0m        0.5346  0.0002  0.2134\n",
      "     24                     \u001b[36m0.7963\u001b[0m        0.4426                     0.7593        0.5363  0.0000  0.2131\n",
      "     25                     0.7897        \u001b[32m0.4393\u001b[0m                     0.7575        0.5374  0.0000  0.2138\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5804\u001b[0m        \u001b[32m1.4425\u001b[0m                     \u001b[35m0.5970\u001b[0m        \u001b[31m1.4775\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6079\u001b[0m        \u001b[32m0.8580\u001b[0m                     \u001b[35m0.6399\u001b[0m        \u001b[31m0.7316\u001b[0m  0.0100  0.2144\n",
      "      3                     \u001b[36m0.6327\u001b[0m        \u001b[32m0.7691\u001b[0m                     \u001b[35m0.6418\u001b[0m        0.7711  0.0098  0.2126\n",
      "      4                     \u001b[36m0.6696\u001b[0m        \u001b[32m0.6290\u001b[0m                     \u001b[35m0.7052\u001b[0m        \u001b[31m0.6026\u001b[0m  0.0096  0.2127\n",
      "      5                     \u001b[36m0.6710\u001b[0m        0.6360                     0.6959        0.6968  0.0093  0.2135\n",
      "      6                     \u001b[36m0.6841\u001b[0m        \u001b[32m0.6279\u001b[0m                     0.6940        0.6895  0.0090  0.2134\n",
      "      7                     0.6706        0.6639                     0.6903        \u001b[31m0.5870\u001b[0m  0.0085  0.2132\n",
      "      8                     \u001b[36m0.6995\u001b[0m        \u001b[32m0.5931\u001b[0m                     0.6922        \u001b[31m0.5827\u001b[0m  0.0080  0.2135\n",
      "      9                     \u001b[36m0.7051\u001b[0m        \u001b[32m0.5718\u001b[0m                     \u001b[35m0.7369\u001b[0m        \u001b[31m0.5465\u001b[0m  0.0075  0.2132\n",
      "     10                     \u001b[36m0.7266\u001b[0m        \u001b[32m0.5538\u001b[0m                     0.7313        0.5505  0.0069  0.2129\n",
      "     11                     0.7248        0.5595                     \u001b[35m0.7388\u001b[0m        0.5532  0.0063  0.2125\n",
      "     12                     \u001b[36m0.7416\u001b[0m        \u001b[32m0.5340\u001b[0m                     0.7332        \u001b[31m0.5290\u001b[0m  0.0057  0.2124\n",
      "     13                     0.7271        \u001b[32m0.5324\u001b[0m                     \u001b[35m0.7500\u001b[0m        0.5920  0.0050  0.2125\n",
      "     14                     \u001b[36m0.7453\u001b[0m        \u001b[32m0.5322\u001b[0m                     0.7481        0.5455  0.0043  0.2090\n",
      "     15                     \u001b[36m0.7612\u001b[0m        \u001b[32m0.5031\u001b[0m                     0.7388        0.5475  0.0037  0.2137\n",
      "     16                     \u001b[36m0.7626\u001b[0m        \u001b[32m0.4980\u001b[0m                     \u001b[35m0.7575\u001b[0m        0.5636  0.0031  0.2114\n",
      "     17                     0.7542        0.5026                     0.7444        0.5483  0.0025  0.2135\n",
      "     18                     \u001b[36m0.7752\u001b[0m        \u001b[32m0.4816\u001b[0m                     \u001b[35m0.7631\u001b[0m        0.5459  0.0020  0.2133\n",
      "     19                     0.7673        0.4825                     0.7593        0.5708  0.0015  0.2145\n",
      "     20                     0.7743        \u001b[32m0.4709\u001b[0m                     0.7444        0.5672  0.0010  0.2140\n",
      "     21                     \u001b[36m0.7776\u001b[0m        \u001b[32m0.4685\u001b[0m                     0.7575        0.5418  0.0007  0.2132\n",
      "     22                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.4608\u001b[0m                     0.7631        0.5373  0.0004  0.2138\n",
      "     23                     0.7752        0.4609                     \u001b[35m0.7649\u001b[0m        0.5358  0.0002  0.2124\n",
      "     24                     0.7827        0.4644                     0.7575        0.5375  0.0000  0.2145\n",
      "     25                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4559\u001b[0m                     0.7593        0.5401  0.0000  0.2138\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5944\u001b[0m        \u001b[32m1.3442\u001b[0m                     \u001b[35m0.5541\u001b[0m        \u001b[31m1.4276\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6621\u001b[0m        \u001b[32m0.7105\u001b[0m                     \u001b[35m0.6362\u001b[0m        \u001b[31m0.8712\u001b[0m  0.0100  0.2149\n",
      "      3                     \u001b[36m0.6766\u001b[0m        \u001b[32m0.6445\u001b[0m                     \u001b[35m0.6847\u001b[0m        \u001b[31m0.6732\u001b[0m  0.0098  0.2135\n",
      "      4                     0.6668        0.6579                     0.6623        0.8158  0.0096  0.2142\n",
      "      5                     0.6687        \u001b[32m0.6430\u001b[0m                     0.6175        \u001b[31m0.6568\u001b[0m  0.0093  0.2149\n",
      "      6                     0.6547        0.7413                     0.6698        0.7378  0.0090  0.2138\n",
      "      7                     \u001b[36m0.6836\u001b[0m        \u001b[32m0.6271\u001b[0m                     0.6474        0.7440  0.0085  0.2118\n",
      "      8                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.5914\u001b[0m                     \u001b[35m0.6866\u001b[0m        \u001b[31m0.6285\u001b[0m  0.0080  0.2151\n",
      "      9                     \u001b[36m0.7257\u001b[0m        \u001b[32m0.5498\u001b[0m                     \u001b[35m0.6996\u001b[0m        \u001b[31m0.5998\u001b[0m  0.0075  0.2154\n",
      "     10                     \u001b[36m0.7262\u001b[0m        \u001b[32m0.5433\u001b[0m                     \u001b[35m0.7034\u001b[0m        \u001b[31m0.5976\u001b[0m  0.0069  0.2144\n",
      "     11                     \u001b[36m0.7276\u001b[0m        \u001b[32m0.5319\u001b[0m                     \u001b[35m0.7146\u001b[0m        0.6563  0.0063  0.2139\n",
      "     12                     \u001b[36m0.7383\u001b[0m        \u001b[32m0.5220\u001b[0m                     \u001b[35m0.7183\u001b[0m        0.6478  0.0057  0.2145\n",
      "     13                     \u001b[36m0.7439\u001b[0m        \u001b[32m0.5167\u001b[0m                     0.6978        0.6151  0.0050  0.2144\n",
      "     14                     0.7388        0.5261                     0.7071        0.6651  0.0043  0.2136\n",
      "     15                     \u001b[36m0.7650\u001b[0m        \u001b[32m0.4979\u001b[0m                     \u001b[35m0.7332\u001b[0m        0.6360  0.0037  0.2155\n",
      "     16                     \u001b[36m0.7682\u001b[0m        0.4998                     0.7313        0.6154  0.0031  0.2145\n",
      "     17                     \u001b[36m0.7729\u001b[0m        \u001b[32m0.4768\u001b[0m                     0.7201        0.6205  0.0025  0.2140\n",
      "     18                     \u001b[36m0.7734\u001b[0m        \u001b[32m0.4711\u001b[0m                     0.7201        0.6207  0.0020  0.2142\n",
      "     19                     0.7701        0.4784                     0.7295        0.6539  0.0015  0.2150\n",
      "     20                     0.7734        0.4757                     0.7276        0.6085  0.0010  0.2138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21                     \u001b[36m0.7799\u001b[0m        \u001b[32m0.4629\u001b[0m                     \u001b[35m0.7407\u001b[0m        0.6078  0.0007  0.2145\n",
      "     22                     \u001b[36m0.7897\u001b[0m        \u001b[32m0.4479\u001b[0m                     0.7407        0.6090  0.0004  0.2155\n",
      "     23                     0.7850        0.4519                     0.7407        0.6091  0.0002  0.2145\n",
      "     24                     0.7860        0.4496                     0.7407        0.6112  0.0000  0.2145\n",
      "     25                     0.7846        0.4488                     0.7407        0.6098  0.0000  0.2131\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5439\u001b[0m        \u001b[32m1.7192\u001b[0m                     \u001b[35m0.5373\u001b[0m        \u001b[31m1.0933\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6187\u001b[0m        \u001b[32m0.7560\u001b[0m                     \u001b[35m0.6772\u001b[0m        \u001b[31m0.6148\u001b[0m  0.0100  0.2152\n",
      "      3                     \u001b[36m0.6607\u001b[0m        \u001b[32m0.6613\u001b[0m                     0.6642        0.7107  0.0098  0.2134\n",
      "      4                     0.6439        0.7882                     0.6455        0.8036  0.0096  0.2127\n",
      "      5                     \u001b[36m0.6953\u001b[0m        \u001b[32m0.6010\u001b[0m                     \u001b[35m0.7015\u001b[0m        \u001b[31m0.5623\u001b[0m  0.0093  0.2120\n",
      "      6                     \u001b[36m0.7061\u001b[0m        \u001b[32m0.5930\u001b[0m                     0.6884        0.6306  0.0090  0.2085\n",
      "      7                     \u001b[36m0.7117\u001b[0m        \u001b[32m0.5914\u001b[0m                     0.6679        0.7615  0.0085  0.2135\n",
      "      8                     0.6986        0.5968                     \u001b[35m0.7034\u001b[0m        0.5624  0.0080  0.2093\n",
      "      9                     \u001b[36m0.7252\u001b[0m        \u001b[32m0.5557\u001b[0m                     0.6716        0.6014  0.0075  0.2111\n",
      "     10                     \u001b[36m0.7341\u001b[0m        \u001b[32m0.5543\u001b[0m                     0.6940        0.5786  0.0069  0.2132\n",
      "     11                     0.7234        \u001b[32m0.5531\u001b[0m                     \u001b[35m0.7108\u001b[0m        0.5789  0.0063  0.2131\n",
      "     12                     \u001b[36m0.7495\u001b[0m        \u001b[32m0.5133\u001b[0m                     \u001b[35m0.7351\u001b[0m        \u001b[31m0.5342\u001b[0m  0.0057  0.2134\n",
      "     13                     0.7364        0.5357                     0.7164        0.5453  0.0050  0.2097\n",
      "     14                     \u001b[36m0.7519\u001b[0m        \u001b[32m0.5109\u001b[0m                     \u001b[35m0.7500\u001b[0m        0.5358  0.0043  0.2124\n",
      "     15                     \u001b[36m0.7682\u001b[0m        \u001b[32m0.4887\u001b[0m                     0.7201        0.5676  0.0037  0.2127\n",
      "     16                     0.7607        \u001b[32m0.4815\u001b[0m                     0.7425        0.5561  0.0031  0.2127\n",
      "     17                     0.7673        \u001b[32m0.4802\u001b[0m                     0.7351        \u001b[31m0.5170\u001b[0m  0.0025  0.2135\n",
      "     18                     \u001b[36m0.7706\u001b[0m        \u001b[32m0.4731\u001b[0m                     0.7481        \u001b[31m0.5162\u001b[0m  0.0020  0.2126\n",
      "     19                     \u001b[36m0.7893\u001b[0m        \u001b[32m0.4510\u001b[0m                     0.7407        0.5296  0.0015  0.2129\n",
      "     20                     0.7850        \u001b[32m0.4486\u001b[0m                     0.7500        0.5208  0.0010  0.2127\n",
      "     21                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4349\u001b[0m                     0.7481        0.5277  0.0007  0.2145\n",
      "     22                     0.7916        0.4530                     0.7425        \u001b[31m0.5092\u001b[0m  0.0004  0.2134\n",
      "     23                     0.7939        0.4435                     0.7500        0.5155  0.0002  0.2135\n",
      "     24                     \u001b[36m0.7963\u001b[0m        0.4402                     0.7388        0.5151  0.0000  0.2126\n",
      "     25                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4335\u001b[0m                     0.7481        0.5141  0.0000  0.2135\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5804\u001b[0m        \u001b[32m1.4676\u001b[0m                     \u001b[35m0.6175\u001b[0m        \u001b[31m0.8780\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6383\u001b[0m        \u001b[32m0.7586\u001b[0m                     0.5896        \u001b[31m0.7196\u001b[0m  0.0100  0.2125\n",
      "      3                     \u001b[36m0.6444\u001b[0m        \u001b[32m0.7496\u001b[0m                     \u001b[35m0.6455\u001b[0m        \u001b[31m0.6335\u001b[0m  0.0098  0.2138\n",
      "      4                     \u001b[36m0.6724\u001b[0m        \u001b[32m0.6514\u001b[0m                     \u001b[35m0.6791\u001b[0m        0.6495  0.0096  0.2134\n",
      "      5                     \u001b[36m0.6776\u001b[0m        \u001b[32m0.6323\u001b[0m                     0.6735        0.6764  0.0093  0.2143\n",
      "      6                     \u001b[36m0.6935\u001b[0m        \u001b[32m0.6190\u001b[0m                     0.6381        0.6461  0.0090  0.2144\n",
      "      7                     \u001b[36m0.7089\u001b[0m        \u001b[32m0.5918\u001b[0m                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.5911\u001b[0m  0.0085  0.2133\n",
      "      8                     \u001b[36m0.7121\u001b[0m        \u001b[32m0.5792\u001b[0m                     0.6399        0.6533  0.0080  0.2174\n",
      "      9                     \u001b[36m0.7210\u001b[0m        \u001b[32m0.5620\u001b[0m                     0.6679        0.6013  0.0075  0.2134\n",
      "     10                     \u001b[36m0.7341\u001b[0m        \u001b[32m0.5517\u001b[0m                     0.6903        0.6332  0.0069  0.2139\n",
      "     11                     \u001b[36m0.7430\u001b[0m        \u001b[32m0.5386\u001b[0m                     0.7015        \u001b[31m0.5655\u001b[0m  0.0063  0.2145\n",
      "     12                     0.7411        \u001b[32m0.5219\u001b[0m                     0.6269        0.6648  0.0057  0.2154\n",
      "     13                     \u001b[36m0.7519\u001b[0m        0.5299                     \u001b[35m0.7108\u001b[0m        0.5805  0.0050  0.2132\n",
      "     14                     \u001b[36m0.7584\u001b[0m        \u001b[32m0.4906\u001b[0m                     \u001b[35m0.7220\u001b[0m        0.5697  0.0043  0.2136\n",
      "     15                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4807\u001b[0m                     0.6959        0.6094  0.0037  0.2134\n",
      "     16                     \u001b[36m0.7724\u001b[0m        \u001b[32m0.4786\u001b[0m                     0.7034        0.5685  0.0031  0.2135\n",
      "     17                     \u001b[36m0.7813\u001b[0m        \u001b[32m0.4690\u001b[0m                     \u001b[35m0.7257\u001b[0m        0.5673  0.0025  0.2143\n",
      "     18                     \u001b[36m0.7902\u001b[0m        \u001b[32m0.4544\u001b[0m                     0.7183        \u001b[31m0.5559\u001b[0m  0.0020  0.2135\n",
      "     19                     0.7897        \u001b[32m0.4523\u001b[0m                     0.7239        0.5591  0.0015  0.2136\n",
      "     20                     0.7790        0.4562                     0.7201        0.5654  0.0010  0.2145\n",
      "     21                     \u001b[36m0.7916\u001b[0m        \u001b[32m0.4451\u001b[0m                     0.7239        0.5678  0.0007  0.2135\n",
      "     22                     0.7832        \u001b[32m0.4448\u001b[0m                     \u001b[35m0.7313\u001b[0m        0.5607  0.0004  0.2144\n",
      "     23                     \u001b[36m0.7986\u001b[0m        \u001b[32m0.4306\u001b[0m                     0.7276        0.5589  0.0002  0.2151\n",
      "     24                     0.7925        0.4371                     0.7313        0.5574  0.0000  0.2145\n",
      "     25                     \u001b[36m0.8042\u001b[0m        0.4408                     0.7257        0.5591  0.0000  0.2138\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5874\u001b[0m        \u001b[32m1.3930\u001b[0m                     \u001b[35m0.6493\u001b[0m        \u001b[31m1.2066\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6421\u001b[0m        \u001b[32m0.7905\u001b[0m                     0.4403        \u001b[31m1.0273\u001b[0m  0.0100  0.2171\n",
      "      3                     \u001b[36m0.6519\u001b[0m        \u001b[32m0.7423\u001b[0m                     \u001b[35m0.6642\u001b[0m        \u001b[31m0.6588\u001b[0m  0.0098  0.2143\n",
      "      4                     \u001b[36m0.6785\u001b[0m        \u001b[32m0.6332\u001b[0m                     \u001b[35m0.6847\u001b[0m        \u001b[31m0.6317\u001b[0m  0.0096  0.2138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5                     \u001b[36m0.7033\u001b[0m        \u001b[32m0.5994\u001b[0m                     0.6493        0.6752  0.0093  0.2134\n",
      "      6                     0.7023        \u001b[32m0.5754\u001b[0m                     \u001b[35m0.7146\u001b[0m        0.6903  0.0090  0.2128\n",
      "      7                     \u001b[36m0.7047\u001b[0m        0.6453                     0.6698        0.6376  0.0085  0.2133\n",
      "      8                     \u001b[36m0.7243\u001b[0m        \u001b[32m0.5554\u001b[0m                     0.6903        0.6599  0.0080  0.2133\n",
      "      9                     \u001b[36m0.7294\u001b[0m        0.5636                     0.6679        0.7781  0.0075  0.2134\n",
      "     10                     0.7224        0.5769                     0.7052        0.6357  0.0069  0.2145\n",
      "     11                     \u001b[36m0.7449\u001b[0m        \u001b[32m0.5286\u001b[0m                     0.6810        0.6474  0.0063  0.2135\n",
      "     12                     \u001b[36m0.7621\u001b[0m        \u001b[32m0.4992\u001b[0m                     0.6586        0.7291  0.0057  0.2132\n",
      "     13                     0.7500        0.5140                     0.6996        0.6838  0.0050  0.2132\n",
      "     14                     \u001b[36m0.7687\u001b[0m        \u001b[32m0.4855\u001b[0m                     0.6996        0.6551  0.0043  0.2118\n",
      "     15                     0.7621        0.4911                     0.7034        0.6711  0.0037  0.2131\n",
      "     16                     0.7631        0.4876                     \u001b[35m0.7220\u001b[0m        0.6433  0.0031  0.2139\n",
      "     17                     \u001b[36m0.7724\u001b[0m        0.4973                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.6187\u001b[0m  0.0025  0.2134\n",
      "     18                     \u001b[36m0.7762\u001b[0m        \u001b[32m0.4746\u001b[0m                     0.7034        0.6383  0.0020  0.2121\n",
      "     19                     \u001b[36m0.7869\u001b[0m        \u001b[32m0.4467\u001b[0m                     0.7164        0.6220  0.0015  0.2128\n",
      "     20                     0.7738        0.4584                     0.7090        0.6207  0.0010  0.2125\n",
      "     21                     \u001b[36m0.7907\u001b[0m        0.4499                     0.7183        \u001b[31m0.6086\u001b[0m  0.0007  0.2140\n",
      "     22                     \u001b[36m0.7977\u001b[0m        \u001b[32m0.4329\u001b[0m                     0.7090        0.6254  0.0004  0.2132\n",
      "     23                     0.7953        0.4436                     0.7108        0.6285  0.0002  0.2124\n",
      "     24                     0.7902        0.4397                     0.7146        0.6261  0.0000  0.2135\n",
      "     25                     \u001b[36m0.8037\u001b[0m        0.4353                     0.7108        0.6275  0.0000  0.2131\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5911\u001b[0m        \u001b[32m1.4102\u001b[0m                     \u001b[35m0.5672\u001b[0m        \u001b[31m0.8446\u001b[0m  0.0100  0.2064\n",
      "      2                     \u001b[36m0.6304\u001b[0m        \u001b[32m0.7616\u001b[0m                     \u001b[35m0.6325\u001b[0m        \u001b[31m0.6403\u001b[0m  0.0100  0.2127\n",
      "      3                     \u001b[36m0.6393\u001b[0m        \u001b[32m0.7462\u001b[0m                     0.6269        0.6739  0.0098  0.2125\n",
      "      4                     \u001b[36m0.6654\u001b[0m        \u001b[32m0.6724\u001b[0m                     \u001b[35m0.6679\u001b[0m        0.9221  0.0096  0.2125\n",
      "      5                     0.6565        0.7033                     0.6437        0.6668  0.0093  0.2135\n",
      "      6                     \u001b[36m0.6822\u001b[0m        \u001b[32m0.6405\u001b[0m                     0.6381        0.6537  0.0090  0.2131\n",
      "      7                     \u001b[36m0.6977\u001b[0m        \u001b[32m0.6123\u001b[0m                     0.6530        \u001b[31m0.6241\u001b[0m  0.0085  0.2135\n",
      "      8                     \u001b[36m0.7131\u001b[0m        \u001b[32m0.5665\u001b[0m                     \u001b[35m0.6847\u001b[0m        \u001b[31m0.6028\u001b[0m  0.0080  0.2133\n",
      "      9                     0.7121        0.5689                     \u001b[35m0.6940\u001b[0m        0.6042  0.0075  0.2149\n",
      "     10                     \u001b[36m0.7159\u001b[0m        \u001b[32m0.5519\u001b[0m                     \u001b[35m0.7034\u001b[0m        \u001b[31m0.5709\u001b[0m  0.0069  0.2125\n",
      "     11                     \u001b[36m0.7332\u001b[0m        \u001b[32m0.5501\u001b[0m                     0.6791        0.6051  0.0063  0.2145\n",
      "     12                     \u001b[36m0.7425\u001b[0m        \u001b[32m0.5367\u001b[0m                     \u001b[35m0.7052\u001b[0m        0.5722  0.0057  0.2144\n",
      "     13                     \u001b[36m0.7458\u001b[0m        \u001b[32m0.5171\u001b[0m                     0.6828        0.6012  0.0050  0.2144\n",
      "     14                     \u001b[36m0.7467\u001b[0m        \u001b[32m0.5142\u001b[0m                     \u001b[35m0.7146\u001b[0m        \u001b[31m0.5627\u001b[0m  0.0043  0.2139\n",
      "     15                     0.7439        0.5232                     0.6828        0.6248  0.0037  0.2144\n",
      "     16                     \u001b[36m0.7589\u001b[0m        \u001b[32m0.5039\u001b[0m                     0.6791        0.6038  0.0031  0.2134\n",
      "     17                     \u001b[36m0.7701\u001b[0m        \u001b[32m0.4968\u001b[0m                     0.6884        0.5886  0.0025  0.2134\n",
      "     18                     \u001b[36m0.7780\u001b[0m        \u001b[32m0.4769\u001b[0m                     0.7127        0.5735  0.0020  0.2126\n",
      "     19                     \u001b[36m0.7794\u001b[0m        0.4773                     0.7052        0.5745  0.0015  0.2131\n",
      "     20                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.4634\u001b[0m                     0.6922        0.5779  0.0010  0.2131\n",
      "     21                     \u001b[36m0.7897\u001b[0m        0.4640                     0.7052        0.5829  0.0007  0.2129\n",
      "     22                     \u001b[36m0.7935\u001b[0m        \u001b[32m0.4607\u001b[0m                     \u001b[35m0.7183\u001b[0m        0.5710  0.0004  0.2155\n",
      "     23                     0.7916        \u001b[32m0.4508\u001b[0m                     \u001b[35m0.7220\u001b[0m        0.5737  0.0002  0.2146\n",
      "     24                     0.7883        \u001b[32m0.4493\u001b[0m                     \u001b[35m0.7239\u001b[0m        0.5729  0.0000  0.2137\n",
      "     25                     0.7935        0.4602                     0.7220        0.5719  0.0000  0.2128\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5570\u001b[0m        \u001b[32m1.4974\u001b[0m                     \u001b[35m0.5784\u001b[0m        \u001b[31m1.1107\u001b[0m  0.0100  0.2064\n",
      "      2                     \u001b[36m0.6042\u001b[0m        \u001b[32m1.1318\u001b[0m                     \u001b[35m0.5970\u001b[0m        1.2785  0.0100  0.2161\n",
      "      3                     \u001b[36m0.6322\u001b[0m        \u001b[32m0.8179\u001b[0m                     \u001b[35m0.5989\u001b[0m        \u001b[31m0.7123\u001b[0m  0.0098  0.2137\n",
      "      4                     \u001b[36m0.6491\u001b[0m        \u001b[32m0.6525\u001b[0m                     \u001b[35m0.6866\u001b[0m        \u001b[31m0.6301\u001b[0m  0.0096  0.2127\n",
      "      5                     \u001b[36m0.6818\u001b[0m        \u001b[32m0.6095\u001b[0m                     0.6437        0.6397  0.0093  0.2147\n",
      "      6                     \u001b[36m0.6930\u001b[0m        \u001b[32m0.5987\u001b[0m                     0.6343        0.6538  0.0090  0.2145\n",
      "      7                     0.6864        0.6146                     \u001b[35m0.6884\u001b[0m        \u001b[31m0.5983\u001b[0m  0.0085  0.2147\n",
      "      8                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.5937\u001b[0m                     \u001b[35m0.6996\u001b[0m        \u001b[31m0.5803\u001b[0m  0.0080  0.2146\n",
      "      9                     \u001b[36m0.7037\u001b[0m        \u001b[32m0.5786\u001b[0m                     0.6996        0.6030  0.0075  0.2139\n",
      "     10                     \u001b[36m0.7164\u001b[0m        \u001b[32m0.5688\u001b[0m                     0.6940        0.5830  0.0069  0.2130\n",
      "     11                     \u001b[36m0.7262\u001b[0m        \u001b[32m0.5419\u001b[0m                     0.6903        0.6115  0.0063  0.2111\n",
      "     12                     \u001b[36m0.7360\u001b[0m        \u001b[32m0.5344\u001b[0m                     0.6996        \u001b[31m0.5736\u001b[0m  0.0057  0.2124\n",
      "     13                     0.7262        0.5390                     0.6959        0.5937  0.0050  0.2145\n",
      "     14                     \u001b[36m0.7458\u001b[0m        \u001b[32m0.5258\u001b[0m                     \u001b[35m0.7146\u001b[0m        0.5831  0.0043  0.2145\n",
      "     15                     0.7402        \u001b[32m0.5197\u001b[0m                     0.7052        0.5869  0.0037  0.2125\n",
      "     16                     0.7416        \u001b[32m0.5184\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5666\u001b[0m  0.0031  0.2144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17                     \u001b[36m0.7528\u001b[0m        \u001b[32m0.5069\u001b[0m                     0.6698        0.6182  0.0025  0.2125\n",
      "     18                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.4958\u001b[0m                     0.7108        \u001b[31m0.5602\u001b[0m  0.0020  0.2145\n",
      "     19                     \u001b[36m0.7589\u001b[0m        \u001b[32m0.4955\u001b[0m                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5545\u001b[0m  0.0015  0.2144\n",
      "     20                     \u001b[36m0.7645\u001b[0m        \u001b[32m0.4828\u001b[0m                     0.7201        0.5560  0.0010  0.2142\n",
      "     21                     \u001b[36m0.7738\u001b[0m        \u001b[32m0.4784\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5478\u001b[0m  0.0007  0.2107\n",
      "     22                     0.7650        0.4808                     0.7295        0.5533  0.0004  0.2139\n",
      "     23                     0.7687        0.4870                     0.7276        0.5545  0.0002  0.2128\n",
      "     24                     0.7710        \u001b[32m0.4751\u001b[0m                     0.7295        0.5551  0.0000  0.2142\n",
      "     25                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4705\u001b[0m                     0.7332        0.5528  0.0000  0.2142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5005\u001b[0m        \u001b[32m1.5925\u001b[0m                     \u001b[35m0.4757\u001b[0m        \u001b[31m1.1740\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5252\u001b[0m        \u001b[32m0.9061\u001b[0m                     \u001b[35m0.5037\u001b[0m        \u001b[31m0.9644\u001b[0m  0.0100  0.2185\n",
      "      3                     \u001b[36m0.5379\u001b[0m        \u001b[32m0.8152\u001b[0m                     \u001b[35m0.5131\u001b[0m        \u001b[31m0.8908\u001b[0m  0.0098  0.2127\n",
      "      4                     \u001b[36m0.5738\u001b[0m        \u001b[32m0.7909\u001b[0m                     \u001b[35m0.5896\u001b[0m        \u001b[31m0.6935\u001b[0m  0.0096  0.2130\n",
      "      5                     \u001b[36m0.5790\u001b[0m        0.8343                     0.5037        1.1364  0.0093  0.2161\n",
      "      6                     \u001b[36m0.6136\u001b[0m        \u001b[32m0.7850\u001b[0m                     0.5709        0.7290  0.0090  0.2126\n",
      "      7                     \u001b[36m0.6238\u001b[0m        \u001b[32m0.6938\u001b[0m                     \u001b[35m0.6082\u001b[0m        \u001b[31m0.6746\u001b[0m  0.0085  0.2113\n",
      "      8                     \u001b[36m0.6364\u001b[0m        0.7008                     0.5299        0.8735  0.0080  0.2130\n",
      "      9                     \u001b[36m0.6439\u001b[0m        \u001b[32m0.6753\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.6437\u001b[0m  0.0075  0.2104\n",
      "     10                     \u001b[36m0.6542\u001b[0m        \u001b[32m0.6751\u001b[0m                     0.6231        \u001b[31m0.6289\u001b[0m  0.0069  0.2135\n",
      "     11                     \u001b[36m0.6645\u001b[0m        \u001b[32m0.6366\u001b[0m                     0.6175        0.6540  0.0063  0.2118\n",
      "     12                     0.6556        0.6561                     0.6231        0.6308  0.0057  0.2120\n",
      "     13                     0.6621        \u001b[32m0.6284\u001b[0m                     0.6287        0.6341  0.0050  0.2126\n",
      "     14                     \u001b[36m0.6696\u001b[0m        \u001b[32m0.6129\u001b[0m                     0.6455        0.6358  0.0043  0.2086\n",
      "     15                     \u001b[36m0.6710\u001b[0m        0.6161                     0.6474        \u001b[31m0.6232\u001b[0m  0.0037  0.2098\n",
      "     16                     \u001b[36m0.6748\u001b[0m        \u001b[32m0.5930\u001b[0m                     0.6362        0.6464  0.0031  0.2124\n",
      "     17                     \u001b[36m0.6799\u001b[0m        \u001b[32m0.5903\u001b[0m                     0.6549        \u001b[31m0.6155\u001b[0m  0.0025  0.2128\n",
      "     18                     \u001b[36m0.6869\u001b[0m        \u001b[32m0.5861\u001b[0m                     0.6493        0.6273  0.0020  0.2121\n",
      "     19                     \u001b[36m0.6925\u001b[0m        \u001b[32m0.5823\u001b[0m                     0.6399        0.6165  0.0015  0.2124\n",
      "     20                     0.6790        0.5924                     0.6455        0.6211  0.0010  0.2132\n",
      "     21                     \u001b[36m0.6981\u001b[0m        \u001b[32m0.5794\u001b[0m                     0.6493        0.6185  0.0007  0.2136\n",
      "     22                     0.6949        \u001b[32m0.5790\u001b[0m                     0.6474        0.6205  0.0004  0.2124\n",
      "     23                     \u001b[36m0.7065\u001b[0m        \u001b[32m0.5688\u001b[0m                     0.6511        0.6197  0.0002  0.2124\n",
      "     24                     0.7047        0.5731                     0.6474        0.6198  0.0000  0.2134\n",
      "     25                     0.6972        0.5725                     0.6530        0.6188  0.0000  0.2131\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5290\u001b[0m        \u001b[32m1.6315\u001b[0m                     \u001b[35m0.5354\u001b[0m        \u001b[31m0.9588\u001b[0m  0.0100  0.2064\n",
      "      2                     \u001b[36m0.5495\u001b[0m        \u001b[32m1.0040\u001b[0m                     0.5056        \u001b[31m0.8499\u001b[0m  0.0100  0.2121\n",
      "      3                     \u001b[36m0.5949\u001b[0m        \u001b[32m0.8182\u001b[0m                     0.5243        \u001b[31m0.7938\u001b[0m  0.0098  0.2114\n",
      "      4                     \u001b[36m0.6121\u001b[0m        \u001b[32m0.7580\u001b[0m                     \u001b[35m0.5970\u001b[0m        \u001b[31m0.7128\u001b[0m  0.0096  0.2117\n",
      "      5                     \u001b[36m0.6173\u001b[0m        0.7593                     \u001b[35m0.6735\u001b[0m        \u001b[31m0.6294\u001b[0m  0.0093  0.2142\n",
      "      6                     \u001b[36m0.6206\u001b[0m        \u001b[32m0.7044\u001b[0m                     0.6287        0.6634  0.0090  0.2127\n",
      "      7                     \u001b[36m0.6355\u001b[0m        0.7365                     0.5951        0.7124  0.0085  0.2133\n",
      "      8                     0.6332        0.7280                     0.6306        0.6640  0.0080  0.2128\n",
      "      9                     \u001b[36m0.6593\u001b[0m        \u001b[32m0.6356\u001b[0m                     0.6549        0.6552  0.0075  0.2128\n",
      "     10                     \u001b[36m0.6654\u001b[0m        0.6593                     0.6045        0.6342  0.0069  0.2139\n",
      "     11                     \u001b[36m0.6710\u001b[0m        0.6407                     0.6269        0.6500  0.0063  0.2104\n",
      "     12                     0.6565        \u001b[32m0.6299\u001b[0m                     0.6250        0.6437  0.0057  0.2134\n",
      "     13                     0.6650        0.6420                     0.6157        0.6301  0.0050  0.2123\n",
      "     14                     \u001b[36m0.6776\u001b[0m        \u001b[32m0.6205\u001b[0m                     0.6343        0.6471  0.0043  0.2125\n",
      "     15                     \u001b[36m0.6869\u001b[0m        \u001b[32m0.6055\u001b[0m                     0.6418        0.6338  0.0037  0.2134\n",
      "     16                     0.6818        0.6158                     0.6474        0.6385  0.0031  0.2125\n",
      "     17                     \u001b[36m0.6921\u001b[0m        \u001b[32m0.5891\u001b[0m                     0.6381        0.6324  0.0025  0.2138\n",
      "     18                     \u001b[36m0.6958\u001b[0m        \u001b[32m0.5859\u001b[0m                     0.6343        \u001b[31m0.6275\u001b[0m  0.0020  0.2148\n",
      "     19                     0.6874        0.5914                     0.6474        0.6367  0.0015  0.2123\n",
      "     20                     0.6874        \u001b[32m0.5776\u001b[0m                     0.6399        0.6320  0.0010  0.2099\n",
      "     21                     0.6958        0.5808                     0.6381        0.6324  0.0007  0.2125\n",
      "     22                     \u001b[36m0.7042\u001b[0m        \u001b[32m0.5739\u001b[0m                     0.6287        0.6309  0.0004  0.2130\n",
      "     23                     0.6967        \u001b[32m0.5667\u001b[0m                     0.6306        0.6314  0.0002  0.2123\n",
      "     24                     0.6935        0.5711                     0.6343        0.6294  0.0000  0.2132\n",
      "     25                     \u001b[36m0.7075\u001b[0m        \u001b[32m0.5666\u001b[0m                     0.6362        0.6295  0.0000  0.2135\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5056\u001b[0m        \u001b[32m1.7091\u001b[0m                     \u001b[35m0.5504\u001b[0m        \u001b[31m1.1577\u001b[0m  0.0100  0.2124\n",
      "      2                     \u001b[36m0.5182\u001b[0m        \u001b[32m0.9950\u001b[0m                     0.5168        \u001b[31m0.7936\u001b[0m  0.0100  0.2324\n",
      "      3                     \u001b[36m0.5430\u001b[0m        \u001b[32m0.8511\u001b[0m                     0.5410        0.8462  0.0098  0.2134\n",
      "      4                     \u001b[36m0.5729\u001b[0m        \u001b[32m0.8049\u001b[0m                     \u001b[35m0.5951\u001b[0m        \u001b[31m0.6936\u001b[0m  0.0096  0.2126\n",
      "      5                     0.5673        \u001b[32m0.7533\u001b[0m                     \u001b[35m0.6157\u001b[0m        0.6944  0.0093  0.2114\n",
      "      6                     \u001b[36m0.6117\u001b[0m        \u001b[32m0.7299\u001b[0m                     0.5802        0.7592  0.0090  0.2154\n",
      "      7                     \u001b[36m0.6271\u001b[0m        \u001b[32m0.7045\u001b[0m                     0.6082        \u001b[31m0.6391\u001b[0m  0.0085  0.2136\n",
      "      8                     \u001b[36m0.6313\u001b[0m        \u001b[32m0.6618\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6166\u001b[0m  0.0080  0.2126\n",
      "      9                     \u001b[36m0.6458\u001b[0m        \u001b[32m0.6452\u001b[0m                     0.6194        0.7649  0.0075  0.2127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     \u001b[36m0.6472\u001b[0m        0.6559                     0.5989        0.8121  0.0069  0.2164\n",
      "     11                     \u001b[36m0.6533\u001b[0m        0.6621                     \u001b[35m0.6511\u001b[0m        0.6374  0.0063  0.2259\n",
      "     12                     \u001b[36m0.6579\u001b[0m        0.6490                     \u001b[35m0.6586\u001b[0m        0.6510  0.0057  0.2125\n",
      "     13                     0.6430        0.6537                     0.6306        0.6167  0.0050  0.2132\n",
      "     14                     \u001b[36m0.6640\u001b[0m        \u001b[32m0.6268\u001b[0m                     0.6287        0.7297  0.0043  0.2120\n",
      "     15                     0.6617        0.6282                     0.6586        \u001b[31m0.6126\u001b[0m  0.0037  0.2135\n",
      "     16                     \u001b[36m0.6902\u001b[0m        \u001b[32m0.5976\u001b[0m                     \u001b[35m0.6679\u001b[0m        0.6331  0.0031  0.2135\n",
      "     17                     0.6855        0.5997                     0.6567        0.6524  0.0025  0.2133\n",
      "     18                     0.6808        0.6073                     0.6511        \u001b[31m0.6073\u001b[0m  0.0020  0.2134\n",
      "     19                     \u001b[36m0.6986\u001b[0m        \u001b[32m0.5864\u001b[0m                     0.6679        \u001b[31m0.6063\u001b[0m  0.0015  0.2125\n",
      "     20                     0.6939        0.5873                     \u001b[35m0.6772\u001b[0m        0.6101  0.0010  0.2142\n",
      "     21                     0.6893        \u001b[32m0.5809\u001b[0m                     0.6679        0.6141  0.0007  0.2126\n",
      "     22                     0.6935        0.5839                     0.6772        0.6067  0.0004  0.2106\n",
      "     23                     \u001b[36m0.7000\u001b[0m        \u001b[32m0.5786\u001b[0m                     0.6698        0.6071  0.0002  0.2134\n",
      "     24                     0.6916        0.5851                     0.6754        0.6077  0.0000  0.2115\n",
      "     25                     0.7000        \u001b[32m0.5756\u001b[0m                     0.6698        0.6071  0.0000  0.2125\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5117\u001b[0m        \u001b[32m1.7784\u001b[0m                     \u001b[35m0.4963\u001b[0m        \u001b[31m1.2251\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5505\u001b[0m        \u001b[32m0.9594\u001b[0m                     \u001b[35m0.5541\u001b[0m        \u001b[31m0.7453\u001b[0m  0.0100  0.2164\n",
      "      3                     0.5449        \u001b[32m0.9150\u001b[0m                     0.5280        0.7491  0.0098  0.2125\n",
      "      4                     \u001b[36m0.5519\u001b[0m        1.0777                     \u001b[35m0.6213\u001b[0m        \u001b[31m0.7289\u001b[0m  0.0096  0.2128\n",
      "      5                     \u001b[36m0.5897\u001b[0m        \u001b[32m0.8328\u001b[0m                     \u001b[35m0.6269\u001b[0m        \u001b[31m0.6657\u001b[0m  0.0093  0.2125\n",
      "      6                     \u001b[36m0.6444\u001b[0m        \u001b[32m0.7049\u001b[0m                     0.6119        0.6774  0.0090  0.2120\n",
      "      7                     \u001b[36m0.6449\u001b[0m        \u001b[32m0.6972\u001b[0m                     0.5877        0.7642  0.0085  0.2124\n",
      "      8                     \u001b[36m0.6505\u001b[0m        \u001b[32m0.6771\u001b[0m                     0.5784        0.6936  0.0080  0.2123\n",
      "      9                     0.6449        \u001b[32m0.6763\u001b[0m                     \u001b[35m0.6399\u001b[0m        \u001b[31m0.6357\u001b[0m  0.0075  0.2123\n",
      "     10                     \u001b[36m0.6593\u001b[0m        \u001b[32m0.6498\u001b[0m                     \u001b[35m0.6455\u001b[0m        0.6399  0.0069  0.2120\n",
      "     11                     \u001b[36m0.6645\u001b[0m        \u001b[32m0.6317\u001b[0m                     0.6213        0.6513  0.0063  0.2126\n",
      "     12                     \u001b[36m0.6738\u001b[0m        \u001b[32m0.6198\u001b[0m                     0.6101        0.6439  0.0057  0.2131\n",
      "     13                     \u001b[36m0.6902\u001b[0m        \u001b[32m0.6034\u001b[0m                     0.6343        \u001b[31m0.6287\u001b[0m  0.0050  0.2126\n",
      "     14                     \u001b[36m0.6911\u001b[0m        0.6051                     0.6455        0.6336  0.0043  0.2154\n",
      "     15                     0.6790        \u001b[32m0.6013\u001b[0m                     0.6437        0.6303  0.0037  0.2127\n",
      "     16                     0.6780        \u001b[32m0.6012\u001b[0m                     \u001b[35m0.6511\u001b[0m        0.6353  0.0031  0.2122\n",
      "     17                     \u001b[36m0.6939\u001b[0m        \u001b[32m0.5905\u001b[0m                     \u001b[35m0.6604\u001b[0m        \u001b[31m0.6231\u001b[0m  0.0025  0.2140\n",
      "     18                     \u001b[36m0.6972\u001b[0m        \u001b[32m0.5792\u001b[0m                     0.6474        0.6275  0.0020  0.2133\n",
      "     19                     \u001b[36m0.7000\u001b[0m        0.5906                     \u001b[35m0.6660\u001b[0m        0.6244  0.0015  0.2134\n",
      "     20                     \u001b[36m0.7126\u001b[0m        0.5812                     0.6549        0.6271  0.0010  0.2144\n",
      "     21                     0.7047        \u001b[32m0.5736\u001b[0m                     0.6455        0.6262  0.0007  0.2129\n",
      "     22                     0.6972        0.5758                     0.6623        0.6252  0.0004  0.2137\n",
      "     23                     0.7047        0.5762                     0.6567        0.6249  0.0002  0.2145\n",
      "     24                     0.7084        0.5738                     \u001b[35m0.6679\u001b[0m        0.6266  0.0000  0.2134\n",
      "     25                     0.7028        \u001b[32m0.5727\u001b[0m                     0.6604        0.6264  0.0000  0.2145\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5150\u001b[0m        \u001b[32m1.4639\u001b[0m                     \u001b[35m0.5187\u001b[0m        \u001b[31m0.7700\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5257\u001b[0m        \u001b[32m0.7722\u001b[0m                     \u001b[35m0.5392\u001b[0m        \u001b[31m0.7274\u001b[0m  0.0100  0.2160\n",
      "      3                     \u001b[36m0.5463\u001b[0m        0.8462                     0.5354        0.9250  0.0098  0.2135\n",
      "      4                     \u001b[36m0.5799\u001b[0m        0.8042                     \u001b[35m0.5653\u001b[0m        \u001b[31m0.6935\u001b[0m  0.0096  0.2135\n",
      "      5                     \u001b[36m0.5804\u001b[0m        0.8048                     0.5634        0.7338  0.0093  0.2140\n",
      "      6                     \u001b[36m0.6070\u001b[0m        \u001b[32m0.7233\u001b[0m                     \u001b[35m0.6306\u001b[0m        \u001b[31m0.6413\u001b[0m  0.0090  0.2134\n",
      "      7                     \u001b[36m0.6407\u001b[0m        \u001b[32m0.6911\u001b[0m                     0.6194        0.6616  0.0085  0.2155\n",
      "      8                     \u001b[36m0.6467\u001b[0m        \u001b[32m0.6564\u001b[0m                     0.5989        0.6975  0.0080  0.2139\n",
      "      9                     \u001b[36m0.6561\u001b[0m        \u001b[32m0.6361\u001b[0m                     \u001b[35m0.6567\u001b[0m        \u001b[31m0.6354\u001b[0m  0.0075  0.2138\n",
      "     10                     \u001b[36m0.6598\u001b[0m        0.6375                     0.6082        0.7021  0.0069  0.2135\n",
      "     11                     \u001b[36m0.6659\u001b[0m        \u001b[32m0.6260\u001b[0m                     0.6418        0.6428  0.0063  0.2143\n",
      "     12                     0.6523        0.6329                     0.6325        0.6538  0.0057  0.2128\n",
      "     13                     \u001b[36m0.6808\u001b[0m        \u001b[32m0.6170\u001b[0m                     0.6287        0.6487  0.0050  0.2139\n",
      "     14                     0.6734        \u001b[32m0.6117\u001b[0m                     0.6231        0.6723  0.0043  0.2126\n",
      "     15                     \u001b[36m0.6841\u001b[0m        \u001b[32m0.6052\u001b[0m                     0.6418        \u001b[31m0.6342\u001b[0m  0.0037  0.2135\n",
      "     16                     \u001b[36m0.6883\u001b[0m        \u001b[32m0.5870\u001b[0m                     0.6511        \u001b[31m0.6293\u001b[0m  0.0031  0.2134\n",
      "     17                     \u001b[36m0.6888\u001b[0m        0.5967                     0.6399        \u001b[31m0.6291\u001b[0m  0.0025  0.2137\n",
      "     18                     \u001b[36m0.7019\u001b[0m        \u001b[32m0.5710\u001b[0m                     0.6549        \u001b[31m0.6286\u001b[0m  0.0020  0.2136\n",
      "     19                     0.7009        0.5796                     0.6530        \u001b[31m0.6286\u001b[0m  0.0015  0.2141\n",
      "     20                     0.7000        0.5767                     \u001b[35m0.6623\u001b[0m        \u001b[31m0.6269\u001b[0m  0.0010  0.2144\n",
      "     21                     \u001b[36m0.7070\u001b[0m        \u001b[32m0.5606\u001b[0m                     0.6530        0.6282  0.0007  0.2141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22                     \u001b[36m0.7154\u001b[0m        0.5624                     0.6567        0.6281  0.0004  0.2130\n",
      "     23                     0.7056        0.5655                     0.6604        0.6281  0.0002  0.2138\n",
      "     24                     \u001b[36m0.7192\u001b[0m        0.5640                     0.6567        0.6271  0.0000  0.2130\n",
      "     25                     0.7121        0.5639                     0.6549        0.6321  0.0000  0.2134\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5145\u001b[0m        \u001b[32m1.8391\u001b[0m                     \u001b[35m0.4701\u001b[0m        \u001b[31m1.1544\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5350\u001b[0m        \u001b[32m0.9298\u001b[0m                     \u001b[35m0.5802\u001b[0m        \u001b[31m0.9170\u001b[0m  0.0100  0.2144\n",
      "      3                     \u001b[36m0.5500\u001b[0m        \u001b[32m0.8868\u001b[0m                     0.5802        \u001b[31m0.6813\u001b[0m  0.0098  0.2135\n",
      "      4                     \u001b[36m0.5939\u001b[0m        \u001b[32m0.7746\u001b[0m                     \u001b[35m0.5821\u001b[0m        \u001b[31m0.6756\u001b[0m  0.0096  0.2094\n",
      "      5                     \u001b[36m0.6397\u001b[0m        \u001b[32m0.7269\u001b[0m                     \u001b[35m0.6119\u001b[0m        \u001b[31m0.6689\u001b[0m  0.0093  0.2109\n",
      "      6                     0.6355        \u001b[32m0.6706\u001b[0m                     \u001b[35m0.6231\u001b[0m        \u001b[31m0.6493\u001b[0m  0.0090  0.2134\n",
      "      7                     0.6360        0.7340                     \u001b[35m0.6325\u001b[0m        0.6622  0.0085  0.2134\n",
      "      8                     0.6285        0.7337                     0.5933        0.7269  0.0080  0.2122\n",
      "      9                     \u001b[36m0.6561\u001b[0m        \u001b[32m0.6496\u001b[0m                     0.6213        0.6709  0.0075  0.2122\n",
      "     10                     0.6542        0.6914                     \u001b[35m0.6381\u001b[0m        \u001b[31m0.6336\u001b[0m  0.0069  0.2125\n",
      "     11                     0.6509        \u001b[32m0.6410\u001b[0m                     0.6343        0.6421  0.0063  0.2124\n",
      "     12                     \u001b[36m0.6748\u001b[0m        \u001b[32m0.6115\u001b[0m                     0.6194        0.6398  0.0057  0.2130\n",
      "     13                     \u001b[36m0.6808\u001b[0m        \u001b[32m0.6106\u001b[0m                     0.6269        0.6825  0.0050  0.2119\n",
      "     14                     \u001b[36m0.6822\u001b[0m        0.6115                     0.6306        0.6777  0.0043  0.2124\n",
      "     15                     0.6766        0.6151                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6297\u001b[0m  0.0037  0.2124\n",
      "     16                     \u001b[36m0.6879\u001b[0m        \u001b[32m0.5967\u001b[0m                     0.6455        0.6391  0.0031  0.2120\n",
      "     17                     \u001b[36m0.6930\u001b[0m        \u001b[32m0.5915\u001b[0m                     0.6381        0.6480  0.0025  0.2132\n",
      "     18                     0.6897        \u001b[32m0.5894\u001b[0m                     0.6418        0.6364  0.0020  0.2119\n",
      "     19                     \u001b[36m0.7084\u001b[0m        \u001b[32m0.5666\u001b[0m                     \u001b[35m0.6586\u001b[0m        \u001b[31m0.6283\u001b[0m  0.0015  0.2114\n",
      "     20                     0.6981        0.5762                     \u001b[35m0.6623\u001b[0m        0.6347  0.0010  0.2125\n",
      "     21                     \u001b[36m0.7089\u001b[0m        0.5731                     0.6493        \u001b[31m0.6272\u001b[0m  0.0007  0.2092\n",
      "     22                     0.7075        0.5754                     0.6418        \u001b[31m0.6239\u001b[0m  0.0004  0.2135\n",
      "     23                     0.6972        0.5786                     0.6511        0.6276  0.0002  0.2124\n",
      "     24                     \u001b[36m0.7103\u001b[0m        0.5682                     0.6530        0.6252  0.0000  0.2119\n",
      "     25                     \u001b[36m0.7131\u001b[0m        \u001b[32m0.5580\u001b[0m                     0.6511        0.6276  0.0000  0.2135\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5290\u001b[0m        \u001b[32m1.5709\u001b[0m                     \u001b[35m0.5019\u001b[0m        \u001b[31m1.1215\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.5336\u001b[0m        \u001b[32m0.9446\u001b[0m                     0.4963        \u001b[31m0.9644\u001b[0m  0.0100  0.2150\n",
      "      3                     \u001b[36m0.5589\u001b[0m        \u001b[32m0.7991\u001b[0m                     \u001b[35m0.5243\u001b[0m        \u001b[31m0.9012\u001b[0m  0.0098  0.2124\n",
      "      4                     \u001b[36m0.5598\u001b[0m        \u001b[32m0.7517\u001b[0m                     \u001b[35m0.5877\u001b[0m        \u001b[31m0.7283\u001b[0m  0.0096  0.2127\n",
      "      5                     \u001b[36m0.5612\u001b[0m        0.8507                     0.5765        0.8723  0.0093  0.2147\n",
      "      6                     \u001b[36m0.6023\u001b[0m        0.7897                     0.5877        0.8027  0.0090  0.2135\n",
      "      7                     \u001b[36m0.6299\u001b[0m        \u001b[32m0.7066\u001b[0m                     \u001b[35m0.6567\u001b[0m        \u001b[31m0.6593\u001b[0m  0.0085  0.2133\n",
      "      8                     \u001b[36m0.6318\u001b[0m        \u001b[32m0.6812\u001b[0m                     0.6175        0.6790  0.0080  0.2144\n",
      "      9                     \u001b[36m0.6425\u001b[0m        \u001b[32m0.6805\u001b[0m                     0.5784        0.7791  0.0075  0.2135\n",
      "     10                     \u001b[36m0.6505\u001b[0m        \u001b[32m0.6413\u001b[0m                     0.6175        0.7010  0.0069  0.2134\n",
      "     11                     \u001b[36m0.6537\u001b[0m        0.6454                     0.6418        \u001b[31m0.6395\u001b[0m  0.0063  0.2115\n",
      "     12                     \u001b[36m0.6575\u001b[0m        \u001b[32m0.6258\u001b[0m                     0.6511        0.6422  0.0057  0.2125\n",
      "     13                     \u001b[36m0.6804\u001b[0m        \u001b[32m0.6242\u001b[0m                     \u001b[35m0.6791\u001b[0m        \u001b[31m0.6179\u001b[0m  0.0050  0.2132\n",
      "     14                     0.6757        \u001b[32m0.6041\u001b[0m                     0.6399        0.6460  0.0043  0.2146\n",
      "     15                     0.6752        0.6061                     0.6082        0.6940  0.0037  0.2134\n",
      "     16                     \u001b[36m0.6967\u001b[0m        \u001b[32m0.5860\u001b[0m                     0.6772        0.6187  0.0031  0.2127\n",
      "     17                     0.6921        0.5895                     0.6642        0.6221  0.0025  0.2134\n",
      "     18                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.5692\u001b[0m                     0.6660        0.6231  0.0020  0.2135\n",
      "     19                     \u001b[36m0.7051\u001b[0m        0.5769                     \u001b[35m0.6903\u001b[0m        \u001b[31m0.6165\u001b[0m  0.0015  0.2145\n",
      "     20                     0.7033        \u001b[32m0.5670\u001b[0m                     0.6549        0.6355  0.0010  0.2106\n",
      "     21                     0.7033        \u001b[32m0.5579\u001b[0m                     \u001b[35m0.6922\u001b[0m        0.6183  0.0007  0.2124\n",
      "     22                     0.6925        0.5662                     0.6698        0.6268  0.0004  0.2140\n",
      "     23                     \u001b[36m0.7126\u001b[0m        \u001b[32m0.5538\u001b[0m                     0.6866        0.6210  0.0002  0.2132\n",
      "     24                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.5503\u001b[0m                     0.6847        0.6242  0.0000  0.2139\n",
      "     25                     0.7042        0.5584                     0.6828        0.6261  0.0000  0.2125\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5117\u001b[0m        \u001b[32m1.7245\u001b[0m                     \u001b[35m0.5280\u001b[0m        \u001b[31m1.2066\u001b[0m  0.0100  0.2074\n",
      "      2                     0.5089        \u001b[32m0.9773\u001b[0m                     0.5093        \u001b[31m0.9766\u001b[0m  0.0100  0.2125\n",
      "      3                     \u001b[36m0.5313\u001b[0m        \u001b[32m0.9738\u001b[0m                     0.4944        \u001b[31m0.8865\u001b[0m  0.0098  0.2135\n",
      "      4                     \u001b[36m0.5472\u001b[0m        \u001b[32m0.7947\u001b[0m                     \u001b[35m0.5504\u001b[0m        \u001b[31m0.7147\u001b[0m  0.0096  0.2140\n",
      "      5                     \u001b[36m0.5743\u001b[0m        0.7991                     \u001b[35m0.5802\u001b[0m        0.7670  0.0093  0.2135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6                     \u001b[36m0.6126\u001b[0m        \u001b[32m0.7300\u001b[0m                     0.5802        0.9111  0.0090  0.2144\n",
      "      7                     0.5818        0.9521                     0.5784        0.7573  0.0085  0.2125\n",
      "      8                     \u001b[36m0.6168\u001b[0m        \u001b[32m0.7062\u001b[0m                     \u001b[35m0.6082\u001b[0m        \u001b[31m0.6586\u001b[0m  0.0080  0.2102\n",
      "      9                     \u001b[36m0.6407\u001b[0m        \u001b[32m0.6417\u001b[0m                     \u001b[35m0.6306\u001b[0m        \u001b[31m0.6536\u001b[0m  0.0075  0.2124\n",
      "     10                     \u001b[36m0.6607\u001b[0m        \u001b[32m0.6370\u001b[0m                     \u001b[35m0.6418\u001b[0m        \u001b[31m0.6387\u001b[0m  0.0069  0.2131\n",
      "     11                     0.6533        0.6678                     0.6213        0.6510  0.0063  0.2135\n",
      "     12                     \u001b[36m0.6692\u001b[0m        \u001b[32m0.6222\u001b[0m                     0.6306        0.6399  0.0057  0.2125\n",
      "     13                     \u001b[36m0.6771\u001b[0m        \u001b[32m0.6070\u001b[0m                     0.6399        \u001b[31m0.6320\u001b[0m  0.0050  0.2129\n",
      "     14                     \u001b[36m0.6804\u001b[0m        \u001b[32m0.6030\u001b[0m                     0.6325        0.6441  0.0043  0.2123\n",
      "     15                     0.6776        0.6046                     0.6343        0.6422  0.0037  0.2142\n",
      "     16                     0.6752        0.6047                     0.6175        0.6470  0.0031  0.2143\n",
      "     17                     \u001b[36m0.6939\u001b[0m        \u001b[32m0.5842\u001b[0m                     0.6175        0.6459  0.0025  0.2086\n",
      "     18                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.5769\u001b[0m                     0.6157        0.6545  0.0020  0.2140\n",
      "     19                     \u001b[36m0.7005\u001b[0m        0.5804                     0.6362        0.6549  0.0015  0.2119\n",
      "     20                     0.6893        0.5810                     0.6250        0.6525  0.0010  0.2093\n",
      "     21                     \u001b[36m0.7089\u001b[0m        \u001b[32m0.5698\u001b[0m                     \u001b[35m0.6493\u001b[0m        0.6446  0.0007  0.2127\n",
      "     22                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.5603\u001b[0m                     0.6399        0.6440  0.0004  0.2127\n",
      "     23                     0.7131        0.5673                     0.6418        0.6435  0.0002  0.2136\n",
      "     24                     0.7131        0.5658                     0.6474        0.6435  0.0000  0.2128\n",
      "     25                     0.7136        0.5637                     0.6455        0.6421  0.0000  0.2132\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5107\u001b[0m        \u001b[32m1.6632\u001b[0m                     \u001b[35m0.5243\u001b[0m        \u001b[31m1.1490\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5505\u001b[0m        \u001b[32m0.8046\u001b[0m                     0.4907        \u001b[31m0.9558\u001b[0m  0.0100  0.2126\n",
      "      3                     \u001b[36m0.5617\u001b[0m        \u001b[32m0.8039\u001b[0m                     \u001b[35m0.5466\u001b[0m        \u001b[31m0.7345\u001b[0m  0.0098  0.2099\n",
      "      4                     \u001b[36m0.5701\u001b[0m        0.8111                     0.5019        0.9030  0.0096  0.2119\n",
      "      5                     \u001b[36m0.5846\u001b[0m        \u001b[32m0.7484\u001b[0m                     \u001b[35m0.5597\u001b[0m        0.7461  0.0093  0.2114\n",
      "      6                     \u001b[36m0.6070\u001b[0m        \u001b[32m0.7420\u001b[0m                     0.5336        0.7892  0.0090  0.2143\n",
      "      7                     \u001b[36m0.6112\u001b[0m        \u001b[32m0.6968\u001b[0m                     \u001b[35m0.5821\u001b[0m        \u001b[31m0.7062\u001b[0m  0.0085  0.2133\n",
      "      8                     \u001b[36m0.6430\u001b[0m        \u001b[32m0.6597\u001b[0m                     \u001b[35m0.6157\u001b[0m        \u001b[31m0.6578\u001b[0m  0.0080  0.2130\n",
      "      9                     0.6393        \u001b[32m0.6525\u001b[0m                     \u001b[35m0.6194\u001b[0m        0.6603  0.0075  0.2134\n",
      "     10                     \u001b[36m0.6472\u001b[0m        \u001b[32m0.6462\u001b[0m                     \u001b[35m0.6325\u001b[0m        \u001b[31m0.6521\u001b[0m  0.0069  0.2136\n",
      "     11                     \u001b[36m0.6822\u001b[0m        \u001b[32m0.6182\u001b[0m                     \u001b[35m0.6567\u001b[0m        \u001b[31m0.6407\u001b[0m  0.0063  0.2129\n",
      "     12                     0.6561        0.6276                     0.6455        0.6487  0.0057  0.2155\n",
      "     13                     0.6706        0.6217                     0.6530        0.6654  0.0050  0.2134\n",
      "     14                     0.6640        0.6252                     \u001b[35m0.6642\u001b[0m        \u001b[31m0.6343\u001b[0m  0.0043  0.2116\n",
      "     15                     0.6617        \u001b[32m0.6109\u001b[0m                     0.6399        0.6388  0.0037  0.2136\n",
      "     16                     0.6678        \u001b[32m0.6037\u001b[0m                     0.6287        0.6653  0.0031  0.2125\n",
      "     17                     \u001b[36m0.6911\u001b[0m        \u001b[32m0.5978\u001b[0m                     \u001b[35m0.6698\u001b[0m        \u001b[31m0.6232\u001b[0m  0.0025  0.2138\n",
      "     18                     \u001b[36m0.6981\u001b[0m        \u001b[32m0.5783\u001b[0m                     0.6530        0.6399  0.0020  0.2135\n",
      "     19                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.5773\u001b[0m                     0.6604        0.6375  0.0015  0.2126\n",
      "     20                     0.6874        0.5857                     \u001b[35m0.6772\u001b[0m        0.6344  0.0010  0.2139\n",
      "     21                     \u001b[36m0.7005\u001b[0m        \u001b[32m0.5755\u001b[0m                     0.6660        0.6258  0.0007  0.2144\n",
      "     22                     \u001b[36m0.7117\u001b[0m        \u001b[32m0.5669\u001b[0m                     0.6642        0.6289  0.0004  0.2142\n",
      "     23                     0.7000        0.5695                     0.6642        0.6237  0.0002  0.2124\n",
      "     24                     0.7103        \u001b[32m0.5665\u001b[0m                     0.6623        0.6248  0.0000  0.2164\n",
      "     25                     0.6986        0.5689                     0.6623        0.6244  0.0000  0.2155\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5089\u001b[0m        \u001b[32m1.5489\u001b[0m                     \u001b[35m0.4851\u001b[0m        \u001b[31m1.5599\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5182\u001b[0m        \u001b[32m1.0675\u001b[0m                     \u001b[35m0.5709\u001b[0m        \u001b[31m0.7212\u001b[0m  0.0100  0.2146\n",
      "      3                     \u001b[36m0.5285\u001b[0m        \u001b[32m0.8894\u001b[0m                     0.5336        0.7414  0.0098  0.2126\n",
      "      4                     \u001b[36m0.5481\u001b[0m        \u001b[32m0.7869\u001b[0m                     0.5354        0.7343  0.0096  0.2134\n",
      "      5                     \u001b[36m0.5664\u001b[0m        \u001b[32m0.7474\u001b[0m                     \u001b[35m0.5970\u001b[0m        \u001b[31m0.6729\u001b[0m  0.0093  0.2141\n",
      "      6                     \u001b[36m0.5944\u001b[0m        \u001b[32m0.7166\u001b[0m                     \u001b[35m0.6082\u001b[0m        \u001b[31m0.6682\u001b[0m  0.0090  0.2133\n",
      "      7                     \u001b[36m0.6121\u001b[0m        \u001b[32m0.7016\u001b[0m                     \u001b[35m0.6250\u001b[0m        0.6971  0.0085  0.2145\n",
      "      8                     0.6061        0.7328                     0.6250        \u001b[31m0.6428\u001b[0m  0.0080  0.2128\n",
      "      9                     \u001b[36m0.6612\u001b[0m        \u001b[32m0.6454\u001b[0m                     \u001b[35m0.6306\u001b[0m        \u001b[31m0.6380\u001b[0m  0.0075  0.2141\n",
      "     10                     0.6449        0.6602                     \u001b[35m0.6437\u001b[0m        0.6604  0.0069  0.2144\n",
      "     11                     0.6453        0.6503                     0.6250        0.6593  0.0063  0.2134\n",
      "     12                     0.6467        0.6475                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6254\u001b[0m  0.0057  0.2128\n",
      "     13                     0.6584        \u001b[32m0.6370\u001b[0m                     0.6362        0.6454  0.0050  0.2145\n",
      "     14                     \u001b[36m0.6626\u001b[0m        \u001b[32m0.6163\u001b[0m                     \u001b[35m0.6623\u001b[0m        \u001b[31m0.6168\u001b[0m  0.0043  0.2131\n",
      "     15                     \u001b[36m0.6874\u001b[0m        \u001b[32m0.5997\u001b[0m                     0.6604        0.6196  0.0037  0.2125\n",
      "     16                     0.6710        0.6086                     0.6567        0.6188  0.0031  0.2125\n",
      "     17                     \u001b[36m0.6958\u001b[0m        \u001b[32m0.5937\u001b[0m                     \u001b[35m0.6642\u001b[0m        0.6390  0.0025  0.2134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18                     0.6864        \u001b[32m0.5865\u001b[0m                     0.6455        0.6301  0.0020  0.2135\n",
      "     19                     0.6888        0.5889                     0.6530        0.6199  0.0015  0.2134\n",
      "     20                     0.6916        \u001b[32m0.5771\u001b[0m                     0.6455        0.6227  0.0010  0.2145\n",
      "     21                     0.6888        \u001b[32m0.5756\u001b[0m                     0.6549        0.6245  0.0007  0.2105\n",
      "     22                     \u001b[36m0.7089\u001b[0m        \u001b[32m0.5686\u001b[0m                     0.6604        0.6208  0.0004  0.2134\n",
      "     23                     0.6981        0.5688                     0.6604        0.6251  0.0002  0.2124\n",
      "     24                     0.7023        0.5733                     0.6567        0.6258  0.0000  0.2129\n",
      "     25                     0.7051        \u001b[32m0.5678\u001b[0m                     0.6604        0.6231  0.0000  0.2129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5234\u001b[0m        \u001b[32m1.3807\u001b[0m                     \u001b[35m0.5075\u001b[0m        \u001b[31m0.9626\u001b[0m  0.0100  0.2863\n",
      "      2                     0.5182        \u001b[32m0.9592\u001b[0m                     \u001b[35m0.5187\u001b[0m        1.0410  0.0100  0.2194\n",
      "      3                     \u001b[36m0.5388\u001b[0m        \u001b[32m0.8683\u001b[0m                     \u001b[35m0.5485\u001b[0m        \u001b[31m0.7607\u001b[0m  0.0098  0.2134\n",
      "      4                     0.5248        \u001b[32m0.8294\u001b[0m                     0.5466        0.8063  0.0096  0.2123\n",
      "      5                     0.5336        0.8452                     0.5168        0.9274  0.0093  0.2133\n",
      "      6                     \u001b[36m0.5491\u001b[0m        \u001b[32m0.7713\u001b[0m                     \u001b[35m0.5578\u001b[0m        \u001b[31m0.7181\u001b[0m  0.0090  0.2094\n",
      "      7                     \u001b[36m0.5687\u001b[0m        \u001b[32m0.7123\u001b[0m                     \u001b[35m0.5746\u001b[0m        \u001b[31m0.6929\u001b[0m  0.0085  0.2092\n",
      "      8                     \u001b[36m0.5701\u001b[0m        \u001b[32m0.7057\u001b[0m                     \u001b[35m0.5896\u001b[0m        \u001b[31m0.6680\u001b[0m  0.0080  0.2091\n",
      "      9                     \u001b[36m0.5771\u001b[0m        \u001b[32m0.7009\u001b[0m                     \u001b[35m0.5933\u001b[0m        0.6693  0.0075  0.2107\n",
      "     10                     \u001b[36m0.5850\u001b[0m        \u001b[32m0.6994\u001b[0m                     0.5485        0.6885  0.0069  0.2108\n",
      "     11                     0.5832        \u001b[32m0.6832\u001b[0m                     0.5354        0.6938  0.0063  0.2124\n",
      "     12                     0.5818        0.7134                     0.5466        0.7016  0.0057  0.2115\n",
      "     13                     \u001b[36m0.6005\u001b[0m        \u001b[32m0.6806\u001b[0m                     \u001b[35m0.6157\u001b[0m        0.6681  0.0050  0.2124\n",
      "     14                     \u001b[36m0.6098\u001b[0m        \u001b[32m0.6690\u001b[0m                     0.5840        0.6778  0.0043  0.2119\n",
      "     15                     0.5879        0.6899                     0.6045        \u001b[31m0.6673\u001b[0m  0.0037  0.2116\n",
      "     16                     0.6037        0.6727                     0.5336        0.6900  0.0031  0.2136\n",
      "     17                     0.6047        0.6699                     0.5951        0.6735  0.0025  0.2115\n",
      "     18                     0.6098        \u001b[32m0.6505\u001b[0m                     0.5858        0.6709  0.0020  0.2114\n",
      "     19                     \u001b[36m0.6145\u001b[0m        0.6579                     \u001b[35m0.6269\u001b[0m        \u001b[31m0.6662\u001b[0m  0.0015  0.2131\n",
      "     20                     \u001b[36m0.6206\u001b[0m        \u001b[32m0.6496\u001b[0m                     0.6026        0.6731  0.0010  0.2119\n",
      "     21                     \u001b[36m0.6262\u001b[0m        0.6500                     0.5858        0.6740  0.0007  0.2114\n",
      "     22                     0.6252        \u001b[32m0.6480\u001b[0m                     0.6269        0.6675  0.0004  0.2115\n",
      "     23                     0.6248        \u001b[32m0.6442\u001b[0m                     0.6119        0.6679  0.0002  0.2122\n",
      "     24                     \u001b[36m0.6379\u001b[0m        \u001b[32m0.6383\u001b[0m                     0.6157        0.6671  0.0000  0.2125\n",
      "     25                     \u001b[36m0.6402\u001b[0m        0.6388                     0.6175        0.6671  0.0000  0.2135\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5276\u001b[0m        \u001b[32m1.6733\u001b[0m                     \u001b[35m0.5653\u001b[0m        \u001b[31m1.0743\u001b[0m  0.0100  0.2074\n",
      "      2                     0.5136        \u001b[32m0.8951\u001b[0m                     0.5616        \u001b[31m0.7094\u001b[0m  0.0100  0.2140\n",
      "      3                     \u001b[36m0.5393\u001b[0m        \u001b[32m0.8438\u001b[0m                     0.5019        0.7788  0.0098  0.2113\n",
      "      4                     \u001b[36m0.5407\u001b[0m        0.8562                     0.5429        0.7165  0.0096  0.2118\n",
      "      5                     \u001b[36m0.5561\u001b[0m        \u001b[32m0.8157\u001b[0m                     0.5317        0.7625  0.0093  0.2117\n",
      "      6                     \u001b[36m0.5659\u001b[0m        \u001b[32m0.7775\u001b[0m                     0.5056        0.7365  0.0090  0.2124\n",
      "      7                     \u001b[36m0.5664\u001b[0m        \u001b[32m0.7567\u001b[0m                     0.4963        0.7615  0.0085  0.2115\n",
      "      8                     \u001b[36m0.5706\u001b[0m        \u001b[32m0.7393\u001b[0m                     0.4832        0.7578  0.0080  0.2134\n",
      "      9                     0.5645        \u001b[32m0.7317\u001b[0m                     0.5299        0.7295  0.0075  0.2111\n",
      "     10                     \u001b[36m0.5738\u001b[0m        0.7388                     0.5634        \u001b[31m0.6826\u001b[0m  0.0069  0.2134\n",
      "     11                     \u001b[36m0.6117\u001b[0m        \u001b[32m0.6680\u001b[0m                     \u001b[35m0.5690\u001b[0m        0.7083  0.0063  0.2126\n",
      "     12                     0.6084        \u001b[32m0.6673\u001b[0m                     \u001b[35m0.5877\u001b[0m        0.6875  0.0057  0.2123\n",
      "     13                     \u001b[36m0.6159\u001b[0m        \u001b[32m0.6640\u001b[0m                     0.5131        0.7546  0.0050  0.2130\n",
      "     14                     \u001b[36m0.6196\u001b[0m        0.6707                     \u001b[35m0.6082\u001b[0m        \u001b[31m0.6685\u001b[0m  0.0043  0.2126\n",
      "     15                     0.6079        \u001b[32m0.6632\u001b[0m                     0.5410        0.7185  0.0037  0.2125\n",
      "     16                     0.6154        \u001b[32m0.6582\u001b[0m                     0.6026        0.6788  0.0031  0.2140\n",
      "     17                     \u001b[36m0.6210\u001b[0m        0.6582                     0.5709        0.7069  0.0025  0.2133\n",
      "     18                     \u001b[36m0.6313\u001b[0m        \u001b[32m0.6397\u001b[0m                     0.5784        0.6921  0.0020  0.2130\n",
      "     19                     0.6304        0.6430                     0.5858        0.7052  0.0015  0.2125\n",
      "     20                     0.6294        0.6443                     0.5709        0.6956  0.0010  0.2093\n",
      "     21                     0.6308        \u001b[32m0.6381\u001b[0m                     0.5914        0.6835  0.0007  0.2148\n",
      "     22                     \u001b[36m0.6327\u001b[0m        \u001b[32m0.6352\u001b[0m                     0.5784        0.6921  0.0004  0.2130\n",
      "     23                     \u001b[36m0.6430\u001b[0m        \u001b[32m0.6305\u001b[0m                     0.5877        0.6875  0.0002  0.2131\n",
      "     24                     \u001b[36m0.6472\u001b[0m        0.6319                     0.5970        0.6853  0.0000  0.2129\n",
      "     25                     0.6439        \u001b[32m0.6260\u001b[0m                     0.5914        0.6861  0.0000  0.2129\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5318\u001b[0m        \u001b[32m1.4791\u001b[0m                     \u001b[35m0.5112\u001b[0m        \u001b[31m0.8836\u001b[0m  0.0100  0.2064\n",
      "      2                     0.5234        \u001b[32m0.8184\u001b[0m                     0.5075        0.9481  0.0100  0.2184\n",
      "      3                     0.5210        0.8571                     \u001b[35m0.5840\u001b[0m        \u001b[31m0.6926\u001b[0m  0.0098  0.2115\n",
      "      4                     \u001b[36m0.5458\u001b[0m        0.8554                     0.5131        0.9822  0.0096  0.2122\n",
      "      5                     0.5383        \u001b[32m0.7494\u001b[0m                     0.5112        0.7874  0.0093  0.2135\n",
      "      6                     0.5416        0.7898                     0.5653        \u001b[31m0.6876\u001b[0m  0.0090  0.2116\n",
      "      7                     \u001b[36m0.5650\u001b[0m        \u001b[32m0.6976\u001b[0m                     0.5504        0.7075  0.0085  0.2137\n",
      "      8                     \u001b[36m0.5888\u001b[0m        0.7516                     0.5168        0.7898  0.0080  0.2134\n",
      "      9                     0.5523        0.7584                     0.5243        0.7413  0.0075  0.2122\n",
      "     10                     0.5869        \u001b[32m0.6902\u001b[0m                     0.5466        0.6962  0.0069  0.2134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     \u001b[36m0.6061\u001b[0m        0.6948                     0.5354        0.7418  0.0063  0.2143\n",
      "     12                     \u001b[36m0.6131\u001b[0m        0.6921                     0.5299        0.6942  0.0057  0.2132\n",
      "     13                     0.5827        \u001b[32m0.6826\u001b[0m                     0.5746        \u001b[31m0.6830\u001b[0m  0.0050  0.2134\n",
      "     14                     0.6047        0.6869                     \u001b[35m0.5914\u001b[0m        0.6874  0.0043  0.2121\n",
      "     15                     0.6019        \u001b[32m0.6766\u001b[0m                     0.5429        0.7266  0.0037  0.2135\n",
      "     16                     \u001b[36m0.6243\u001b[0m        \u001b[32m0.6628\u001b[0m                     0.5709        0.7092  0.0031  0.2125\n",
      "     17                     0.6210        \u001b[32m0.6565\u001b[0m                     0.5821        0.6954  0.0025  0.2134\n",
      "     18                     0.6089        \u001b[32m0.6491\u001b[0m                     0.5578        0.6853  0.0020  0.2122\n",
      "     19                     \u001b[36m0.6308\u001b[0m        \u001b[32m0.6435\u001b[0m                     0.5840        0.6847  0.0015  0.2095\n",
      "     20                     \u001b[36m0.6402\u001b[0m        \u001b[32m0.6416\u001b[0m                     0.5858        \u001b[31m0.6811\u001b[0m  0.0010  0.2128\n",
      "     21                     0.6262        0.6437                     \u001b[35m0.5933\u001b[0m        0.6830  0.0007  0.2135\n",
      "     22                     \u001b[36m0.6416\u001b[0m        0.6428                     0.5933        0.6851  0.0004  0.2124\n",
      "     23                     \u001b[36m0.6467\u001b[0m        \u001b[32m0.6312\u001b[0m                     0.5933        0.6841  0.0002  0.2127\n",
      "     24                     0.6397        0.6406                     0.5877        0.6824  0.0000  0.2092\n",
      "     25                     0.6453        0.6355                     0.5896        0.6868  0.0000  0.2098\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5000\u001b[0m        \u001b[32m1.7480\u001b[0m                     \u001b[35m0.5299\u001b[0m        \u001b[31m0.9109\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.5051\u001b[0m        \u001b[32m1.1404\u001b[0m                     \u001b[35m0.5448\u001b[0m        \u001b[31m0.7825\u001b[0m  0.0100  0.2135\n",
      "      3                     \u001b[36m0.5505\u001b[0m        \u001b[32m0.8391\u001b[0m                     0.5131        0.9312  0.0098  0.2126\n",
      "      4                     \u001b[36m0.5659\u001b[0m        0.8640                     0.5000        1.0262  0.0096  0.2126\n",
      "      5                     \u001b[36m0.5762\u001b[0m        \u001b[32m0.7905\u001b[0m                     \u001b[35m0.5784\u001b[0m        \u001b[31m0.7160\u001b[0m  0.0093  0.2127\n",
      "      6                     0.5579        \u001b[32m0.7298\u001b[0m                     \u001b[35m0.5896\u001b[0m        0.7616  0.0090  0.2114\n",
      "      7                     \u001b[36m0.5832\u001b[0m        0.7628                     0.5690        0.7174  0.0085  0.2114\n",
      "      8                     0.5682        0.8119                     0.5466        0.7394  0.0080  0.2122\n",
      "      9                     0.5678        0.7446                     0.5858        \u001b[31m0.6917\u001b[0m  0.0075  0.2184\n",
      "     10                     \u001b[36m0.5846\u001b[0m        \u001b[32m0.7138\u001b[0m                     0.5187        0.7773  0.0069  0.2294\n",
      "     11                     \u001b[36m0.5963\u001b[0m        0.7151                     0.5765        0.6995  0.0063  0.2234\n",
      "     12                     \u001b[36m0.6098\u001b[0m        \u001b[32m0.7009\u001b[0m                     0.5765        0.6987  0.0057  0.2242\n",
      "     13                     \u001b[36m0.6257\u001b[0m        \u001b[32m0.6826\u001b[0m                     0.5672        0.7014  0.0050  0.2224\n",
      "     14                     0.6234        \u001b[32m0.6614\u001b[0m                     0.5784        0.7018  0.0043  0.2393\n",
      "     15                     \u001b[36m0.6322\u001b[0m        \u001b[32m0.6511\u001b[0m                     0.5709        0.6961  0.0037  0.2463\n",
      "     16                     0.6257        0.6596                     0.5597        \u001b[31m0.6876\u001b[0m  0.0031  0.2384\n",
      "     17                     \u001b[36m0.6397\u001b[0m        \u001b[32m0.6486\u001b[0m                     0.5728        0.6916  0.0025  0.2344\n",
      "     18                     0.6178        \u001b[32m0.6470\u001b[0m                     0.5765        \u001b[31m0.6867\u001b[0m  0.0020  0.2424\n",
      "     19                     \u001b[36m0.6421\u001b[0m        \u001b[32m0.6406\u001b[0m                     0.5653        0.6924  0.0015  0.2344\n",
      "     20                     \u001b[36m0.6458\u001b[0m        \u001b[32m0.6378\u001b[0m                     0.5690        0.6937  0.0010  0.2274\n",
      "     21                     0.6444        \u001b[32m0.6330\u001b[0m                     0.5840        0.7028  0.0007  0.2264\n",
      "     22                     0.6397        0.6353                     0.5728        0.6942  0.0004  0.2244\n",
      "     23                     \u001b[36m0.6463\u001b[0m        \u001b[32m0.6312\u001b[0m                     0.5746        0.6911  0.0002  0.2234\n",
      "     24                     \u001b[36m0.6477\u001b[0m        0.6320                     0.5784        0.6969  0.0000  0.2294\n",
      "     25                     \u001b[36m0.6607\u001b[0m        \u001b[32m0.6238\u001b[0m                     0.5653        0.6899  0.0000  0.2453\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5173\u001b[0m        \u001b[32m1.6335\u001b[0m                     \u001b[35m0.4907\u001b[0m        \u001b[31m1.0863\u001b[0m  0.0100  0.2314\n",
      "      2                     0.5093        \u001b[32m0.9848\u001b[0m                     0.4888        1.1004  0.0100  0.2374\n",
      "      3                     \u001b[36m0.5332\u001b[0m        \u001b[32m0.8661\u001b[0m                     \u001b[35m0.5000\u001b[0m        \u001b[31m0.7953\u001b[0m  0.0098  0.2453\n",
      "      4                     \u001b[36m0.5579\u001b[0m        \u001b[32m0.7818\u001b[0m                     0.4925        \u001b[31m0.7730\u001b[0m  0.0096  0.2424\n",
      "      5                     \u001b[36m0.5617\u001b[0m        \u001b[32m0.7249\u001b[0m                     \u001b[35m0.5354\u001b[0m        \u001b[31m0.7142\u001b[0m  0.0093  0.2284\n",
      "      6                     \u001b[36m0.5659\u001b[0m        0.7413                     0.5056        1.1121  0.0090  0.2264\n",
      "      7                     0.5519        0.8515                     0.5037        0.7349  0.0085  0.2244\n",
      "      8                     \u001b[36m0.5696\u001b[0m        0.7781                     \u001b[35m0.5597\u001b[0m        0.7239  0.0080  0.2304\n",
      "      9                     \u001b[36m0.5818\u001b[0m        \u001b[32m0.7044\u001b[0m                     0.5336        0.7247  0.0075  0.2314\n",
      "     10                     \u001b[36m0.6000\u001b[0m        \u001b[32m0.6781\u001b[0m                     0.5504        \u001b[31m0.6935\u001b[0m  0.0069  0.2294\n",
      "     11                     0.5944        0.6863                     0.5541        0.7011  0.0063  0.2384\n",
      "     12                     0.5879        \u001b[32m0.6772\u001b[0m                     \u001b[35m0.5709\u001b[0m        \u001b[31m0.6860\u001b[0m  0.0057  0.2324\n",
      "     13                     \u001b[36m0.6065\u001b[0m        0.6906                     0.5578        0.7006  0.0050  0.2394\n",
      "     14                     \u001b[36m0.6079\u001b[0m        \u001b[32m0.6657\u001b[0m                     0.5709        0.7004  0.0043  0.2364\n",
      "     15                     \u001b[36m0.6276\u001b[0m        \u001b[32m0.6551\u001b[0m                     0.5504        0.7268  0.0037  0.2314\n",
      "     16                     0.6271        \u001b[32m0.6535\u001b[0m                     \u001b[35m0.5765\u001b[0m        0.6941  0.0031  0.2294\n",
      "     17                     \u001b[36m0.6322\u001b[0m        \u001b[32m0.6490\u001b[0m                     \u001b[35m0.5933\u001b[0m        0.6899  0.0025  0.2274\n",
      "     18                     0.6294        \u001b[32m0.6483\u001b[0m                     0.5840        0.6868  0.0020  0.2334\n",
      "     19                     \u001b[36m0.6425\u001b[0m        \u001b[32m0.6388\u001b[0m                     0.5672        0.6997  0.0015  0.2354\n",
      "     20                     0.6318        0.6414                     0.5840        0.6876  0.0010  0.2374\n",
      "     21                     0.6346        \u001b[32m0.6313\u001b[0m                     0.5840        0.6879  0.0007  0.2334\n",
      "     22                     \u001b[36m0.6556\u001b[0m        \u001b[32m0.6235\u001b[0m                     0.5802        0.6925  0.0004  0.2304\n",
      "     23                     0.6547        0.6254                     0.5728        0.6911  0.0002  0.2294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24                     \u001b[36m0.6579\u001b[0m        \u001b[32m0.6225\u001b[0m                     0.5746        0.6940  0.0000  0.2274\n",
      "     25                     0.6565        0.6340                     0.5690        0.6929  0.0000  0.2294\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5126\u001b[0m        \u001b[32m1.5333\u001b[0m                     \u001b[35m0.5429\u001b[0m        \u001b[31m0.7545\u001b[0m  0.0100  0.2254\n",
      "      2                     \u001b[36m0.5210\u001b[0m        \u001b[32m1.1984\u001b[0m                     \u001b[35m0.5522\u001b[0m        \u001b[31m0.7399\u001b[0m  0.0100  0.2264\n",
      "      3                     \u001b[36m0.5332\u001b[0m        \u001b[32m0.9035\u001b[0m                     0.5466        \u001b[31m0.6941\u001b[0m  0.0098  0.2304\n",
      "      4                     \u001b[36m0.5607\u001b[0m        \u001b[32m0.7357\u001b[0m                     \u001b[35m0.5690\u001b[0m        \u001b[31m0.6847\u001b[0m  0.0096  0.2294\n",
      "      5                     0.5481        0.8220                     0.5410        0.7945  0.0093  0.2284\n",
      "      6                     0.5294        0.8884                     0.5485        0.7020  0.0090  0.2274\n",
      "      7                     0.5313        0.7785                     0.5672        \u001b[31m0.6811\u001b[0m  0.0085  0.2274\n",
      "      8                     \u001b[36m0.5617\u001b[0m        \u001b[32m0.7185\u001b[0m                     0.5224        0.7128  0.0080  0.2304\n",
      "      9                     \u001b[36m0.5706\u001b[0m        0.7487                     0.5168        0.7346  0.0075  0.2274\n",
      "     10                     \u001b[36m0.5958\u001b[0m        \u001b[32m0.6920\u001b[0m                     0.5672        \u001b[31m0.6789\u001b[0m  0.0069  0.2264\n",
      "     11                     \u001b[36m0.6042\u001b[0m        0.6943                     \u001b[35m0.5728\u001b[0m        0.6866  0.0063  0.2274\n",
      "     12                     0.5953        \u001b[32m0.6732\u001b[0m                     \u001b[35m0.5802\u001b[0m        0.7223  0.0057  0.2284\n",
      "     13                     0.5963        0.7219                     0.5093        0.7792  0.0050  0.2284\n",
      "     14                     \u001b[36m0.6178\u001b[0m        0.6811                     0.5634        0.6992  0.0043  0.2274\n",
      "     15                     0.6070        0.6736                     0.5728        0.6818  0.0037  0.2254\n",
      "     16                     0.6126        \u001b[32m0.6542\u001b[0m                     0.5149        0.7054  0.0031  0.2294\n",
      "     17                     0.6164        0.6586                     0.5560        0.7012  0.0025  0.2274\n",
      "     18                     \u001b[36m0.6369\u001b[0m        \u001b[32m0.6523\u001b[0m                     0.5802        0.6904  0.0020  0.2284\n",
      "     19                     0.6280        0.6527                     0.5784        0.6839  0.0015  0.2274\n",
      "     20                     0.6280        0.6546                     \u001b[35m0.5821\u001b[0m        0.6861  0.0010  0.2254\n",
      "     21                     0.6294        \u001b[32m0.6452\u001b[0m                     \u001b[35m0.5896\u001b[0m        \u001b[31m0.6784\u001b[0m  0.0007  0.2244\n",
      "     22                     \u001b[36m0.6579\u001b[0m        \u001b[32m0.6281\u001b[0m                     \u001b[35m0.6045\u001b[0m        0.6787  0.0004  0.2304\n",
      "     23                     0.6495        0.6314                     0.5672        0.6837  0.0002  0.2274\n",
      "     24                     0.6463        0.6293                     0.5672        0.6820  0.0000  0.2264\n",
      "     25                     0.6472        \u001b[32m0.6267\u001b[0m                     0.5709        0.6805  0.0000  0.2264\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5238\u001b[0m        \u001b[32m1.7381\u001b[0m                     \u001b[35m0.5354\u001b[0m        \u001b[31m1.3596\u001b[0m  0.0100  0.2204\n",
      "      2                     0.4958        \u001b[32m0.9271\u001b[0m                     \u001b[35m0.5541\u001b[0m        \u001b[31m0.7361\u001b[0m  0.0100  0.2254\n",
      "      3                     0.5187        \u001b[32m0.8317\u001b[0m                     0.5224        0.7708  0.0098  0.2224\n",
      "      4                     \u001b[36m0.5551\u001b[0m        \u001b[32m0.7836\u001b[0m                     0.5019        0.8000  0.0096  0.2244\n",
      "      5                     0.5486        0.8449                     \u001b[35m0.5634\u001b[0m        0.7806  0.0093  0.2214\n",
      "      6                     0.5519        0.7883                     0.5522        \u001b[31m0.6945\u001b[0m  0.0090  0.2234\n",
      "      7                     \u001b[36m0.5565\u001b[0m        \u001b[32m0.7540\u001b[0m                     \u001b[35m0.5896\u001b[0m        \u001b[31m0.6682\u001b[0m  0.0085  0.2244\n",
      "      8                     \u001b[36m0.5650\u001b[0m        \u001b[32m0.7129\u001b[0m                     0.5896        0.6873  0.0080  0.2235\n",
      "      9                     \u001b[36m0.5692\u001b[0m        \u001b[32m0.6901\u001b[0m                     0.5821        0.6706  0.0075  0.2224\n",
      "     10                     \u001b[36m0.5776\u001b[0m        0.7171                     0.5466        0.7598  0.0069  0.2234\n",
      "     11                     0.5771        0.7063                     \u001b[35m0.6101\u001b[0m        \u001b[31m0.6608\u001b[0m  0.0063  0.2254\n",
      "     12                     \u001b[36m0.5911\u001b[0m        0.7002                     0.5690        0.6959  0.0057  0.2244\n",
      "     13                     0.5617        0.7294                     0.5877        0.6631  0.0050  0.2264\n",
      "     14                     \u001b[36m0.6182\u001b[0m        \u001b[32m0.6612\u001b[0m                     0.5951        0.6789  0.0043  0.2214\n",
      "     15                     0.5808        0.6716                     0.5896        0.6668  0.0037  0.2234\n",
      "     16                     0.6154        \u001b[32m0.6564\u001b[0m                     0.5877        0.6863  0.0031  0.2229\n",
      "     17                     0.6150        \u001b[32m0.6533\u001b[0m                     \u001b[35m0.6138\u001b[0m        0.6631  0.0025  0.2214\n",
      "     18                     \u001b[36m0.6257\u001b[0m        \u001b[32m0.6491\u001b[0m                     0.6063        0.6652  0.0020  0.2234\n",
      "     19                     0.6210        0.6533                     \u001b[35m0.6287\u001b[0m        0.6633  0.0015  0.2244\n",
      "     20                     \u001b[36m0.6369\u001b[0m        \u001b[32m0.6383\u001b[0m                     0.6101        0.6665  0.0010  0.2224\n",
      "     21                     \u001b[36m0.6374\u001b[0m        \u001b[32m0.6350\u001b[0m                     0.6082        0.6623  0.0007  0.2224\n",
      "     22                     0.6318        \u001b[32m0.6309\u001b[0m                     0.6007        0.6655  0.0004  0.2224\n",
      "     23                     0.6327        0.6388                     0.6007        0.6661  0.0002  0.2244\n",
      "     24                     0.6308        0.6375                     0.6063        0.6650  0.0000  0.2234\n",
      "     25                     \u001b[36m0.6439\u001b[0m        0.6402                     0.6138        0.6642  0.0000  0.2234\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5056\u001b[0m        \u001b[32m2.0402\u001b[0m                     \u001b[35m0.5093\u001b[0m        \u001b[31m1.4971\u001b[0m  0.0100  0.2164\n",
      "      2                     \u001b[36m0.5336\u001b[0m        \u001b[32m0.9106\u001b[0m                     \u001b[35m0.5224\u001b[0m        \u001b[31m0.8137\u001b[0m  0.0100  0.2254\n",
      "      3                     0.5159        \u001b[32m0.7987\u001b[0m                     0.5112        1.0168  0.0098  0.2214\n",
      "      4                     \u001b[36m0.5579\u001b[0m        0.8094                     \u001b[35m0.5597\u001b[0m        \u001b[31m0.7886\u001b[0m  0.0096  0.2214\n",
      "      5                     0.5360        \u001b[32m0.7908\u001b[0m                     0.5299        0.8189  0.0093  0.2194\n",
      "      6                     \u001b[36m0.5879\u001b[0m        \u001b[32m0.7152\u001b[0m                     0.5131        0.7981  0.0090  0.2204\n",
      "      7                     0.5808        0.7180                     0.5299        \u001b[31m0.7276\u001b[0m  0.0085  0.2214\n",
      "      8                     0.5864        \u001b[32m0.7081\u001b[0m                     \u001b[35m0.5616\u001b[0m        \u001b[31m0.6962\u001b[0m  0.0080  0.2214\n",
      "      9                     \u001b[36m0.5907\u001b[0m        \u001b[32m0.6961\u001b[0m                     0.5354        0.6985  0.0075  0.2214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     \u001b[36m0.6023\u001b[0m        \u001b[32m0.6731\u001b[0m                     0.5485        0.7293  0.0069  0.2194\n",
      "     11                     0.5944        0.6858                     0.5448        0.6962  0.0063  0.2224\n",
      "     12                     0.5939        0.6843                     0.5466        0.7408  0.0057  0.2224\n",
      "     13                     \u001b[36m0.6112\u001b[0m        0.6746                     0.5616        \u001b[31m0.6952\u001b[0m  0.0050  0.2234\n",
      "     14                     0.6047        \u001b[32m0.6652\u001b[0m                     \u001b[35m0.5840\u001b[0m        \u001b[31m0.6858\u001b[0m  0.0043  0.2194\n",
      "     15                     \u001b[36m0.6332\u001b[0m        \u001b[32m0.6424\u001b[0m                     0.5653        0.7249  0.0037  0.2234\n",
      "     16                     0.6276        0.6519                     0.5802        0.6885  0.0031  0.2214\n",
      "     17                     \u001b[36m0.6449\u001b[0m        \u001b[32m0.6405\u001b[0m                     \u001b[35m0.6026\u001b[0m        0.6910  0.0025  0.2194\n",
      "     18                     \u001b[36m0.6458\u001b[0m        \u001b[32m0.6330\u001b[0m                     0.5784        0.6997  0.0020  0.2194\n",
      "     19                     \u001b[36m0.6537\u001b[0m        \u001b[32m0.6329\u001b[0m                     \u001b[35m0.6045\u001b[0m        \u001b[31m0.6826\u001b[0m  0.0015  0.2224\n",
      "     20                     \u001b[36m0.6589\u001b[0m        \u001b[32m0.6190\u001b[0m                     0.5802        0.6986  0.0010  0.2204\n",
      "     21                     0.6411        0.6326                     0.5858        0.7048  0.0007  0.2164\n",
      "     22                     \u001b[36m0.6640\u001b[0m        \u001b[32m0.6167\u001b[0m                     0.5784        0.7066  0.0004  0.2214\n",
      "     23                     \u001b[36m0.6771\u001b[0m        \u001b[32m0.6107\u001b[0m                     0.5802        0.6992  0.0002  0.2204\n",
      "     24                     0.6542        0.6190                     0.5877        0.6966  0.0000  0.2194\n",
      "     25                     0.6612        0.6148                     0.5914        0.6918  0.0000  0.2204\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5145\u001b[0m        \u001b[32m1.4746\u001b[0m                     \u001b[35m0.5093\u001b[0m        \u001b[31m0.9069\u001b[0m  0.0100  0.2164\n",
      "      2                     0.5033        \u001b[32m0.9150\u001b[0m                     \u001b[35m0.5261\u001b[0m        \u001b[31m0.8053\u001b[0m  0.0100  0.2214\n",
      "      3                     \u001b[36m0.5196\u001b[0m        \u001b[32m0.8235\u001b[0m                     0.4963        \u001b[31m0.7884\u001b[0m  0.0098  0.2204\n",
      "      4                     \u001b[36m0.5215\u001b[0m        0.9125                     0.4963        \u001b[31m0.7453\u001b[0m  0.0096  0.2184\n",
      "      5                     \u001b[36m0.5234\u001b[0m        0.8607                     0.5112        0.8861  0.0093  0.2194\n",
      "      6                     \u001b[36m0.5430\u001b[0m        \u001b[32m0.7857\u001b[0m                     \u001b[35m0.5560\u001b[0m        \u001b[31m0.7255\u001b[0m  0.0090  0.2194\n",
      "      7                     \u001b[36m0.5593\u001b[0m        \u001b[32m0.7217\u001b[0m                     0.5392        \u001b[31m0.6897\u001b[0m  0.0085  0.2204\n",
      "      8                     \u001b[36m0.5902\u001b[0m        \u001b[32m0.7064\u001b[0m                     0.5280        0.7658  0.0080  0.2194\n",
      "      9                     0.5790        0.7337                     0.5410        0.7717  0.0075  0.2234\n",
      "     10                     \u001b[36m0.6019\u001b[0m        \u001b[32m0.7008\u001b[0m                     0.5522        0.7061  0.0069  0.2254\n",
      "     11                     \u001b[36m0.6164\u001b[0m        \u001b[32m0.6672\u001b[0m                     \u001b[35m0.5578\u001b[0m        0.7267  0.0063  0.2224\n",
      "     12                     0.6047        0.6915                     0.4907        0.8228  0.0057  0.2224\n",
      "     13                     0.6033        0.6850                     0.5578        0.6971  0.0050  0.2220\n",
      "     14                     \u001b[36m0.6220\u001b[0m        \u001b[32m0.6638\u001b[0m                     0.5429        0.7080  0.0043  0.2224\n",
      "     15                     0.6220        \u001b[32m0.6558\u001b[0m                     0.5410        0.7265  0.0037  0.2214\n",
      "     16                     0.6201        \u001b[32m0.6493\u001b[0m                     0.5485        0.7161  0.0031  0.2184\n",
      "     17                     \u001b[36m0.6238\u001b[0m        \u001b[32m0.6446\u001b[0m                     0.5429        0.7162  0.0025  0.2204\n",
      "     18                     \u001b[36m0.6463\u001b[0m        \u001b[32m0.6306\u001b[0m                     0.5429        0.7419  0.0020  0.2214\n",
      "     19                     0.6383        0.6307                     0.5560        0.7436  0.0015  0.2234\n",
      "     20                     0.6374        0.6310                     0.5336        0.7161  0.0010  0.2224\n",
      "     21                     \u001b[36m0.6565\u001b[0m        \u001b[32m0.6200\u001b[0m                     0.5354        0.7181  0.0007  0.2214\n",
      "     22                     \u001b[36m0.6612\u001b[0m        \u001b[32m0.6167\u001b[0m                     0.5522        0.7100  0.0004  0.2222\n",
      "     23                     0.6547        0.6175                     0.5485        0.7213  0.0002  0.2229\n",
      "     24                     0.6463        0.6221                     0.5541        0.7108  0.0000  0.2215\n",
      "     25                     0.6565        0.6198                     0.5485        0.7191  0.0000  0.2244\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5112\u001b[0m        \u001b[32m1.4214\u001b[0m                     \u001b[35m0.4907\u001b[0m        \u001b[31m0.9698\u001b[0m  0.0100  0.2164\n",
      "      2                     \u001b[36m0.5304\u001b[0m        \u001b[32m0.8692\u001b[0m                     \u001b[35m0.5410\u001b[0m        \u001b[31m0.8675\u001b[0m  0.0100  0.2234\n",
      "      3                     0.5131        0.9938                     0.5410        \u001b[31m0.7057\u001b[0m  0.0098  0.2204\n",
      "      4                     0.5276        \u001b[32m0.8239\u001b[0m                     \u001b[35m0.5746\u001b[0m        0.7736  0.0096  0.2214\n",
      "      5                     \u001b[36m0.5463\u001b[0m        \u001b[32m0.8005\u001b[0m                     0.5541        0.7932  0.0093  0.2204\n",
      "      6                     0.5435        \u001b[32m0.7843\u001b[0m                     \u001b[35m0.6045\u001b[0m        \u001b[31m0.6764\u001b[0m  0.0090  0.2214\n",
      "      7                     \u001b[36m0.5500\u001b[0m        \u001b[32m0.7493\u001b[0m                     \u001b[35m0.6175\u001b[0m        \u001b[31m0.6723\u001b[0m  0.0085  0.2224\n",
      "      8                     \u001b[36m0.5659\u001b[0m        \u001b[32m0.6964\u001b[0m                     0.5578        0.7105  0.0080  0.2224\n",
      "      9                     \u001b[36m0.5678\u001b[0m        0.7320                     0.6045        0.6790  0.0075  0.2224\n",
      "     10                     \u001b[36m0.5738\u001b[0m        0.7481                     0.5653        0.6767  0.0069  0.2222\n",
      "     11                     \u001b[36m0.5864\u001b[0m        \u001b[32m0.6942\u001b[0m                     0.6157        0.6849  0.0063  0.2224\n",
      "     12                     \u001b[36m0.5879\u001b[0m        \u001b[32m0.6901\u001b[0m                     0.6101        0.6728  0.0057  0.2224\n",
      "     13                     \u001b[36m0.5935\u001b[0m        \u001b[32m0.6737\u001b[0m                     0.6045        \u001b[31m0.6646\u001b[0m  0.0050  0.2214\n",
      "     14                     \u001b[36m0.6098\u001b[0m        0.6738                     \u001b[35m0.6194\u001b[0m        \u001b[31m0.6580\u001b[0m  0.0043  0.2224\n",
      "     15                     \u001b[36m0.6229\u001b[0m        \u001b[32m0.6572\u001b[0m                     0.5746        0.6689  0.0037  0.2234\n",
      "     16                     0.6178        0.6621                     0.6045        0.6634  0.0031  0.2234\n",
      "     17                     0.6224        \u001b[32m0.6570\u001b[0m                     0.5560        0.6820  0.0025  0.2234\n",
      "     18                     0.6126        0.6726                     0.6026        0.6699  0.0020  0.2204\n",
      "     19                     0.6220        \u001b[32m0.6548\u001b[0m                     0.5634        0.6764  0.0015  0.2234\n",
      "     20                     \u001b[36m0.6364\u001b[0m        \u001b[32m0.6397\u001b[0m                     \u001b[35m0.6343\u001b[0m        0.6654  0.0010  0.2224\n",
      "     21                     0.6243        0.6399                     0.5896        0.6686  0.0007  0.2234\n",
      "     22                     0.6285        0.6401                     0.5933        0.6719  0.0004  0.2226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23                     0.6332        \u001b[32m0.6387\u001b[0m                     0.5914        0.6700  0.0002  0.2214\n",
      "     24                     \u001b[36m0.6463\u001b[0m        \u001b[32m0.6338\u001b[0m                     0.5858        0.6705  0.0000  0.2234\n",
      "     25                     0.6360        0.6350                     0.5896        0.6697  0.0000  0.2234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7089\u001b[0m        \u001b[32m1.3592\u001b[0m                     \u001b[35m0.8060\u001b[0m        \u001b[31m0.7920\u001b[0m  0.0100  0.2134\n",
      "      2                     \u001b[36m0.8019\u001b[0m        \u001b[32m0.4982\u001b[0m                     0.7873        \u001b[31m0.5207\u001b[0m  0.0100  0.2214\n",
      "      3                     \u001b[36m0.8187\u001b[0m        \u001b[32m0.4371\u001b[0m                     0.7966        \u001b[31m0.4224\u001b[0m  0.0098  0.2214\n",
      "      4                     0.8154        \u001b[32m0.4275\u001b[0m                     \u001b[35m0.8284\u001b[0m        \u001b[31m0.3893\u001b[0m  0.0096  0.2224\n",
      "      5                     \u001b[36m0.8313\u001b[0m        \u001b[32m0.4130\u001b[0m                     \u001b[35m0.8340\u001b[0m        0.3932  0.0093  0.2234\n",
      "      6                     \u001b[36m0.8388\u001b[0m        \u001b[32m0.4012\u001b[0m                     0.8340        \u001b[31m0.3885\u001b[0m  0.0090  0.2224\n",
      "      7                     \u001b[36m0.8505\u001b[0m        \u001b[32m0.3739\u001b[0m                     \u001b[35m0.8507\u001b[0m        0.4074  0.0085  0.2214\n",
      "      8                     \u001b[36m0.8533\u001b[0m        \u001b[32m0.3570\u001b[0m                     \u001b[35m0.8638\u001b[0m        \u001b[31m0.3468\u001b[0m  0.0080  0.2214\n",
      "      9                     0.8505        \u001b[32m0.3497\u001b[0m                     0.8619        0.3468  0.0075  0.2214\n",
      "     10                     \u001b[36m0.8640\u001b[0m        \u001b[32m0.3378\u001b[0m                     0.8582        0.3473  0.0069  0.2214\n",
      "     11                     0.8636        \u001b[32m0.3290\u001b[0m                     0.8265        0.3770  0.0063  0.2205\n",
      "     12                     \u001b[36m0.8752\u001b[0m        \u001b[32m0.3182\u001b[0m                     0.8601        \u001b[31m0.3273\u001b[0m  0.0057  0.2208\n",
      "     13                     \u001b[36m0.8762\u001b[0m        \u001b[32m0.3117\u001b[0m                     0.8619        0.3328  0.0050  0.2234\n",
      "     14                     \u001b[36m0.8893\u001b[0m        \u001b[32m0.2887\u001b[0m                     \u001b[35m0.8675\u001b[0m        \u001b[31m0.3208\u001b[0m  0.0043  0.2214\n",
      "     15                     0.8850        \u001b[32m0.2796\u001b[0m                     \u001b[35m0.8769\u001b[0m        \u001b[31m0.3143\u001b[0m  0.0037  0.2224\n",
      "     16                     \u001b[36m0.8944\u001b[0m        \u001b[32m0.2701\u001b[0m                     0.8713        \u001b[31m0.3042\u001b[0m  0.0031  0.2217\n",
      "     17                     0.8921        \u001b[32m0.2636\u001b[0m                     \u001b[35m0.8787\u001b[0m        \u001b[31m0.3025\u001b[0m  0.0025  0.2214\n",
      "     18                     \u001b[36m0.8944\u001b[0m        \u001b[32m0.2466\u001b[0m                     0.8769        0.3071  0.0020  0.2224\n",
      "     19                     \u001b[36m0.9023\u001b[0m        \u001b[32m0.2430\u001b[0m                     \u001b[35m0.8918\u001b[0m        \u001b[31m0.3013\u001b[0m  0.0015  0.2224\n",
      "     20                     0.8991        0.2478                     0.8843        \u001b[31m0.2996\u001b[0m  0.0010  0.2224\n",
      "     21                     \u001b[36m0.9042\u001b[0m        \u001b[32m0.2398\u001b[0m                     0.8806        0.3013  0.0007  0.2227\n",
      "     22                     \u001b[36m0.9140\u001b[0m        \u001b[32m0.2289\u001b[0m                     0.8825        0.3024  0.0004  0.2224\n",
      "     23                     0.9089        0.2384                     0.8825        0.3057  0.0002  0.2224\n",
      "     24                     0.9140        0.2324                     0.8787        0.3066  0.0000  0.2238\n",
      "     25                     \u001b[36m0.9201\u001b[0m        \u001b[32m0.2240\u001b[0m                     0.8769        0.3100  0.0000  0.2224\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7327\u001b[0m        \u001b[32m1.0974\u001b[0m                     \u001b[35m0.8284\u001b[0m        \u001b[31m0.4227\u001b[0m  0.0100  0.2164\n",
      "      2                     \u001b[36m0.7888\u001b[0m        \u001b[32m0.5427\u001b[0m                     \u001b[35m0.8545\u001b[0m        \u001b[31m0.3719\u001b[0m  0.0100  0.2224\n",
      "      3                     \u001b[36m0.8126\u001b[0m        \u001b[32m0.4461\u001b[0m                     \u001b[35m0.8750\u001b[0m        \u001b[31m0.3619\u001b[0m  0.0098  0.2214\n",
      "      4                     \u001b[36m0.8210\u001b[0m        \u001b[32m0.4222\u001b[0m                     0.8619        0.3934  0.0096  0.2224\n",
      "      5                     \u001b[36m0.8355\u001b[0m        \u001b[32m0.3982\u001b[0m                     0.8582        \u001b[31m0.3540\u001b[0m  0.0093  0.2201\n",
      "      6                     \u001b[36m0.8360\u001b[0m        \u001b[32m0.3981\u001b[0m                     \u001b[35m0.8881\u001b[0m        \u001b[31m0.3237\u001b[0m  0.0090  0.2214\n",
      "      7                     \u001b[36m0.8393\u001b[0m        \u001b[32m0.3743\u001b[0m                     0.8713        0.3250  0.0085  0.2224\n",
      "      8                     \u001b[36m0.8505\u001b[0m        \u001b[32m0.3712\u001b[0m                     0.8787        \u001b[31m0.3235\u001b[0m  0.0080  0.2224\n",
      "      9                     \u001b[36m0.8593\u001b[0m        \u001b[32m0.3420\u001b[0m                     0.8526        0.3558  0.0075  0.2214\n",
      "     10                     0.8593        0.3525                     0.8526        0.3733  0.0069  0.2230\n",
      "     11                     0.8533        \u001b[32m0.3418\u001b[0m                     0.8582        0.3281  0.0063  0.2264\n",
      "     12                     \u001b[36m0.8734\u001b[0m        \u001b[32m0.3072\u001b[0m                     0.8675        0.3404  0.0057  0.2224\n",
      "     13                     0.8706        0.3202                     0.8638        0.3549  0.0050  0.2214\n",
      "     14                     0.8650        0.3284                     \u001b[35m0.8955\u001b[0m        \u001b[31m0.3065\u001b[0m  0.0043  0.2224\n",
      "     15                     \u001b[36m0.8897\u001b[0m        \u001b[32m0.2779\u001b[0m                     0.8937        \u001b[31m0.2979\u001b[0m  0.0037  0.2234\n",
      "     16                     0.8827        0.2801                     0.8843        \u001b[31m0.2862\u001b[0m  0.0031  0.2214\n",
      "     17                     0.8879        \u001b[32m0.2699\u001b[0m                     \u001b[35m0.8993\u001b[0m        0.3057  0.0025  0.2216\n",
      "     18                     0.8860        0.2758                     0.8955        0.2986  0.0020  0.2214\n",
      "     19                     \u001b[36m0.8902\u001b[0m        \u001b[32m0.2583\u001b[0m                     0.8843        0.3024  0.0015  0.2194\n",
      "     20                     \u001b[36m0.9037\u001b[0m        \u001b[32m0.2523\u001b[0m                     0.8974        \u001b[31m0.2814\u001b[0m  0.0010  0.2235\n",
      "     21                     \u001b[36m0.9098\u001b[0m        \u001b[32m0.2364\u001b[0m                     \u001b[35m0.9011\u001b[0m        \u001b[31m0.2771\u001b[0m  0.0007  0.2363\n",
      "     22                     0.9028        0.2423                     0.8918        0.2772  0.0004  0.2274\n",
      "     23                     0.9019        0.2374                     0.9011        0.2771  0.0002  0.2244\n",
      "     24                     0.9098        \u001b[32m0.2305\u001b[0m                     0.8937        \u001b[31m0.2766\u001b[0m  0.0000  0.2234\n",
      "     25                     0.9056        0.2320                     0.8955        \u001b[31m0.2762\u001b[0m  0.0000  0.2224\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7364\u001b[0m        \u001b[32m1.0250\u001b[0m                     \u001b[35m0.8190\u001b[0m        \u001b[31m0.5737\u001b[0m  0.0100  0.2174\n",
      "      2                     \u001b[36m0.8126\u001b[0m        \u001b[32m0.4889\u001b[0m                     \u001b[35m0.8396\u001b[0m        \u001b[31m0.3776\u001b[0m  0.0100  0.2224\n",
      "      3                     0.8042        0.5149                     0.8302        0.4091  0.0098  0.2224\n",
      "      4                     0.8019        \u001b[32m0.4743\u001b[0m                     0.8116        0.4000  0.0096  0.2234\n",
      "      5                     \u001b[36m0.8318\u001b[0m        \u001b[32m0.3859\u001b[0m                     \u001b[35m0.8675\u001b[0m        \u001b[31m0.3495\u001b[0m  0.0093  0.2383\n",
      "      6                     \u001b[36m0.8519\u001b[0m        \u001b[32m0.3712\u001b[0m                     \u001b[35m0.8713\u001b[0m        \u001b[31m0.3295\u001b[0m  0.0090  0.2244\n",
      "      7                     0.8495        \u001b[32m0.3665\u001b[0m                     0.8470        0.3814  0.0085  0.2224\n",
      "      8                     \u001b[36m0.8533\u001b[0m        0.3753                     0.8619        0.3440  0.0080  0.2224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.8551\u001b[0m        \u001b[32m0.3618\u001b[0m                     0.8545        0.3540  0.0075  0.2184\n",
      "     10                     0.8449        0.3838                     0.8451        0.3562  0.0069  0.2214\n",
      "     11                     \u001b[36m0.8603\u001b[0m        \u001b[32m0.3324\u001b[0m                     \u001b[35m0.8993\u001b[0m        \u001b[31m0.2904\u001b[0m  0.0063  0.2234\n",
      "     12                     \u001b[36m0.8785\u001b[0m        \u001b[32m0.3003\u001b[0m                     0.8899        \u001b[31m0.2770\u001b[0m  0.0057  0.2234\n",
      "     13                     0.8766        0.3162                     0.8881        0.2806  0.0050  0.2224\n",
      "     14                     \u001b[36m0.8832\u001b[0m        \u001b[32m0.2909\u001b[0m                     0.8918        \u001b[31m0.2762\u001b[0m  0.0043  0.2196\n",
      "     15                     0.8832        \u001b[32m0.2886\u001b[0m                     \u001b[35m0.9030\u001b[0m        \u001b[31m0.2733\u001b[0m  0.0037  0.2234\n",
      "     16                     0.8827        \u001b[32m0.2832\u001b[0m                     0.8619        0.2930  0.0031  0.2220\n",
      "     17                     \u001b[36m0.8949\u001b[0m        \u001b[32m0.2706\u001b[0m                     0.8881        0.2819  0.0025  0.2234\n",
      "     18                     0.8939        \u001b[32m0.2680\u001b[0m                     0.8899        \u001b[31m0.2587\u001b[0m  0.0020  0.2224\n",
      "     19                     \u001b[36m0.9009\u001b[0m        \u001b[32m0.2487\u001b[0m                     0.9030        \u001b[31m0.2550\u001b[0m  0.0015  0.2224\n",
      "     20                     \u001b[36m0.9028\u001b[0m        \u001b[32m0.2474\u001b[0m                     0.8937        0.2645  0.0010  0.2232\n",
      "     21                     0.9028        \u001b[32m0.2376\u001b[0m                     0.8937        0.2663  0.0007  0.2184\n",
      "     22                     \u001b[36m0.9051\u001b[0m        0.2405                     0.8918        0.2670  0.0004  0.2214\n",
      "     23                     \u001b[36m0.9079\u001b[0m        \u001b[32m0.2267\u001b[0m                     0.9011        0.2605  0.0002  0.2224\n",
      "     24                     \u001b[36m0.9089\u001b[0m        \u001b[32m0.2237\u001b[0m                     0.8974        0.2616  0.0000  0.2214\n",
      "     25                     0.9023        0.2389                     0.8955        0.2625  0.0000  0.2214\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7107\u001b[0m        \u001b[32m1.2279\u001b[0m                     \u001b[35m0.8321\u001b[0m        \u001b[31m0.4010\u001b[0m  0.0100  0.2164\n",
      "      2                     \u001b[36m0.7967\u001b[0m        \u001b[32m0.5023\u001b[0m                     0.8041        0.4693  0.0100  0.2224\n",
      "      3                     0.7822        0.5825                     0.8209        0.4148  0.0098  0.2204\n",
      "      4                     \u001b[36m0.8033\u001b[0m        0.5053                     0.8321        \u001b[31m0.3767\u001b[0m  0.0096  0.2209\n",
      "      5                     \u001b[36m0.8290\u001b[0m        \u001b[32m0.4476\u001b[0m                     0.7929        0.5661  0.0093  0.2222\n",
      "      6                     \u001b[36m0.8341\u001b[0m        \u001b[32m0.3921\u001b[0m                     0.8134        0.4730  0.0090  0.2224\n",
      "      7                     0.8037        0.5179                     0.8172        0.5131  0.0085  0.2204\n",
      "      8                     \u001b[36m0.8495\u001b[0m        0.3963                     \u001b[35m0.8750\u001b[0m        \u001b[31m0.3041\u001b[0m  0.0080  0.2244\n",
      "      9                     \u001b[36m0.8575\u001b[0m        \u001b[32m0.3448\u001b[0m                     0.8619        \u001b[31m0.3022\u001b[0m  0.0075  0.2254\n",
      "     10                     \u001b[36m0.8650\u001b[0m        \u001b[32m0.3421\u001b[0m                     0.8675        0.3070  0.0069  0.2234\n",
      "     11                     0.8636        \u001b[32m0.3374\u001b[0m                     \u001b[35m0.8843\u001b[0m        \u001b[31m0.2881\u001b[0m  0.0063  0.2224\n",
      "     12                     \u001b[36m0.8738\u001b[0m        \u001b[32m0.3172\u001b[0m                     0.8731        0.2902  0.0057  0.2234\n",
      "     13                     \u001b[36m0.8748\u001b[0m        \u001b[32m0.3138\u001b[0m                     0.8731        0.2953  0.0050  0.2234\n",
      "     14                     \u001b[36m0.8808\u001b[0m        \u001b[32m0.3001\u001b[0m                     0.8694        0.2957  0.0043  0.2234\n",
      "     15                     \u001b[36m0.8897\u001b[0m        \u001b[32m0.2872\u001b[0m                     \u001b[35m0.8881\u001b[0m        \u001b[31m0.2756\u001b[0m  0.0037  0.2214\n",
      "     16                     0.8813        \u001b[32m0.2864\u001b[0m                     0.8843        0.2786  0.0031  0.2244\n",
      "     17                     0.8869        \u001b[32m0.2781\u001b[0m                     \u001b[35m0.8974\u001b[0m        \u001b[31m0.2697\u001b[0m  0.0025  0.2244\n",
      "     18                     \u001b[36m0.8967\u001b[0m        \u001b[32m0.2699\u001b[0m                     0.8881        0.2741  0.0020  0.2224\n",
      "     19                     0.8911        \u001b[32m0.2657\u001b[0m                     0.8918        \u001b[31m0.2618\u001b[0m  0.0015  0.2234\n",
      "     20                     \u001b[36m0.8995\u001b[0m        \u001b[32m0.2564\u001b[0m                     \u001b[35m0.8993\u001b[0m        0.2661  0.0010  0.2214\n",
      "     21                     \u001b[36m0.9023\u001b[0m        \u001b[32m0.2455\u001b[0m                     0.8993        0.2640  0.0007  0.2222\n",
      "     22                     0.9005        0.2497                     \u001b[35m0.9049\u001b[0m        \u001b[31m0.2594\u001b[0m  0.0004  0.2204\n",
      "     23                     \u001b[36m0.9093\u001b[0m        \u001b[32m0.2405\u001b[0m                     0.8993        0.2602  0.0002  0.2184\n",
      "     24                     0.9051        0.2439                     \u001b[35m0.9067\u001b[0m        0.2597  0.0000  0.2224\n",
      "     25                     0.9028        \u001b[32m0.2382\u001b[0m                     0.9011        \u001b[31m0.2591\u001b[0m  0.0000  0.2214\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7364\u001b[0m        \u001b[32m0.9654\u001b[0m                     \u001b[35m0.8321\u001b[0m        \u001b[31m0.4800\u001b[0m  0.0100  0.2164\n",
      "      2                     \u001b[36m0.8056\u001b[0m        \u001b[32m0.4557\u001b[0m                     0.8060        \u001b[31m0.4500\u001b[0m  0.0100  0.2254\n",
      "      3                     \u001b[36m0.8182\u001b[0m        0.4750                     0.7575        0.6479  0.0098  0.2234\n",
      "      4                     0.7832        0.6790                     0.8284        \u001b[31m0.4182\u001b[0m  0.0096  0.2214\n",
      "      5                     \u001b[36m0.8308\u001b[0m        \u001b[32m0.4032\u001b[0m                     0.8172        \u001b[31m0.3998\u001b[0m  0.0093  0.2214\n",
      "      6                     \u001b[36m0.8393\u001b[0m        \u001b[32m0.3927\u001b[0m                     \u001b[35m0.8470\u001b[0m        \u001b[31m0.3692\u001b[0m  0.0090  0.2214\n",
      "      7                     \u001b[36m0.8430\u001b[0m        \u001b[32m0.3813\u001b[0m                     0.8302        0.3870  0.0085  0.2234\n",
      "      8                     \u001b[36m0.8458\u001b[0m        \u001b[32m0.3473\u001b[0m                     0.8433        \u001b[31m0.3637\u001b[0m  0.0080  0.2234\n",
      "      9                     \u001b[36m0.8603\u001b[0m        0.3480                     \u001b[35m0.8601\u001b[0m        \u001b[31m0.3434\u001b[0m  0.0075  0.2224\n",
      "     10                     \u001b[36m0.8617\u001b[0m        \u001b[32m0.3383\u001b[0m                     \u001b[35m0.8731\u001b[0m        \u001b[31m0.3302\u001b[0m  0.0069  0.2234\n",
      "     11                     \u001b[36m0.8701\u001b[0m        \u001b[32m0.3267\u001b[0m                     0.8694        \u001b[31m0.3276\u001b[0m  0.0063  0.2214\n",
      "     12                     \u001b[36m0.8780\u001b[0m        \u001b[32m0.2980\u001b[0m                     0.8582        \u001b[31m0.3267\u001b[0m  0.0057  0.2244\n",
      "     13                     \u001b[36m0.8794\u001b[0m        \u001b[32m0.2919\u001b[0m                     0.8451        0.3540  0.0050  0.2234\n",
      "     14                     0.8790        \u001b[32m0.2856\u001b[0m                     0.8713        0.3428  0.0043  0.2244\n",
      "     15                     \u001b[36m0.8850\u001b[0m        0.2885                     0.8638        0.3398  0.0037  0.2214\n",
      "     16                     0.8841        0.2899                     \u001b[35m0.8750\u001b[0m        0.3361  0.0031  0.2230\n",
      "     17                     \u001b[36m0.8949\u001b[0m        \u001b[32m0.2613\u001b[0m                     0.8545        0.3457  0.0025  0.2224\n",
      "     18                     \u001b[36m0.8963\u001b[0m        0.2615                     0.8731        0.3300  0.0020  0.2224\n",
      "     19                     0.8916        \u001b[32m0.2556\u001b[0m                     0.8675        \u001b[31m0.3232\u001b[0m  0.0015  0.2254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                     \u001b[36m0.9014\u001b[0m        \u001b[32m0.2491\u001b[0m                     \u001b[35m0.8825\u001b[0m        \u001b[31m0.3147\u001b[0m  0.0010  0.2244\n",
      "     21                     \u001b[36m0.9047\u001b[0m        \u001b[32m0.2411\u001b[0m                     0.8713        0.3213  0.0007  0.2234\n",
      "     22                     0.8953        \u001b[32m0.2403\u001b[0m                     0.8806        \u001b[31m0.3129\u001b[0m  0.0004  0.2224\n",
      "     23                     0.9033        0.2443                     0.8769        0.3155  0.0002  0.2174\n",
      "     24                     \u001b[36m0.9065\u001b[0m        \u001b[32m0.2334\u001b[0m                     0.8731        0.3157  0.0000  0.2204\n",
      "     25                     0.8958        0.2436                     0.8806        \u001b[31m0.3129\u001b[0m  0.0000  0.2224\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7028\u001b[0m        \u001b[32m1.1225\u001b[0m                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.6473\u001b[0m  0.0100  0.2164\n",
      "      2                     \u001b[36m0.8023\u001b[0m        \u001b[32m0.4851\u001b[0m                     \u001b[35m0.8340\u001b[0m        \u001b[31m0.4366\u001b[0m  0.0100  0.2214\n",
      "      3                     \u001b[36m0.8098\u001b[0m        \u001b[32m0.4744\u001b[0m                     0.8041        0.5265  0.0098  0.2224\n",
      "      4                     \u001b[36m0.8257\u001b[0m        \u001b[32m0.4143\u001b[0m                     0.8004        0.5153  0.0096  0.2214\n",
      "      5                     0.8173        0.4833                     0.8097        0.4667  0.0093  0.2234\n",
      "      6                     \u001b[36m0.8477\u001b[0m        \u001b[32m0.3722\u001b[0m                     \u001b[35m0.8414\u001b[0m        \u001b[31m0.4153\u001b[0m  0.0090  0.2224\n",
      "      7                     0.8411        \u001b[32m0.3696\u001b[0m                     \u001b[35m0.8470\u001b[0m        0.4828  0.0085  0.2214\n",
      "      8                     \u001b[36m0.8645\u001b[0m        \u001b[32m0.3407\u001b[0m                     0.8302        0.4442  0.0080  0.2194\n",
      "      9                     0.8458        0.3589                     0.8396        \u001b[31m0.4132\u001b[0m  0.0075  0.2194\n",
      "     10                     0.8640        \u001b[32m0.3380\u001b[0m                     \u001b[35m0.8507\u001b[0m        \u001b[31m0.4071\u001b[0m  0.0069  0.2224\n",
      "     11                     \u001b[36m0.8790\u001b[0m        \u001b[32m0.3133\u001b[0m                     \u001b[35m0.8526\u001b[0m        0.4100  0.0063  0.2224\n",
      "     12                     0.8790        \u001b[32m0.3085\u001b[0m                     0.8451        \u001b[31m0.3988\u001b[0m  0.0057  0.2224\n",
      "     13                     0.8617        0.3250                     \u001b[35m0.8657\u001b[0m        \u001b[31m0.3949\u001b[0m  0.0050  0.2171\n",
      "     14                     \u001b[36m0.8850\u001b[0m        \u001b[32m0.2838\u001b[0m                     0.8601        \u001b[31m0.3756\u001b[0m  0.0043  0.2175\n",
      "     15                     \u001b[36m0.8860\u001b[0m        \u001b[32m0.2755\u001b[0m                     0.8433        0.3826  0.0037  0.2224\n",
      "     16                     0.8846        0.2794                     0.8582        0.3987  0.0031  0.2274\n",
      "     17                     \u001b[36m0.8939\u001b[0m        \u001b[32m0.2578\u001b[0m                     0.8433        0.4093  0.0025  0.2214\n",
      "     18                     \u001b[36m0.8958\u001b[0m        \u001b[32m0.2572\u001b[0m                     0.8563        0.3884  0.0020  0.2224\n",
      "     19                     \u001b[36m0.9037\u001b[0m        \u001b[32m0.2472\u001b[0m                     0.8545        0.3767  0.0015  0.2234\n",
      "     20                     \u001b[36m0.9089\u001b[0m        \u001b[32m0.2378\u001b[0m                     0.8451        0.3955  0.0010  0.2224\n",
      "     21                     0.9023        0.2479                     0.8451        0.3953  0.0007  0.2224\n",
      "     22                     \u001b[36m0.9117\u001b[0m        0.2407                     0.8545        0.3887  0.0004  0.2224\n",
      "     23                     0.9103        \u001b[32m0.2324\u001b[0m                     0.8601        0.3825  0.0002  0.2210\n",
      "     24                     0.9065        0.2419                     0.8582        0.3818  0.0000  0.2194\n",
      "     25                     0.9117        0.2338                     0.8582        0.3834  0.0000  0.2254\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7308\u001b[0m        \u001b[32m1.0557\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.7118\u001b[0m  0.0100  0.2164\n",
      "      2                     \u001b[36m0.8051\u001b[0m        \u001b[32m0.4647\u001b[0m                     \u001b[35m0.8433\u001b[0m        \u001b[31m0.4262\u001b[0m  0.0100  0.2164\n",
      "      3                     \u001b[36m0.8093\u001b[0m        \u001b[32m0.4630\u001b[0m                     0.8041        0.5071  0.0098  0.2144\n",
      "      4                     \u001b[36m0.8131\u001b[0m        \u001b[32m0.4524\u001b[0m                     \u001b[35m0.8470\u001b[0m        \u001b[31m0.3783\u001b[0m  0.0096  0.2144\n",
      "      5                     \u001b[36m0.8276\u001b[0m        \u001b[32m0.4396\u001b[0m                     0.7761        0.4991  0.0093  0.2144\n",
      "      6                     \u001b[36m0.8346\u001b[0m        \u001b[32m0.3821\u001b[0m                     \u001b[35m0.8545\u001b[0m        0.3904  0.0090  0.2164\n",
      "      7                     \u001b[36m0.8514\u001b[0m        \u001b[32m0.3629\u001b[0m                     0.8433        0.3843  0.0085  0.2144\n",
      "      8                     0.8491        0.3696                     0.8545        \u001b[31m0.3529\u001b[0m  0.0080  0.2154\n",
      "      9                     \u001b[36m0.8626\u001b[0m        \u001b[32m0.3217\u001b[0m                     \u001b[35m0.8601\u001b[0m        0.3783  0.0075  0.2134\n",
      "     10                     0.8603        0.3419                     \u001b[35m0.8694\u001b[0m        \u001b[31m0.3377\u001b[0m  0.0069  0.2135\n",
      "     11                     \u001b[36m0.8710\u001b[0m        0.3261                     0.8545        0.3549  0.0063  0.2144\n",
      "     12                     \u001b[36m0.8799\u001b[0m        \u001b[32m0.2980\u001b[0m                     0.8507        0.3842  0.0057  0.2144\n",
      "     13                     0.8771        0.3005                     0.8582        0.3680  0.0050  0.2155\n",
      "     14                     \u001b[36m0.8855\u001b[0m        \u001b[32m0.2809\u001b[0m                     0.8470        0.3699  0.0043  0.2154\n",
      "     15                     \u001b[36m0.8907\u001b[0m        \u001b[32m0.2736\u001b[0m                     \u001b[35m0.8750\u001b[0m        0.3440  0.0037  0.2184\n",
      "     16                     \u001b[36m0.9005\u001b[0m        \u001b[32m0.2515\u001b[0m                     0.8619        0.3445  0.0031  0.2194\n",
      "     17                     \u001b[36m0.9028\u001b[0m        \u001b[32m0.2496\u001b[0m                     \u001b[35m0.8974\u001b[0m        \u001b[31m0.3253\u001b[0m  0.0025  0.2224\n",
      "     18                     \u001b[36m0.9070\u001b[0m        \u001b[32m0.2389\u001b[0m                     0.8787        0.3257  0.0020  0.2204\n",
      "     19                     0.9023        0.2489                     0.8787        0.3299  0.0015  0.2184\n",
      "     20                     \u001b[36m0.9098\u001b[0m        \u001b[32m0.2254\u001b[0m                     0.8769        0.3400  0.0010  0.2184\n",
      "     21                     0.9093        0.2338                     0.8825        \u001b[31m0.3176\u001b[0m  0.0007  0.2184\n",
      "     22                     \u001b[36m0.9103\u001b[0m        0.2268                     0.8843        0.3200  0.0004  0.2154\n",
      "     23                     \u001b[36m0.9168\u001b[0m        \u001b[32m0.2243\u001b[0m                     0.8787        0.3229  0.0002  0.2194\n",
      "     24                     0.9112        0.2250                     0.8825        0.3206  0.0000  0.2234\n",
      "     25                     0.9117        \u001b[32m0.2226\u001b[0m                     0.8843        0.3200  0.0000  0.2214\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7121\u001b[0m        \u001b[32m1.3649\u001b[0m                     \u001b[35m0.7929\u001b[0m        \u001b[31m0.6353\u001b[0m  0.0100  0.2124\n",
      "      2                     \u001b[36m0.7869\u001b[0m        \u001b[32m0.4896\u001b[0m                     \u001b[35m0.8134\u001b[0m        \u001b[31m0.4758\u001b[0m  0.0100  0.2144\n",
      "      3                     \u001b[36m0.8154\u001b[0m        0.4922                     \u001b[35m0.8265\u001b[0m        \u001b[31m0.4708\u001b[0m  0.0098  0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4                     0.8061        0.4965                     0.7799        0.5739  0.0096  0.2124\n",
      "      5                     \u001b[36m0.8234\u001b[0m        \u001b[32m0.4608\u001b[0m                     \u001b[35m0.8284\u001b[0m        \u001b[31m0.3969\u001b[0m  0.0093  0.2164\n",
      "      6                     \u001b[36m0.8481\u001b[0m        \u001b[32m0.3776\u001b[0m                     \u001b[35m0.8358\u001b[0m        \u001b[31m0.3788\u001b[0m  0.0090  0.2174\n",
      "      7                     0.8364        0.3899                     \u001b[35m0.8377\u001b[0m        0.4098  0.0085  0.2144\n",
      "      8                     0.8467        \u001b[32m0.3692\u001b[0m                     \u001b[35m0.8526\u001b[0m        \u001b[31m0.3623\u001b[0m  0.0080  0.2124\n",
      "      9                     \u001b[36m0.8607\u001b[0m        \u001b[32m0.3238\u001b[0m                     0.7705        0.5995  0.0075  0.2144\n",
      "     10                     0.8584        0.3423                     0.8451        0.4544  0.0069  0.2154\n",
      "     11                     \u001b[36m0.8640\u001b[0m        0.3354                     0.8470        0.3885  0.0063  0.2154\n",
      "     12                     \u001b[36m0.8752\u001b[0m        \u001b[32m0.2980\u001b[0m                     0.8228        0.4413  0.0057  0.2154\n",
      "     13                     \u001b[36m0.8855\u001b[0m        \u001b[32m0.2878\u001b[0m                     \u001b[35m0.8675\u001b[0m        \u001b[31m0.3524\u001b[0m  0.0050  0.2144\n",
      "     14                     \u001b[36m0.8864\u001b[0m        \u001b[32m0.2727\u001b[0m                     0.8638        0.3531  0.0043  0.2154\n",
      "     15                     \u001b[36m0.8911\u001b[0m        \u001b[32m0.2670\u001b[0m                     0.8526        0.3899  0.0037  0.2144\n",
      "     16                     \u001b[36m0.8963\u001b[0m        \u001b[32m0.2522\u001b[0m                     \u001b[35m0.8787\u001b[0m        \u001b[31m0.3403\u001b[0m  0.0031  0.2174\n",
      "     17                     \u001b[36m0.9019\u001b[0m        \u001b[32m0.2451\u001b[0m                     0.8769        0.3483  0.0025  0.2124\n",
      "     18                     \u001b[36m0.9028\u001b[0m        0.2454                     0.8638        0.3443  0.0020  0.2154\n",
      "     19                     \u001b[36m0.9037\u001b[0m        \u001b[32m0.2445\u001b[0m                     \u001b[35m0.8806\u001b[0m        0.3418  0.0015  0.2184\n",
      "     20                     \u001b[36m0.9061\u001b[0m        \u001b[32m0.2350\u001b[0m                     \u001b[35m0.8825\u001b[0m        0.3444  0.0010  0.2154\n",
      "     21                     0.9042        0.2359                     0.8787        \u001b[31m0.3362\u001b[0m  0.0007  0.2114\n",
      "     22                     \u001b[36m0.9070\u001b[0m        \u001b[32m0.2245\u001b[0m                     0.8806        0.3374  0.0004  0.2134\n",
      "     23                     \u001b[36m0.9098\u001b[0m        0.2270                     \u001b[35m0.8843\u001b[0m        \u001b[31m0.3349\u001b[0m  0.0002  0.2124\n",
      "     24                     0.9093        \u001b[32m0.2228\u001b[0m                     0.8843        0.3359  0.0000  0.2145\n",
      "     25                     \u001b[36m0.9201\u001b[0m        \u001b[32m0.2191\u001b[0m                     0.8825        0.3373  0.0000  0.2154\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6972\u001b[0m        \u001b[32m1.2933\u001b[0m                     \u001b[35m0.8097\u001b[0m        \u001b[31m0.4732\u001b[0m  0.0100  0.2114\n",
      "      2                     \u001b[36m0.8089\u001b[0m        \u001b[32m0.4690\u001b[0m                     0.7444        0.5927  0.0100  0.2132\n",
      "      3                     \u001b[36m0.8107\u001b[0m        0.4733                     0.7948        \u001b[31m0.4363\u001b[0m  0.0098  0.2124\n",
      "      4                     0.7935        0.5424                     \u001b[35m0.8153\u001b[0m        0.4369  0.0096  0.2119\n",
      "      5                     \u001b[36m0.8341\u001b[0m        \u001b[32m0.3983\u001b[0m                     \u001b[35m0.8358\u001b[0m        \u001b[31m0.3685\u001b[0m  0.0093  0.2138\n",
      "      6                     \u001b[36m0.8486\u001b[0m        \u001b[32m0.3743\u001b[0m                     0.8190        0.3797  0.0090  0.2135\n",
      "      7                     0.8350        0.4112                     0.8060        0.4278  0.0085  0.2138\n",
      "      8                     0.8425        0.3885                     \u001b[35m0.8563\u001b[0m        0.4067  0.0080  0.2145\n",
      "      9                     \u001b[36m0.8491\u001b[0m        \u001b[32m0.3659\u001b[0m                     \u001b[35m0.8638\u001b[0m        0.3775  0.0075  0.2128\n",
      "     10                     \u001b[36m0.8598\u001b[0m        \u001b[32m0.3574\u001b[0m                     0.8601        \u001b[31m0.3502\u001b[0m  0.0069  0.2135\n",
      "     11                     \u001b[36m0.8715\u001b[0m        \u001b[32m0.3128\u001b[0m                     \u001b[35m0.8657\u001b[0m        \u001b[31m0.3252\u001b[0m  0.0063  0.2134\n",
      "     12                     \u001b[36m0.8766\u001b[0m        \u001b[32m0.2978\u001b[0m                     0.8545        0.3368  0.0057  0.2149\n",
      "     13                     \u001b[36m0.8818\u001b[0m        \u001b[32m0.2912\u001b[0m                     \u001b[35m0.8731\u001b[0m        0.3332  0.0050  0.2130\n",
      "     14                     \u001b[36m0.8874\u001b[0m        \u001b[32m0.2873\u001b[0m                     0.8619        \u001b[31m0.3156\u001b[0m  0.0043  0.2145\n",
      "     15                     0.8818        \u001b[32m0.2815\u001b[0m                     0.8545        0.3610  0.0037  0.2165\n",
      "     16                     \u001b[36m0.8958\u001b[0m        \u001b[32m0.2704\u001b[0m                     \u001b[35m0.8750\u001b[0m        0.3303  0.0031  0.2125\n",
      "     17                     0.8953        \u001b[32m0.2585\u001b[0m                     0.8489        0.3206  0.0025  0.2125\n",
      "     18                     \u001b[36m0.9028\u001b[0m        0.2602                     0.8638        \u001b[31m0.3142\u001b[0m  0.0020  0.2135\n",
      "     19                     \u001b[36m0.9047\u001b[0m        \u001b[32m0.2558\u001b[0m                     0.8675        0.3228  0.0015  0.2852\n",
      "     20                     0.9028        \u001b[32m0.2400\u001b[0m                     0.8526        0.3144  0.0010  0.2144\n",
      "     21                     \u001b[36m0.9107\u001b[0m        \u001b[32m0.2341\u001b[0m                     0.8675        0.3199  0.0007  0.2135\n",
      "     22                     \u001b[36m0.9126\u001b[0m        \u001b[32m0.2287\u001b[0m                     0.8563        \u001b[31m0.3138\u001b[0m  0.0004  0.2135\n",
      "     23                     0.9056        0.2325                     0.8619        \u001b[31m0.3130\u001b[0m  0.0002  0.2135\n",
      "     24                     0.9098        0.2349                     0.8619        0.3159  0.0000  0.2141\n",
      "     25                     0.9098        0.2337                     0.8563        0.3146  0.0000  0.2140\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7322\u001b[0m        \u001b[32m1.0927\u001b[0m                     \u001b[35m0.7612\u001b[0m        \u001b[31m0.5479\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.8224\u001b[0m        \u001b[32m0.4294\u001b[0m                     \u001b[35m0.8433\u001b[0m        \u001b[31m0.4180\u001b[0m  0.0100  0.2135\n",
      "      3                     0.8220        0.4409                     0.8396        \u001b[31m0.3819\u001b[0m  0.0098  0.2139\n",
      "      4                     0.8201        0.4408                     0.8078        0.4466  0.0096  0.2137\n",
      "      5                     \u001b[36m0.8271\u001b[0m        0.4377                     \u001b[35m0.8507\u001b[0m        0.3950  0.0093  0.2128\n",
      "      6                     \u001b[36m0.8584\u001b[0m        \u001b[32m0.3531\u001b[0m                     0.8116        0.4345  0.0090  0.2184\n",
      "      7                     0.8407        0.4152                     0.8451        \u001b[31m0.3572\u001b[0m  0.0085  0.2130\n",
      "      8                     0.8556        0.3672                     \u001b[35m0.8601\u001b[0m        \u001b[31m0.3524\u001b[0m  0.0080  0.2127\n",
      "      9                     \u001b[36m0.8607\u001b[0m        \u001b[32m0.3514\u001b[0m                     0.8377        0.3591  0.0075  0.2134\n",
      "     10                     0.8598        0.3630                     0.8526        0.3535  0.0069  0.2124\n",
      "     11                     \u001b[36m0.8701\u001b[0m        \u001b[32m0.3180\u001b[0m                     0.8489        0.4362  0.0063  0.2137\n",
      "     12                     0.8593        0.3263                     0.8470        0.3663  0.0057  0.2137\n",
      "     13                     0.8701        \u001b[32m0.3127\u001b[0m                     0.8414        0.3793  0.0050  0.2130\n",
      "     14                     \u001b[36m0.8893\u001b[0m        \u001b[32m0.2810\u001b[0m                     0.8340        0.3763  0.0043  0.2136\n",
      "     15                     0.8836        \u001b[32m0.2760\u001b[0m                     \u001b[35m0.8694\u001b[0m        \u001b[31m0.3123\u001b[0m  0.0037  0.2143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16                     \u001b[36m0.9009\u001b[0m        \u001b[32m0.2576\u001b[0m                     0.8601        0.3333  0.0031  0.2128\n",
      "     17                     0.8888        0.2584                     0.8563        0.3550  0.0025  0.2129\n",
      "     18                     0.8897        0.2644                     0.8657        0.3307  0.0020  0.2135\n",
      "     19                     \u001b[36m0.9033\u001b[0m        \u001b[32m0.2435\u001b[0m                     0.8694        0.3267  0.0015  0.2135\n",
      "     20                     \u001b[36m0.9061\u001b[0m        \u001b[32m0.2386\u001b[0m                     0.8582        0.3369  0.0010  0.2130\n",
      "     21                     0.9009        0.2404                     0.8657        0.3194  0.0007  0.2148\n",
      "     22                     0.9047        \u001b[32m0.2318\u001b[0m                     0.8638        0.3253  0.0004  0.2128\n",
      "     23                     0.9056        0.2324                     0.8619        0.3273  0.0002  0.2116\n",
      "     24                     0.9051        0.2370                     0.8619        0.3281  0.0000  0.2116\n",
      "     25                     \u001b[36m0.9089\u001b[0m        \u001b[32m0.2221\u001b[0m                     0.8601        0.3227  0.0000  0.2105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5383\u001b[0m        \u001b[32m1.5496\u001b[0m                     \u001b[35m0.5261\u001b[0m        \u001b[31m1.5090\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.6519\u001b[0m        \u001b[32m0.7145\u001b[0m                     \u001b[35m0.6810\u001b[0m        \u001b[31m0.5977\u001b[0m  0.0100  0.2145\n",
      "      3                     \u001b[36m0.7093\u001b[0m        \u001b[32m0.6430\u001b[0m                     0.6567        0.7590  0.0098  0.2132\n",
      "      4                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.6072\u001b[0m                     \u001b[35m0.6940\u001b[0m        0.6557  0.0096  0.2134\n",
      "      5                     \u001b[36m0.7360\u001b[0m        \u001b[32m0.5834\u001b[0m                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5871\u001b[0m  0.0093  0.2135\n",
      "      6                     \u001b[36m0.7542\u001b[0m        \u001b[32m0.5399\u001b[0m                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.4943\u001b[0m  0.0090  0.2157\n",
      "      7                     \u001b[36m0.7738\u001b[0m        \u001b[32m0.4736\u001b[0m                     0.7425        0.5409  0.0085  0.2085\n",
      "      8                     0.7738        0.4983                     \u001b[35m0.7780\u001b[0m        \u001b[31m0.4920\u001b[0m  0.0080  0.2118\n",
      "      9                     \u001b[36m0.7930\u001b[0m        \u001b[32m0.4552\u001b[0m                     0.7425        0.5292  0.0075  0.2124\n",
      "     10                     0.7930        0.4574                     0.7780        0.5059  0.0069  0.2125\n",
      "     11                     \u001b[36m0.8014\u001b[0m        \u001b[32m0.4430\u001b[0m                     0.7556        0.5398  0.0063  0.2124\n",
      "     12                     \u001b[36m0.8051\u001b[0m        \u001b[32m0.4301\u001b[0m                     0.7668        0.5113  0.0057  0.2124\n",
      "     13                     \u001b[36m0.8084\u001b[0m        \u001b[32m0.4237\u001b[0m                     0.7724        \u001b[31m0.4815\u001b[0m  0.0050  0.2127\n",
      "     14                     \u001b[36m0.8126\u001b[0m        \u001b[32m0.4127\u001b[0m                     \u001b[35m0.7799\u001b[0m        0.4819  0.0043  0.2122\n",
      "     15                     \u001b[36m0.8187\u001b[0m        \u001b[32m0.4056\u001b[0m                     0.7593        0.5163  0.0037  0.2119\n",
      "     16                     \u001b[36m0.8262\u001b[0m        \u001b[32m0.3912\u001b[0m                     \u001b[35m0.7929\u001b[0m        \u001b[31m0.4678\u001b[0m  0.0031  0.2122\n",
      "     17                     \u001b[36m0.8355\u001b[0m        \u001b[32m0.3770\u001b[0m                     0.7743        0.4829  0.0025  0.2090\n",
      "     18                     0.8304        \u001b[32m0.3748\u001b[0m                     0.7892        0.4849  0.0020  0.2124\n",
      "     19                     0.8336        \u001b[32m0.3716\u001b[0m                     0.7892        0.4803  0.0015  0.2128\n",
      "     20                     \u001b[36m0.8402\u001b[0m        \u001b[32m0.3518\u001b[0m                     0.7854        0.4816  0.0010  0.2129\n",
      "     21                     \u001b[36m0.8463\u001b[0m        0.3539                     0.7892        0.4831  0.0007  0.2123\n",
      "     22                     \u001b[36m0.8607\u001b[0m        \u001b[32m0.3404\u001b[0m                     0.7910        0.4717  0.0004  0.2125\n",
      "     23                     0.8393        0.3534                     0.7873        0.4740  0.0002  0.2119\n",
      "     24                     0.8355        0.3587                     0.7910        0.4712  0.0000  0.2154\n",
      "     25                     0.8551        0.3416                     0.7854        0.4722  0.0000  0.2125\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5463\u001b[0m        \u001b[32m1.6149\u001b[0m                     \u001b[35m0.5448\u001b[0m        \u001b[31m1.2803\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.6145\u001b[0m        \u001b[32m0.9026\u001b[0m                     \u001b[35m0.6698\u001b[0m        \u001b[31m0.6312\u001b[0m  0.0100  0.2161\n",
      "      3                     \u001b[36m0.6888\u001b[0m        \u001b[32m0.6766\u001b[0m                     0.6101        0.8588  0.0098  0.2182\n",
      "      4                     \u001b[36m0.7065\u001b[0m        \u001b[32m0.6675\u001b[0m                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5871\u001b[0m  0.0096  0.2184\n",
      "      5                     \u001b[36m0.7430\u001b[0m        \u001b[32m0.5305\u001b[0m                     \u001b[35m0.7444\u001b[0m        0.5885  0.0093  0.2114\n",
      "      6                     \u001b[36m0.7556\u001b[0m        \u001b[32m0.5106\u001b[0m                     0.6884        0.7275  0.0090  0.2125\n",
      "      7                     0.7421        0.5893                     0.7146        0.6229  0.0085  0.2125\n",
      "      8                     \u001b[36m0.7607\u001b[0m        0.5220                     \u001b[35m0.7500\u001b[0m        \u001b[31m0.5161\u001b[0m  0.0080  0.2135\n",
      "      9                     \u001b[36m0.7706\u001b[0m        \u001b[32m0.4941\u001b[0m                     \u001b[35m0.7612\u001b[0m        \u001b[31m0.4987\u001b[0m  0.0075  0.2125\n",
      "     10                     \u001b[36m0.7930\u001b[0m        \u001b[32m0.4531\u001b[0m                     \u001b[35m0.7668\u001b[0m        0.5208  0.0069  0.2134\n",
      "     11                     \u001b[36m0.7986\u001b[0m        \u001b[32m0.4406\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.4944\u001b[0m  0.0063  0.2129\n",
      "     12                     0.7916        0.4413                     0.7705        \u001b[31m0.4718\u001b[0m  0.0057  0.2129\n",
      "     13                     \u001b[36m0.8098\u001b[0m        \u001b[32m0.4156\u001b[0m                     0.7724        0.5063  0.0050  0.2128\n",
      "     14                     \u001b[36m0.8117\u001b[0m        0.4200                     0.7612        0.5255  0.0043  0.2132\n",
      "     15                     \u001b[36m0.8210\u001b[0m        \u001b[32m0.4052\u001b[0m                     0.7500        0.5284  0.0037  0.2123\n",
      "     16                     0.8196        \u001b[32m0.3953\u001b[0m                     \u001b[35m0.7743\u001b[0m        0.4900  0.0031  0.2125\n",
      "     17                     0.8210        \u001b[32m0.3879\u001b[0m                     \u001b[35m0.7780\u001b[0m        0.4828  0.0025  0.2136\n",
      "     18                     \u001b[36m0.8243\u001b[0m        \u001b[32m0.3807\u001b[0m                     \u001b[35m0.7854\u001b[0m        0.4899  0.0020  0.2133\n",
      "     19                     \u001b[36m0.8341\u001b[0m        \u001b[32m0.3754\u001b[0m                     0.7761        \u001b[31m0.4693\u001b[0m  0.0015  0.2131\n",
      "     20                     0.8285        \u001b[32m0.3623\u001b[0m                     0.7743        0.4730  0.0010  0.2135\n",
      "     21                     \u001b[36m0.8523\u001b[0m        \u001b[32m0.3487\u001b[0m                     0.7836        0.4707  0.0007  0.2137\n",
      "     22                     0.8495        \u001b[32m0.3421\u001b[0m                     0.7761        0.4733  0.0004  0.2140\n",
      "     23                     0.8397        0.3534                     0.7780        0.4752  0.0002  0.2144\n",
      "     24                     0.8505        \u001b[32m0.3416\u001b[0m                     0.7761        0.4802  0.0000  0.2103\n",
      "     25                     0.8444        \u001b[32m0.3396\u001b[0m                     0.7799        0.4759  0.0000  0.2132\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5112\u001b[0m        \u001b[32m1.7610\u001b[0m                     \u001b[35m0.5802\u001b[0m        \u001b[31m0.8150\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5472\u001b[0m        \u001b[32m0.7933\u001b[0m                     0.5168        0.9344  0.0100  0.2127\n",
      "      3                     \u001b[36m0.6393\u001b[0m        \u001b[32m0.7439\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.6413\u001b[0m  0.0098  0.2126\n",
      "      4                     \u001b[36m0.6804\u001b[0m        \u001b[32m0.6557\u001b[0m                     \u001b[35m0.7649\u001b[0m        \u001b[31m0.5012\u001b[0m  0.0096  0.2135\n",
      "      5                     \u001b[36m0.7159\u001b[0m        \u001b[32m0.5736\u001b[0m                     0.7519        0.5388  0.0093  0.2145\n",
      "      6                     \u001b[36m0.7397\u001b[0m        \u001b[32m0.5573\u001b[0m                     \u001b[35m0.7743\u001b[0m        0.5178  0.0090  0.2136\n",
      "      7                     \u001b[36m0.7407\u001b[0m        0.5667                     0.7668        \u001b[31m0.4934\u001b[0m  0.0085  0.2146\n",
      "      8                     \u001b[36m0.7523\u001b[0m        \u001b[32m0.5330\u001b[0m                     0.7575        0.5100  0.0080  0.2145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.7715\u001b[0m        \u001b[32m0.4933\u001b[0m                     \u001b[35m0.7873\u001b[0m        \u001b[31m0.4599\u001b[0m  0.0075  0.2144\n",
      "     10                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.4548\u001b[0m                     \u001b[35m0.8060\u001b[0m        \u001b[31m0.4493\u001b[0m  0.0069  0.2135\n",
      "     11                     \u001b[36m0.7893\u001b[0m        \u001b[32m0.4535\u001b[0m                     0.7575        0.5431  0.0063  0.2125\n",
      "     12                     0.7860        0.4590                     0.7687        0.4887  0.0057  0.2120\n",
      "     13                     0.7818        0.5022                     0.7593        0.5179  0.0050  0.2125\n",
      "     14                     \u001b[36m0.8117\u001b[0m        \u001b[32m0.4317\u001b[0m                     0.7743        0.4757  0.0043  0.2134\n",
      "     15                     0.8075        \u001b[32m0.4243\u001b[0m                     0.7817        0.5005  0.0037  0.2117\n",
      "     16                     \u001b[36m0.8210\u001b[0m        \u001b[32m0.3949\u001b[0m                     0.7780        0.4804  0.0031  0.2124\n",
      "     17                     0.8159        0.4070                     0.7817        0.4900  0.0025  0.2132\n",
      "     18                     \u001b[36m0.8322\u001b[0m        \u001b[32m0.3794\u001b[0m                     0.7817        0.4560  0.0020  0.2125\n",
      "     19                     \u001b[36m0.8383\u001b[0m        \u001b[32m0.3699\u001b[0m                     0.7892        0.4614  0.0015  0.2144\n",
      "     20                     0.8360        \u001b[32m0.3586\u001b[0m                     0.7873        0.4610  0.0010  0.2119\n",
      "     21                     0.8379        0.3632                     0.7817        0.4596  0.0007  0.2090\n",
      "     22                     0.8350        0.3636                     0.7780        0.4674  0.0004  0.2100\n",
      "     23                     \u001b[36m0.8467\u001b[0m        \u001b[32m0.3537\u001b[0m                     0.7817        0.4675  0.0002  0.2130\n",
      "     24                     0.8449        0.3574                     0.7817        0.4678  0.0000  0.2133\n",
      "     25                     \u001b[36m0.8500\u001b[0m        \u001b[32m0.3482\u001b[0m                     0.7780        0.4686  0.0000  0.2132\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5687\u001b[0m        \u001b[32m1.5805\u001b[0m                     \u001b[35m0.5709\u001b[0m        \u001b[31m0.8708\u001b[0m  0.0100  0.2064\n",
      "      2                     \u001b[36m0.6369\u001b[0m        \u001b[32m0.7520\u001b[0m                     \u001b[35m0.6493\u001b[0m        \u001b[31m0.6924\u001b[0m  0.0100  0.2163\n",
      "      3                     \u001b[36m0.6766\u001b[0m        \u001b[32m0.7106\u001b[0m                     \u001b[35m0.7407\u001b[0m        \u001b[31m0.6248\u001b[0m  0.0098  0.2122\n",
      "      4                     \u001b[36m0.7206\u001b[0m        \u001b[32m0.6395\u001b[0m                     0.7276        0.6473  0.0096  0.2128\n",
      "      5                     0.7093        0.7350                     0.7351        \u001b[31m0.5967\u001b[0m  0.0093  0.2124\n",
      "      6                     \u001b[36m0.7565\u001b[0m        \u001b[32m0.5785\u001b[0m                     \u001b[35m0.7761\u001b[0m        \u001b[31m0.5003\u001b[0m  0.0090  0.2152\n",
      "      7                     0.7528        0.5912                     0.7724        0.5110  0.0085  0.2135\n",
      "      8                     0.7500        0.5984                     0.7444        0.5309  0.0080  0.2137\n",
      "      9                     \u001b[36m0.7678\u001b[0m        \u001b[32m0.5042\u001b[0m                     0.7444        0.6200  0.0075  0.2131\n",
      "     10                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4646\u001b[0m                     0.7276        0.5481  0.0069  0.2119\n",
      "     11                     0.7902        0.4695                     0.7631        0.5208  0.0063  0.2133\n",
      "     12                     \u001b[36m0.8121\u001b[0m        \u001b[32m0.4359\u001b[0m                     \u001b[35m0.7799\u001b[0m        \u001b[31m0.4857\u001b[0m  0.0057  0.2135\n",
      "     13                     0.8037        \u001b[32m0.4269\u001b[0m                     0.7631        0.5074  0.0050  0.2125\n",
      "     14                     0.7981        \u001b[32m0.4225\u001b[0m                     \u001b[35m0.7854\u001b[0m        0.4870  0.0043  0.2126\n",
      "     15                     0.8112        \u001b[32m0.4096\u001b[0m                     \u001b[35m0.7873\u001b[0m        \u001b[31m0.4748\u001b[0m  0.0037  0.2138\n",
      "     16                     \u001b[36m0.8304\u001b[0m        \u001b[32m0.3818\u001b[0m                     \u001b[35m0.7910\u001b[0m        0.4772  0.0031  0.2123\n",
      "     17                     \u001b[36m0.8341\u001b[0m        \u001b[32m0.3703\u001b[0m                     \u001b[35m0.8060\u001b[0m        \u001b[31m0.4699\u001b[0m  0.0025  0.2139\n",
      "     18                     \u001b[36m0.8369\u001b[0m        0.3761                     0.7985        \u001b[31m0.4693\u001b[0m  0.0020  0.2129\n",
      "     19                     \u001b[36m0.8425\u001b[0m        \u001b[32m0.3589\u001b[0m                     0.8060        \u001b[31m0.4639\u001b[0m  0.0015  0.2134\n",
      "     20                     0.8360        \u001b[32m0.3563\u001b[0m                     \u001b[35m0.8153\u001b[0m        \u001b[31m0.4628\u001b[0m  0.0010  0.2130\n",
      "     21                     0.8402        \u001b[32m0.3528\u001b[0m                     0.8153        0.4665  0.0007  0.2135\n",
      "     22                     \u001b[36m0.8453\u001b[0m        \u001b[32m0.3454\u001b[0m                     0.8116        0.4691  0.0004  0.2105\n",
      "     23                     \u001b[36m0.8514\u001b[0m        \u001b[32m0.3392\u001b[0m                     0.8060        0.4684  0.0002  0.2137\n",
      "     24                     \u001b[36m0.8593\u001b[0m        0.3392                     0.8060        0.4685  0.0000  0.2129\n",
      "     25                     0.8505        0.3401                     0.8060        0.4696  0.0000  0.2142\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5425\u001b[0m        \u001b[32m1.6839\u001b[0m                     \u001b[35m0.5299\u001b[0m        \u001b[31m1.1406\u001b[0m  0.0100  0.2064\n",
      "      2                     \u001b[36m0.5995\u001b[0m        \u001b[32m0.8457\u001b[0m                     \u001b[35m0.6884\u001b[0m        \u001b[31m0.6382\u001b[0m  0.0100  0.2155\n",
      "      3                     \u001b[36m0.6752\u001b[0m        \u001b[32m0.7046\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5950\u001b[0m  0.0098  0.2137\n",
      "      4                     \u001b[36m0.7037\u001b[0m        \u001b[32m0.6472\u001b[0m                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5367\u001b[0m  0.0096  0.2144\n",
      "      5                     \u001b[36m0.7308\u001b[0m        \u001b[32m0.6127\u001b[0m                     \u001b[35m0.7761\u001b[0m        \u001b[31m0.4723\u001b[0m  0.0093  0.2143\n",
      "      6                     \u001b[36m0.7421\u001b[0m        \u001b[32m0.5343\u001b[0m                     \u001b[35m0.7892\u001b[0m        \u001b[31m0.4604\u001b[0m  0.0090  0.2142\n",
      "      7                     \u001b[36m0.7621\u001b[0m        \u001b[32m0.4974\u001b[0m                     0.7854        \u001b[31m0.4515\u001b[0m  0.0085  0.2132\n",
      "      8                     \u001b[36m0.7626\u001b[0m        0.5104                     0.7612        0.5195  0.0080  0.2145\n",
      "      9                     \u001b[36m0.7650\u001b[0m        0.5154                     0.7854        \u001b[31m0.4513\u001b[0m  0.0075  0.2143\n",
      "     10                     \u001b[36m0.7981\u001b[0m        \u001b[32m0.4477\u001b[0m                     \u001b[35m0.8022\u001b[0m        \u001b[31m0.4321\u001b[0m  0.0069  0.2133\n",
      "     11                     0.7916        \u001b[32m0.4475\u001b[0m                     0.7929        0.4447  0.0063  0.2139\n",
      "     12                     \u001b[36m0.7986\u001b[0m        \u001b[32m0.4362\u001b[0m                     \u001b[35m0.8078\u001b[0m        0.4344  0.0057  0.2155\n",
      "     13                     \u001b[36m0.8014\u001b[0m        \u001b[32m0.4266\u001b[0m                     0.7780        0.4672  0.0050  0.2137\n",
      "     14                     \u001b[36m0.8042\u001b[0m        \u001b[32m0.4181\u001b[0m                     0.7985        0.4504  0.0043  0.2139\n",
      "     15                     \u001b[36m0.8107\u001b[0m        \u001b[32m0.4119\u001b[0m                     0.7705        0.4879  0.0037  0.2135\n",
      "     16                     0.8037        0.4168                     0.8004        0.4336  0.0031  0.2090\n",
      "     17                     \u001b[36m0.8206\u001b[0m        \u001b[32m0.3809\u001b[0m                     \u001b[35m0.8097\u001b[0m        0.4390  0.0025  0.2108\n",
      "     18                     \u001b[36m0.8220\u001b[0m        \u001b[32m0.3777\u001b[0m                     \u001b[35m0.8228\u001b[0m        \u001b[31m0.4106\u001b[0m  0.0020  0.2150\n",
      "     19                     0.8178        \u001b[32m0.3760\u001b[0m                     0.8116        \u001b[31m0.4105\u001b[0m  0.0015  0.2136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                     \u001b[36m0.8421\u001b[0m        \u001b[32m0.3690\u001b[0m                     0.8041        0.4322  0.0010  0.2145\n",
      "     21                     0.8374        \u001b[32m0.3674\u001b[0m                     0.8097        \u001b[31m0.4102\u001b[0m  0.0007  0.2132\n",
      "     22                     0.8332        \u001b[32m0.3608\u001b[0m                     0.8060        0.4192  0.0004  0.2134\n",
      "     23                     \u001b[36m0.8477\u001b[0m        \u001b[32m0.3600\u001b[0m                     0.8153        0.4142  0.0002  0.2125\n",
      "     24                     0.8449        \u001b[32m0.3465\u001b[0m                     0.8172        0.4153  0.0000  0.2134\n",
      "     25                     \u001b[36m0.8495\u001b[0m        0.3481                     0.8172        0.4114  0.0000  0.2133\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5252\u001b[0m        \u001b[32m1.5035\u001b[0m                     \u001b[35m0.5429\u001b[0m        \u001b[31m0.9293\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5336\u001b[0m        \u001b[32m0.9177\u001b[0m                     \u001b[35m0.6269\u001b[0m        \u001b[31m0.6489\u001b[0m  0.0100  0.2150\n",
      "      3                     \u001b[36m0.6350\u001b[0m        \u001b[32m0.7622\u001b[0m                     \u001b[35m0.6772\u001b[0m        \u001b[31m0.6465\u001b[0m  0.0098  0.2131\n",
      "      4                     \u001b[36m0.6846\u001b[0m        \u001b[32m0.6284\u001b[0m                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.4755\u001b[0m  0.0096  0.2104\n",
      "      5                     \u001b[36m0.6874\u001b[0m        0.6929                     0.6679        0.6775  0.0093  0.2125\n",
      "      6                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5567\u001b[0m                     0.7332        0.5496  0.0090  0.2144\n",
      "      7                     0.7495        \u001b[32m0.5211\u001b[0m                     0.7257        0.5669  0.0085  0.2125\n",
      "      8                     \u001b[36m0.7734\u001b[0m        \u001b[32m0.5002\u001b[0m                     \u001b[35m0.7873\u001b[0m        \u001b[31m0.4534\u001b[0m  0.0080  0.2135\n",
      "      9                     0.7692        0.5085                     0.7799        0.4880  0.0075  0.2134\n",
      "     10                     0.7687        \u001b[32m0.4879\u001b[0m                     \u001b[35m0.7985\u001b[0m        0.4561  0.0069  0.2125\n",
      "     11                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4744\u001b[0m                     0.7575        0.4944  0.0063  0.2134\n",
      "     12                     \u001b[36m0.7921\u001b[0m        \u001b[32m0.4620\u001b[0m                     0.7780        0.4579  0.0057  0.2144\n",
      "     13                     \u001b[36m0.8042\u001b[0m        \u001b[32m0.4417\u001b[0m                     \u001b[35m0.8004\u001b[0m        \u001b[31m0.4339\u001b[0m  0.0050  0.2117\n",
      "     14                     \u001b[36m0.8065\u001b[0m        0.4433                     0.7892        0.4509  0.0043  0.2127\n",
      "     15                     0.7939        \u001b[32m0.4342\u001b[0m                     0.7966        \u001b[31m0.4167\u001b[0m  0.0037  0.2130\n",
      "     16                     \u001b[36m0.8252\u001b[0m        \u001b[32m0.4011\u001b[0m                     \u001b[35m0.8060\u001b[0m        0.4185  0.0031  0.2145\n",
      "     17                     0.8201        \u001b[32m0.3990\u001b[0m                     0.8041        \u001b[31m0.4144\u001b[0m  0.0025  0.2125\n",
      "     18                     \u001b[36m0.8350\u001b[0m        \u001b[32m0.3777\u001b[0m                     \u001b[35m0.8116\u001b[0m        0.4258  0.0020  0.2125\n",
      "     19                     0.8318        0.3826                     0.7948        0.4253  0.0015  0.2134\n",
      "     20                     0.8238        0.3853                     0.8078        \u001b[31m0.4119\u001b[0m  0.0010  0.2125\n",
      "     21                     \u001b[36m0.8407\u001b[0m        \u001b[32m0.3759\u001b[0m                     0.7929        0.4203  0.0007  0.2142\n",
      "     22                     \u001b[36m0.8439\u001b[0m        \u001b[32m0.3644\u001b[0m                     0.8022        0.4191  0.0004  0.2137\n",
      "     23                     0.8308        0.3652                     0.8097        0.4125  0.0002  0.2135\n",
      "     24                     0.8383        0.3660                     0.8060        0.4142  0.0000  0.2129\n",
      "     25                     0.8290        0.3670                     0.8060        0.4127  0.0000  0.2154\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5112\u001b[0m        \u001b[32m1.6947\u001b[0m                     \u001b[35m0.5522\u001b[0m        \u001b[31m1.3634\u001b[0m  0.0100  0.2084\n",
      "      2                     \u001b[36m0.5350\u001b[0m        \u001b[32m0.9176\u001b[0m                     0.4981        \u001b[31m1.1434\u001b[0m  0.0100  0.2144\n",
      "      3                     \u001b[36m0.5715\u001b[0m        \u001b[32m0.7696\u001b[0m                     \u001b[35m0.6306\u001b[0m        \u001b[31m0.6823\u001b[0m  0.0098  0.2132\n",
      "      4                     \u001b[36m0.6107\u001b[0m        0.8033                     \u001b[35m0.6698\u001b[0m        \u001b[31m0.6130\u001b[0m  0.0096  0.2131\n",
      "      5                     \u001b[36m0.6575\u001b[0m        0.7960                     0.6567        0.7946  0.0093  0.2131\n",
      "      6                     \u001b[36m0.7304\u001b[0m        \u001b[32m0.5778\u001b[0m                     \u001b[35m0.7481\u001b[0m        \u001b[31m0.4997\u001b[0m  0.0090  0.2105\n",
      "      7                     \u001b[36m0.7603\u001b[0m        \u001b[32m0.5213\u001b[0m                     0.7425        0.5375  0.0085  0.2141\n",
      "      8                     0.7495        0.5337                     \u001b[35m0.7575\u001b[0m        0.5207  0.0080  0.2144\n",
      "      9                     \u001b[36m0.7860\u001b[0m        \u001b[32m0.4738\u001b[0m                     \u001b[35m0.7631\u001b[0m        0.5012  0.0075  0.2135\n",
      "     10                     0.7636        0.5160                     \u001b[35m0.7799\u001b[0m        \u001b[31m0.4938\u001b[0m  0.0069  0.2134\n",
      "     11                     0.7841        \u001b[32m0.4589\u001b[0m                     0.7799        \u001b[31m0.4635\u001b[0m  0.0063  0.2134\n",
      "     12                     \u001b[36m0.8079\u001b[0m        \u001b[32m0.4393\u001b[0m                     0.7556        0.5323  0.0057  0.2127\n",
      "     13                     0.7949        0.4512                     0.7500        0.5127  0.0050  0.2138\n",
      "     14                     0.8056        \u001b[32m0.4313\u001b[0m                     \u001b[35m0.7854\u001b[0m        \u001b[31m0.4577\u001b[0m  0.0043  0.2143\n",
      "     15                     \u001b[36m0.8182\u001b[0m        \u001b[32m0.4133\u001b[0m                     \u001b[35m0.7892\u001b[0m        \u001b[31m0.4545\u001b[0m  0.0037  0.2147\n",
      "     16                     \u001b[36m0.8248\u001b[0m        \u001b[32m0.4014\u001b[0m                     0.7836        0.4587  0.0031  0.2133\n",
      "     17                     \u001b[36m0.8252\u001b[0m        \u001b[32m0.3892\u001b[0m                     0.7873        \u001b[31m0.4417\u001b[0m  0.0025  0.2158\n",
      "     18                     \u001b[36m0.8308\u001b[0m        0.3940                     \u001b[35m0.7910\u001b[0m        0.4451  0.0020  0.2141\n",
      "     19                     0.8276        0.3894                     0.7836        0.4526  0.0015  0.2135\n",
      "     20                     \u001b[36m0.8369\u001b[0m        \u001b[32m0.3766\u001b[0m                     0.7910        \u001b[31m0.4391\u001b[0m  0.0010  0.2145\n",
      "     21                     \u001b[36m0.8407\u001b[0m        \u001b[32m0.3660\u001b[0m                     \u001b[35m0.7966\u001b[0m        0.4393  0.0007  0.2134\n",
      "     22                     \u001b[36m0.8416\u001b[0m        \u001b[32m0.3657\u001b[0m                     0.7892        0.4428  0.0004  0.2136\n",
      "     23                     \u001b[36m0.8472\u001b[0m        \u001b[32m0.3530\u001b[0m                     0.7929        0.4409  0.0002  0.2147\n",
      "     24                     0.8453        0.3605                     0.7929        0.4409  0.0000  0.2144\n",
      "     25                     0.8374        0.3660                     0.7929        0.4418  0.0000  0.2144\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5173\u001b[0m        \u001b[32m1.7168\u001b[0m                     \u001b[35m0.5354\u001b[0m        \u001b[31m0.8157\u001b[0m  0.0100  0.2074\n",
      "      2                     \u001b[36m0.5561\u001b[0m        \u001b[32m0.8213\u001b[0m                     \u001b[35m0.5896\u001b[0m        \u001b[31m0.6862\u001b[0m  0.0100  0.2124\n",
      "      3                     \u001b[36m0.6210\u001b[0m        \u001b[32m0.7967\u001b[0m                     0.5896        0.8210  0.0098  0.2124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4                     \u001b[36m0.6720\u001b[0m        \u001b[32m0.6568\u001b[0m                     \u001b[35m0.7537\u001b[0m        \u001b[31m0.4986\u001b[0m  0.0096  0.2130\n",
      "      5                     \u001b[36m0.7047\u001b[0m        0.6602                     0.6959        0.7019  0.0093  0.2144\n",
      "      6                     \u001b[36m0.7491\u001b[0m        \u001b[32m0.5527\u001b[0m                     0.6996        0.5586  0.0090  0.2129\n",
      "      7                     \u001b[36m0.7603\u001b[0m        \u001b[32m0.5076\u001b[0m                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.4772\u001b[0m  0.0085  0.2133\n",
      "      8                     \u001b[36m0.7626\u001b[0m        \u001b[32m0.5066\u001b[0m                     0.7519        0.5347  0.0080  0.2124\n",
      "      9                     \u001b[36m0.7818\u001b[0m        \u001b[32m0.4920\u001b[0m                     \u001b[35m0.7948\u001b[0m        \u001b[31m0.4627\u001b[0m  0.0075  0.2134\n",
      "     10                     0.7818        0.4964                     0.7668        0.4861  0.0069  0.2127\n",
      "     11                     \u001b[36m0.7822\u001b[0m        \u001b[32m0.4786\u001b[0m                     0.7854        0.4700  0.0063  0.2125\n",
      "     12                     \u001b[36m0.7907\u001b[0m        \u001b[32m0.4522\u001b[0m                     0.7780        0.4717  0.0057  0.2125\n",
      "     13                     \u001b[36m0.8079\u001b[0m        \u001b[32m0.4206\u001b[0m                     \u001b[35m0.7966\u001b[0m        \u001b[31m0.4391\u001b[0m  0.0050  0.2088\n",
      "     14                     0.8079        0.4323                     0.7817        0.4445  0.0043  0.2135\n",
      "     15                     \u001b[36m0.8089\u001b[0m        \u001b[32m0.4042\u001b[0m                     0.7799        0.4594  0.0037  0.2117\n",
      "     16                     \u001b[36m0.8206\u001b[0m        \u001b[32m0.3996\u001b[0m                     0.7910        0.4437  0.0031  0.2126\n",
      "     17                     \u001b[36m0.8229\u001b[0m        \u001b[32m0.3907\u001b[0m                     0.7929        0.4478  0.0025  0.2135\n",
      "     18                     \u001b[36m0.8280\u001b[0m        \u001b[32m0.3822\u001b[0m                     0.7910        0.4685  0.0020  0.2125\n",
      "     19                     0.8271        0.3842                     0.7929        0.4672  0.0015  0.2129\n",
      "     20                     \u001b[36m0.8290\u001b[0m        \u001b[32m0.3795\u001b[0m                     0.7948        0.4540  0.0010  0.2125\n",
      "     21                     \u001b[36m0.8364\u001b[0m        \u001b[32m0.3577\u001b[0m                     0.7892        0.4575  0.0007  0.2124\n",
      "     22                     0.8318        0.3777                     0.7929        0.4538  0.0004  0.2123\n",
      "     23                     \u001b[36m0.8411\u001b[0m        0.3667                     0.7836        0.4541  0.0002  0.2124\n",
      "     24                     \u001b[36m0.8425\u001b[0m        \u001b[32m0.3568\u001b[0m                     0.7854        0.4490  0.0000  0.2125\n",
      "     25                     0.8416        \u001b[32m0.3544\u001b[0m                     0.7854        0.4535  0.0000  0.2146\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5061\u001b[0m        \u001b[32m1.7512\u001b[0m                     \u001b[35m0.5746\u001b[0m        \u001b[31m0.9296\u001b[0m  0.0100  0.2064\n",
      "      2                     \u001b[36m0.6327\u001b[0m        \u001b[32m0.7272\u001b[0m                     \u001b[35m0.5989\u001b[0m        \u001b[31m0.7812\u001b[0m  0.0100  0.2117\n",
      "      3                     \u001b[36m0.6589\u001b[0m        \u001b[32m0.6641\u001b[0m                     \u001b[35m0.6959\u001b[0m        \u001b[31m0.6165\u001b[0m  0.0098  0.2125\n",
      "      4                     \u001b[36m0.7131\u001b[0m        0.6746                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5917\u001b[0m  0.0096  0.2132\n",
      "      5                     \u001b[36m0.7393\u001b[0m        \u001b[32m0.5690\u001b[0m                     \u001b[35m0.7687\u001b[0m        \u001b[31m0.5189\u001b[0m  0.0093  0.2135\n",
      "      6                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5460\u001b[0m                     \u001b[35m0.7854\u001b[0m        \u001b[31m0.4713\u001b[0m  0.0090  0.2125\n",
      "      7                     \u001b[36m0.7678\u001b[0m        \u001b[32m0.4997\u001b[0m                     0.7332        0.5370  0.0085  0.2133\n",
      "      8                     0.7631        0.5126                     0.7817        0.4933  0.0080  0.2122\n",
      "      9                     \u001b[36m0.7743\u001b[0m        \u001b[32m0.4854\u001b[0m                     0.7724        0.4787  0.0075  0.2121\n",
      "     10                     \u001b[36m0.7794\u001b[0m        \u001b[32m0.4798\u001b[0m                     0.7220        0.7091  0.0069  0.2130\n",
      "     11                     0.7631        0.6102                     \u001b[35m0.7929\u001b[0m        \u001b[31m0.4629\u001b[0m  0.0063  0.2134\n",
      "     12                     \u001b[36m0.7893\u001b[0m        \u001b[32m0.4449\u001b[0m                     0.7724        0.4945  0.0057  0.2132\n",
      "     13                     \u001b[36m0.8047\u001b[0m        \u001b[32m0.4233\u001b[0m                     0.7631        0.5003  0.0050  0.2135\n",
      "     14                     0.8037        0.4278                     \u001b[35m0.8022\u001b[0m        \u001b[31m0.4508\u001b[0m  0.0043  0.2135\n",
      "     15                     \u001b[36m0.8061\u001b[0m        \u001b[32m0.4151\u001b[0m                     \u001b[35m0.8134\u001b[0m        \u001b[31m0.4464\u001b[0m  0.0037  0.2135\n",
      "     16                     \u001b[36m0.8150\u001b[0m        \u001b[32m0.4049\u001b[0m                     0.8004        0.4653  0.0031  0.2114\n",
      "     17                     \u001b[36m0.8262\u001b[0m        \u001b[32m0.3928\u001b[0m                     \u001b[35m0.8153\u001b[0m        \u001b[31m0.4368\u001b[0m  0.0025  0.2150\n",
      "     18                     \u001b[36m0.8271\u001b[0m        \u001b[32m0.3827\u001b[0m                     0.8004        0.4407  0.0020  0.2144\n",
      "     19                     \u001b[36m0.8313\u001b[0m        \u001b[32m0.3737\u001b[0m                     0.8153        0.4475  0.0015  0.2135\n",
      "     20                     \u001b[36m0.8336\u001b[0m        \u001b[32m0.3645\u001b[0m                     0.8041        0.4568  0.0010  0.2125\n",
      "     21                     \u001b[36m0.8379\u001b[0m        0.3697                     0.8097        0.4449  0.0007  0.2131\n",
      "     22                     \u001b[36m0.8416\u001b[0m        \u001b[32m0.3596\u001b[0m                     0.8060        0.4510  0.0004  0.2141\n",
      "     23                     0.8397        0.3619                     0.8097        0.4508  0.0002  0.2139\n",
      "     24                     0.8322        \u001b[32m0.3595\u001b[0m                     0.8078        0.4502  0.0000  0.2132\n",
      "     25                     \u001b[36m0.8477\u001b[0m        \u001b[32m0.3554\u001b[0m                     0.8116        0.4525  0.0000  0.2164\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5229\u001b[0m        \u001b[32m1.6086\u001b[0m                     \u001b[35m0.5056\u001b[0m        \u001b[31m0.8485\u001b[0m  0.0100  0.2084\n",
      "      2                     0.5154        \u001b[32m0.8768\u001b[0m                     \u001b[35m0.5243\u001b[0m        \u001b[31m0.8464\u001b[0m  0.0100  0.2174\n",
      "      3                     \u001b[36m0.5841\u001b[0m        \u001b[32m0.7387\u001b[0m                     \u001b[35m0.6586\u001b[0m        \u001b[31m0.6334\u001b[0m  0.0098  0.2128\n",
      "      4                     \u001b[36m0.6995\u001b[0m        \u001b[32m0.6135\u001b[0m                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5733\u001b[0m  0.0096  0.2134\n",
      "      5                     \u001b[36m0.7192\u001b[0m        \u001b[32m0.5538\u001b[0m                     \u001b[35m0.7351\u001b[0m        \u001b[31m0.5404\u001b[0m  0.0093  0.2133\n",
      "      6                     \u001b[36m0.7379\u001b[0m        0.5762                     \u001b[35m0.7463\u001b[0m        0.5513  0.0090  0.2134\n",
      "      7                     \u001b[36m0.7537\u001b[0m        0.5777                     \u001b[35m0.7481\u001b[0m        \u001b[31m0.5179\u001b[0m  0.0085  0.2131\n",
      "      8                     \u001b[36m0.7822\u001b[0m        \u001b[32m0.4655\u001b[0m                     0.6940        0.6394  0.0080  0.2125\n",
      "      9                     \u001b[36m0.7888\u001b[0m        0.4776                     \u001b[35m0.7724\u001b[0m        0.5190  0.0075  0.2128\n",
      "     10                     0.7757        0.5142                     0.7649        \u001b[31m0.4874\u001b[0m  0.0069  0.2124\n",
      "     11                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4334\u001b[0m                     0.7631        0.5000  0.0063  0.2118\n",
      "     12                     \u001b[36m0.8023\u001b[0m        \u001b[32m0.4320\u001b[0m                     0.7612        0.5039  0.0057  0.2119\n",
      "     13                     \u001b[36m0.8140\u001b[0m        \u001b[32m0.4094\u001b[0m                     \u001b[35m0.7799\u001b[0m        0.4967  0.0050  0.2122\n",
      "     14                     0.8000        0.4448                     0.7575        0.5374  0.0043  0.2135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15                     \u001b[36m0.8168\u001b[0m        0.4190                     0.7799        \u001b[31m0.4751\u001b[0m  0.0037  0.2125\n",
      "     16                     \u001b[36m0.8182\u001b[0m        \u001b[32m0.3922\u001b[0m                     0.7687        0.4761  0.0031  0.2135\n",
      "     17                     \u001b[36m0.8252\u001b[0m        \u001b[32m0.3840\u001b[0m                     0.7705        0.4789  0.0025  0.2130\n",
      "     18                     \u001b[36m0.8336\u001b[0m        \u001b[32m0.3723\u001b[0m                     \u001b[35m0.7854\u001b[0m        \u001b[31m0.4628\u001b[0m  0.0020  0.2135\n",
      "     19                     \u001b[36m0.8421\u001b[0m        \u001b[32m0.3598\u001b[0m                     0.7817        0.4876  0.0015  0.2125\n",
      "     20                     0.8416        \u001b[32m0.3578\u001b[0m                     0.7761        0.4724  0.0010  0.2125\n",
      "     21                     \u001b[36m0.8425\u001b[0m        \u001b[32m0.3517\u001b[0m                     \u001b[35m0.7873\u001b[0m        0.4695  0.0007  0.2110\n",
      "     22                     \u001b[36m0.8439\u001b[0m        \u001b[32m0.3512\u001b[0m                     0.7854        0.4665  0.0004  0.2128\n",
      "     23                     0.8374        \u001b[32m0.3458\u001b[0m                     0.7854        0.4689  0.0002  0.2096\n",
      "     24                     \u001b[36m0.8444\u001b[0m        0.3508                     0.7854        0.4693  0.0000  0.2135\n",
      "     25                     \u001b[36m0.8514\u001b[0m        0.3505                     0.7873        0.4699  0.0000  0.2136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5804\u001b[0m        \u001b[32m0.7080\u001b[0m                     \u001b[35m0.5896\u001b[0m        \u001b[31m0.6726\u001b[0m  0.0100  0.7135\n",
      "      2                     \u001b[36m0.6584\u001b[0m        \u001b[32m0.6264\u001b[0m                     \u001b[35m0.6362\u001b[0m        \u001b[31m0.6427\u001b[0m  0.0100  0.6446\n",
      "      3                     \u001b[36m0.6762\u001b[0m        \u001b[32m0.6012\u001b[0m                     \u001b[35m0.6437\u001b[0m        0.6433  0.0098  0.6370\n",
      "      4                     \u001b[36m0.6911\u001b[0m        \u001b[32m0.5885\u001b[0m                     \u001b[35m0.6754\u001b[0m        \u001b[31m0.6072\u001b[0m  0.0096  0.6368\n",
      "      5                     \u001b[36m0.7061\u001b[0m        \u001b[32m0.5784\u001b[0m                     \u001b[35m0.6791\u001b[0m        0.6186  0.0093  0.6366\n",
      "      6                     \u001b[36m0.7196\u001b[0m        \u001b[32m0.5602\u001b[0m                     0.6754        \u001b[31m0.5862\u001b[0m  0.0090  0.6380\n",
      "      7                     \u001b[36m0.7374\u001b[0m        \u001b[32m0.5508\u001b[0m                     0.6754        0.6235  0.0085  0.6375\n",
      "      8                     0.7322        \u001b[32m0.5311\u001b[0m                     \u001b[35m0.7108\u001b[0m        0.6011  0.0080  0.6372\n",
      "      9                     \u001b[36m0.7449\u001b[0m        \u001b[32m0.5211\u001b[0m                     0.6735        0.6545  0.0075  0.6367\n",
      "     10                     \u001b[36m0.7533\u001b[0m        \u001b[32m0.5164\u001b[0m                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5842\u001b[0m  0.0069  0.6386\n",
      "     11                     \u001b[36m0.7626\u001b[0m        \u001b[32m0.4973\u001b[0m                     0.6754        0.6259  0.0063  0.6368\n",
      "     12                     \u001b[36m0.7692\u001b[0m        \u001b[32m0.4902\u001b[0m                     0.7127        0.5971  0.0057  0.6376\n",
      "     13                     \u001b[36m0.7748\u001b[0m        \u001b[32m0.4772\u001b[0m                     0.6996        0.6348  0.0050  0.6370\n",
      "     14                     0.7682        0.4828                     0.7015        0.5855  0.0043  0.6379\n",
      "     15                     0.7748        \u001b[32m0.4749\u001b[0m                     \u001b[35m0.7164\u001b[0m        \u001b[31m0.5682\u001b[0m  0.0037  0.6384\n",
      "     16                     \u001b[36m0.7794\u001b[0m        \u001b[32m0.4715\u001b[0m                     0.7071        0.5825  0.0031  0.6367\n",
      "     17                     0.7762        \u001b[32m0.4674\u001b[0m                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5567\u001b[0m  0.0025  0.6379\n",
      "     18                     \u001b[36m0.8028\u001b[0m        \u001b[32m0.4477\u001b[0m                     0.7164        0.5633  0.0020  0.6362\n",
      "     19                     0.8000        \u001b[32m0.4369\u001b[0m                     \u001b[35m0.7220\u001b[0m        0.5703  0.0015  0.6365\n",
      "     20                     \u001b[36m0.8047\u001b[0m        \u001b[32m0.4369\u001b[0m                     \u001b[35m0.7239\u001b[0m        0.5665  0.0010  0.6365\n",
      "     21                     0.7963        0.4390                     0.7127        0.5680  0.0007  0.6380\n",
      "     22                     0.7958        \u001b[32m0.4325\u001b[0m                     0.7183        0.5688  0.0004  0.6372\n",
      "     23                     \u001b[36m0.8061\u001b[0m        \u001b[32m0.4318\u001b[0m                     0.7183        0.5684  0.0002  0.6369\n",
      "     24                     0.8033        \u001b[32m0.4304\u001b[0m                     0.7183        0.5678  0.0000  0.6384\n",
      "     25                     0.8042        \u001b[32m0.4231\u001b[0m                     0.7164        0.5676  0.0000  0.6612\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5551\u001b[0m        \u001b[32m0.7023\u001b[0m                     \u001b[35m0.5373\u001b[0m        \u001b[31m0.7241\u001b[0m  0.0100  0.6362\n",
      "      2                     \u001b[36m0.6467\u001b[0m        \u001b[32m0.6274\u001b[0m                     \u001b[35m0.6530\u001b[0m        \u001b[31m0.6286\u001b[0m  0.0100  0.6374\n",
      "      3                     \u001b[36m0.6813\u001b[0m        \u001b[32m0.5982\u001b[0m                     0.6362        0.6541  0.0098  0.6582\n",
      "      4                     \u001b[36m0.6869\u001b[0m        \u001b[32m0.5856\u001b[0m                     0.6007        0.7538  0.0096  0.6377\n",
      "      5                     \u001b[36m0.6883\u001b[0m        \u001b[32m0.5852\u001b[0m                     \u001b[35m0.6772\u001b[0m        0.6354  0.0093  0.6373\n",
      "      6                     \u001b[36m0.7178\u001b[0m        \u001b[32m0.5645\u001b[0m                     \u001b[35m0.6866\u001b[0m        0.6859  0.0090  0.6377\n",
      "      7                     \u001b[36m0.7332\u001b[0m        \u001b[32m0.5489\u001b[0m                     0.6642        0.7078  0.0085  0.6374\n",
      "      8                     0.7271        \u001b[32m0.5415\u001b[0m                     \u001b[35m0.7015\u001b[0m        \u001b[31m0.6100\u001b[0m  0.0080  0.6387\n",
      "      9                     \u001b[36m0.7481\u001b[0m        \u001b[32m0.5326\u001b[0m                     \u001b[35m0.7071\u001b[0m        \u001b[31m0.6074\u001b[0m  0.0075  0.6375\n",
      "     10                     \u001b[36m0.7528\u001b[0m        \u001b[32m0.5132\u001b[0m                     0.7015        \u001b[31m0.5923\u001b[0m  0.0069  0.6381\n",
      "     11                     \u001b[36m0.7612\u001b[0m        \u001b[32m0.5087\u001b[0m                     \u001b[35m0.7444\u001b[0m        \u001b[31m0.5567\u001b[0m  0.0063  0.6381\n",
      "     12                     \u001b[36m0.7654\u001b[0m        \u001b[32m0.4972\u001b[0m                     0.7015        0.6265  0.0057  0.6374\n",
      "     13                     0.7565        \u001b[32m0.4940\u001b[0m                     0.7332        0.5793  0.0050  0.6389\n",
      "     14                     \u001b[36m0.7766\u001b[0m        \u001b[32m0.4820\u001b[0m                     0.7276        0.6070  0.0043  0.6367\n",
      "     15                     0.7636        0.4869                     0.7276        0.5724  0.0037  0.6378\n",
      "     16                     \u001b[36m0.7850\u001b[0m        \u001b[32m0.4623\u001b[0m                     0.7351        0.5661  0.0031  0.6373\n",
      "     17                     0.7836        0.4648                     0.7444        0.5663  0.0025  0.6372\n",
      "     18                     \u001b[36m0.7949\u001b[0m        \u001b[32m0.4456\u001b[0m                     \u001b[35m0.7481\u001b[0m        0.5691  0.0020  0.6385\n",
      "     19                     0.7907        0.4504                     \u001b[35m0.7631\u001b[0m        0.5785  0.0015  0.6370\n",
      "     20                     \u001b[36m0.8047\u001b[0m        \u001b[32m0.4339\u001b[0m                     0.7407        0.5776  0.0010  0.6381\n",
      "     21                     0.8000        0.4407                     0.7500        0.5788  0.0007  0.6375\n",
      "     22                     0.7953        \u001b[32m0.4244\u001b[0m                     0.7407        0.5714  0.0004  0.6371\n",
      "     23                     0.7953        0.4298                     0.7369        0.5752  0.0002  0.6376\n",
      "     24                     0.8000        0.4252                     0.7407        0.5745  0.0000  0.6378\n",
      "     25                     0.8033        0.4285                     0.7369        0.5750  0.0000  0.6368\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5322\u001b[0m        \u001b[32m0.7516\u001b[0m                     \u001b[35m0.5821\u001b[0m        \u001b[31m0.6787\u001b[0m  0.0100  0.6353\n",
      "      2                     \u001b[36m0.6107\u001b[0m        \u001b[32m0.6684\u001b[0m                     0.5634        0.7021  0.0100  0.6369\n",
      "      3                     \u001b[36m0.6430\u001b[0m        \u001b[32m0.6353\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6197\u001b[0m  0.0098  0.6363\n",
      "      4                     \u001b[36m0.6631\u001b[0m        \u001b[32m0.6170\u001b[0m                     0.6399        0.6292  0.0096  0.6375\n",
      "      5                     \u001b[36m0.6874\u001b[0m        \u001b[32m0.5989\u001b[0m                     \u001b[35m0.6903\u001b[0m        \u001b[31m0.5895\u001b[0m  0.0093  0.6377\n",
      "      6                     \u001b[36m0.6977\u001b[0m        \u001b[32m0.5826\u001b[0m                     \u001b[35m0.6940\u001b[0m        0.5898  0.0090  0.6378\n",
      "      7                     \u001b[36m0.7019\u001b[0m        \u001b[32m0.5683\u001b[0m                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5586\u001b[0m  0.0085  0.6373\n",
      "      8                     \u001b[36m0.7178\u001b[0m        \u001b[32m0.5514\u001b[0m                     0.7183        0.5649  0.0080  0.6379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                     \u001b[36m0.7360\u001b[0m        \u001b[32m0.5420\u001b[0m                     0.7108        0.5847  0.0075  0.6377\n",
      "     10                     0.7313        \u001b[32m0.5408\u001b[0m                     \u001b[35m0.7220\u001b[0m        \u001b[31m0.5453\u001b[0m  0.0069  0.6374\n",
      "     11                     \u001b[36m0.7453\u001b[0m        \u001b[32m0.5253\u001b[0m                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.5350\u001b[0m  0.0063  0.6376\n",
      "     12                     \u001b[36m0.7593\u001b[0m        \u001b[32m0.5075\u001b[0m                     0.7071        0.5655  0.0057  0.6382\n",
      "     13                     0.7486        \u001b[32m0.5028\u001b[0m                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5307\u001b[0m  0.0050  0.6377\n",
      "     14                     \u001b[36m0.7603\u001b[0m        \u001b[32m0.4864\u001b[0m                     0.7164        0.5592  0.0043  0.6381\n",
      "     15                     \u001b[36m0.7710\u001b[0m        \u001b[32m0.4781\u001b[0m                     \u001b[35m0.7444\u001b[0m        0.5345  0.0037  0.6377\n",
      "     16                     \u001b[36m0.7902\u001b[0m        \u001b[32m0.4723\u001b[0m                     \u001b[35m0.7612\u001b[0m        \u001b[31m0.5204\u001b[0m  0.0031  0.6376\n",
      "     17                     0.7808        \u001b[32m0.4692\u001b[0m                     0.7537        \u001b[31m0.5153\u001b[0m  0.0025  0.6377\n",
      "     18                     0.7841        \u001b[32m0.4589\u001b[0m                     0.7369        0.5338  0.0020  0.6383\n",
      "     19                     \u001b[36m0.7953\u001b[0m        \u001b[32m0.4541\u001b[0m                     0.7612        \u001b[31m0.5140\u001b[0m  0.0015  0.6383\n",
      "     20                     \u001b[36m0.8009\u001b[0m        \u001b[32m0.4533\u001b[0m                     0.7444        0.5235  0.0010  0.6378\n",
      "     21                     0.7888        \u001b[32m0.4439\u001b[0m                     0.7425        0.5274  0.0007  0.6380\n",
      "     22                     0.7916        0.4480                     0.7481        0.5172  0.0004  0.6379\n",
      "     23                     0.7986        \u001b[32m0.4381\u001b[0m                     0.7519        0.5179  0.0002  0.6378\n",
      "     24                     0.7939        \u001b[32m0.4344\u001b[0m                     0.7556        0.5177  0.0000  0.6380\n",
      "     25                     \u001b[36m0.8061\u001b[0m        0.4424                     0.7575        0.5176  0.0000  0.6383\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5640\u001b[0m        \u001b[32m0.7346\u001b[0m                     \u001b[35m0.5802\u001b[0m        \u001b[31m0.6781\u001b[0m  0.0100  0.6355\n",
      "      2                     \u001b[36m0.6248\u001b[0m        \u001b[32m0.6513\u001b[0m                     0.5765        \u001b[31m0.6721\u001b[0m  0.0100  0.6379\n",
      "      3                     \u001b[36m0.6790\u001b[0m        \u001b[32m0.6189\u001b[0m                     0.5784        0.6765  0.0098  0.6372\n",
      "      4                     \u001b[36m0.6897\u001b[0m        \u001b[32m0.5933\u001b[0m                     \u001b[35m0.6063\u001b[0m        0.7026  0.0096  0.6371\n",
      "      5                     0.6893        \u001b[32m0.5919\u001b[0m                     \u001b[35m0.6549\u001b[0m        \u001b[31m0.6182\u001b[0m  0.0093  0.6367\n",
      "      6                     \u001b[36m0.7037\u001b[0m        \u001b[32m0.5721\u001b[0m                     0.5746        0.7900  0.0090  0.6380\n",
      "      7                     \u001b[36m0.7154\u001b[0m        \u001b[32m0.5633\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.6055\u001b[0m  0.0085  0.6369\n",
      "      8                     \u001b[36m0.7243\u001b[0m        \u001b[32m0.5397\u001b[0m                     \u001b[35m0.6978\u001b[0m        \u001b[31m0.6051\u001b[0m  0.0080  0.6391\n",
      "      9                     \u001b[36m0.7402\u001b[0m        0.5418                     0.6418        0.6577  0.0075  0.6410\n",
      "     10                     \u001b[36m0.7491\u001b[0m        \u001b[32m0.5257\u001b[0m                     0.6735        0.6733  0.0069  0.6368\n",
      "     11                     \u001b[36m0.7542\u001b[0m        \u001b[32m0.5172\u001b[0m                     \u001b[35m0.7034\u001b[0m        \u001b[31m0.5860\u001b[0m  0.0063  0.6378\n",
      "     12                     0.7533        \u001b[32m0.5092\u001b[0m                     0.6959        0.6091  0.0057  0.6370\n",
      "     13                     \u001b[36m0.7617\u001b[0m        \u001b[32m0.4987\u001b[0m                     0.7015        \u001b[31m0.5769\u001b[0m  0.0050  0.6369\n",
      "     14                     \u001b[36m0.7757\u001b[0m        \u001b[32m0.4852\u001b[0m                     \u001b[35m0.7127\u001b[0m        0.5826  0.0043  0.6369\n",
      "     15                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4701\u001b[0m                     0.7127        0.6024  0.0037  0.6385\n",
      "     16                     0.7776        0.4760                     \u001b[35m0.7146\u001b[0m        0.5927  0.0031  0.6373\n",
      "     17                     \u001b[36m0.7930\u001b[0m        \u001b[32m0.4642\u001b[0m                     0.7108        0.5897  0.0025  0.6367\n",
      "     18                     0.7874        \u001b[32m0.4560\u001b[0m                     0.7090        0.5829  0.0020  0.6361\n",
      "     19                     \u001b[36m0.7953\u001b[0m        \u001b[32m0.4423\u001b[0m                     \u001b[35m0.7220\u001b[0m        \u001b[31m0.5604\u001b[0m  0.0015  0.6375\n",
      "     20                     0.7949        0.4490                     0.7146        0.5606  0.0010  0.6370\n",
      "     21                     \u001b[36m0.7963\u001b[0m        0.4436                     0.7164        0.5611  0.0007  0.6370\n",
      "     22                     \u001b[36m0.7977\u001b[0m        0.4488                     0.7164        \u001b[31m0.5588\u001b[0m  0.0004  0.6382\n",
      "     23                     0.7949        0.4432                     0.7127        0.5588  0.0002  0.6368\n",
      "     24                     0.7902        \u001b[32m0.4350\u001b[0m                     0.7183        \u001b[31m0.5581\u001b[0m  0.0000  0.6363\n",
      "     25                     \u001b[36m0.8098\u001b[0m        \u001b[32m0.4286\u001b[0m                     \u001b[35m0.7257\u001b[0m        0.5586  0.0000  0.6377\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5607\u001b[0m        \u001b[32m0.7349\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6358\u001b[0m  0.0100  0.6350\n",
      "      2                     \u001b[36m0.6215\u001b[0m        \u001b[32m0.6568\u001b[0m                     \u001b[35m0.6754\u001b[0m        \u001b[31m0.6178\u001b[0m  0.0100  0.6377\n",
      "      3                     \u001b[36m0.6607\u001b[0m        \u001b[32m0.6128\u001b[0m                     0.6175        0.6640  0.0098  0.6368\n",
      "      4                     \u001b[36m0.6692\u001b[0m        \u001b[32m0.6063\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.6045\u001b[0m  0.0096  0.6370\n",
      "      5                     \u001b[36m0.7014\u001b[0m        \u001b[32m0.5764\u001b[0m                     \u001b[35m0.6996\u001b[0m        \u001b[31m0.6024\u001b[0m  0.0093  0.6366\n",
      "      6                     \u001b[36m0.7178\u001b[0m        \u001b[32m0.5604\u001b[0m                     \u001b[35m0.7164\u001b[0m        0.6139  0.0090  0.6365\n",
      "      7                     0.7084        0.5661                     \u001b[35m0.7183\u001b[0m        0.6041  0.0085  0.6364\n",
      "      8                     \u001b[36m0.7379\u001b[0m        \u001b[32m0.5348\u001b[0m                     \u001b[35m0.7276\u001b[0m        \u001b[31m0.5790\u001b[0m  0.0080  0.6370\n",
      "      9                     \u001b[36m0.7481\u001b[0m        \u001b[32m0.5184\u001b[0m                     \u001b[35m0.7388\u001b[0m        \u001b[31m0.5694\u001b[0m  0.0075  0.6366\n",
      "     10                     \u001b[36m0.7547\u001b[0m        0.5222                     0.7257        0.5864  0.0069  0.6373\n",
      "     11                     \u001b[36m0.7556\u001b[0m        \u001b[32m0.5087\u001b[0m                     0.6791        0.6891  0.0063  0.6371\n",
      "     12                     0.7495        0.5137                     0.7257        0.5812  0.0057  0.6373\n",
      "     13                     \u001b[36m0.7617\u001b[0m        \u001b[32m0.4966\u001b[0m                     \u001b[35m0.7444\u001b[0m        \u001b[31m0.5488\u001b[0m  0.0050  0.6369\n",
      "     14                     \u001b[36m0.7748\u001b[0m        \u001b[32m0.4675\u001b[0m                     \u001b[35m0.7463\u001b[0m        0.5518  0.0043  0.6368\n",
      "     15                     0.7692        0.4715                     0.7276        \u001b[31m0.5357\u001b[0m  0.0037  0.6357\n",
      "     16                     0.7678        0.4756                     0.7369        0.5449  0.0031  0.6382\n",
      "     17                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4664\u001b[0m                     0.7407        0.5363  0.0025  0.6375\n",
      "     18                     \u001b[36m0.7888\u001b[0m        \u001b[32m0.4592\u001b[0m                     0.7425        0.5466  0.0020  0.6365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19                     \u001b[36m0.7991\u001b[0m        \u001b[32m0.4409\u001b[0m                     0.7444        0.5443  0.0015  0.6362\n",
      "     20                     \u001b[36m0.8009\u001b[0m        \u001b[32m0.4405\u001b[0m                     \u001b[35m0.7519\u001b[0m        0.5403  0.0010  0.6373\n",
      "     21                     0.7972        0.4449                     \u001b[35m0.7575\u001b[0m        0.5375  0.0007  0.6379\n",
      "     22                     0.7977        0.4405                     0.7519        0.5383  0.0004  0.6375\n",
      "     23                     0.8000        \u001b[32m0.4398\u001b[0m                     0.7519        0.5362  0.0002  0.6375\n",
      "     24                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4332\u001b[0m                     0.7481        \u001b[31m0.5353\u001b[0m  0.0000  0.6368\n",
      "     25                     0.8023        0.4373                     0.7444        \u001b[31m0.5351\u001b[0m  0.0000  0.6370\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5481\u001b[0m        \u001b[32m0.7317\u001b[0m                     \u001b[35m0.6325\u001b[0m        \u001b[31m0.6506\u001b[0m  0.0100  0.6352\n",
      "      2                     \u001b[36m0.6150\u001b[0m        \u001b[32m0.6512\u001b[0m                     \u001b[35m0.6623\u001b[0m        \u001b[31m0.5969\u001b[0m  0.0100  0.6373\n",
      "      3                     \u001b[36m0.6612\u001b[0m        \u001b[32m0.6171\u001b[0m                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.5952\u001b[0m  0.0098  0.6365\n",
      "      4                     \u001b[36m0.6729\u001b[0m        \u001b[32m0.5959\u001b[0m                     0.7052        \u001b[31m0.5767\u001b[0m  0.0096  0.6367\n",
      "      5                     \u001b[36m0.6925\u001b[0m        \u001b[32m0.5865\u001b[0m                     \u001b[35m0.7220\u001b[0m        \u001b[31m0.5628\u001b[0m  0.0093  0.6371\n",
      "      6                     \u001b[36m0.7201\u001b[0m        \u001b[32m0.5673\u001b[0m                     \u001b[35m0.7388\u001b[0m        \u001b[31m0.5545\u001b[0m  0.0090  0.6391\n",
      "      7                     \u001b[36m0.7294\u001b[0m        \u001b[32m0.5515\u001b[0m                     0.7369        \u001b[31m0.5445\u001b[0m  0.0085  0.6376\n",
      "      8                     \u001b[36m0.7336\u001b[0m        \u001b[32m0.5452\u001b[0m                     0.7034        0.5894  0.0080  0.6374\n",
      "      9                     \u001b[36m0.7411\u001b[0m        \u001b[32m0.5244\u001b[0m                     0.7201        0.5642  0.0075  0.6374\n",
      "     10                     0.7397        0.5348                     0.7257        0.5533  0.0069  0.6372\n",
      "     11                     \u001b[36m0.7533\u001b[0m        \u001b[32m0.5157\u001b[0m                     0.7369        \u001b[31m0.5370\u001b[0m  0.0063  0.6375\n",
      "     12                     \u001b[36m0.7659\u001b[0m        \u001b[32m0.5028\u001b[0m                     0.7388        \u001b[31m0.5357\u001b[0m  0.0057  0.6372\n",
      "     13                     0.7561        \u001b[32m0.5012\u001b[0m                     \u001b[35m0.7612\u001b[0m        \u001b[31m0.5221\u001b[0m  0.0050  0.6383\n",
      "     14                     0.7645        \u001b[32m0.4910\u001b[0m                     0.7500        0.5460  0.0043  0.6375\n",
      "     15                     \u001b[36m0.7678\u001b[0m        \u001b[32m0.4907\u001b[0m                     0.7556        0.5310  0.0037  0.6382\n",
      "     16                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4705\u001b[0m                     0.7519        0.5387  0.0031  0.6367\n",
      "     17                     0.7701        0.4830                     0.7500        0.5273  0.0025  0.6371\n",
      "     18                     0.7738        \u001b[32m0.4612\u001b[0m                     0.7444        0.5359  0.0020  0.6379\n",
      "     19                     \u001b[36m0.7916\u001b[0m        \u001b[32m0.4535\u001b[0m                     0.7519        0.5342  0.0015  0.6380\n",
      "     20                     0.7902        \u001b[32m0.4471\u001b[0m                     0.7481        \u001b[31m0.5220\u001b[0m  0.0010  0.6368\n",
      "     21                     0.7869        0.4475                     0.7537        \u001b[31m0.5220\u001b[0m  0.0007  0.6375\n",
      "     22                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4441\u001b[0m                     0.7519        0.5254  0.0004  0.6367\n",
      "     23                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4399\u001b[0m                     0.7519        0.5235  0.0002  0.6364\n",
      "     24                     \u001b[36m0.8019\u001b[0m        0.4431                     0.7519        0.5233  0.0000  0.6386\n",
      "     25                     0.7981        \u001b[32m0.4347\u001b[0m                     0.7519        0.5230  0.0000  0.6377\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5224\u001b[0m        \u001b[32m0.7552\u001b[0m                     \u001b[35m0.5709\u001b[0m        \u001b[31m0.6958\u001b[0m  0.0100  0.6356\n",
      "      2                     \u001b[36m0.5907\u001b[0m        \u001b[32m0.6771\u001b[0m                     \u001b[35m0.6642\u001b[0m        \u001b[31m0.6449\u001b[0m  0.0100  0.6387\n",
      "      3                     \u001b[36m0.6294\u001b[0m        \u001b[32m0.6453\u001b[0m                     0.6530        \u001b[31m0.6212\u001b[0m  0.0098  0.6379\n",
      "      4                     \u001b[36m0.6729\u001b[0m        \u001b[32m0.6113\u001b[0m                     \u001b[35m0.6847\u001b[0m        \u001b[31m0.6114\u001b[0m  0.0096  0.6372\n",
      "      5                     \u001b[36m0.6841\u001b[0m        \u001b[32m0.5978\u001b[0m                     \u001b[35m0.6978\u001b[0m        \u001b[31m0.6063\u001b[0m  0.0093  0.6373\n",
      "      6                     0.6804        0.5991                     0.6810        0.6115  0.0090  0.6379\n",
      "      7                     \u001b[36m0.7061\u001b[0m        \u001b[32m0.5829\u001b[0m                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.5783\u001b[0m  0.0085  0.6381\n",
      "      8                     0.6963        \u001b[32m0.5731\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5770\u001b[0m  0.0080  0.6378\n",
      "      9                     \u001b[36m0.7070\u001b[0m        0.5774                     0.6810        0.6189  0.0075  0.6382\n",
      "     10                     0.7070        \u001b[32m0.5677\u001b[0m                     0.6903        0.6137  0.0069  0.6369\n",
      "     11                     \u001b[36m0.7234\u001b[0m        \u001b[32m0.5501\u001b[0m                     0.7015        0.5855  0.0063  0.6380\n",
      "     12                     0.7121        0.5625                     0.7090        \u001b[31m0.5647\u001b[0m  0.0057  0.6374\n",
      "     13                     0.7234        \u001b[32m0.5462\u001b[0m                     0.7034        0.5756  0.0050  0.6375\n",
      "     14                     \u001b[36m0.7369\u001b[0m        \u001b[32m0.5335\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5534\u001b[0m  0.0043  0.6379\n",
      "     15                     0.7364        \u001b[32m0.5239\u001b[0m                     0.7015        0.5667  0.0037  0.6372\n",
      "     16                     \u001b[36m0.7458\u001b[0m        \u001b[32m0.5164\u001b[0m                     0.7034        0.5606  0.0031  0.6381\n",
      "     17                     0.7439        \u001b[32m0.5146\u001b[0m                     0.7201        0.5563  0.0025  0.6371\n",
      "     18                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.4954\u001b[0m                     0.7313        \u001b[31m0.5419\u001b[0m  0.0020  0.6371\n",
      "     19                     \u001b[36m0.7528\u001b[0m        0.5031                     0.7239        0.5481  0.0015  0.6382\n",
      "     20                     \u001b[36m0.7533\u001b[0m        0.5005                     0.7276        0.5506  0.0010  0.6388\n",
      "     21                     \u001b[36m0.7593\u001b[0m        \u001b[32m0.4952\u001b[0m                     0.6996        0.5629  0.0007  0.6373\n",
      "     22                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4868\u001b[0m                     0.7071        0.5574  0.0004  0.6381\n",
      "     23                     \u001b[36m0.7682\u001b[0m        \u001b[32m0.4806\u001b[0m                     0.7146        0.5474  0.0002  0.6375\n",
      "     24                     \u001b[36m0.7692\u001b[0m        0.4807                     0.7257        0.5432  0.0000  0.6374\n",
      "     25                     0.7678        0.4815                     0.7220        \u001b[31m0.5406\u001b[0m  0.0000  0.6383\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5500\u001b[0m        \u001b[32m0.7601\u001b[0m                     \u001b[35m0.5504\u001b[0m        \u001b[31m0.7498\u001b[0m  0.0100  0.6372\n",
      "      2                     \u001b[36m0.5907\u001b[0m        \u001b[32m0.6657\u001b[0m                     \u001b[35m0.5914\u001b[0m        \u001b[31m0.6815\u001b[0m  0.0100  0.6412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3                     \u001b[36m0.6271\u001b[0m        \u001b[32m0.6447\u001b[0m                     0.5466        0.7434  0.0098  0.6371\n",
      "      4                     \u001b[36m0.6589\u001b[0m        \u001b[32m0.6144\u001b[0m                     \u001b[35m0.6063\u001b[0m        \u001b[31m0.6808\u001b[0m  0.0096  0.6369\n",
      "      5                     \u001b[36m0.6715\u001b[0m        \u001b[32m0.6117\u001b[0m                     0.6026        0.6907  0.0093  0.6369\n",
      "      6                     \u001b[36m0.6981\u001b[0m        \u001b[32m0.5886\u001b[0m                     \u001b[35m0.6343\u001b[0m        \u001b[31m0.6301\u001b[0m  0.0090  0.6370\n",
      "      7                     0.6935        \u001b[32m0.5845\u001b[0m                     0.6007        0.7304  0.0085  0.6362\n",
      "      8                     \u001b[36m0.7192\u001b[0m        \u001b[32m0.5727\u001b[0m                     0.5784        0.7255  0.0080  0.6364\n",
      "      9                     0.7112        \u001b[32m0.5681\u001b[0m                     \u001b[35m0.6978\u001b[0m        \u001b[31m0.5803\u001b[0m  0.0075  0.6373\n",
      "     10                     \u001b[36m0.7346\u001b[0m        \u001b[32m0.5470\u001b[0m                     \u001b[35m0.6996\u001b[0m        0.5867  0.0069  0.6378\n",
      "     11                     0.7140        0.5533                     0.6996        \u001b[31m0.5794\u001b[0m  0.0063  0.6370\n",
      "     12                     \u001b[36m0.7397\u001b[0m        \u001b[32m0.5444\u001b[0m                     \u001b[35m0.7146\u001b[0m        \u001b[31m0.5695\u001b[0m  0.0057  0.6367\n",
      "     13                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5319\u001b[0m                     0.7052        \u001b[31m0.5652\u001b[0m  0.0050  0.6372\n",
      "     14                     \u001b[36m0.7495\u001b[0m        \u001b[32m0.5252\u001b[0m                     0.6940        0.5802  0.0043  0.6386\n",
      "     15                     \u001b[36m0.7593\u001b[0m        \u001b[32m0.5093\u001b[0m                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.5551\u001b[0m  0.0037  0.6376\n",
      "     16                     \u001b[36m0.7654\u001b[0m        \u001b[32m0.5064\u001b[0m                     0.7201        0.5776  0.0031  0.6378\n",
      "     17                     0.7551        \u001b[32m0.5021\u001b[0m                     0.7108        0.5663  0.0025  0.6380\n",
      "     18                     0.7589        0.5026                     0.7108        \u001b[31m0.5485\u001b[0m  0.0020  0.6379\n",
      "     19                     \u001b[36m0.7696\u001b[0m        \u001b[32m0.4820\u001b[0m                     0.7146        0.5580  0.0015  0.6373\n",
      "     20                     \u001b[36m0.7771\u001b[0m        \u001b[32m0.4816\u001b[0m                     0.7164        0.5569  0.0010  0.6367\n",
      "     21                     \u001b[36m0.7818\u001b[0m        \u001b[32m0.4794\u001b[0m                     0.7146        0.5502  0.0007  0.6375\n",
      "     22                     0.7799        \u001b[32m0.4760\u001b[0m                     \u001b[35m0.7257\u001b[0m        0.5487  0.0004  0.6388\n",
      "     23                     0.7659        0.4826                     0.7146        \u001b[31m0.5467\u001b[0m  0.0002  0.6366\n",
      "     24                     0.7687        0.4790                     0.7201        \u001b[31m0.5459\u001b[0m  0.0000  0.6371\n",
      "     25                     0.7724        0.4812                     0.7220        \u001b[31m0.5456\u001b[0m  0.0000  0.6369\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5243\u001b[0m        \u001b[32m0.7464\u001b[0m                     \u001b[35m0.5989\u001b[0m        \u001b[31m0.6584\u001b[0m  0.0100  0.6350\n",
      "      2                     \u001b[36m0.6192\u001b[0m        \u001b[32m0.6532\u001b[0m                     \u001b[35m0.6213\u001b[0m        \u001b[31m0.6501\u001b[0m  0.0100  0.6402\n",
      "      3                     \u001b[36m0.6467\u001b[0m        \u001b[32m0.6229\u001b[0m                     \u001b[35m0.6679\u001b[0m        \u001b[31m0.6139\u001b[0m  0.0098  0.6393\n",
      "      4                     \u001b[36m0.6776\u001b[0m        \u001b[32m0.6023\u001b[0m                     0.6623        0.6154  0.0096  0.6389\n",
      "      5                     \u001b[36m0.7075\u001b[0m        \u001b[32m0.5850\u001b[0m                     \u001b[35m0.6978\u001b[0m        \u001b[31m0.6037\u001b[0m  0.0093  0.6374\n",
      "      6                     \u001b[36m0.7107\u001b[0m        \u001b[32m0.5806\u001b[0m                     0.6903        0.6093  0.0090  0.6371\n",
      "      7                     \u001b[36m0.7168\u001b[0m        \u001b[32m0.5646\u001b[0m                     0.6959        0.6196  0.0085  0.6375\n",
      "      8                     \u001b[36m0.7313\u001b[0m        \u001b[32m0.5483\u001b[0m                     0.6922        \u001b[31m0.5872\u001b[0m  0.0080  0.6373\n",
      "      9                     \u001b[36m0.7491\u001b[0m        \u001b[32m0.5287\u001b[0m                     \u001b[35m0.7369\u001b[0m        \u001b[31m0.5666\u001b[0m  0.0075  0.6389\n",
      "     10                     0.7397        0.5451                     0.6791        0.5998  0.0069  0.6656\n",
      "     11                     0.7486        \u001b[32m0.5257\u001b[0m                     0.7183        0.5835  0.0063  0.6376\n",
      "     12                     \u001b[36m0.7533\u001b[0m        \u001b[32m0.5102\u001b[0m                     0.7034        0.5825  0.0057  0.6377\n",
      "     13                     \u001b[36m0.7664\u001b[0m        \u001b[32m0.5075\u001b[0m                     \u001b[35m0.7388\u001b[0m        \u001b[31m0.5554\u001b[0m  0.0050  0.6513\n",
      "     14                     \u001b[36m0.7678\u001b[0m        \u001b[32m0.4928\u001b[0m                     0.7164        \u001b[31m0.5507\u001b[0m  0.0043  0.6434\n",
      "     15                     \u001b[36m0.7794\u001b[0m        \u001b[32m0.4846\u001b[0m                     0.7220        0.5591  0.0037  0.6408\n",
      "     16                     0.7687        0.4873                     0.7257        \u001b[31m0.5472\u001b[0m  0.0031  0.6375\n",
      "     17                     0.7771        \u001b[32m0.4751\u001b[0m                     0.7108        0.5621  0.0025  0.6383\n",
      "     18                     \u001b[36m0.7921\u001b[0m        \u001b[32m0.4620\u001b[0m                     0.7369        \u001b[31m0.5437\u001b[0m  0.0020  0.6377\n",
      "     19                     0.7879        \u001b[32m0.4531\u001b[0m                     0.7276        0.5476  0.0015  0.6381\n",
      "     20                     0.7921        0.4563                     0.7313        0.5442  0.0010  0.6370\n",
      "     21                     \u001b[36m0.8005\u001b[0m        \u001b[32m0.4432\u001b[0m                     0.7313        \u001b[31m0.5425\u001b[0m  0.0007  0.6365\n",
      "     22                     0.7953        0.4437                     0.7239        0.5450  0.0004  0.6393\n",
      "     23                     0.7930        0.4533                     0.7313        0.5437  0.0002  0.6393\n",
      "     24                     0.7907        0.4519                     0.7276        0.5438  0.0000  0.6375\n",
      "     25                     \u001b[36m0.8009\u001b[0m        0.4437                     0.7257        0.5439  0.0000  0.6391\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5640\u001b[0m        \u001b[32m0.7249\u001b[0m                     \u001b[35m0.5485\u001b[0m        \u001b[31m0.6992\u001b[0m  0.0100  0.6365\n",
      "      2                     \u001b[36m0.6379\u001b[0m        \u001b[32m0.6498\u001b[0m                     \u001b[35m0.6231\u001b[0m        \u001b[31m0.6394\u001b[0m  0.0100  0.6367\n",
      "      3                     \u001b[36m0.6706\u001b[0m        \u001b[32m0.6143\u001b[0m                     \u001b[35m0.6679\u001b[0m        \u001b[31m0.6096\u001b[0m  0.0098  0.6379\n",
      "      4                     \u001b[36m0.6949\u001b[0m        \u001b[32m0.5856\u001b[0m                     \u001b[35m0.6828\u001b[0m        \u001b[31m0.6049\u001b[0m  0.0096  0.6362\n",
      "      5                     \u001b[36m0.7065\u001b[0m        \u001b[32m0.5675\u001b[0m                     0.6679        0.6248  0.0093  0.6371\n",
      "      6                     \u001b[36m0.7192\u001b[0m        \u001b[32m0.5638\u001b[0m                     \u001b[35m0.7108\u001b[0m        \u001b[31m0.5816\u001b[0m  0.0090  0.6394\n",
      "      7                     \u001b[36m0.7425\u001b[0m        \u001b[32m0.5457\u001b[0m                     0.7034        0.6040  0.0085  0.6370\n",
      "      8                     0.7285        \u001b[32m0.5440\u001b[0m                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5782\u001b[0m  0.0080  0.6383\n",
      "      9                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5163\u001b[0m                     0.7183        \u001b[31m0.5635\u001b[0m  0.0075  0.6372\n",
      "     10                     \u001b[36m0.7533\u001b[0m        \u001b[32m0.5120\u001b[0m                     0.7146        \u001b[31m0.5602\u001b[0m  0.0069  0.6377\n",
      "     11                     0.7514        \u001b[32m0.5078\u001b[0m                     0.7164        0.5673  0.0063  0.6396\n",
      "     12                     0.7523        \u001b[32m0.5032\u001b[0m                     \u001b[35m0.7500\u001b[0m        \u001b[31m0.5357\u001b[0m  0.0057  0.6374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13                     \u001b[36m0.7584\u001b[0m        0.5036                     0.7183        0.5521  0.0050  0.6378\n",
      "     14                     \u001b[36m0.7645\u001b[0m        \u001b[32m0.4787\u001b[0m                     0.7201        0.5522  0.0043  0.6370\n",
      "     15                     \u001b[36m0.7757\u001b[0m        \u001b[32m0.4769\u001b[0m                     0.7369        0.5547  0.0037  0.6385\n",
      "     16                     \u001b[36m0.7883\u001b[0m        \u001b[32m0.4592\u001b[0m                     \u001b[35m0.7705\u001b[0m        0.5371  0.0031  0.6380\n",
      "     17                     0.7841        0.4644                     0.7500        0.5430  0.0025  0.6377\n",
      "     18                     0.7766        0.4667                     0.7668        0.5368  0.0020  0.6394\n",
      "     19                     \u001b[36m0.7953\u001b[0m        \u001b[32m0.4507\u001b[0m                     0.7463        0.5363  0.0015  0.6386\n",
      "     20                     \u001b[36m0.7995\u001b[0m        \u001b[32m0.4485\u001b[0m                     0.7593        0.5392  0.0010  0.6377\n",
      "     21                     0.7897        \u001b[32m0.4455\u001b[0m                     0.7463        \u001b[31m0.5316\u001b[0m  0.0007  0.6387\n",
      "     22                     0.7907        \u001b[32m0.4399\u001b[0m                     0.7537        \u001b[31m0.5301\u001b[0m  0.0004  0.6390\n",
      "     23                     0.7874        0.4406                     0.7556        0.5310  0.0002  0.6375\n",
      "     24                     \u001b[36m0.8061\u001b[0m        \u001b[32m0.4323\u001b[0m                     0.7556        \u001b[31m0.5297\u001b[0m  0.0000  0.6383\n",
      "     25                     0.8051        0.4380                     0.7556        \u001b[31m0.5291\u001b[0m  0.0000  0.6382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6201\u001b[0m        \u001b[32m0.6670\u001b[0m                     \u001b[35m0.6866\u001b[0m        \u001b[31m0.6168\u001b[0m  0.0100  0.6378\n",
      "      2                     \u001b[36m0.7107\u001b[0m        \u001b[32m0.5748\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5438\u001b[0m  0.0100  0.6414\n",
      "      3                     \u001b[36m0.7224\u001b[0m        \u001b[32m0.5526\u001b[0m                     0.7332        \u001b[31m0.5229\u001b[0m  0.0098  0.6392\n",
      "      4                     \u001b[36m0.7285\u001b[0m        \u001b[32m0.5334\u001b[0m                     \u001b[35m0.7425\u001b[0m        0.5546  0.0096  0.6381\n",
      "      5                     0.7266        0.5373                     0.7220        0.5725  0.0093  0.6374\n",
      "      6                     \u001b[36m0.7430\u001b[0m        \u001b[32m0.5237\u001b[0m                     0.7407        \u001b[31m0.5122\u001b[0m  0.0090  0.6390\n",
      "      7                     0.7393        \u001b[32m0.5189\u001b[0m                     0.7071        0.5823  0.0085  0.6374\n",
      "      8                     \u001b[36m0.7654\u001b[0m        \u001b[32m0.5057\u001b[0m                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.4940\u001b[0m  0.0080  0.6391\n",
      "      9                     0.7523        \u001b[32m0.5056\u001b[0m                     0.7481        0.5041  0.0075  0.6391\n",
      "     10                     \u001b[36m0.7724\u001b[0m        \u001b[32m0.4817\u001b[0m                     \u001b[35m0.7705\u001b[0m        0.5020  0.0069  0.6381\n",
      "     11                     0.7607        \u001b[32m0.4742\u001b[0m                     \u001b[35m0.7743\u001b[0m        \u001b[31m0.4848\u001b[0m  0.0063  0.6400\n",
      "     12                     0.7701        0.4835                     0.6679        0.8655  0.0057  0.6380\n",
      "     13                     0.7668        0.4758                     0.7257        0.6225  0.0050  0.6375\n",
      "     14                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4598\u001b[0m                     0.7743        0.4914  0.0043  0.6391\n",
      "     15                     \u001b[36m0.7893\u001b[0m        \u001b[32m0.4473\u001b[0m                     0.7649        0.5132  0.0037  0.6413\n",
      "     16                     0.7841        0.4476                     0.7537        0.5248  0.0031  0.6433\n",
      "     17                     \u001b[36m0.7902\u001b[0m        \u001b[32m0.4411\u001b[0m                     0.7444        0.5177  0.0025  0.6423\n",
      "     18                     0.7804        0.4521                     0.7631        0.4987  0.0020  0.6413\n",
      "     19                     \u001b[36m0.8019\u001b[0m        \u001b[32m0.4290\u001b[0m                     0.7631        0.4949  0.0015  0.6492\n",
      "     20                     0.7995        0.4354                     \u001b[35m0.7780\u001b[0m        0.4981  0.0010  0.6413\n",
      "     21                     \u001b[36m0.8028\u001b[0m        \u001b[32m0.4274\u001b[0m                     0.7743        0.4936  0.0007  0.6413\n",
      "     22                     0.7981        0.4305                     0.7780        0.4921  0.0004  0.6383\n",
      "     23                     0.8028        \u001b[32m0.4239\u001b[0m                     0.7761        0.4931  0.0002  0.6363\n",
      "     24                     0.8019        \u001b[32m0.4214\u001b[0m                     0.7724        0.4930  0.0000  0.6373\n",
      "     25                     0.7949        0.4316                     0.7724        0.4934  0.0000  0.6393\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6421\u001b[0m        \u001b[32m0.6632\u001b[0m                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5782\u001b[0m  0.0100  0.6393\n",
      "      2                     \u001b[36m0.7248\u001b[0m        \u001b[32m0.5645\u001b[0m                     \u001b[35m0.7257\u001b[0m        \u001b[31m0.5285\u001b[0m  0.0100  0.6433\n",
      "      3                     \u001b[36m0.7252\u001b[0m        \u001b[32m0.5406\u001b[0m                     0.7090        0.5907  0.0098  0.6423\n",
      "      4                     \u001b[36m0.7379\u001b[0m        \u001b[32m0.5372\u001b[0m                     0.7164        0.5783  0.0096  0.6393\n",
      "      5                     0.7369        \u001b[32m0.5301\u001b[0m                     0.7127        0.5655  0.0093  0.6413\n",
      "      6                     \u001b[36m0.7551\u001b[0m        \u001b[32m0.5202\u001b[0m                     \u001b[35m0.7481\u001b[0m        0.5310  0.0090  0.6712\n",
      "      7                     \u001b[36m0.7598\u001b[0m        \u001b[32m0.4950\u001b[0m                     \u001b[35m0.7537\u001b[0m        \u001b[31m0.5038\u001b[0m  0.0085  0.6443\n",
      "      8                     0.7584        0.4977                     0.7257        0.6099  0.0080  0.6463\n",
      "      9                     \u001b[36m0.7696\u001b[0m        \u001b[32m0.4808\u001b[0m                     0.7463        0.5332  0.0075  0.6443\n",
      "     10                     \u001b[36m0.7715\u001b[0m        \u001b[32m0.4807\u001b[0m                     \u001b[35m0.7593\u001b[0m        0.5072  0.0069  0.6453\n",
      "     11                     0.7696        \u001b[32m0.4740\u001b[0m                     \u001b[35m0.7631\u001b[0m        0.5283  0.0063  0.6443\n",
      "     12                     \u001b[36m0.7822\u001b[0m        \u001b[32m0.4572\u001b[0m                     0.7071        0.7212  0.0057  0.6443\n",
      "     13                     \u001b[36m0.7836\u001b[0m        0.4619                     0.7239        0.6390  0.0050  0.6453\n",
      "     14                     0.7832        0.4582                     0.7313        0.6700  0.0043  0.6453\n",
      "     15                     0.7808        \u001b[32m0.4551\u001b[0m                     0.7481        0.5236  0.0037  0.6443\n",
      "     16                     \u001b[36m0.7883\u001b[0m        \u001b[32m0.4447\u001b[0m                     0.6567        0.6559  0.0031  0.6433\n",
      "     17                     \u001b[36m0.7930\u001b[0m        \u001b[32m0.4355\u001b[0m                     0.7519        0.5347  0.0025  0.6423\n",
      "     18                     \u001b[36m0.8014\u001b[0m        \u001b[32m0.4301\u001b[0m                     \u001b[35m0.7705\u001b[0m        0.5127  0.0020  0.6443\n",
      "     19                     0.7963        \u001b[32m0.4275\u001b[0m                     0.7649        0.5149  0.0015  0.6453\n",
      "     20                     \u001b[36m0.8061\u001b[0m        \u001b[32m0.4163\u001b[0m                     0.7463        0.5320  0.0010  0.6423\n",
      "     21                     \u001b[36m0.8089\u001b[0m        0.4180                     0.7575        0.5181  0.0007  0.6443\n",
      "     22                     \u001b[36m0.8098\u001b[0m        0.4194                     0.7556        0.5143  0.0004  0.6453\n",
      "     23                     0.8033        \u001b[32m0.4139\u001b[0m                     \u001b[35m0.7761\u001b[0m        0.5108  0.0002  0.6463\n",
      "     24                     \u001b[36m0.8103\u001b[0m        0.4142                     0.7724        0.5110  0.0000  0.6453\n",
      "     25                     0.8070        \u001b[32m0.4125\u001b[0m                     0.7705        0.5111  0.0000  0.6453\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6411\u001b[0m        \u001b[32m0.6638\u001b[0m                     \u001b[35m0.7071\u001b[0m        \u001b[31m0.5667\u001b[0m  0.0100  0.6443\n",
      "      2                     \u001b[36m0.7145\u001b[0m        \u001b[32m0.5637\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5509\u001b[0m  0.0100  0.6453\n",
      "      3                     \u001b[36m0.7266\u001b[0m        \u001b[32m0.5565\u001b[0m                     0.7239        \u001b[31m0.5423\u001b[0m  0.0098  0.6453\n",
      "      4                     \u001b[36m0.7360\u001b[0m        \u001b[32m0.5258\u001b[0m                     0.7220        0.5988  0.0096  0.6443\n",
      "      5                     \u001b[36m0.7444\u001b[0m        \u001b[32m0.5227\u001b[0m                     0.7276        0.5823  0.0093  0.6453\n",
      "      6                     \u001b[36m0.7491\u001b[0m        \u001b[32m0.5137\u001b[0m                     0.7090        0.6079  0.0090  0.6453\n",
      "      7                     \u001b[36m0.7551\u001b[0m        \u001b[32m0.5078\u001b[0m                     0.7220        0.5663  0.0085  0.6453\n",
      "      8                     \u001b[36m0.7636\u001b[0m        \u001b[32m0.4911\u001b[0m                     0.7052        0.5705  0.0080  0.6462\n",
      "      9                     \u001b[36m0.7762\u001b[0m        \u001b[32m0.4901\u001b[0m                     0.7220        0.5505  0.0075  0.6443\n",
      "     10                     0.7650        0.5007                     0.7201        \u001b[31m0.5292\u001b[0m  0.0069  0.6453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     0.7710        \u001b[32m0.4827\u001b[0m                     0.7332        0.5369  0.0063  0.6453\n",
      "     12                     \u001b[36m0.7780\u001b[0m        \u001b[32m0.4730\u001b[0m                     0.7239        0.5338  0.0057  0.6433\n",
      "     13                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4613\u001b[0m                     \u001b[35m0.7444\u001b[0m        0.5440  0.0050  0.6403\n",
      "     14                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4598\u001b[0m                     0.7071        0.5975  0.0043  0.6403\n",
      "     15                     0.7846        \u001b[32m0.4586\u001b[0m                     0.7108        0.5633  0.0037  0.6393\n",
      "     16                     0.7925        \u001b[32m0.4426\u001b[0m                     0.7407        \u001b[31m0.5182\u001b[0m  0.0031  0.6393\n",
      "     17                     \u001b[36m0.8056\u001b[0m        \u001b[32m0.4361\u001b[0m                     0.7127        0.5295  0.0025  0.6413\n",
      "     18                     0.8051        \u001b[32m0.4336\u001b[0m                     0.7351        \u001b[31m0.5099\u001b[0m  0.0020  0.6433\n",
      "     19                     0.7958        \u001b[32m0.4290\u001b[0m                     0.7332        0.5338  0.0015  0.6423\n",
      "     20                     \u001b[36m0.8065\u001b[0m        \u001b[32m0.4285\u001b[0m                     0.7388        0.5123  0.0010  0.6423\n",
      "     21                     0.7953        0.4386                     0.7407        0.5181  0.0007  0.6413\n",
      "     22                     \u001b[36m0.8103\u001b[0m        \u001b[32m0.4283\u001b[0m                     0.7407        \u001b[31m0.5077\u001b[0m  0.0004  0.6403\n",
      "     23                     \u001b[36m0.8126\u001b[0m        \u001b[32m0.4228\u001b[0m                     0.7407        0.5078  0.0002  0.6413\n",
      "     24                     \u001b[36m0.8140\u001b[0m        \u001b[32m0.4204\u001b[0m                     0.7369        \u001b[31m0.5068\u001b[0m  0.0000  0.6403\n",
      "     25                     0.8047        \u001b[32m0.4183\u001b[0m                     0.7369        0.5068  0.0000  0.6403\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6252\u001b[0m        \u001b[32m0.6650\u001b[0m                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.5486\u001b[0m  0.0100  0.6373\n",
      "      2                     \u001b[36m0.7196\u001b[0m        \u001b[32m0.5568\u001b[0m                     \u001b[35m0.7257\u001b[0m        \u001b[31m0.5395\u001b[0m  0.0100  0.6413\n",
      "      3                     \u001b[36m0.7248\u001b[0m        \u001b[32m0.5420\u001b[0m                     0.7183        \u001b[31m0.5394\u001b[0m  0.0098  0.6413\n",
      "      4                     \u001b[36m0.7364\u001b[0m        \u001b[32m0.5267\u001b[0m                     \u001b[35m0.7425\u001b[0m        \u001b[31m0.5055\u001b[0m  0.0096  0.6413\n",
      "      5                     \u001b[36m0.7425\u001b[0m        \u001b[32m0.5248\u001b[0m                     \u001b[35m0.7556\u001b[0m        0.5208  0.0093  0.6393\n",
      "      6                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5101\u001b[0m                     0.6604        0.8281  0.0090  0.6363\n",
      "      7                     0.7407        0.5110                     0.7444        0.5270  0.0085  0.6393\n",
      "      8                     0.7491        \u001b[32m0.5010\u001b[0m                     0.7519        0.5193  0.0080  0.6413\n",
      "      9                     \u001b[36m0.7715\u001b[0m        \u001b[32m0.4831\u001b[0m                     0.7313        0.5412  0.0075  0.6423\n",
      "     10                     0.7692        \u001b[32m0.4782\u001b[0m                     0.7034        0.6222  0.0069  0.6413\n",
      "     11                     0.7682        \u001b[32m0.4705\u001b[0m                     0.7537        \u001b[31m0.5036\u001b[0m  0.0063  0.6413\n",
      "     12                     0.7687        \u001b[32m0.4669\u001b[0m                     0.6269        0.7337  0.0057  0.6413\n",
      "     13                     \u001b[36m0.7776\u001b[0m        \u001b[32m0.4576\u001b[0m                     0.7332        0.5581  0.0050  0.6413\n",
      "     14                     \u001b[36m0.7841\u001b[0m        0.4585                     0.7537        0.5192  0.0043  0.6413\n",
      "     15                     \u001b[36m0.7916\u001b[0m        \u001b[32m0.4390\u001b[0m                     0.7034        0.5577  0.0037  0.6453\n",
      "     16                     0.7888        0.4395                     0.7183        0.7262  0.0031  0.6423\n",
      "     17                     0.7916        \u001b[32m0.4371\u001b[0m                     0.7388        0.5242  0.0025  0.6403\n",
      "     18                     \u001b[36m0.7981\u001b[0m        \u001b[32m0.4330\u001b[0m                     0.7351        0.5221  0.0020  0.6423\n",
      "     19                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4206\u001b[0m                     0.7332        0.5323  0.0015  0.6433\n",
      "     20                     0.8028        0.4206                     0.7388        0.5347  0.0010  0.6433\n",
      "     21                     \u001b[36m0.8103\u001b[0m        \u001b[32m0.4093\u001b[0m                     0.7407        0.5246  0.0007  0.6423\n",
      "     22                     0.8098        \u001b[32m0.4084\u001b[0m                     0.7444        0.5248  0.0004  0.6433\n",
      "     23                     0.8098        0.4107                     0.7351        0.5236  0.0002  0.6423\n",
      "     24                     0.8098        0.4132                     0.7351        0.5208  0.0000  0.6413\n",
      "     25                     0.8089        \u001b[32m0.4063\u001b[0m                     0.7369        0.5200  0.0000  0.6423\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6444\u001b[0m        \u001b[32m0.6703\u001b[0m                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5612\u001b[0m  0.0100  0.6403\n",
      "      2                     \u001b[36m0.7154\u001b[0m        \u001b[32m0.5715\u001b[0m                     \u001b[35m0.7220\u001b[0m        0.5633  0.0100  0.6383\n",
      "      3                     \u001b[36m0.7159\u001b[0m        \u001b[32m0.5574\u001b[0m                     \u001b[35m0.7388\u001b[0m        \u001b[31m0.5443\u001b[0m  0.0098  0.6393\n",
      "      4                     \u001b[36m0.7453\u001b[0m        \u001b[32m0.5221\u001b[0m                     0.7090        0.5879  0.0096  0.6433\n",
      "      5                     0.7439        0.5250                     0.7351        \u001b[31m0.5403\u001b[0m  0.0093  0.6373\n",
      "      6                     \u001b[36m0.7477\u001b[0m        \u001b[32m0.5046\u001b[0m                     0.7257        0.6461  0.0090  0.6423\n",
      "      7                     0.7416        0.5099                     \u001b[35m0.7444\u001b[0m        0.5612  0.0085  0.6401\n",
      "      8                     \u001b[36m0.7514\u001b[0m        \u001b[32m0.4925\u001b[0m                     0.6549        0.9418  0.0080  0.6380\n",
      "      9                     \u001b[36m0.7640\u001b[0m        \u001b[32m0.4798\u001b[0m                     0.7090        0.6938  0.0075  0.6384\n",
      "     10                     \u001b[36m0.7748\u001b[0m        \u001b[32m0.4734\u001b[0m                     \u001b[35m0.7575\u001b[0m        0.5657  0.0069  0.6375\n",
      "     11                     0.7687        \u001b[32m0.4663\u001b[0m                     0.7276        0.6623  0.0063  0.6423\n",
      "     12                     \u001b[36m0.7799\u001b[0m        \u001b[32m0.4607\u001b[0m                     0.7481        0.5718  0.0057  0.6373\n",
      "     13                     0.7790        \u001b[32m0.4598\u001b[0m                     0.7556        \u001b[31m0.5339\u001b[0m  0.0050  0.6367\n",
      "     14                     \u001b[36m0.7855\u001b[0m        \u001b[32m0.4424\u001b[0m                     0.7537        0.5502  0.0043  0.6374\n",
      "     15                     0.7771        0.4557                     0.7463        0.5502  0.0037  0.6375\n",
      "     16                     \u001b[36m0.7930\u001b[0m        \u001b[32m0.4406\u001b[0m                     0.7519        \u001b[31m0.5283\u001b[0m  0.0031  0.6374\n",
      "     17                     0.7902        0.4433                     0.7425        0.6001  0.0025  0.6376\n",
      "     18                     0.7930        \u001b[32m0.4350\u001b[0m                     0.7537        0.5358  0.0020  0.6382\n",
      "     19                     \u001b[36m0.7977\u001b[0m        \u001b[32m0.4265\u001b[0m                     0.7463        0.5342  0.0015  0.6387\n",
      "     20                     \u001b[36m0.8009\u001b[0m        0.4325                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.5196\u001b[0m  0.0010  0.6381\n",
      "     21                     0.7986        0.4269                     0.7519        0.5243  0.0007  0.6380\n",
      "     22                     \u001b[36m0.8056\u001b[0m        \u001b[32m0.4182\u001b[0m                     0.7612        0.5225  0.0004  0.6374\n",
      "     23                     \u001b[36m0.8098\u001b[0m        \u001b[32m0.4159\u001b[0m                     0.7575        0.5239  0.0002  0.6376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24                     0.8070        0.4201                     0.7593        0.5244  0.0000  0.6383\n",
      "     25                     0.8051        0.4214                     0.7612        0.5245  0.0000  0.6380\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6402\u001b[0m        \u001b[32m0.6599\u001b[0m                     \u001b[35m0.6903\u001b[0m        \u001b[31m0.6113\u001b[0m  0.0100  0.6354\n",
      "      2                     \u001b[36m0.7145\u001b[0m        \u001b[32m0.5760\u001b[0m                     \u001b[35m0.7090\u001b[0m        \u001b[31m0.5526\u001b[0m  0.0100  0.6371\n",
      "      3                     \u001b[36m0.7313\u001b[0m        \u001b[32m0.5527\u001b[0m                     \u001b[35m0.7276\u001b[0m        \u001b[31m0.5373\u001b[0m  0.0098  0.6366\n",
      "      4                     0.7271        \u001b[32m0.5498\u001b[0m                     \u001b[35m0.7537\u001b[0m        \u001b[31m0.5239\u001b[0m  0.0096  0.6372\n",
      "      5                     \u001b[36m0.7467\u001b[0m        \u001b[32m0.5253\u001b[0m                     \u001b[35m0.7631\u001b[0m        0.5281  0.0093  0.6365\n",
      "      6                     0.7425        \u001b[32m0.5239\u001b[0m                     0.7351        \u001b[31m0.5100\u001b[0m  0.0090  0.6382\n",
      "      7                     \u001b[36m0.7537\u001b[0m        \u001b[32m0.5078\u001b[0m                     0.7015        0.6281  0.0085  0.6374\n",
      "      8                     \u001b[36m0.7565\u001b[0m        0.5101                     0.7164        0.5388  0.0080  0.6375\n",
      "      9                     0.7533        \u001b[32m0.4988\u001b[0m                     0.7164        0.5437  0.0075  0.6377\n",
      "     10                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4899\u001b[0m                     0.7276        0.5286  0.0069  0.6368\n",
      "     11                     0.7612        \u001b[32m0.4895\u001b[0m                     0.7444        0.5223  0.0063  0.6370\n",
      "     12                     \u001b[36m0.7743\u001b[0m        \u001b[32m0.4764\u001b[0m                     0.7276        \u001b[31m0.5078\u001b[0m  0.0057  0.6381\n",
      "     13                     \u001b[36m0.7846\u001b[0m        \u001b[32m0.4703\u001b[0m                     0.7593        \u001b[31m0.4922\u001b[0m  0.0050  0.6375\n",
      "     14                     0.7794        \u001b[32m0.4644\u001b[0m                     0.7575        \u001b[31m0.4906\u001b[0m  0.0043  0.6377\n",
      "     15                     \u001b[36m0.7855\u001b[0m        \u001b[32m0.4529\u001b[0m                     0.6959        0.5630  0.0037  0.6379\n",
      "     16                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4481\u001b[0m                     0.7481        0.5123  0.0031  0.6375\n",
      "     17                     0.7911        \u001b[32m0.4433\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.4828\u001b[0m  0.0025  0.6368\n",
      "     18                     0.7855        0.4440                     0.7537        0.5020  0.0020  0.6379\n",
      "     19                     \u001b[36m0.8019\u001b[0m        \u001b[32m0.4278\u001b[0m                     0.7612        0.4916  0.0015  0.6378\n",
      "     20                     \u001b[36m0.8028\u001b[0m        0.4305                     0.7649        \u001b[31m0.4798\u001b[0m  0.0010  0.6656\n",
      "     21                     \u001b[36m0.8028\u001b[0m        \u001b[32m0.4265\u001b[0m                     0.7724        \u001b[31m0.4764\u001b[0m  0.0007  0.6380\n",
      "     22                     \u001b[36m0.8047\u001b[0m        0.4278                     \u001b[35m0.7743\u001b[0m        \u001b[31m0.4751\u001b[0m  0.0004  0.6377\n",
      "     23                     \u001b[36m0.8093\u001b[0m        \u001b[32m0.4202\u001b[0m                     0.7743        0.4756  0.0002  0.6593\n",
      "     24                     0.7930        0.4319                     0.7743        0.4755  0.0000  0.6378\n",
      "     25                     0.7939        0.4366                     0.7705        0.4754  0.0000  0.6383\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6393\u001b[0m        \u001b[32m0.6550\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.5805\u001b[0m  0.0100  0.6359\n",
      "      2                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.5781\u001b[0m                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5573\u001b[0m  0.0100  0.6385\n",
      "      3                     \u001b[36m0.7322\u001b[0m        \u001b[32m0.5355\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5403\u001b[0m  0.0098  0.6365\n",
      "      4                     \u001b[36m0.7411\u001b[0m        \u001b[32m0.5179\u001b[0m                     0.7201        0.5674  0.0096  0.6371\n",
      "      5                     \u001b[36m0.7444\u001b[0m        0.5224                     \u001b[35m0.7631\u001b[0m        \u001b[31m0.5210\u001b[0m  0.0093  0.6375\n",
      "      6                     \u001b[36m0.7495\u001b[0m        \u001b[32m0.4975\u001b[0m                     0.7332        0.6055  0.0090  0.6372\n",
      "      7                     \u001b[36m0.7565\u001b[0m        0.5073                     0.7556        0.5423  0.0085  0.6413\n",
      "      8                     0.7551        \u001b[32m0.4878\u001b[0m                     0.7631        0.5655  0.0080  0.6403\n",
      "      9                     \u001b[36m0.7678\u001b[0m        \u001b[32m0.4794\u001b[0m                     0.7183        0.5908  0.0075  0.6373\n",
      "     10                     \u001b[36m0.7734\u001b[0m        \u001b[32m0.4660\u001b[0m                     0.7257        0.6024  0.0069  0.6372\n",
      "     11                     \u001b[36m0.7813\u001b[0m        0.4696                     0.7537        \u001b[31m0.5140\u001b[0m  0.0063  0.6383\n",
      "     12                     \u001b[36m0.7864\u001b[0m        \u001b[32m0.4570\u001b[0m                     0.7164        0.6243  0.0057  0.6374\n",
      "     13                     0.7836        \u001b[32m0.4515\u001b[0m                     0.6754        0.6527  0.0050  0.6373\n",
      "     14                     \u001b[36m0.7916\u001b[0m        0.4556                     0.7500        0.5294  0.0043  0.6374\n",
      "     15                     0.7794        \u001b[32m0.4440\u001b[0m                     0.7052        0.5637  0.0037  0.6372\n",
      "     16                     0.7864        \u001b[32m0.4350\u001b[0m                     0.6847        0.5942  0.0031  0.6376\n",
      "     17                     \u001b[36m0.7995\u001b[0m        \u001b[32m0.4348\u001b[0m                     0.7351        0.5185  0.0025  0.6377\n",
      "     18                     0.7953        \u001b[32m0.4331\u001b[0m                     0.7220        0.5310  0.0020  0.6375\n",
      "     19                     0.7963        \u001b[32m0.4281\u001b[0m                     0.7052        0.5748  0.0015  0.6377\n",
      "     20                     \u001b[36m0.8047\u001b[0m        \u001b[32m0.4207\u001b[0m                     0.7034        0.5626  0.0010  0.6368\n",
      "     21                     \u001b[36m0.8061\u001b[0m        \u001b[32m0.4142\u001b[0m                     0.7034        0.5548  0.0007  0.6363\n",
      "     22                     0.8023        0.4214                     0.7201        0.5316  0.0004  0.6381\n",
      "     23                     \u001b[36m0.8150\u001b[0m        \u001b[32m0.4097\u001b[0m                     0.7276        0.5152  0.0002  0.6368\n",
      "     24                     0.8084        0.4140                     0.7425        \u001b[31m0.5124\u001b[0m  0.0000  0.6376\n",
      "     25                     0.8117        0.4108                     0.7463        \u001b[31m0.5113\u001b[0m  0.0000  0.6385\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6425\u001b[0m        \u001b[32m0.6636\u001b[0m                     \u001b[35m0.6810\u001b[0m        \u001b[31m0.5848\u001b[0m  0.0100  0.6343\n",
      "      2                     \u001b[36m0.7192\u001b[0m        \u001b[32m0.5535\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5328\u001b[0m  0.0100  0.6376\n",
      "      3                     \u001b[36m0.7346\u001b[0m        \u001b[32m0.5349\u001b[0m                     0.7090        0.5504  0.0098  0.6378\n",
      "      4                     \u001b[36m0.7472\u001b[0m        \u001b[32m0.5189\u001b[0m                     \u001b[35m0.7463\u001b[0m        \u001b[31m0.5266\u001b[0m  0.0096  0.6374\n",
      "      5                     \u001b[36m0.7505\u001b[0m        0.5221                     0.7388        0.5432  0.0093  0.6396\n",
      "      6                     \u001b[36m0.7617\u001b[0m        \u001b[32m0.4980\u001b[0m                     0.7444        0.5523  0.0090  0.6367\n",
      "      7                     0.7598        \u001b[32m0.4955\u001b[0m                     0.7369        0.5290  0.0085  0.6369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8                     0.7537        \u001b[32m0.4947\u001b[0m                     0.7313        0.5513  0.0080  0.6378\n",
      "      9                     \u001b[36m0.7715\u001b[0m        \u001b[32m0.4741\u001b[0m                     \u001b[35m0.7537\u001b[0m        \u001b[31m0.4977\u001b[0m  0.0075  0.6380\n",
      "     10                     0.7519        0.4918                     0.7183        0.6511  0.0069  0.6361\n",
      "     11                     \u001b[36m0.7766\u001b[0m        \u001b[32m0.4620\u001b[0m                     0.7220        0.5625  0.0063  0.6355\n",
      "     12                     \u001b[36m0.7836\u001b[0m        \u001b[32m0.4561\u001b[0m                     0.7257        0.5349  0.0057  0.6371\n",
      "     13                     \u001b[36m0.7869\u001b[0m        \u001b[32m0.4499\u001b[0m                     0.7332        0.5407  0.0050  0.6374\n",
      "     14                     \u001b[36m0.7907\u001b[0m        \u001b[32m0.4402\u001b[0m                     0.6679        0.8768  0.0043  0.6373\n",
      "     15                     \u001b[36m0.7981\u001b[0m        \u001b[32m0.4328\u001b[0m                     0.7257        0.6184  0.0037  0.6381\n",
      "     16                     \u001b[36m0.8079\u001b[0m        \u001b[32m0.4265\u001b[0m                     0.7332        0.5294  0.0031  0.6374\n",
      "     17                     0.7967        0.4291                     0.7500        0.5215  0.0025  0.6360\n",
      "     18                     0.8070        \u001b[32m0.4161\u001b[0m                     \u001b[35m0.7575\u001b[0m        0.5163  0.0020  0.6380\n",
      "     19                     0.7953        0.4238                     0.7425        0.5225  0.0015  0.6373\n",
      "     20                     \u001b[36m0.8145\u001b[0m        \u001b[32m0.4093\u001b[0m                     0.7537        0.5177  0.0010  0.6370\n",
      "     21                     0.8065        \u001b[32m0.4086\u001b[0m                     \u001b[35m0.7593\u001b[0m        0.5169  0.0007  0.6365\n",
      "     22                     0.8103        \u001b[32m0.4029\u001b[0m                     0.7593        0.5177  0.0004  0.6375\n",
      "     23                     \u001b[36m0.8173\u001b[0m        \u001b[32m0.3994\u001b[0m                     0.7575        0.5176  0.0002  0.6370\n",
      "     24                     0.8159        0.4045                     0.7556        0.5175  0.0000  0.6369\n",
      "     25                     0.8173        \u001b[32m0.3965\u001b[0m                     0.7537        0.5174  0.0000  0.6368\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6500\u001b[0m        \u001b[32m0.6528\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.5755\u001b[0m  0.0100  0.6351\n",
      "      2                     \u001b[36m0.7206\u001b[0m        \u001b[32m0.5629\u001b[0m                     \u001b[35m0.7108\u001b[0m        \u001b[31m0.5754\u001b[0m  0.0100  0.6374\n",
      "      3                     \u001b[36m0.7341\u001b[0m        \u001b[32m0.5474\u001b[0m                     \u001b[35m0.7444\u001b[0m        \u001b[31m0.5308\u001b[0m  0.0098  0.6387\n",
      "      4                     0.7294        \u001b[32m0.5216\u001b[0m                     0.7220        0.5371  0.0096  0.6401\n",
      "      5                     \u001b[36m0.7411\u001b[0m        \u001b[32m0.5133\u001b[0m                     0.6996        0.5690  0.0093  0.6378\n",
      "      6                     \u001b[36m0.7463\u001b[0m        0.5136                     \u001b[35m0.7481\u001b[0m        0.5458  0.0090  0.6374\n",
      "      7                     \u001b[36m0.7710\u001b[0m        \u001b[32m0.4933\u001b[0m                     0.7220        0.5789  0.0085  0.6379\n",
      "      8                     0.7556        \u001b[32m0.4869\u001b[0m                     0.6903        0.8115  0.0080  0.6369\n",
      "      9                     \u001b[36m0.7715\u001b[0m        \u001b[32m0.4826\u001b[0m                     0.7295        0.5591  0.0075  0.6376\n",
      "     10                     \u001b[36m0.7776\u001b[0m        \u001b[32m0.4663\u001b[0m                     0.6437        0.7306  0.0069  0.6374\n",
      "     11                     0.7734        0.4793                     0.7295        0.5347  0.0063  0.6384\n",
      "     12                     0.7776        \u001b[32m0.4644\u001b[0m                     0.7425        \u001b[31m0.5269\u001b[0m  0.0057  0.6373\n",
      "     13                     \u001b[36m0.7860\u001b[0m        \u001b[32m0.4613\u001b[0m                     \u001b[35m0.7537\u001b[0m        \u001b[31m0.5268\u001b[0m  0.0050  0.6381\n",
      "     14                     \u001b[36m0.7925\u001b[0m        \u001b[32m0.4494\u001b[0m                     0.7444        0.5286  0.0043  0.6379\n",
      "     15                     0.7883        \u001b[32m0.4425\u001b[0m                     0.7313        \u001b[31m0.5197\u001b[0m  0.0037  0.6370\n",
      "     16                     \u001b[36m0.7986\u001b[0m        \u001b[32m0.4395\u001b[0m                     0.6959        0.5744  0.0031  0.6377\n",
      "     17                     0.7958        \u001b[32m0.4368\u001b[0m                     0.7519        \u001b[31m0.4988\u001b[0m  0.0025  0.6381\n",
      "     18                     0.7967        \u001b[32m0.4352\u001b[0m                     0.7351        0.5084  0.0020  0.6386\n",
      "     19                     \u001b[36m0.8075\u001b[0m        0.4396                     0.7183        0.5424  0.0015  0.6374\n",
      "     20                     0.8051        \u001b[32m0.4187\u001b[0m                     0.7127        0.5428  0.0010  0.6374\n",
      "     21                     0.7977        0.4264                     0.7090        0.5480  0.0007  0.6371\n",
      "     22                     \u001b[36m0.8131\u001b[0m        \u001b[32m0.4138\u001b[0m                     0.7257        0.5305  0.0004  0.6373\n",
      "     23                     0.8061        \u001b[32m0.4137\u001b[0m                     0.7276        0.5208  0.0002  0.6376\n",
      "     24                     0.8121        \u001b[32m0.4086\u001b[0m                     0.7276        0.5144  0.0000  0.6384\n",
      "     25                     \u001b[36m0.8206\u001b[0m        \u001b[32m0.4085\u001b[0m                     0.7351        0.5107  0.0000  0.6362\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6407\u001b[0m        \u001b[32m0.6615\u001b[0m                     \u001b[35m0.6828\u001b[0m        \u001b[31m0.6138\u001b[0m  0.0100  0.6353\n",
      "      2                     \u001b[36m0.7075\u001b[0m        \u001b[32m0.5642\u001b[0m                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.5705\u001b[0m  0.0100  0.6375\n",
      "      3                     \u001b[36m0.7280\u001b[0m        \u001b[32m0.5363\u001b[0m                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5414\u001b[0m  0.0098  0.6374\n",
      "      4                     \u001b[36m0.7388\u001b[0m        \u001b[32m0.5292\u001b[0m                     \u001b[35m0.7388\u001b[0m        0.5487  0.0096  0.6388\n",
      "      5                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.5112\u001b[0m                     \u001b[35m0.7575\u001b[0m        \u001b[31m0.5292\u001b[0m  0.0093  0.6369\n",
      "      6                     0.7561        \u001b[32m0.4974\u001b[0m                     0.7407        0.5464  0.0090  0.6378\n",
      "      7                     \u001b[36m0.7603\u001b[0m        \u001b[32m0.4887\u001b[0m                     0.7034        0.6013  0.0085  0.6387\n",
      "      8                     0.7542        \u001b[32m0.4861\u001b[0m                     0.7220        0.5796  0.0080  0.6379\n",
      "      9                     \u001b[36m0.7621\u001b[0m        \u001b[32m0.4798\u001b[0m                     0.7313        0.6037  0.0075  0.6370\n",
      "     10                     \u001b[36m0.7729\u001b[0m        \u001b[32m0.4634\u001b[0m                     0.7183        0.6541  0.0069  0.6385\n",
      "     11                     \u001b[36m0.7757\u001b[0m        0.4659                     0.7239        0.6620  0.0063  0.6370\n",
      "     12                     0.7687        \u001b[32m0.4603\u001b[0m                     0.7481        0.5894  0.0057  0.6385\n",
      "     13                     \u001b[36m0.7822\u001b[0m        \u001b[32m0.4505\u001b[0m                     0.7201        0.6802  0.0050  0.6380\n",
      "     14                     0.7808        \u001b[32m0.4421\u001b[0m                     0.7332        0.5657  0.0043  0.6377\n",
      "     15                     \u001b[36m0.7911\u001b[0m        0.4422                     0.7220        0.5837  0.0037  0.6375\n",
      "     16                     0.7841        \u001b[32m0.4345\u001b[0m                     0.7090        0.7295  0.0031  0.6378\n",
      "     17                     0.7902        0.4348                     0.7015        0.7577  0.0025  0.6370\n",
      "     18                     \u001b[36m0.8009\u001b[0m        \u001b[32m0.4271\u001b[0m                     0.7090        0.7276  0.0020  0.6383\n",
      "     19                     \u001b[36m0.8047\u001b[0m        \u001b[32m0.4231\u001b[0m                     0.7090        0.6977  0.0015  0.6378\n",
      "     20                     0.8042        \u001b[32m0.4172\u001b[0m                     0.7425        0.5517  0.0010  0.6376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21                     \u001b[36m0.8075\u001b[0m        \u001b[32m0.4096\u001b[0m                     0.7351        0.5647  0.0007  0.6384\n",
      "     22                     0.8065        0.4158                     0.7369        0.5637  0.0004  0.6382\n",
      "     23                     \u001b[36m0.8131\u001b[0m        \u001b[32m0.4004\u001b[0m                     0.7463        0.5530  0.0002  0.6373\n",
      "     24                     0.8019        0.4100                     0.7556        0.5496  0.0000  0.6379\n",
      "     25                     0.8093        0.4046                     0.7500        0.5461  0.0000  0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6182\u001b[0m        \u001b[32m0.6843\u001b[0m                     \u001b[35m0.6045\u001b[0m        \u001b[31m0.6713\u001b[0m  0.0100  0.6373\n",
      "      2                     \u001b[36m0.6822\u001b[0m        \u001b[32m0.6096\u001b[0m                     \u001b[35m0.6399\u001b[0m        \u001b[31m0.6285\u001b[0m  0.0100  0.6370\n",
      "      3                     \u001b[36m0.7005\u001b[0m        \u001b[32m0.5688\u001b[0m                     \u001b[35m0.6604\u001b[0m        0.6305  0.0098  0.6384\n",
      "      4                     \u001b[36m0.7196\u001b[0m        \u001b[32m0.5598\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.5752\u001b[0m  0.0096  0.6359\n",
      "      5                     \u001b[36m0.7262\u001b[0m        \u001b[32m0.5392\u001b[0m                     \u001b[35m0.7071\u001b[0m        0.5896  0.0093  0.6356\n",
      "      6                     \u001b[36m0.7327\u001b[0m        \u001b[32m0.5313\u001b[0m                     0.6847        0.5987  0.0090  0.6366\n",
      "      7                     0.7318        0.5356                     0.6791        0.6302  0.0085  0.6374\n",
      "      8                     \u001b[36m0.7393\u001b[0m        \u001b[32m0.5151\u001b[0m                     0.6306        0.8503  0.0080  0.6362\n",
      "      9                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.5034\u001b[0m                     0.6903        0.6001  0.0075  0.6371\n",
      "     10                     \u001b[36m0.7621\u001b[0m        \u001b[32m0.5030\u001b[0m                     0.7052        0.6048  0.0069  0.6360\n",
      "     11                     0.7500        \u001b[32m0.5019\u001b[0m                     0.6959        0.5857  0.0063  0.6370\n",
      "     12                     \u001b[36m0.7673\u001b[0m        \u001b[32m0.4891\u001b[0m                     0.6772        0.6834  0.0057  0.6374\n",
      "     13                     0.7645        \u001b[32m0.4890\u001b[0m                     0.6716        0.6492  0.0050  0.6381\n",
      "     14                     0.7664        \u001b[32m0.4827\u001b[0m                     0.7034        \u001b[31m0.5597\u001b[0m  0.0043  0.6370\n",
      "     15                     \u001b[36m0.7771\u001b[0m        \u001b[32m0.4678\u001b[0m                     0.6922        0.5642  0.0037  0.6373\n",
      "     16                     0.7706        0.4696                     \u001b[35m0.7146\u001b[0m        0.5756  0.0031  0.6372\n",
      "     17                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4630\u001b[0m                     0.6940        0.5936  0.0025  0.6373\n",
      "     18                     0.7794        \u001b[32m0.4548\u001b[0m                     0.7034        0.5760  0.0020  0.6363\n",
      "     19                     0.7766        0.4550                     0.7071        0.5771  0.0015  0.6365\n",
      "     20                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4511\u001b[0m                     0.7090        0.5763  0.0010  0.6373\n",
      "     21                     0.7869        \u001b[32m0.4437\u001b[0m                     0.7071        0.5713  0.0007  0.6370\n",
      "     22                     0.7874        0.4496                     0.7146        0.5660  0.0004  0.6359\n",
      "     23                     \u001b[36m0.7944\u001b[0m        \u001b[32m0.4405\u001b[0m                     0.7090        0.5646  0.0002  0.6367\n",
      "     24                     \u001b[36m0.8042\u001b[0m        0.4428                     0.7108        0.5647  0.0000  0.6368\n",
      "     25                     0.7911        0.4407                     0.7108        0.5651  0.0000  0.6372\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6271\u001b[0m        \u001b[32m0.6683\u001b[0m                     \u001b[35m0.5672\u001b[0m        \u001b[31m1.0275\u001b[0m  0.0100  0.6353\n",
      "      2                     \u001b[36m0.6813\u001b[0m        \u001b[32m0.6045\u001b[0m                     \u001b[35m0.6791\u001b[0m        \u001b[31m0.6683\u001b[0m  0.0100  0.6374\n",
      "      3                     \u001b[36m0.7164\u001b[0m        \u001b[32m0.5615\u001b[0m                     \u001b[35m0.6828\u001b[0m        \u001b[31m0.6625\u001b[0m  0.0098  0.6373\n",
      "      4                     \u001b[36m0.7210\u001b[0m        \u001b[32m0.5495\u001b[0m                     0.5989        0.7910  0.0096  0.6381\n",
      "      5                     0.7201        \u001b[32m0.5486\u001b[0m                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.6244\u001b[0m  0.0093  0.6364\n",
      "      6                     \u001b[36m0.7416\u001b[0m        \u001b[32m0.5266\u001b[0m                     0.7146        \u001b[31m0.6181\u001b[0m  0.0090  0.6372\n",
      "      7                     \u001b[36m0.7439\u001b[0m        \u001b[32m0.5144\u001b[0m                     0.7220        \u001b[31m0.5991\u001b[0m  0.0085  0.6381\n",
      "      8                     0.7402        \u001b[32m0.5115\u001b[0m                     \u001b[35m0.7388\u001b[0m        \u001b[31m0.5878\u001b[0m  0.0080  0.6378\n",
      "      9                     \u001b[36m0.7523\u001b[0m        \u001b[32m0.4998\u001b[0m                     0.7164        0.6111  0.0075  0.6375\n",
      "     10                     \u001b[36m0.7607\u001b[0m        \u001b[32m0.4950\u001b[0m                     0.7369        \u001b[31m0.5840\u001b[0m  0.0069  0.6369\n",
      "     11                     \u001b[36m0.7621\u001b[0m        0.4972                     0.7313        \u001b[31m0.5668\u001b[0m  0.0063  0.6375\n",
      "     12                     0.7617        \u001b[32m0.4783\u001b[0m                     0.7388        0.5674  0.0057  0.6375\n",
      "     13                     \u001b[36m0.7720\u001b[0m        0.4811                     0.7146        0.5897  0.0050  0.6368\n",
      "     14                     \u001b[36m0.7766\u001b[0m        \u001b[32m0.4694\u001b[0m                     0.7257        0.5766  0.0043  0.6390\n",
      "     15                     \u001b[36m0.7822\u001b[0m        \u001b[32m0.4493\u001b[0m                     0.7351        0.5837  0.0037  0.6375\n",
      "     16                     0.7729        0.4656                     0.7220        0.5800  0.0031  0.6372\n",
      "     17                     \u001b[36m0.7832\u001b[0m        0.4634                     0.6996        0.6304  0.0025  0.6373\n",
      "     18                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4474\u001b[0m                     0.7108        0.5987  0.0020  0.6384\n",
      "     19                     0.7869        \u001b[32m0.4468\u001b[0m                     0.7257        0.6019  0.0015  0.6374\n",
      "     20                     \u001b[36m0.8075\u001b[0m        \u001b[32m0.4299\u001b[0m                     0.7276        0.5893  0.0010  0.6385\n",
      "     21                     0.8023        0.4345                     0.7164        0.5964  0.0007  0.6416\n",
      "     22                     0.7897        0.4408                     0.7183        0.5919  0.0004  0.6374\n",
      "     23                     0.7972        0.4319                     0.7164        0.5940  0.0002  0.6384\n",
      "     24                     0.8023        \u001b[32m0.4275\u001b[0m                     0.7146        0.5940  0.0000  0.6381\n",
      "     25                     0.7930        0.4394                     0.7164        0.5945  0.0000  0.6381\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6140\u001b[0m        \u001b[32m0.6924\u001b[0m                     \u001b[35m0.6343\u001b[0m        \u001b[31m0.7125\u001b[0m  0.0100  0.6360\n",
      "      2                     \u001b[36m0.6696\u001b[0m        \u001b[32m0.6152\u001b[0m                     0.6250        0.7498  0.0100  0.6367\n",
      "      3                     \u001b[36m0.6911\u001b[0m        \u001b[32m0.5943\u001b[0m                     \u001b[35m0.6922\u001b[0m        \u001b[31m0.6880\u001b[0m  0.0098  0.6374\n",
      "      4                     \u001b[36m0.7140\u001b[0m        \u001b[32m0.5642\u001b[0m                     0.6660        \u001b[31m0.6668\u001b[0m  0.0096  0.6365\n",
      "      5                     \u001b[36m0.7224\u001b[0m        \u001b[32m0.5548\u001b[0m                     \u001b[35m0.7146\u001b[0m        \u001b[31m0.5947\u001b[0m  0.0093  0.6374\n",
      "      6                     0.7093        0.5558                     \u001b[35m0.7369\u001b[0m        0.6158  0.0090  0.6377\n",
      "      7                     \u001b[36m0.7350\u001b[0m        \u001b[32m0.5409\u001b[0m                     0.7164        0.6087  0.0085  0.6374\n",
      "      8                     \u001b[36m0.7360\u001b[0m        \u001b[32m0.5367\u001b[0m                     \u001b[35m0.7687\u001b[0m        \u001b[31m0.5378\u001b[0m  0.0080  0.6367\n",
      "      9                     0.7346        \u001b[32m0.5292\u001b[0m                     0.7071        0.6123  0.0075  0.6370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     0.7238        0.5397                     0.7519        0.5385  0.0069  0.6376\n",
      "     11                     \u001b[36m0.7411\u001b[0m        \u001b[32m0.5188\u001b[0m                     0.7295        0.5952  0.0063  0.6374\n",
      "     12                     \u001b[36m0.7584\u001b[0m        \u001b[32m0.4985\u001b[0m                     0.7239        0.5693  0.0057  0.6371\n",
      "     13                     \u001b[36m0.7626\u001b[0m        0.4993                     0.7612        \u001b[31m0.5178\u001b[0m  0.0050  0.6379\n",
      "     14                     \u001b[36m0.7631\u001b[0m        0.5014                     0.7500        0.5373  0.0043  0.6362\n",
      "     15                     \u001b[36m0.7696\u001b[0m        \u001b[32m0.4837\u001b[0m                     0.7575        0.5322  0.0037  0.6375\n",
      "     16                     0.7650        \u001b[32m0.4835\u001b[0m                     0.7444        0.5577  0.0031  0.6370\n",
      "     17                     0.7673        \u001b[32m0.4826\u001b[0m                     0.7257        0.5775  0.0025  0.6366\n",
      "     18                     \u001b[36m0.7715\u001b[0m        \u001b[32m0.4776\u001b[0m                     0.7425        0.5504  0.0020  0.6376\n",
      "     19                     \u001b[36m0.7729\u001b[0m        \u001b[32m0.4694\u001b[0m                     0.7052        0.6134  0.0015  0.6365\n",
      "     20                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4615\u001b[0m                     0.7108        0.5941  0.0010  0.6375\n",
      "     21                     \u001b[36m0.7850\u001b[0m        0.4644                     0.7220        0.5801  0.0007  0.6376\n",
      "     22                     0.7794        \u001b[32m0.4494\u001b[0m                     0.7500        0.5618  0.0004  0.6385\n",
      "     23                     0.7832        0.4522                     0.7537        0.5598  0.0002  0.6369\n",
      "     24                     0.7850        0.4515                     0.7500        0.5564  0.0000  0.6368\n",
      "     25                     \u001b[36m0.7855\u001b[0m        0.4559                     0.7481        0.5542  0.0000  0.6374\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6350\u001b[0m        \u001b[32m0.6753\u001b[0m                     \u001b[35m0.6530\u001b[0m        \u001b[31m0.6315\u001b[0m  0.0100  0.6349\n",
      "      2                     \u001b[36m0.6925\u001b[0m        \u001b[32m0.5917\u001b[0m                     \u001b[35m0.7146\u001b[0m        \u001b[31m0.5815\u001b[0m  0.0100  0.6382\n",
      "      3                     \u001b[36m0.7257\u001b[0m        \u001b[32m0.5527\u001b[0m                     0.6940        0.6134  0.0098  0.6361\n",
      "      4                     0.7140        \u001b[32m0.5513\u001b[0m                     \u001b[35m0.7276\u001b[0m        \u001b[31m0.5638\u001b[0m  0.0096  0.6469\n",
      "      5                     \u001b[36m0.7285\u001b[0m        \u001b[32m0.5311\u001b[0m                     \u001b[35m0.7295\u001b[0m        0.5709  0.0093  0.6522\n",
      "      6                     \u001b[36m0.7407\u001b[0m        \u001b[32m0.5204\u001b[0m                     0.7183        0.5941  0.0090  0.6375\n",
      "      7                     \u001b[36m0.7416\u001b[0m        \u001b[32m0.5183\u001b[0m                     0.7239        0.5796  0.0085  0.6378\n",
      "      8                     0.7388        \u001b[32m0.5172\u001b[0m                     \u001b[35m0.7388\u001b[0m        0.5769  0.0080  0.6596\n",
      "      9                     \u001b[36m0.7556\u001b[0m        \u001b[32m0.5010\u001b[0m                     \u001b[35m0.7444\u001b[0m        0.5874  0.0075  0.6370\n",
      "     10                     0.7542        \u001b[32m0.4953\u001b[0m                     0.7201        0.6204  0.0069  0.6370\n",
      "     11                     \u001b[36m0.7598\u001b[0m        \u001b[32m0.4875\u001b[0m                     0.7090        0.6472  0.0063  0.6368\n",
      "     12                     \u001b[36m0.7687\u001b[0m        \u001b[32m0.4781\u001b[0m                     0.7201        0.5926  0.0057  0.6366\n",
      "     13                     \u001b[36m0.7715\u001b[0m        \u001b[32m0.4729\u001b[0m                     \u001b[35m0.7668\u001b[0m        \u001b[31m0.5416\u001b[0m  0.0050  0.6372\n",
      "     14                     \u001b[36m0.7766\u001b[0m        \u001b[32m0.4593\u001b[0m                     0.7463        0.5493  0.0043  0.6372\n",
      "     15                     0.7724        \u001b[32m0.4576\u001b[0m                     0.7537        0.5475  0.0037  0.6376\n",
      "     16                     \u001b[36m0.7799\u001b[0m        0.4608                     0.7295        0.5976  0.0031  0.6367\n",
      "     17                     0.7780        \u001b[32m0.4520\u001b[0m                     0.7537        0.5531  0.0025  0.6375\n",
      "     18                     \u001b[36m0.7846\u001b[0m        \u001b[32m0.4460\u001b[0m                     0.7444        0.5816  0.0020  0.6373\n",
      "     19                     \u001b[36m0.7935\u001b[0m        \u001b[32m0.4449\u001b[0m                     0.7593        0.5540  0.0015  0.6360\n",
      "     20                     0.7902        0.4470                     0.7631        0.5432  0.0010  0.6370\n",
      "     21                     0.7846        \u001b[32m0.4424\u001b[0m                     0.7649        0.5458  0.0007  0.6365\n",
      "     22                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4300\u001b[0m                     0.7593        0.5474  0.0004  0.6372\n",
      "     23                     0.7827        0.4465                     0.7612        0.5460  0.0002  0.6375\n",
      "     24                     0.7935        0.4322                     0.7612        0.5465  0.0000  0.6373\n",
      "     25                     0.7893        0.4376                     0.7612        0.5464  0.0000  0.6365\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6243\u001b[0m        \u001b[32m0.6726\u001b[0m                     \u001b[35m0.6250\u001b[0m        \u001b[31m0.6938\u001b[0m  0.0100  0.6344\n",
      "      2                     \u001b[36m0.6724\u001b[0m        \u001b[32m0.6019\u001b[0m                     \u001b[35m0.6810\u001b[0m        \u001b[31m0.6244\u001b[0m  0.0100  0.6380\n",
      "      3                     \u001b[36m0.7089\u001b[0m        \u001b[32m0.5603\u001b[0m                     0.6772        0.6318  0.0098  0.6363\n",
      "      4                     \u001b[36m0.7252\u001b[0m        \u001b[32m0.5419\u001b[0m                     0.6306        0.7086  0.0096  0.6373\n",
      "      5                     \u001b[36m0.7299\u001b[0m        0.5506                     0.6306        0.7704  0.0093  0.6365\n",
      "      6                     0.7271        \u001b[32m0.5326\u001b[0m                     \u001b[35m0.6847\u001b[0m        0.6549  0.0090  0.6383\n",
      "      7                     \u001b[36m0.7519\u001b[0m        \u001b[32m0.5150\u001b[0m                     \u001b[35m0.6996\u001b[0m        0.6489  0.0085  0.6368\n",
      "      8                     0.7416        0.5158                     0.6847        0.6473  0.0080  0.6365\n",
      "      9                     \u001b[36m0.7607\u001b[0m        \u001b[32m0.4993\u001b[0m                     \u001b[35m0.7090\u001b[0m        0.6451  0.0075  0.6370\n",
      "     10                     0.7579        \u001b[32m0.4918\u001b[0m                     0.6884        0.6908  0.0069  0.6367\n",
      "     11                     0.7444        0.5000                     0.6828        0.6604  0.0063  0.6365\n",
      "     12                     \u001b[36m0.7617\u001b[0m        \u001b[32m0.4846\u001b[0m                     0.6847        \u001b[31m0.6221\u001b[0m  0.0057  0.6378\n",
      "     13                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4762\u001b[0m                     0.6978        0.6536  0.0050  0.6363\n",
      "     14                     \u001b[36m0.7757\u001b[0m        \u001b[32m0.4662\u001b[0m                     0.6903        0.6245  0.0043  0.6368\n",
      "     15                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4608\u001b[0m                     0.7071        \u001b[31m0.6105\u001b[0m  0.0037  0.6371\n",
      "     16                     0.7813        \u001b[32m0.4539\u001b[0m                     0.7090        \u001b[31m0.6033\u001b[0m  0.0031  0.6372\n",
      "     17                     \u001b[36m0.7883\u001b[0m        \u001b[32m0.4483\u001b[0m                     \u001b[35m0.7146\u001b[0m        \u001b[31m0.5954\u001b[0m  0.0025  0.6368\n",
      "     18                     0.7827        0.4524                     0.7127        \u001b[31m0.5935\u001b[0m  0.0020  0.6359\n",
      "     19                     \u001b[36m0.7921\u001b[0m        \u001b[32m0.4407\u001b[0m                     0.7127        0.5992  0.0015  0.6365\n",
      "     20                     \u001b[36m0.8000\u001b[0m        \u001b[32m0.4333\u001b[0m                     \u001b[35m0.7164\u001b[0m        \u001b[31m0.5906\u001b[0m  0.0010  0.6373\n",
      "     21                     0.7949        0.4382                     \u001b[35m0.7220\u001b[0m        0.5930  0.0007  0.6373\n",
      "     22                     0.7893        0.4349                     0.7183        0.5910  0.0004  0.6380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23                     0.7907        0.4402                     0.7201        0.5921  0.0002  0.6370\n",
      "     24                     \u001b[36m0.8065\u001b[0m        \u001b[32m0.4222\u001b[0m                     0.7183        0.5919  0.0000  0.6369\n",
      "     25                     0.7832        0.4424                     0.7183        0.5919  0.0000  0.6370\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6421\u001b[0m        \u001b[32m0.6704\u001b[0m                     \u001b[35m0.6978\u001b[0m        \u001b[31m0.5805\u001b[0m  0.0100  0.6364\n",
      "      2                     \u001b[36m0.6902\u001b[0m        \u001b[32m0.5874\u001b[0m                     0.6549        0.5872  0.0100  0.6370\n",
      "      3                     \u001b[36m0.6972\u001b[0m        \u001b[32m0.5766\u001b[0m                     0.6959        \u001b[31m0.5625\u001b[0m  0.0098  0.6369\n",
      "      4                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.5630\u001b[0m                     \u001b[35m0.7239\u001b[0m        0.5785  0.0096  0.6371\n",
      "      5                     \u001b[36m0.7266\u001b[0m        \u001b[32m0.5441\u001b[0m                     \u001b[35m0.7276\u001b[0m        0.5723  0.0093  0.6374\n",
      "      6                     \u001b[36m0.7444\u001b[0m        \u001b[32m0.5282\u001b[0m                     0.6959        0.5858  0.0090  0.6364\n",
      "      7                     0.7364        0.5328                     0.7239        \u001b[31m0.5593\u001b[0m  0.0085  0.6370\n",
      "      8                     0.7393        \u001b[32m0.5219\u001b[0m                     \u001b[35m0.7407\u001b[0m        \u001b[31m0.5474\u001b[0m  0.0080  0.6368\n",
      "      9                     \u001b[36m0.7659\u001b[0m        \u001b[32m0.5062\u001b[0m                     \u001b[35m0.7425\u001b[0m        0.5519  0.0075  0.6369\n",
      "     10                     0.7505        0.5080                     0.7276        0.5803  0.0069  0.6374\n",
      "     11                     0.7477        0.5075                     0.7388        0.5595  0.0063  0.6381\n",
      "     12                     0.7542        \u001b[32m0.4942\u001b[0m                     0.7425        \u001b[31m0.5428\u001b[0m  0.0057  0.6365\n",
      "     13                     \u001b[36m0.7706\u001b[0m        \u001b[32m0.4771\u001b[0m                     0.7425        \u001b[31m0.5280\u001b[0m  0.0050  0.6385\n",
      "     14                     \u001b[36m0.7734\u001b[0m        0.4870                     0.7351        0.5526  0.0043  0.6399\n",
      "     15                     \u001b[36m0.7818\u001b[0m        \u001b[32m0.4642\u001b[0m                     0.7164        0.5410  0.0037  0.6369\n",
      "     16                     \u001b[36m0.7832\u001b[0m        0.4662                     \u001b[35m0.7425\u001b[0m        0.5380  0.0031  0.6372\n",
      "     17                     0.7748        0.4698                     \u001b[35m0.7481\u001b[0m        0.5325  0.0025  0.6375\n",
      "     18                     0.7832        0.4646                     0.7407        0.5291  0.0020  0.6375\n",
      "     19                     \u001b[36m0.7888\u001b[0m        \u001b[32m0.4517\u001b[0m                     0.7369        0.5535  0.0015  0.6368\n",
      "     20                     0.7804        0.4588                     0.7481        \u001b[31m0.5199\u001b[0m  0.0010  0.6375\n",
      "     21                     \u001b[36m0.7944\u001b[0m        \u001b[32m0.4471\u001b[0m                     \u001b[35m0.7537\u001b[0m        0.5202  0.0007  0.6377\n",
      "     22                     0.7813        0.4573                     \u001b[35m0.7575\u001b[0m        0.5200  0.0004  0.6371\n",
      "     23                     0.7869        0.4527                     0.7556        0.5202  0.0002  0.6379\n",
      "     24                     0.7916        \u001b[32m0.4467\u001b[0m                     0.7537        0.5201  0.0000  0.6369\n",
      "     25                     0.7879        0.4487                     0.7556        0.5203  0.0000  0.6373\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6243\u001b[0m        \u001b[32m0.6923\u001b[0m                     \u001b[35m0.6623\u001b[0m        \u001b[31m0.6338\u001b[0m  0.0100  0.6354\n",
      "      2                     \u001b[36m0.6598\u001b[0m        \u001b[32m0.6190\u001b[0m                     \u001b[35m0.7220\u001b[0m        \u001b[31m0.5770\u001b[0m  0.0100  0.6373\n",
      "      3                     \u001b[36m0.6958\u001b[0m        \u001b[32m0.5695\u001b[0m                     0.6586        0.6038  0.0098  0.6372\n",
      "      4                     \u001b[36m0.7374\u001b[0m        \u001b[32m0.5488\u001b[0m                     0.7071        \u001b[31m0.5646\u001b[0m  0.0096  0.6382\n",
      "      5                     0.7285        \u001b[32m0.5374\u001b[0m                     0.7090        0.5772  0.0093  0.6365\n",
      "      6                     0.7374        \u001b[32m0.5287\u001b[0m                     0.7183        \u001b[31m0.5397\u001b[0m  0.0090  0.6373\n",
      "      7                     \u001b[36m0.7435\u001b[0m        \u001b[32m0.5164\u001b[0m                     \u001b[35m0.7388\u001b[0m        0.5483  0.0085  0.6374\n",
      "      8                     0.7430        0.5235                     0.6623        0.6796  0.0080  0.6376\n",
      "      9                     \u001b[36m0.7556\u001b[0m        \u001b[32m0.5092\u001b[0m                     0.7164        0.5675  0.0075  0.6367\n",
      "     10                     0.7551        \u001b[32m0.4978\u001b[0m                     0.7146        0.5757  0.0069  0.6373\n",
      "     11                     \u001b[36m0.7678\u001b[0m        \u001b[32m0.4913\u001b[0m                     0.7220        0.6177  0.0063  0.6378\n",
      "     12                     0.7575        \u001b[32m0.4856\u001b[0m                     0.7332        0.5689  0.0057  0.6372\n",
      "     13                     \u001b[36m0.7813\u001b[0m        \u001b[32m0.4765\u001b[0m                     0.7127        0.6153  0.0050  0.6375\n",
      "     14                     0.7710        0.4766                     \u001b[35m0.7425\u001b[0m        0.5511  0.0043  0.6367\n",
      "     15                     0.7640        \u001b[32m0.4743\u001b[0m                     0.7407        0.5484  0.0037  0.6368\n",
      "     16                     0.7785        \u001b[32m0.4655\u001b[0m                     0.7164        0.5961  0.0031  0.6377\n",
      "     17                     0.7734        \u001b[32m0.4609\u001b[0m                     0.7257        0.5480  0.0025  0.6367\n",
      "     18                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4484\u001b[0m                     0.7239        0.5804  0.0020  0.6369\n",
      "     19                     0.7799        0.4591                     0.7332        0.5454  0.0015  0.6373\n",
      "     20                     \u001b[36m0.7921\u001b[0m        \u001b[32m0.4470\u001b[0m                     \u001b[35m0.7463\u001b[0m        0.5544  0.0010  0.6372\n",
      "     21                     0.7869        \u001b[32m0.4375\u001b[0m                     \u001b[35m0.7481\u001b[0m        0.5422  0.0007  0.6383\n",
      "     22                     \u001b[36m0.8005\u001b[0m        \u001b[32m0.4322\u001b[0m                     \u001b[35m0.7500\u001b[0m        0.5429  0.0004  0.6385\n",
      "     23                     0.7925        0.4378                     \u001b[35m0.7556\u001b[0m        0.5429  0.0002  0.6379\n",
      "     24                     0.7907        0.4421                     0.7444        0.5439  0.0000  0.6382\n",
      "     25                     0.7883        0.4439                     0.7481        0.5453  0.0000  0.6375\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6280\u001b[0m        \u001b[32m0.6787\u001b[0m                     \u001b[35m0.6511\u001b[0m        \u001b[31m0.6459\u001b[0m  0.0100  0.6355\n",
      "      2                     \u001b[36m0.6841\u001b[0m        \u001b[32m0.5966\u001b[0m                     \u001b[35m0.7015\u001b[0m        \u001b[31m0.5926\u001b[0m  0.0100  0.6365\n",
      "      3                     \u001b[36m0.7150\u001b[0m        \u001b[32m0.5548\u001b[0m                     0.6604        0.6105  0.0098  0.6372\n",
      "      4                     \u001b[36m0.7327\u001b[0m        \u001b[32m0.5408\u001b[0m                     0.6772        0.6283  0.0096  0.6368\n",
      "      5                     0.7299        \u001b[32m0.5376\u001b[0m                     \u001b[35m0.7295\u001b[0m        \u001b[31m0.5780\u001b[0m  0.0093  0.6373\n",
      "      6                     \u001b[36m0.7430\u001b[0m        \u001b[32m0.5264\u001b[0m                     0.6735        0.6089  0.0090  0.6371\n",
      "      7                     \u001b[36m0.7495\u001b[0m        \u001b[32m0.5080\u001b[0m                     0.7015        0.6460  0.0085  0.6367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8                     \u001b[36m0.7537\u001b[0m        \u001b[32m0.5007\u001b[0m                     0.6847        0.6292  0.0080  0.6354\n",
      "      9                     0.7523        0.5054                     0.6959        0.6396  0.0075  0.6369\n",
      "     10                     \u001b[36m0.7575\u001b[0m        \u001b[32m0.4866\u001b[0m                     0.7201        0.6251  0.0069  0.6369\n",
      "     11                     \u001b[36m0.7678\u001b[0m        0.4875                     0.7257        0.6085  0.0063  0.6360\n",
      "     12                     \u001b[36m0.7701\u001b[0m        \u001b[32m0.4735\u001b[0m                     0.7146        0.6393  0.0057  0.6424\n",
      "     13                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4661\u001b[0m                     \u001b[35m0.7313\u001b[0m        0.5882  0.0050  0.6423\n",
      "     14                     0.7696        0.4751                     0.6940        0.6188  0.0043  0.6360\n",
      "     15                     \u001b[36m0.7836\u001b[0m        \u001b[32m0.4624\u001b[0m                     0.7034        0.6457  0.0037  0.6381\n",
      "     16                     0.7794        \u001b[32m0.4562\u001b[0m                     0.7257        0.6079  0.0031  0.6436\n",
      "     17                     0.7818        \u001b[32m0.4548\u001b[0m                     0.7220        0.5926  0.0025  0.6633\n",
      "     18                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4417\u001b[0m                     0.7257        0.6028  0.0020  0.6401\n",
      "     19                     0.7813        \u001b[32m0.4380\u001b[0m                     0.7276        0.6119  0.0015  0.6376\n",
      "     20                     \u001b[36m0.8023\u001b[0m        \u001b[32m0.4349\u001b[0m                     \u001b[35m0.7351\u001b[0m        0.6022  0.0010  0.6373\n",
      "     21                     0.8023        \u001b[32m0.4321\u001b[0m                     0.7183        0.6037  0.0007  0.6376\n",
      "     22                     0.7967        \u001b[32m0.4288\u001b[0m                     0.7332        0.6014  0.0004  0.6379\n",
      "     23                     \u001b[36m0.8089\u001b[0m        \u001b[32m0.4208\u001b[0m                     0.7295        0.6002  0.0002  0.6384\n",
      "     24                     0.7930        0.4272                     0.7295        0.6004  0.0000  0.6369\n",
      "     25                     0.7991        0.4293                     0.7276        0.6001  0.0000  0.6370\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6360\u001b[0m        \u001b[32m0.6736\u001b[0m                     \u001b[35m0.6623\u001b[0m        \u001b[31m0.6595\u001b[0m  0.0100  0.6353\n",
      "      2                     \u001b[36m0.6748\u001b[0m        \u001b[32m0.6142\u001b[0m                     \u001b[35m0.6660\u001b[0m        \u001b[31m0.6191\u001b[0m  0.0100  0.6373\n",
      "      3                     \u001b[36m0.7056\u001b[0m        \u001b[32m0.5717\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.5985\u001b[0m  0.0098  0.6371\n",
      "      4                     \u001b[36m0.7308\u001b[0m        \u001b[32m0.5473\u001b[0m                     \u001b[35m0.6754\u001b[0m        0.6246  0.0096  0.6363\n",
      "      5                     \u001b[36m0.7336\u001b[0m        \u001b[32m0.5336\u001b[0m                     \u001b[35m0.7127\u001b[0m        \u001b[31m0.5790\u001b[0m  0.0093  0.6371\n",
      "      6                     0.7327        \u001b[32m0.5289\u001b[0m                     \u001b[35m0.7257\u001b[0m        \u001b[31m0.5602\u001b[0m  0.0090  0.6363\n",
      "      7                     \u001b[36m0.7350\u001b[0m        \u001b[32m0.5225\u001b[0m                     0.6045        0.7751  0.0085  0.6372\n",
      "      8                     \u001b[36m0.7486\u001b[0m        \u001b[32m0.5123\u001b[0m                     0.6623        0.7162  0.0080  0.6360\n",
      "      9                     \u001b[36m0.7556\u001b[0m        0.5128                     0.6399        0.8123  0.0075  0.6371\n",
      "     10                     \u001b[36m0.7692\u001b[0m        \u001b[32m0.4943\u001b[0m                     0.6586        0.6172  0.0069  0.6376\n",
      "     11                     \u001b[36m0.7748\u001b[0m        \u001b[32m0.4876\u001b[0m                     0.6866        0.5944  0.0063  0.6386\n",
      "     12                     0.7621        \u001b[32m0.4841\u001b[0m                     0.6866        0.5997  0.0057  0.6369\n",
      "     13                     0.7659        \u001b[32m0.4822\u001b[0m                     0.7146        \u001b[31m0.5536\u001b[0m  0.0050  0.6366\n",
      "     14                     \u001b[36m0.7799\u001b[0m        \u001b[32m0.4734\u001b[0m                     0.6884        0.5998  0.0043  0.6369\n",
      "     15                     \u001b[36m0.7850\u001b[0m        \u001b[32m0.4633\u001b[0m                     0.6996        0.6435  0.0037  0.6364\n",
      "     16                     \u001b[36m0.7860\u001b[0m        0.4652                     0.6772        0.6330  0.0031  0.6370\n",
      "     17                     0.7808        \u001b[32m0.4611\u001b[0m                     0.6866        0.6016  0.0025  0.6377\n",
      "     18                     0.7813        0.4614                     0.6996        0.5728  0.0020  0.6374\n",
      "     19                     \u001b[36m0.7911\u001b[0m        \u001b[32m0.4533\u001b[0m                     0.7071        0.5579  0.0015  0.6363\n",
      "     20                     \u001b[36m0.7939\u001b[0m        \u001b[32m0.4443\u001b[0m                     0.6828        0.5874  0.0010  0.6346\n",
      "     21                     \u001b[36m0.8005\u001b[0m        \u001b[32m0.4346\u001b[0m                     0.7108        0.5694  0.0007  0.6372\n",
      "     22                     0.7879        0.4444                     0.7015        0.5644  0.0004  0.6373\n",
      "     23                     \u001b[36m0.8037\u001b[0m        \u001b[32m0.4329\u001b[0m                     0.7015        0.5645  0.0002  0.6371\n",
      "     24                     0.8023        0.4381                     0.7034        0.5650  0.0000  0.6375\n",
      "     25                     \u001b[36m0.8051\u001b[0m        0.4361                     0.7052        0.5658  0.0000  0.6366\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6397\u001b[0m        \u001b[32m0.6599\u001b[0m                     \u001b[35m0.6306\u001b[0m        \u001b[31m0.6568\u001b[0m  0.0100  0.6341\n",
      "      2                     \u001b[36m0.6813\u001b[0m        \u001b[32m0.6006\u001b[0m                     \u001b[35m0.6716\u001b[0m        \u001b[31m0.6206\u001b[0m  0.0100  0.6379\n",
      "      3                     \u001b[36m0.7103\u001b[0m        \u001b[32m0.5666\u001b[0m                     \u001b[35m0.6866\u001b[0m        \u001b[31m0.5928\u001b[0m  0.0098  0.6370\n",
      "      4                     \u001b[36m0.7304\u001b[0m        \u001b[32m0.5418\u001b[0m                     \u001b[35m0.6978\u001b[0m        \u001b[31m0.5808\u001b[0m  0.0096  0.6365\n",
      "      5                     0.7266        0.5444                     \u001b[35m0.7127\u001b[0m        0.5861  0.0093  0.6376\n",
      "      6                     \u001b[36m0.7346\u001b[0m        \u001b[32m0.5304\u001b[0m                     0.6437        0.7080  0.0090  0.6396\n",
      "      7                     \u001b[36m0.7439\u001b[0m        \u001b[32m0.5141\u001b[0m                     0.6847        0.5859  0.0085  0.6421\n",
      "      8                     \u001b[36m0.7514\u001b[0m        \u001b[32m0.5094\u001b[0m                     0.6847        0.6218  0.0080  0.6374\n",
      "      9                     \u001b[36m0.7593\u001b[0m        \u001b[32m0.5007\u001b[0m                     0.6828        0.6231  0.0075  0.6383\n",
      "     10                     0.7547        \u001b[32m0.5006\u001b[0m                     0.7052        0.6013  0.0069  0.6371\n",
      "     11                     0.7537        \u001b[32m0.4963\u001b[0m                     0.6978        0.6200  0.0063  0.6379\n",
      "     12                     \u001b[36m0.7720\u001b[0m        \u001b[32m0.4842\u001b[0m                     0.7108        0.6105  0.0057  0.6368\n",
      "     13                     0.7640        \u001b[32m0.4759\u001b[0m                     \u001b[35m0.7183\u001b[0m        \u001b[31m0.5719\u001b[0m  0.0050  0.6381\n",
      "     14                     0.7710        \u001b[32m0.4661\u001b[0m                     0.7015        0.5854  0.0043  0.6367\n",
      "     15                     0.7710        \u001b[32m0.4656\u001b[0m                     \u001b[35m0.7201\u001b[0m        0.6010  0.0037  0.6377\n",
      "     16                     \u001b[36m0.7729\u001b[0m        0.4712                     0.7164        \u001b[31m0.5714\u001b[0m  0.0031  0.6367\n",
      "     17                     \u001b[36m0.7874\u001b[0m        \u001b[32m0.4521\u001b[0m                     \u001b[35m0.7220\u001b[0m        \u001b[31m0.5673\u001b[0m  0.0025  0.6379\n",
      "     18                     \u001b[36m0.7902\u001b[0m        \u001b[32m0.4514\u001b[0m                     0.7127        \u001b[31m0.5656\u001b[0m  0.0020  0.6373\n",
      "     19                     \u001b[36m0.7953\u001b[0m        \u001b[32m0.4422\u001b[0m                     \u001b[35m0.7257\u001b[0m        \u001b[31m0.5570\u001b[0m  0.0015  0.6380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                     0.7827        0.4513                     0.7201        \u001b[31m0.5527\u001b[0m  0.0010  0.6388\n",
      "     21                     \u001b[36m0.7991\u001b[0m        \u001b[32m0.4349\u001b[0m                     \u001b[35m0.7295\u001b[0m        0.5585  0.0007  0.6381\n",
      "     22                     \u001b[36m0.8023\u001b[0m        0.4361                     \u001b[35m0.7313\u001b[0m        0.5580  0.0004  0.6376\n",
      "     23                     0.7995        0.4370                     0.7295        0.5593  0.0002  0.6372\n",
      "     24                     0.7953        \u001b[32m0.4318\u001b[0m                     0.7276        0.5587  0.0000  0.6371\n",
      "     25                     0.7911        0.4424                     0.7276        0.5580  0.0000  0.6381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5416\u001b[0m        \u001b[32m0.7478\u001b[0m                     \u001b[35m0.5709\u001b[0m        \u001b[31m0.6934\u001b[0m  0.0100  0.6373\n",
      "      2                     \u001b[36m0.5977\u001b[0m        \u001b[32m0.6831\u001b[0m                     \u001b[35m0.6026\u001b[0m        \u001b[31m0.6684\u001b[0m  0.0100  0.6374\n",
      "      3                     \u001b[36m0.6121\u001b[0m        \u001b[32m0.6587\u001b[0m                     0.6026        \u001b[31m0.6511\u001b[0m  0.0098  0.6388\n",
      "      4                     \u001b[36m0.6364\u001b[0m        \u001b[32m0.6357\u001b[0m                     \u001b[35m0.6194\u001b[0m        \u001b[31m0.6408\u001b[0m  0.0096  0.6385\n",
      "      5                     \u001b[36m0.6491\u001b[0m        \u001b[32m0.6309\u001b[0m                     \u001b[35m0.6343\u001b[0m        \u001b[31m0.6366\u001b[0m  0.0093  0.6369\n",
      "      6                     \u001b[36m0.6542\u001b[0m        \u001b[32m0.6235\u001b[0m                     \u001b[35m0.6399\u001b[0m        \u001b[31m0.6276\u001b[0m  0.0090  0.6374\n",
      "      7                     \u001b[36m0.6724\u001b[0m        \u001b[32m0.6094\u001b[0m                     \u001b[35m0.6530\u001b[0m        0.6287  0.0085  0.6369\n",
      "      8                     0.6701        0.6136                     \u001b[35m0.6623\u001b[0m        \u001b[31m0.6239\u001b[0m  0.0080  0.6386\n",
      "      9                     0.6682        \u001b[32m0.6019\u001b[0m                     0.6362        0.6297  0.0075  0.6378\n",
      "     10                     \u001b[36m0.6762\u001b[0m        \u001b[32m0.6002\u001b[0m                     0.6269        0.6347  0.0069  0.6375\n",
      "     11                     \u001b[36m0.6785\u001b[0m        0.6005                     0.6604        \u001b[31m0.6210\u001b[0m  0.0063  0.6376\n",
      "     12                     0.6771        \u001b[32m0.5994\u001b[0m                     0.6549        0.6211  0.0057  0.6378\n",
      "     13                     \u001b[36m0.6967\u001b[0m        \u001b[32m0.5795\u001b[0m                     0.6567        0.6224  0.0050  0.6432\n",
      "     14                     0.6883        0.5798                     0.6530        \u001b[31m0.6156\u001b[0m  0.0043  0.6618\n",
      "     15                     0.6921        \u001b[32m0.5775\u001b[0m                     0.6362        0.6203  0.0037  0.6391\n",
      "     16                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.5744\u001b[0m                     0.6567        0.6263  0.0031  0.6384\n",
      "     17                     0.6991        \u001b[32m0.5706\u001b[0m                     0.6586        \u001b[31m0.6130\u001b[0m  0.0025  0.6578\n",
      "     18                     \u001b[36m0.7182\u001b[0m        \u001b[32m0.5602\u001b[0m                     0.6567        0.6220  0.0020  0.6381\n",
      "     19                     0.7084        \u001b[32m0.5601\u001b[0m                     \u001b[35m0.6642\u001b[0m        0.6162  0.0015  0.6374\n",
      "     20                     0.7075        \u001b[32m0.5580\u001b[0m                     0.6586        0.6158  0.0010  0.6372\n",
      "     21                     0.7089        \u001b[32m0.5561\u001b[0m                     0.6474        0.6174  0.0007  0.6378\n",
      "     22                     0.7079        \u001b[32m0.5535\u001b[0m                     0.6511        0.6151  0.0004  0.6384\n",
      "     23                     \u001b[36m0.7257\u001b[0m        \u001b[32m0.5514\u001b[0m                     0.6530        0.6151  0.0002  0.6390\n",
      "     24                     0.7220        \u001b[32m0.5468\u001b[0m                     0.6511        0.6152  0.0000  0.6379\n",
      "     25                     \u001b[36m0.7299\u001b[0m        \u001b[32m0.5414\u001b[0m                     0.6530        0.6152  0.0000  0.6367\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5416\u001b[0m        \u001b[32m0.7533\u001b[0m                     \u001b[35m0.5392\u001b[0m        \u001b[31m0.7364\u001b[0m  0.0100  0.6360\n",
      "      2                     \u001b[36m0.6065\u001b[0m        \u001b[32m0.6743\u001b[0m                     \u001b[35m0.5970\u001b[0m        \u001b[31m0.6642\u001b[0m  0.0100  0.6383\n",
      "      3                     \u001b[36m0.6589\u001b[0m        \u001b[32m0.6292\u001b[0m                     \u001b[35m0.5989\u001b[0m        0.6690  0.0098  0.6383\n",
      "      4                     0.6369        0.6347                     \u001b[35m0.6157\u001b[0m        \u001b[31m0.6583\u001b[0m  0.0096  0.6384\n",
      "      5                     \u001b[36m0.6766\u001b[0m        \u001b[32m0.6077\u001b[0m                     \u001b[35m0.6343\u001b[0m        \u001b[31m0.6561\u001b[0m  0.0093  0.6375\n",
      "      6                     0.6692        0.6100                     \u001b[35m0.6362\u001b[0m        \u001b[31m0.6482\u001b[0m  0.0090  0.6380\n",
      "      7                     0.6514        0.6143                     \u001b[35m0.6418\u001b[0m        0.6513  0.0085  0.6393\n",
      "      8                     \u001b[36m0.6818\u001b[0m        \u001b[32m0.6012\u001b[0m                     0.6213        0.6487  0.0080  0.6381\n",
      "      9                     \u001b[36m0.6832\u001b[0m        \u001b[32m0.5928\u001b[0m                     0.6399        0.6711  0.0075  0.6377\n",
      "     10                     0.6790        0.6002                     0.6418        0.6537  0.0069  0.6383\n",
      "     11                     \u001b[36m0.6991\u001b[0m        \u001b[32m0.5894\u001b[0m                     0.6362        0.6568  0.0063  0.6385\n",
      "     12                     0.6935        \u001b[32m0.5791\u001b[0m                     0.6213        0.6493  0.0057  0.6384\n",
      "     13                     \u001b[36m0.6995\u001b[0m        \u001b[32m0.5782\u001b[0m                     0.6343        0.6529  0.0050  0.6381\n",
      "     14                     \u001b[36m0.7084\u001b[0m        \u001b[32m0.5637\u001b[0m                     0.6325        0.6620  0.0043  0.6385\n",
      "     15                     \u001b[36m0.7093\u001b[0m        0.5731                     0.6381        0.6604  0.0037  0.6379\n",
      "     16                     \u001b[36m0.7112\u001b[0m        \u001b[32m0.5618\u001b[0m                     0.6362        0.6536  0.0031  0.6380\n",
      "     17                     0.7112        \u001b[32m0.5599\u001b[0m                     0.6399        0.6496  0.0025  0.6383\n",
      "     18                     \u001b[36m0.7210\u001b[0m        \u001b[32m0.5531\u001b[0m                     0.6343        0.6516  0.0020  0.6384\n",
      "     19                     \u001b[36m0.7243\u001b[0m        \u001b[32m0.5438\u001b[0m                     0.6418        0.6512  0.0015  0.6381\n",
      "     20                     \u001b[36m0.7266\u001b[0m        0.5479                     0.6306        0.6588  0.0010  0.6397\n",
      "     21                     \u001b[36m0.7294\u001b[0m        \u001b[32m0.5350\u001b[0m                     0.6362        0.6604  0.0007  0.6388\n",
      "     22                     \u001b[36m0.7416\u001b[0m        \u001b[32m0.5334\u001b[0m                     0.6325        0.6609  0.0004  0.6377\n",
      "     23                     0.7238        0.5417                     0.6343        0.6604  0.0002  0.6378\n",
      "     24                     0.7336        0.5376                     0.6343        0.6615  0.0000  0.6396\n",
      "     25                     0.7290        0.5396                     0.6325        0.6624  0.0000  0.6376\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5084\u001b[0m        \u001b[32m0.7440\u001b[0m                     \u001b[35m0.5709\u001b[0m        \u001b[31m0.6789\u001b[0m  0.0100  0.6362\n",
      "      2                     \u001b[36m0.5869\u001b[0m        \u001b[32m0.6836\u001b[0m                     \u001b[35m0.6026\u001b[0m        \u001b[31m0.6589\u001b[0m  0.0100  0.6381\n",
      "      3                     \u001b[36m0.6379\u001b[0m        \u001b[32m0.6466\u001b[0m                     \u001b[35m0.6604\u001b[0m        \u001b[31m0.6314\u001b[0m  0.0098  0.6382\n",
      "      4                     \u001b[36m0.6547\u001b[0m        \u001b[32m0.6356\u001b[0m                     0.5858        0.6837  0.0096  0.6381\n",
      "      5                     0.6425        \u001b[32m0.6272\u001b[0m                     0.6381        0.6489  0.0093  0.6380\n",
      "      6                     0.6528        \u001b[32m0.6176\u001b[0m                     0.6399        0.6366  0.0090  0.6383\n",
      "      7                     \u001b[36m0.6822\u001b[0m        \u001b[32m0.6103\u001b[0m                     0.6437        \u001b[31m0.6300\u001b[0m  0.0085  0.6367\n",
      "      8                     0.6743        0.6116                     0.6175        0.6662  0.0080  0.6373\n",
      "      9                     \u001b[36m0.6874\u001b[0m        \u001b[32m0.5957\u001b[0m                     0.6343        0.6354  0.0075  0.6377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     0.6701        0.6045                     0.6343        \u001b[31m0.6284\u001b[0m  0.0069  0.6378\n",
      "     11                     \u001b[36m0.6874\u001b[0m        \u001b[32m0.5854\u001b[0m                     0.6549        \u001b[31m0.6224\u001b[0m  0.0063  0.6387\n",
      "     12                     \u001b[36m0.6883\u001b[0m        0.5856                     \u001b[35m0.6698\u001b[0m        \u001b[31m0.6215\u001b[0m  0.0057  0.6383\n",
      "     13                     \u001b[36m0.6995\u001b[0m        \u001b[32m0.5810\u001b[0m                     0.6437        0.6336  0.0050  0.6377\n",
      "     14                     \u001b[36m0.7005\u001b[0m        \u001b[32m0.5810\u001b[0m                     0.6325        0.6314  0.0043  0.6374\n",
      "     15                     0.6991        \u001b[32m0.5751\u001b[0m                     0.6549        \u001b[31m0.6175\u001b[0m  0.0037  0.6387\n",
      "     16                     \u001b[36m0.7164\u001b[0m        \u001b[32m0.5629\u001b[0m                     0.6660        \u001b[31m0.6146\u001b[0m  0.0031  0.6380\n",
      "     17                     \u001b[36m0.7182\u001b[0m        \u001b[32m0.5599\u001b[0m                     0.6567        0.6164  0.0025  0.6379\n",
      "     18                     0.7145        \u001b[32m0.5567\u001b[0m                     \u001b[35m0.6754\u001b[0m        0.6158  0.0020  0.6395\n",
      "     19                     0.7154        \u001b[32m0.5498\u001b[0m                     0.6754        \u001b[31m0.6124\u001b[0m  0.0015  0.6394\n",
      "     20                     \u001b[36m0.7215\u001b[0m        \u001b[32m0.5490\u001b[0m                     0.6642        \u001b[31m0.6105\u001b[0m  0.0010  0.6379\n",
      "     21                     \u001b[36m0.7304\u001b[0m        \u001b[32m0.5401\u001b[0m                     0.6698        0.6116  0.0007  0.6390\n",
      "     22                     0.7252        0.5448                     0.6679        0.6129  0.0004  0.6403\n",
      "     23                     0.7266        \u001b[32m0.5359\u001b[0m                     0.6660        0.6124  0.0002  0.6412\n",
      "     24                     \u001b[36m0.7355\u001b[0m        0.5376                     0.6698        0.6126  0.0000  0.6378\n",
      "     25                     \u001b[36m0.7388\u001b[0m        0.5395                     0.6679        0.6129  0.0000  0.6387\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5435\u001b[0m        \u001b[32m0.7525\u001b[0m                     \u001b[35m0.5410\u001b[0m        \u001b[31m0.6918\u001b[0m  0.0100  0.6344\n",
      "      2                     \u001b[36m0.6014\u001b[0m        \u001b[32m0.6705\u001b[0m                     \u001b[35m0.5933\u001b[0m        \u001b[31m0.6602\u001b[0m  0.0100  0.6375\n",
      "      3                     \u001b[36m0.6341\u001b[0m        \u001b[32m0.6434\u001b[0m                     \u001b[35m0.6306\u001b[0m        \u001b[31m0.6504\u001b[0m  0.0098  0.6378\n",
      "      4                     \u001b[36m0.6603\u001b[0m        \u001b[32m0.6262\u001b[0m                     \u001b[35m0.6474\u001b[0m        \u001b[31m0.6319\u001b[0m  0.0096  0.6380\n",
      "      5                     \u001b[36m0.6617\u001b[0m        \u001b[32m0.6103\u001b[0m                     0.6306        0.6425  0.0093  0.6374\n",
      "      6                     \u001b[36m0.6724\u001b[0m        \u001b[32m0.6100\u001b[0m                     0.6119        0.6475  0.0090  0.6377\n",
      "      7                     0.6720        \u001b[32m0.6027\u001b[0m                     0.6194        0.6654  0.0085  0.6374\n",
      "      8                     \u001b[36m0.6808\u001b[0m        \u001b[32m0.5972\u001b[0m                     0.6325        0.6572  0.0080  0.6370\n",
      "      9                     \u001b[36m0.6930\u001b[0m        \u001b[32m0.5924\u001b[0m                     \u001b[35m0.6586\u001b[0m        0.6384  0.0075  0.6377\n",
      "     10                     \u001b[36m0.6977\u001b[0m        \u001b[32m0.5838\u001b[0m                     0.6213        0.6460  0.0069  0.6375\n",
      "     11                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.5767\u001b[0m                     0.6101        0.6926  0.0063  0.6376\n",
      "     12                     \u001b[36m0.7037\u001b[0m        0.5795                     0.6269        0.6628  0.0057  0.6374\n",
      "     13                     0.7009        \u001b[32m0.5726\u001b[0m                     0.6362        0.6838  0.0050  0.6374\n",
      "     14                     \u001b[36m0.7089\u001b[0m        0.5736                     0.6362        0.6449  0.0043  0.6380\n",
      "     15                     \u001b[36m0.7107\u001b[0m        \u001b[32m0.5604\u001b[0m                     0.6549        0.6484  0.0037  0.6366\n",
      "     16                     \u001b[36m0.7164\u001b[0m        \u001b[32m0.5552\u001b[0m                     0.6381        0.6707  0.0031  0.6375\n",
      "     17                     \u001b[36m0.7201\u001b[0m        \u001b[32m0.5487\u001b[0m                     0.6063        0.6722  0.0025  0.6369\n",
      "     18                     0.7196        \u001b[32m0.5447\u001b[0m                     0.6511        0.6474  0.0020  0.6384\n",
      "     19                     \u001b[36m0.7271\u001b[0m        \u001b[32m0.5428\u001b[0m                     \u001b[35m0.6604\u001b[0m        0.6449  0.0015  0.6374\n",
      "     20                     0.7234        \u001b[32m0.5395\u001b[0m                     0.6511        0.6479  0.0010  0.6378\n",
      "     21                     \u001b[36m0.7383\u001b[0m        \u001b[32m0.5336\u001b[0m                     0.6586        0.6552  0.0007  0.6377\n",
      "     22                     \u001b[36m0.7402\u001b[0m        \u001b[32m0.5314\u001b[0m                     0.6549        0.6515  0.0004  0.6384\n",
      "     23                     0.7355        0.5383                     0.6549        0.6531  0.0002  0.6385\n",
      "     24                     0.7304        0.5329                     0.6511        0.6526  0.0000  0.6359\n",
      "     25                     0.7290        0.5333                     0.6530        0.6525  0.0000  0.6373\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5290\u001b[0m        \u001b[32m0.7451\u001b[0m                     \u001b[35m0.5205\u001b[0m        \u001b[31m0.6958\u001b[0m  0.0100  0.6357\n",
      "      2                     \u001b[36m0.5762\u001b[0m        \u001b[32m0.6832\u001b[0m                     \u001b[35m0.5634\u001b[0m        \u001b[31m0.6692\u001b[0m  0.0100  0.6370\n",
      "      3                     \u001b[36m0.6248\u001b[0m        \u001b[32m0.6431\u001b[0m                     \u001b[35m0.6362\u001b[0m        \u001b[31m0.6431\u001b[0m  0.0098  0.6378\n",
      "      4                     \u001b[36m0.6547\u001b[0m        \u001b[32m0.6221\u001b[0m                     \u001b[35m0.6455\u001b[0m        0.6488  0.0096  0.6373\n",
      "      5                     \u001b[36m0.6575\u001b[0m        \u001b[32m0.6120\u001b[0m                     0.6306        0.6472  0.0093  0.6374\n",
      "      6                     \u001b[36m0.6804\u001b[0m        \u001b[32m0.6089\u001b[0m                     \u001b[35m0.6474\u001b[0m        0.6455  0.0090  0.6381\n",
      "      7                     0.6743        \u001b[32m0.5987\u001b[0m                     \u001b[35m0.6604\u001b[0m        \u001b[31m0.6430\u001b[0m  0.0085  0.6371\n",
      "      8                     0.6794        \u001b[32m0.5948\u001b[0m                     0.6157        0.6758  0.0080  0.6381\n",
      "      9                     \u001b[36m0.6836\u001b[0m        \u001b[32m0.5851\u001b[0m                     0.6455        0.6510  0.0075  0.6372\n",
      "     10                     \u001b[36m0.6930\u001b[0m        \u001b[32m0.5837\u001b[0m                     0.6530        \u001b[31m0.6381\u001b[0m  0.0069  0.6378\n",
      "     11                     \u001b[36m0.7028\u001b[0m        \u001b[32m0.5729\u001b[0m                     0.6399        0.6616  0.0063  0.6377\n",
      "     12                     0.6855        0.5765                     0.6437        0.6574  0.0057  0.6376\n",
      "     13                     0.6935        0.5748                     0.6325        0.6610  0.0050  0.6383\n",
      "     14                     0.7005        \u001b[32m0.5655\u001b[0m                     0.5746        0.7631  0.0043  0.6374\n",
      "     15                     0.6977        0.5685                     0.6213        0.6605  0.0037  0.6377\n",
      "     16                     0.6967        0.5664                     0.6549        0.6461  0.0031  0.6381\n",
      "     17                     \u001b[36m0.7187\u001b[0m        \u001b[32m0.5556\u001b[0m                     0.6362        0.6534  0.0025  0.6374\n",
      "     18                     \u001b[36m0.7238\u001b[0m        \u001b[32m0.5435\u001b[0m                     0.6231        0.6659  0.0020  0.6383\n",
      "     19                     0.7215        \u001b[32m0.5428\u001b[0m                     0.6399        0.6556  0.0015  0.6376\n",
      "     20                     \u001b[36m0.7266\u001b[0m        \u001b[32m0.5407\u001b[0m                     0.6586        0.6506  0.0010  0.6380\n",
      "     21                     \u001b[36m0.7327\u001b[0m        \u001b[32m0.5381\u001b[0m                     0.6493        0.6532  0.0007  0.6376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22                     0.7280        \u001b[32m0.5297\u001b[0m                     0.6418        0.6524  0.0004  0.6378\n",
      "     23                     0.7257        0.5350                     0.6474        0.6526  0.0002  0.6377\n",
      "     24                     0.7285        0.5330                     0.6455        0.6526  0.0000  0.6378\n",
      "     25                     0.7318        \u001b[32m0.5255\u001b[0m                     0.6437        0.6527  0.0000  0.6375\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5117\u001b[0m        \u001b[32m0.7744\u001b[0m                     \u001b[35m0.5336\u001b[0m        \u001b[31m0.6888\u001b[0m  0.0100  0.6352\n",
      "      2                     \u001b[36m0.6019\u001b[0m        \u001b[32m0.6696\u001b[0m                     \u001b[35m0.5784\u001b[0m        0.6958  0.0100  0.6376\n",
      "      3                     \u001b[36m0.6364\u001b[0m        \u001b[32m0.6486\u001b[0m                     \u001b[35m0.5970\u001b[0m        \u001b[31m0.6735\u001b[0m  0.0098  0.6372\n",
      "      4                     \u001b[36m0.6453\u001b[0m        \u001b[32m0.6348\u001b[0m                     \u001b[35m0.6157\u001b[0m        \u001b[31m0.6528\u001b[0m  0.0096  0.6374\n",
      "      5                     \u001b[36m0.6636\u001b[0m        \u001b[32m0.6285\u001b[0m                     \u001b[35m0.6325\u001b[0m        \u001b[31m0.6383\u001b[0m  0.0093  0.6376\n",
      "      6                     \u001b[36m0.6734\u001b[0m        \u001b[32m0.6140\u001b[0m                     0.6250        0.6448  0.0090  0.6377\n",
      "      7                     \u001b[36m0.6785\u001b[0m        \u001b[32m0.6030\u001b[0m                     0.6269        0.6566  0.0085  0.6376\n",
      "      8                     0.6743        0.6045                     0.6250        0.6707  0.0080  0.6378\n",
      "      9                     \u001b[36m0.6822\u001b[0m        \u001b[32m0.6004\u001b[0m                     \u001b[35m0.6362\u001b[0m        0.6571  0.0075  0.6371\n",
      "     10                     \u001b[36m0.6864\u001b[0m        \u001b[32m0.5927\u001b[0m                     0.6250        0.6527  0.0069  0.6380\n",
      "     11                     \u001b[36m0.6874\u001b[0m        \u001b[32m0.5884\u001b[0m                     0.6343        0.6412  0.0063  0.6377\n",
      "     12                     \u001b[36m0.6907\u001b[0m        0.5897                     0.6231        0.6460  0.0057  0.6379\n",
      "     13                     \u001b[36m0.7009\u001b[0m        \u001b[32m0.5804\u001b[0m                     0.6325        0.6460  0.0050  0.6384\n",
      "     14                     \u001b[36m0.7121\u001b[0m        \u001b[32m0.5673\u001b[0m                     0.6250        0.6481  0.0043  0.6379\n",
      "     15                     0.7051        0.5713                     0.6250        0.6449  0.0037  0.6383\n",
      "     16                     0.7037        0.5708                     0.6362        0.6447  0.0031  0.6373\n",
      "     17                     \u001b[36m0.7182\u001b[0m        \u001b[32m0.5619\u001b[0m                     0.6325        0.6450  0.0025  0.6379\n",
      "     18                     0.7056        \u001b[32m0.5583\u001b[0m                     \u001b[35m0.6455\u001b[0m        0.6458  0.0020  0.6378\n",
      "     19                     \u001b[36m0.7192\u001b[0m        \u001b[32m0.5533\u001b[0m                     0.6343        0.6515  0.0015  0.6380\n",
      "     20                     0.7140        0.5538                     0.6287        0.6472  0.0010  0.6373\n",
      "     21                     \u001b[36m0.7220\u001b[0m        \u001b[32m0.5464\u001b[0m                     0.6325        0.6475  0.0007  0.6376\n",
      "     22                     0.7196        \u001b[32m0.5448\u001b[0m                     0.6325        0.6477  0.0004  0.6376\n",
      "     23                     \u001b[36m0.7243\u001b[0m        \u001b[32m0.5425\u001b[0m                     0.6306        0.6477  0.0002  0.6376\n",
      "     24                     0.7196        0.5494                     0.6287        0.6476  0.0000  0.6378\n",
      "     25                     0.7229        0.5434                     0.6287        0.6477  0.0000  0.6373\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5449\u001b[0m        \u001b[32m0.7387\u001b[0m                     \u001b[35m0.5485\u001b[0m        \u001b[31m0.7051\u001b[0m  0.0100  0.6363\n",
      "      2                     \u001b[36m0.6009\u001b[0m        \u001b[32m0.6629\u001b[0m                     \u001b[35m0.6026\u001b[0m        \u001b[31m0.6754\u001b[0m  0.0100  0.6372\n",
      "      3                     \u001b[36m0.6388\u001b[0m        \u001b[32m0.6367\u001b[0m                     \u001b[35m0.6511\u001b[0m        \u001b[31m0.6418\u001b[0m  0.0098  0.6373\n",
      "      4                     \u001b[36m0.6495\u001b[0m        \u001b[32m0.6304\u001b[0m                     0.6455        0.6437  0.0096  0.6373\n",
      "      5                     \u001b[36m0.6738\u001b[0m        \u001b[32m0.6132\u001b[0m                     0.6362        0.6669  0.0093  0.6380\n",
      "      6                     0.6551        0.6190                     0.6381        \u001b[31m0.6402\u001b[0m  0.0090  0.6371\n",
      "      7                     0.6654        0.6148                     0.6511        \u001b[31m0.6396\u001b[0m  0.0085  0.6384\n",
      "      8                     0.6696        \u001b[32m0.6052\u001b[0m                     0.6381        0.6478  0.0080  0.6382\n",
      "      9                     \u001b[36m0.6771\u001b[0m        \u001b[32m0.6012\u001b[0m                     \u001b[35m0.6567\u001b[0m        \u001b[31m0.6322\u001b[0m  0.0075  0.6372\n",
      "     10                     \u001b[36m0.6832\u001b[0m        0.6031                     0.6437        0.6478  0.0069  0.6380\n",
      "     11                     0.6822        \u001b[32m0.5977\u001b[0m                     \u001b[35m0.6642\u001b[0m        0.6396  0.0063  0.6392\n",
      "     12                     \u001b[36m0.6967\u001b[0m        \u001b[32m0.5832\u001b[0m                     \u001b[35m0.6716\u001b[0m        0.6397  0.0057  0.6376\n",
      "     13                     \u001b[36m0.6981\u001b[0m        \u001b[32m0.5800\u001b[0m                     0.6250        0.6562  0.0050  0.6527\n",
      "     14                     \u001b[36m0.7019\u001b[0m        \u001b[32m0.5762\u001b[0m                     0.6567        0.6495  0.0043  0.7018\n",
      "     15                     0.6963        0.5813                     0.6287        0.6464  0.0037  0.6715\n",
      "     16                     \u001b[36m0.7070\u001b[0m        \u001b[32m0.5683\u001b[0m                     0.6455        0.6480  0.0031  0.7134\n",
      "     17                     \u001b[36m0.7126\u001b[0m        \u001b[32m0.5606\u001b[0m                     0.6567        0.6497  0.0025  0.6504\n",
      "     18                     \u001b[36m0.7145\u001b[0m        \u001b[32m0.5595\u001b[0m                     0.6530        0.6492  0.0020  0.6599\n",
      "     19                     \u001b[36m0.7178\u001b[0m        \u001b[32m0.5559\u001b[0m                     0.6549        0.6458  0.0015  0.6604\n",
      "     20                     \u001b[36m0.7206\u001b[0m        \u001b[32m0.5461\u001b[0m                     0.6549        0.6483  0.0010  0.6387\n",
      "     21                     \u001b[36m0.7220\u001b[0m        0.5485                     0.6586        0.6469  0.0007  0.6381\n",
      "     22                     \u001b[36m0.7238\u001b[0m        \u001b[32m0.5442\u001b[0m                     0.6511        0.6481  0.0004  0.6380\n",
      "     23                     \u001b[36m0.7449\u001b[0m        \u001b[32m0.5367\u001b[0m                     0.6530        0.6490  0.0002  0.6372\n",
      "     24                     0.7238        0.5407                     0.6549        0.6491  0.0000  0.6376\n",
      "     25                     0.7229        0.5426                     0.6549        0.6491  0.0000  0.6388\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5290\u001b[0m        \u001b[32m0.7436\u001b[0m                     \u001b[35m0.5896\u001b[0m        \u001b[31m0.6835\u001b[0m  0.0100  0.6343\n",
      "      2                     \u001b[36m0.6023\u001b[0m        \u001b[32m0.6647\u001b[0m                     \u001b[35m0.6138\u001b[0m        \u001b[31m0.6653\u001b[0m  0.0100  0.6370\n",
      "      3                     \u001b[36m0.6364\u001b[0m        \u001b[32m0.6405\u001b[0m                     \u001b[35m0.6231\u001b[0m        \u001b[31m0.6512\u001b[0m  0.0098  0.6367\n",
      "      4                     \u001b[36m0.6467\u001b[0m        \u001b[32m0.6340\u001b[0m                     0.6157        0.6518  0.0096  0.6373\n",
      "      5                     \u001b[36m0.6603\u001b[0m        \u001b[32m0.6246\u001b[0m                     \u001b[35m0.6493\u001b[0m        0.6618  0.0093  0.6383\n",
      "      6                     \u001b[36m0.6636\u001b[0m        \u001b[32m0.6156\u001b[0m                     0.6325        \u001b[31m0.6339\u001b[0m  0.0090  0.6371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7                     0.6621        \u001b[32m0.6133\u001b[0m                     0.6437        0.6392  0.0085  0.6375\n",
      "      8                     \u001b[36m0.6729\u001b[0m        \u001b[32m0.6020\u001b[0m                     0.6269        0.6615  0.0080  0.6370\n",
      "      9                     \u001b[36m0.6757\u001b[0m        0.6068                     0.6287        0.6597  0.0075  0.6375\n",
      "     10                     \u001b[36m0.6822\u001b[0m        \u001b[32m0.5997\u001b[0m                     \u001b[35m0.6567\u001b[0m        0.6383  0.0069  0.6377\n",
      "     11                     \u001b[36m0.6921\u001b[0m        \u001b[32m0.5917\u001b[0m                     \u001b[35m0.6586\u001b[0m        0.6502  0.0063  0.6370\n",
      "     12                     0.6883        \u001b[32m0.5865\u001b[0m                     0.6418        0.6572  0.0057  0.6381\n",
      "     13                     \u001b[36m0.7089\u001b[0m        \u001b[32m0.5744\u001b[0m                     0.6325        0.6519  0.0050  0.6378\n",
      "     14                     0.6897        0.5863                     0.6530        0.6348  0.0043  0.6381\n",
      "     15                     0.7065        \u001b[32m0.5689\u001b[0m                     0.6213        0.6667  0.0037  0.6370\n",
      "     16                     \u001b[36m0.7093\u001b[0m        0.5736                     0.6250        0.6658  0.0031  0.6388\n",
      "     17                     0.7033        \u001b[32m0.5675\u001b[0m                     0.6250        0.6601  0.0025  0.6369\n",
      "     18                     0.7033        \u001b[32m0.5652\u001b[0m                     0.6362        0.6612  0.0020  0.6378\n",
      "     19                     \u001b[36m0.7224\u001b[0m        \u001b[32m0.5554\u001b[0m                     0.6362        0.6484  0.0015  0.6372\n",
      "     20                     0.7154        \u001b[32m0.5492\u001b[0m                     0.6399        0.6489  0.0010  0.6380\n",
      "     21                     \u001b[36m0.7257\u001b[0m        0.5493                     0.6418        0.6490  0.0007  0.6376\n",
      "     22                     0.7182        0.5523                     0.6418        0.6467  0.0004  0.6380\n",
      "     23                     0.7224        \u001b[32m0.5473\u001b[0m                     0.6399        0.6466  0.0002  0.6377\n",
      "     24                     \u001b[36m0.7299\u001b[0m        \u001b[32m0.5457\u001b[0m                     0.6399        0.6469  0.0000  0.6702\n",
      "     25                     0.7150        0.5498                     0.6362        0.6474  0.0000  0.6386\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5304\u001b[0m        \u001b[32m0.7605\u001b[0m                     \u001b[35m0.5261\u001b[0m        \u001b[31m0.7025\u001b[0m  0.0100  0.6352\n",
      "      2                     \u001b[36m0.6033\u001b[0m        \u001b[32m0.6726\u001b[0m                     \u001b[35m0.5802\u001b[0m        \u001b[31m0.6899\u001b[0m  0.0100  0.6636\n",
      "      3                     \u001b[36m0.6182\u001b[0m        \u001b[32m0.6522\u001b[0m                     \u001b[35m0.5933\u001b[0m        \u001b[31m0.6602\u001b[0m  0.0098  0.6378\n",
      "      4                     \u001b[36m0.6346\u001b[0m        \u001b[32m0.6362\u001b[0m                     \u001b[35m0.6343\u001b[0m        \u001b[31m0.6542\u001b[0m  0.0096  0.6369\n",
      "      5                     \u001b[36m0.6495\u001b[0m        \u001b[32m0.6177\u001b[0m                     \u001b[35m0.6493\u001b[0m        \u001b[31m0.6538\u001b[0m  0.0093  0.6370\n",
      "      6                     \u001b[36m0.6509\u001b[0m        0.6224                     \u001b[35m0.6567\u001b[0m        \u001b[31m0.6422\u001b[0m  0.0090  0.6364\n",
      "      7                     \u001b[36m0.6603\u001b[0m        \u001b[32m0.6105\u001b[0m                     \u001b[35m0.6698\u001b[0m        \u001b[31m0.6303\u001b[0m  0.0085  0.6372\n",
      "      8                     \u001b[36m0.6729\u001b[0m        \u001b[32m0.5985\u001b[0m                     0.6679        0.6524  0.0080  0.6368\n",
      "      9                     \u001b[36m0.6794\u001b[0m        \u001b[32m0.5937\u001b[0m                     0.6455        0.6736  0.0075  0.6361\n",
      "     10                     0.6776        0.5960                     0.6586        0.6388  0.0069  0.6369\n",
      "     11                     0.6748        \u001b[32m0.5884\u001b[0m                     0.6604        0.6506  0.0063  0.6367\n",
      "     12                     \u001b[36m0.6836\u001b[0m        \u001b[32m0.5862\u001b[0m                     0.6623        0.6423  0.0057  0.6370\n",
      "     13                     \u001b[36m0.6869\u001b[0m        \u001b[32m0.5814\u001b[0m                     0.6549        0.6560  0.0050  0.6371\n",
      "     14                     \u001b[36m0.7140\u001b[0m        \u001b[32m0.5652\u001b[0m                     0.6586        0.6602  0.0043  0.6375\n",
      "     15                     0.7009        0.5677                     0.6437        0.6560  0.0037  0.6370\n",
      "     16                     0.7126        \u001b[32m0.5595\u001b[0m                     0.6642        0.6538  0.0031  0.6377\n",
      "     17                     0.6986        0.5662                     0.6586        0.6432  0.0025  0.6373\n",
      "     18                     0.7117        \u001b[32m0.5561\u001b[0m                     \u001b[35m0.6791\u001b[0m        0.6488  0.0020  0.6369\n",
      "     19                     \u001b[36m0.7150\u001b[0m        \u001b[32m0.5510\u001b[0m                     0.6642        0.6512  0.0015  0.6369\n",
      "     20                     \u001b[36m0.7164\u001b[0m        \u001b[32m0.5476\u001b[0m                     0.6530        0.6546  0.0010  0.6365\n",
      "     21                     \u001b[36m0.7178\u001b[0m        \u001b[32m0.5387\u001b[0m                     0.6604        0.6520  0.0007  0.6366\n",
      "     22                     0.7178        0.5460                     0.6716        0.6521  0.0004  0.6380\n",
      "     23                     0.7136        0.5482                     0.6698        0.6524  0.0002  0.6369\n",
      "     24                     \u001b[36m0.7257\u001b[0m        0.5402                     0.6679        0.6529  0.0000  0.6376\n",
      "     25                     0.7196        0.5469                     0.6716        0.6532  0.0000  0.6379\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5294\u001b[0m        \u001b[32m0.7543\u001b[0m                     \u001b[35m0.5373\u001b[0m        \u001b[31m0.7236\u001b[0m  0.0100  0.6350\n",
      "      2                     \u001b[36m0.5748\u001b[0m        \u001b[32m0.6893\u001b[0m                     \u001b[35m0.5709\u001b[0m        \u001b[31m0.6789\u001b[0m  0.0100  0.6380\n",
      "      3                     \u001b[36m0.6079\u001b[0m        \u001b[32m0.6667\u001b[0m                     \u001b[35m0.6138\u001b[0m        \u001b[31m0.6594\u001b[0m  0.0098  0.6378\n",
      "      4                     \u001b[36m0.6402\u001b[0m        \u001b[32m0.6452\u001b[0m                     \u001b[35m0.6194\u001b[0m        \u001b[31m0.6585\u001b[0m  0.0096  0.6379\n",
      "      5                     \u001b[36m0.6449\u001b[0m        \u001b[32m0.6322\u001b[0m                     \u001b[35m0.6287\u001b[0m        \u001b[31m0.6357\u001b[0m  0.0093  0.6378\n",
      "      6                     \u001b[36m0.6589\u001b[0m        \u001b[32m0.6197\u001b[0m                     \u001b[35m0.6567\u001b[0m        \u001b[31m0.6164\u001b[0m  0.0090  0.6374\n",
      "      7                     \u001b[36m0.6785\u001b[0m        \u001b[32m0.6115\u001b[0m                     0.6362        0.6629  0.0085  0.6373\n",
      "      8                     0.6715        \u001b[32m0.6090\u001b[0m                     0.6362        0.6674  0.0080  0.6385\n",
      "      9                     \u001b[36m0.6827\u001b[0m        \u001b[32m0.5997\u001b[0m                     \u001b[35m0.6772\u001b[0m        0.6260  0.0075  0.6384\n",
      "     10                     0.6808        \u001b[32m0.5930\u001b[0m                     0.6623        \u001b[31m0.6089\u001b[0m  0.0069  0.6367\n",
      "     11                     \u001b[36m0.6935\u001b[0m        \u001b[32m0.5873\u001b[0m                     0.6735        0.6202  0.0063  0.6377\n",
      "     12                     0.6879        0.5907                     0.6716        0.6279  0.0057  0.6381\n",
      "     13                     \u001b[36m0.7047\u001b[0m        \u001b[32m0.5757\u001b[0m                     0.6642        0.6293  0.0050  0.6377\n",
      "     14                     0.6841        0.5838                     0.6269        0.6778  0.0043  0.6374\n",
      "     15                     0.6953        0.5780                     0.6623        0.6202  0.0037  0.6381\n",
      "     16                     \u001b[36m0.7061\u001b[0m        \u001b[32m0.5666\u001b[0m                     \u001b[35m0.6847\u001b[0m        0.6209  0.0031  0.6373\n",
      "     17                     \u001b[36m0.7107\u001b[0m        \u001b[32m0.5649\u001b[0m                     0.6698        0.6256  0.0025  0.6393\n",
      "     18                     \u001b[36m0.7192\u001b[0m        \u001b[32m0.5521\u001b[0m                     0.6772        0.6226  0.0020  0.6375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19                     0.7173        0.5603                     0.6716        0.6279  0.0015  0.6375\n",
      "     20                     \u001b[36m0.7266\u001b[0m        \u001b[32m0.5483\u001b[0m                     0.6698        0.6270  0.0010  0.6366\n",
      "     21                     0.7178        0.5495                     0.6604        0.6269  0.0007  0.6378\n",
      "     22                     0.7164        \u001b[32m0.5441\u001b[0m                     0.6660        0.6258  0.0004  0.6379\n",
      "     23                     0.7182        0.5484                     0.6642        0.6263  0.0002  0.6380\n",
      "     24                     \u001b[36m0.7364\u001b[0m        \u001b[32m0.5400\u001b[0m                     0.6604        0.6259  0.0000  0.6376\n",
      "     25                     0.7355        \u001b[32m0.5363\u001b[0m                     0.6604        0.6255  0.0000  0.6381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5234\u001b[0m        \u001b[32m0.7652\u001b[0m                     \u001b[35m0.5597\u001b[0m        \u001b[31m0.6932\u001b[0m  0.0100  0.7132\n",
      "      2                     \u001b[36m0.5336\u001b[0m        \u001b[32m0.7005\u001b[0m                     0.5410        \u001b[31m0.6874\u001b[0m  0.0100  0.6396\n",
      "      3                     \u001b[36m0.5771\u001b[0m        \u001b[32m0.6885\u001b[0m                     \u001b[35m0.5765\u001b[0m        \u001b[31m0.6770\u001b[0m  0.0098  0.6428\n",
      "      4                     \u001b[36m0.5780\u001b[0m        \u001b[32m0.6772\u001b[0m                     0.5746        0.6852  0.0096  0.6381\n",
      "      5                     \u001b[36m0.5836\u001b[0m        \u001b[32m0.6700\u001b[0m                     \u001b[35m0.5802\u001b[0m        0.6829  0.0093  0.6377\n",
      "      6                     \u001b[36m0.5967\u001b[0m        \u001b[32m0.6680\u001b[0m                     \u001b[35m0.5914\u001b[0m        \u001b[31m0.6756\u001b[0m  0.0090  0.6375\n",
      "      7                     \u001b[36m0.6070\u001b[0m        \u001b[32m0.6617\u001b[0m                     0.5522        0.6980  0.0085  0.6371\n",
      "      8                     0.5935        0.6623                     0.5709        0.6967  0.0080  0.6384\n",
      "      9                     \u001b[36m0.6136\u001b[0m        \u001b[32m0.6573\u001b[0m                     0.5541        0.6903  0.0075  0.6374\n",
      "     10                     \u001b[36m0.6215\u001b[0m        \u001b[32m0.6508\u001b[0m                     0.5728        0.6859  0.0069  0.6370\n",
      "     11                     0.6168        \u001b[32m0.6475\u001b[0m                     0.5821        0.6827  0.0063  0.6377\n",
      "     12                     \u001b[36m0.6215\u001b[0m        \u001b[32m0.6469\u001b[0m                     \u001b[35m0.6007\u001b[0m        0.6857  0.0057  0.6371\n",
      "     13                     \u001b[36m0.6285\u001b[0m        \u001b[32m0.6447\u001b[0m                     0.5858        0.6884  0.0050  0.6368\n",
      "     14                     \u001b[36m0.6318\u001b[0m        \u001b[32m0.6398\u001b[0m                     0.5896        0.6764  0.0043  0.6375\n",
      "     15                     \u001b[36m0.6369\u001b[0m        \u001b[32m0.6345\u001b[0m                     0.5616        0.6924  0.0037  0.6373\n",
      "     16                     \u001b[36m0.6430\u001b[0m        \u001b[32m0.6341\u001b[0m                     0.5933        0.6809  0.0031  0.6378\n",
      "     17                     0.6327        \u001b[32m0.6282\u001b[0m                     0.5951        0.6798  0.0025  0.6388\n",
      "     18                     \u001b[36m0.6603\u001b[0m        \u001b[32m0.6197\u001b[0m                     0.5970        0.6805  0.0020  0.6372\n",
      "     19                     0.6523        \u001b[32m0.6194\u001b[0m                     0.5970        0.6882  0.0015  0.6380\n",
      "     20                     0.6603        \u001b[32m0.6157\u001b[0m                     0.5970        0.6873  0.0010  0.6379\n",
      "     21                     0.6533        \u001b[32m0.6067\u001b[0m                     \u001b[35m0.6045\u001b[0m        0.6841  0.0007  0.6389\n",
      "     22                     0.6528        0.6119                     0.6026        0.6874  0.0004  0.6381\n",
      "     23                     0.6551        0.6156                     0.6007        0.6893  0.0002  0.6376\n",
      "     24                     0.6561        0.6121                     0.6007        0.6898  0.0000  0.6372\n",
      "     25                     \u001b[36m0.6621\u001b[0m        0.6127                     0.6007        0.6901  0.0000  0.6386\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5229\u001b[0m        \u001b[32m0.7720\u001b[0m                     \u001b[35m0.5056\u001b[0m        \u001b[31m0.7039\u001b[0m  0.0100  0.6341\n",
      "      2                     \u001b[36m0.5528\u001b[0m        \u001b[32m0.6957\u001b[0m                     \u001b[35m0.5354\u001b[0m        \u001b[31m0.6917\u001b[0m  0.0100  0.6380\n",
      "      3                     \u001b[36m0.5673\u001b[0m        \u001b[32m0.6816\u001b[0m                     \u001b[35m0.5541\u001b[0m        \u001b[31m0.6871\u001b[0m  0.0098  0.6380\n",
      "      4                     \u001b[36m0.5986\u001b[0m        \u001b[32m0.6689\u001b[0m                     \u001b[35m0.5858\u001b[0m        \u001b[31m0.6821\u001b[0m  0.0096  0.6382\n",
      "      5                     \u001b[36m0.6056\u001b[0m        \u001b[32m0.6667\u001b[0m                     \u001b[35m0.5989\u001b[0m        \u001b[31m0.6793\u001b[0m  0.0093  0.6372\n",
      "      6                     0.5949        \u001b[32m0.6652\u001b[0m                     0.5728        0.6868  0.0090  0.6384\n",
      "      7                     \u001b[36m0.6294\u001b[0m        \u001b[32m0.6553\u001b[0m                     0.5560        0.6946  0.0085  0.6386\n",
      "      8                     0.6234        \u001b[32m0.6511\u001b[0m                     0.5802        0.6933  0.0080  0.6377\n",
      "      9                     0.6238        0.6516                     0.5933        0.6861  0.0075  0.6374\n",
      "     10                     0.6178        0.6523                     0.5690        0.6842  0.0069  0.6377\n",
      "     11                     0.6192        \u001b[32m0.6427\u001b[0m                     0.5728        0.6908  0.0063  0.6370\n",
      "     12                     0.6294        \u001b[32m0.6376\u001b[0m                     0.5522        0.7049  0.0057  0.6373\n",
      "     13                     \u001b[36m0.6444\u001b[0m        \u001b[32m0.6357\u001b[0m                     0.5690        0.7027  0.0050  0.6372\n",
      "     14                     0.6369        \u001b[32m0.6336\u001b[0m                     0.5560        0.6918  0.0043  0.6371\n",
      "     15                     0.6397        \u001b[32m0.6280\u001b[0m                     0.5616        0.6966  0.0037  0.6383\n",
      "     16                     \u001b[36m0.6556\u001b[0m        \u001b[32m0.6242\u001b[0m                     0.5541        0.6990  0.0031  0.6366\n",
      "     17                     \u001b[36m0.6631\u001b[0m        \u001b[32m0.6212\u001b[0m                     0.5578        0.7004  0.0025  0.6379\n",
      "     18                     \u001b[36m0.6650\u001b[0m        0.6227                     0.5653        0.7021  0.0020  0.6376\n",
      "     19                     0.6561        \u001b[32m0.6187\u001b[0m                     0.5448        0.7043  0.0015  0.6377\n",
      "     20                     \u001b[36m0.6762\u001b[0m        \u001b[32m0.6088\u001b[0m                     0.5578        0.7073  0.0010  0.6391\n",
      "     21                     0.6720        0.6133                     0.5541        0.7050  0.0007  0.6382\n",
      "     22                     0.6621        0.6115                     0.5597        0.7079  0.0004  0.6379\n",
      "     23                     0.6734        \u001b[32m0.6072\u001b[0m                     0.5578        0.7078  0.0002  0.6387\n",
      "     24                     \u001b[36m0.6813\u001b[0m        \u001b[32m0.5994\u001b[0m                     0.5541        0.7083  0.0000  0.6382\n",
      "     25                     0.6706        0.6037                     0.5560        0.7087  0.0000  0.6379\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5131\u001b[0m        \u001b[32m0.7601\u001b[0m                     \u001b[35m0.5093\u001b[0m        \u001b[31m0.7142\u001b[0m  0.0100  0.6335\n",
      "      2                     \u001b[36m0.5509\u001b[0m        \u001b[32m0.6961\u001b[0m                     \u001b[35m0.5784\u001b[0m        \u001b[31m0.6961\u001b[0m  0.0100  0.6395\n",
      "      3                     \u001b[36m0.5682\u001b[0m        \u001b[32m0.6819\u001b[0m                     0.5765        \u001b[31m0.6837\u001b[0m  0.0098  0.6374\n",
      "      4                     \u001b[36m0.5706\u001b[0m        \u001b[32m0.6744\u001b[0m                     0.5578        0.6899  0.0096  0.6371\n",
      "      5                     \u001b[36m0.5972\u001b[0m        \u001b[32m0.6690\u001b[0m                     0.5672        \u001b[31m0.6770\u001b[0m  0.0093  0.6371\n",
      "      6                     0.5944        \u001b[32m0.6647\u001b[0m                     \u001b[35m0.5970\u001b[0m        0.6842  0.0090  0.6377\n",
      "      7                     0.5911        0.6652                     0.5840        0.6811  0.0085  0.6373\n",
      "      8                     \u001b[36m0.6014\u001b[0m        \u001b[32m0.6557\u001b[0m                     0.5784        0.6907  0.0080  0.6378\n",
      "      9                     \u001b[36m0.6168\u001b[0m        0.6564                     0.5746        0.6923  0.0075  0.6378\n",
      "     10                     \u001b[36m0.6238\u001b[0m        \u001b[32m0.6504\u001b[0m                     0.5746        0.6965  0.0069  0.6374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11                     0.6136        0.6509                     0.5914        0.6873  0.0063  0.6378\n",
      "     12                     \u001b[36m0.6322\u001b[0m        \u001b[32m0.6423\u001b[0m                     \u001b[35m0.6007\u001b[0m        0.6825  0.0057  0.6368\n",
      "     13                     \u001b[36m0.6327\u001b[0m        \u001b[32m0.6389\u001b[0m                     0.5746        0.6987  0.0050  0.6381\n",
      "     14                     0.6308        \u001b[32m0.6369\u001b[0m                     0.5672        0.7017  0.0043  0.6378\n",
      "     15                     \u001b[36m0.6336\u001b[0m        0.6407                     0.5634        0.6944  0.0037  0.6376\n",
      "     16                     \u001b[36m0.6388\u001b[0m        \u001b[32m0.6315\u001b[0m                     0.5504        0.7003  0.0031  0.6379\n",
      "     17                     \u001b[36m0.6598\u001b[0m        \u001b[32m0.6202\u001b[0m                     0.5896        0.6888  0.0025  0.6376\n",
      "     18                     0.6486        0.6246                     0.5690        0.6958  0.0020  0.6372\n",
      "     19                     0.6589        \u001b[32m0.6121\u001b[0m                     0.5597        0.7005  0.0015  0.6373\n",
      "     20                     0.6570        0.6151                     0.5746        0.6962  0.0010  0.6377\n",
      "     21                     0.6598        \u001b[32m0.6118\u001b[0m                     0.5765        0.6959  0.0007  0.6375\n",
      "     22                     \u001b[36m0.6664\u001b[0m        \u001b[32m0.6081\u001b[0m                     0.5821        0.6971  0.0004  0.6373\n",
      "     23                     \u001b[36m0.6696\u001b[0m        \u001b[32m0.6028\u001b[0m                     0.5784        0.6984  0.0002  0.6391\n",
      "     24                     0.6682        0.6069                     0.5746        0.6991  0.0000  0.6371\n",
      "     25                     \u001b[36m0.6715\u001b[0m        \u001b[32m0.6018\u001b[0m                     0.5690        0.6991  0.0000  0.6371\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5313\u001b[0m        \u001b[32m0.7632\u001b[0m                     \u001b[35m0.5522\u001b[0m        \u001b[31m0.7059\u001b[0m  0.0100  0.6346\n",
      "      2                     \u001b[36m0.5612\u001b[0m        \u001b[32m0.6956\u001b[0m                     0.5093        \u001b[31m0.6940\u001b[0m  0.0100  0.6382\n",
      "      3                     \u001b[36m0.5738\u001b[0m        \u001b[32m0.6834\u001b[0m                     0.5410        0.6993  0.0098  0.6366\n",
      "      4                     \u001b[36m0.5902\u001b[0m        \u001b[32m0.6710\u001b[0m                     \u001b[35m0.5746\u001b[0m        \u001b[31m0.6932\u001b[0m  0.0096  0.6404\n",
      "      5                     \u001b[36m0.6107\u001b[0m        \u001b[32m0.6659\u001b[0m                     0.5616        \u001b[31m0.6920\u001b[0m  0.0093  0.6473\n",
      "      6                     \u001b[36m0.6131\u001b[0m        \u001b[32m0.6569\u001b[0m                     0.5578        0.6977  0.0090  0.6423\n",
      "      7                     0.6051        0.6636                     0.5690        \u001b[31m0.6838\u001b[0m  0.0085  0.6443\n",
      "      8                     0.6061        0.6578                     0.5672        0.6944  0.0080  0.6432\n",
      "      9                     \u001b[36m0.6206\u001b[0m        \u001b[32m0.6514\u001b[0m                     0.5597        0.7004  0.0075  0.6423\n",
      "     10                     \u001b[36m0.6280\u001b[0m        \u001b[32m0.6469\u001b[0m                     0.5354        0.7020  0.0069  0.6443\n",
      "     11                     0.6164        \u001b[32m0.6439\u001b[0m                     0.5690        0.6918  0.0063  0.6443\n",
      "     12                     \u001b[36m0.6313\u001b[0m        \u001b[32m0.6387\u001b[0m                     0.5653        0.7008  0.0057  0.6433\n",
      "     13                     \u001b[36m0.6322\u001b[0m        0.6415                     0.5690        0.6999  0.0050  0.6423\n",
      "     14                     \u001b[36m0.6505\u001b[0m        \u001b[32m0.6292\u001b[0m                     0.5578        0.7029  0.0043  0.6423\n",
      "     15                     0.6360        \u001b[32m0.6284\u001b[0m                     0.5410        0.6987  0.0037  0.6438\n",
      "     16                     0.6458        \u001b[32m0.6254\u001b[0m                     0.5466        0.7039  0.0031  0.6424\n",
      "     17                     \u001b[36m0.6537\u001b[0m        \u001b[32m0.6228\u001b[0m                     0.5429        0.7074  0.0025  0.6404\n",
      "     18                     \u001b[36m0.6598\u001b[0m        \u001b[32m0.6122\u001b[0m                     0.5653        0.7040  0.0020  0.6385\n",
      "     19                     0.6551        0.6156                     0.5560        0.7061  0.0015  0.6381\n",
      "     20                     0.6547        0.6144                     0.5504        0.7092  0.0010  0.6364\n",
      "     21                     0.6593        \u001b[32m0.6120\u001b[0m                     0.5541        0.7095  0.0007  0.6385\n",
      "     22                     \u001b[36m0.6668\u001b[0m        \u001b[32m0.6080\u001b[0m                     0.5616        0.7093  0.0004  0.6376\n",
      "     23                     0.6640        0.6087                     0.5616        0.7096  0.0002  0.6366\n",
      "     24                     \u001b[36m0.6766\u001b[0m        \u001b[32m0.6071\u001b[0m                     0.5634        0.7098  0.0000  0.6368\n",
      "     25                     0.6636        0.6104                     0.5616        0.7104  0.0000  0.6387\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5229\u001b[0m        \u001b[32m0.7675\u001b[0m                     \u001b[35m0.5224\u001b[0m        \u001b[31m0.6957\u001b[0m  0.0100  0.6351\n",
      "      2                     \u001b[36m0.5542\u001b[0m        \u001b[32m0.6935\u001b[0m                     \u001b[35m0.5653\u001b[0m        \u001b[31m0.6807\u001b[0m  0.0100  0.6370\n",
      "      3                     \u001b[36m0.5846\u001b[0m        \u001b[32m0.6727\u001b[0m                     \u001b[35m0.5933\u001b[0m        \u001b[31m0.6804\u001b[0m  0.0098  0.6366\n",
      "      4                     \u001b[36m0.5953\u001b[0m        \u001b[32m0.6640\u001b[0m                     0.5597        0.6966  0.0096  0.6364\n",
      "      5                     \u001b[36m0.5995\u001b[0m        \u001b[32m0.6627\u001b[0m                     0.5690        0.6935  0.0093  0.6361\n",
      "      6                     \u001b[36m0.6121\u001b[0m        \u001b[32m0.6596\u001b[0m                     0.5560        0.7039  0.0090  0.6363\n",
      "      7                     \u001b[36m0.6173\u001b[0m        \u001b[32m0.6504\u001b[0m                     0.5728        0.6969  0.0085  0.6371\n",
      "      8                     \u001b[36m0.6215\u001b[0m        \u001b[32m0.6492\u001b[0m                     0.5709        0.7024  0.0080  0.6368\n",
      "      9                     0.6126        0.6533                     0.5373        0.7069  0.0075  0.6363\n",
      "     10                     0.6131        \u001b[32m0.6439\u001b[0m                     0.5840        0.6986  0.0069  0.6363\n",
      "     11                     \u001b[36m0.6393\u001b[0m        \u001b[32m0.6404\u001b[0m                     0.5653        0.7114  0.0063  0.6371\n",
      "     12                     \u001b[36m0.6444\u001b[0m        \u001b[32m0.6352\u001b[0m                     0.5560        0.7178  0.0057  0.6369\n",
      "     13                     0.6341        \u001b[32m0.6314\u001b[0m                     0.5504        0.7157  0.0050  0.6385\n",
      "     14                     0.6397        \u001b[32m0.6275\u001b[0m                     0.5597        0.7257  0.0043  0.6362\n",
      "     15                     \u001b[36m0.6481\u001b[0m        \u001b[32m0.6251\u001b[0m                     0.5522        0.7149  0.0037  0.6353\n",
      "     16                     \u001b[36m0.6636\u001b[0m        \u001b[32m0.6215\u001b[0m                     0.5802        0.7132  0.0031  0.6364\n",
      "     17                     \u001b[36m0.6678\u001b[0m        \u001b[32m0.6148\u001b[0m                     0.5746        0.7211  0.0025  0.6370\n",
      "     18                     \u001b[36m0.6710\u001b[0m        \u001b[32m0.6062\u001b[0m                     0.5560        0.7217  0.0020  0.6376\n",
      "     19                     \u001b[36m0.6748\u001b[0m        \u001b[32m0.6024\u001b[0m                     0.5578        0.7268  0.0015  0.6375\n",
      "     20                     0.6715        \u001b[32m0.5998\u001b[0m                     0.5560        0.7242  0.0010  0.6370\n",
      "     21                     \u001b[36m0.6776\u001b[0m        \u001b[32m0.5993\u001b[0m                     0.5690        0.7266  0.0007  0.6369\n",
      "     22                     \u001b[36m0.6804\u001b[0m        0.6008                     0.5634        0.7300  0.0004  0.6371\n",
      "     23                     0.6780        \u001b[32m0.5949\u001b[0m                     0.5672        0.7312  0.0002  0.6371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24                     0.6659        0.5989                     0.5672        0.7314  0.0000  0.6364\n",
      "     25                     0.6799        0.5996                     0.5616        0.7312  0.0000  0.6670\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5098\u001b[0m        \u001b[32m0.7735\u001b[0m                     \u001b[35m0.5485\u001b[0m        \u001b[31m0.6868\u001b[0m  0.0100  0.6351\n",
      "      2                     \u001b[36m0.5500\u001b[0m        \u001b[32m0.7098\u001b[0m                     0.5354        0.6875  0.0100  0.6373\n",
      "      3                     \u001b[36m0.5617\u001b[0m        \u001b[32m0.6852\u001b[0m                     \u001b[35m0.5560\u001b[0m        \u001b[31m0.6861\u001b[0m  0.0098  0.6597\n",
      "      4                     \u001b[36m0.5799\u001b[0m        \u001b[32m0.6817\u001b[0m                     \u001b[35m0.5933\u001b[0m        \u001b[31m0.6650\u001b[0m  0.0096  0.6375\n",
      "      5                     \u001b[36m0.5897\u001b[0m        \u001b[32m0.6728\u001b[0m                     \u001b[35m0.5989\u001b[0m        0.6704  0.0093  0.6375\n",
      "      6                     \u001b[36m0.5986\u001b[0m        \u001b[32m0.6624\u001b[0m                     0.5802        0.6771  0.0090  0.6379\n",
      "      7                     \u001b[36m0.6126\u001b[0m        \u001b[32m0.6619\u001b[0m                     \u001b[35m0.6082\u001b[0m        0.6812  0.0085  0.6375\n",
      "      8                     0.6093        0.6627                     0.5616        0.6877  0.0080  0.6384\n",
      "      9                     0.6107        \u001b[32m0.6588\u001b[0m                     0.5746        0.6827  0.0075  0.6367\n",
      "     10                     \u001b[36m0.6248\u001b[0m        \u001b[32m0.6509\u001b[0m                     0.5672        0.6892  0.0069  0.6376\n",
      "     11                     \u001b[36m0.6280\u001b[0m        0.6510                     0.5560        0.7011  0.0063  0.6367\n",
      "     12                     \u001b[36m0.6290\u001b[0m        \u001b[32m0.6497\u001b[0m                     0.5280        0.6944  0.0057  0.6373\n",
      "     13                     \u001b[36m0.6308\u001b[0m        \u001b[32m0.6418\u001b[0m                     0.5765        0.6882  0.0050  0.6377\n",
      "     14                     0.6308        \u001b[32m0.6412\u001b[0m                     0.5466        0.6881  0.0043  0.6371\n",
      "     15                     \u001b[36m0.6486\u001b[0m        \u001b[32m0.6319\u001b[0m                     0.5112        0.7667  0.0037  0.6371\n",
      "     16                     0.6421        0.6334                     0.5466        0.7058  0.0031  0.6374\n",
      "     17                     0.6458        \u001b[32m0.6287\u001b[0m                     0.5560        0.6938  0.0025  0.6359\n",
      "     18                     \u001b[36m0.6607\u001b[0m        \u001b[32m0.6283\u001b[0m                     0.5485        0.7026  0.0020  0.6368\n",
      "     19                     0.6551        \u001b[32m0.6221\u001b[0m                     0.5728        0.6881  0.0015  0.6376\n",
      "     20                     0.6435        0.6232                     0.5485        0.6937  0.0010  0.6385\n",
      "     21                     0.6575        \u001b[32m0.6187\u001b[0m                     0.5541        0.6946  0.0007  0.6370\n",
      "     22                     0.6519        \u001b[32m0.6180\u001b[0m                     0.5690        0.6918  0.0004  0.6373\n",
      "     23                     0.6584        \u001b[32m0.6180\u001b[0m                     0.5616        0.6936  0.0002  0.6382\n",
      "     24                     0.6528        0.6229                     0.5672        0.6928  0.0000  0.6375\n",
      "     25                     0.6575        0.6204                     0.5784        0.6921  0.0000  0.6371\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5280\u001b[0m        \u001b[32m0.7589\u001b[0m                     \u001b[35m0.5653\u001b[0m        \u001b[31m0.6800\u001b[0m  0.0100  0.6338\n",
      "      2                     \u001b[36m0.5467\u001b[0m        \u001b[32m0.7026\u001b[0m                     0.5466        0.6838  0.0100  0.6366\n",
      "      3                     \u001b[36m0.5593\u001b[0m        \u001b[32m0.6781\u001b[0m                     \u001b[35m0.5914\u001b[0m        \u001b[31m0.6737\u001b[0m  0.0098  0.6366\n",
      "      4                     \u001b[36m0.5916\u001b[0m        \u001b[32m0.6724\u001b[0m                     0.5224        0.6914  0.0096  0.6373\n",
      "      5                     \u001b[36m0.6089\u001b[0m        \u001b[32m0.6704\u001b[0m                     \u001b[35m0.5970\u001b[0m        \u001b[31m0.6681\u001b[0m  0.0093  0.6366\n",
      "      6                     \u001b[36m0.6117\u001b[0m        \u001b[32m0.6645\u001b[0m                     0.5448        0.6861  0.0090  0.6370\n",
      "      7                     0.5958        \u001b[32m0.6615\u001b[0m                     0.5709        0.6869  0.0085  0.6383\n",
      "      8                     0.6061        \u001b[32m0.6565\u001b[0m                     0.5522        0.6899  0.0080  0.6384\n",
      "      9                     \u001b[36m0.6280\u001b[0m        \u001b[32m0.6548\u001b[0m                     0.5765        0.6848  0.0075  0.6380\n",
      "     10                     \u001b[36m0.6290\u001b[0m        \u001b[32m0.6501\u001b[0m                     0.5765        0.6862  0.0069  0.6367\n",
      "     11                     0.6243        0.6525                     0.5914        0.6891  0.0063  0.6370\n",
      "     12                     0.6276        \u001b[32m0.6453\u001b[0m                     0.5205        0.7089  0.0057  0.6379\n",
      "     13                     \u001b[36m0.6435\u001b[0m        \u001b[32m0.6336\u001b[0m                     0.5933        0.6813  0.0050  0.6374\n",
      "     14                     \u001b[36m0.6481\u001b[0m        0.6340                     0.5672        0.6954  0.0043  0.6381\n",
      "     15                     0.6477        \u001b[32m0.6279\u001b[0m                     0.5634        0.6983  0.0037  0.6381\n",
      "     16                     0.6397        0.6307                     0.5541        0.7000  0.0031  0.6368\n",
      "     17                     \u001b[36m0.6678\u001b[0m        \u001b[32m0.6199\u001b[0m                     0.5728        0.6934  0.0025  0.6377\n",
      "     18                     0.6547        \u001b[32m0.6157\u001b[0m                     0.5877        0.6963  0.0020  0.6384\n",
      "     19                     0.6607        0.6163                     0.5709        0.6949  0.0015  0.6379\n",
      "     20                     \u001b[36m0.6692\u001b[0m        \u001b[32m0.6090\u001b[0m                     0.5840        0.6951  0.0010  0.6367\n",
      "     21                     \u001b[36m0.6776\u001b[0m        0.6123                     0.5746        0.6950  0.0007  0.6375\n",
      "     22                     0.6696        \u001b[32m0.6078\u001b[0m                     0.5765        0.6944  0.0004  0.6373\n",
      "     23                     0.6776        \u001b[32m0.6058\u001b[0m                     0.5728        0.6948  0.0002  0.6380\n",
      "     24                     \u001b[36m0.6921\u001b[0m        \u001b[32m0.5963\u001b[0m                     0.5746        0.6952  0.0000  0.6376\n",
      "     25                     0.6743        0.6066                     0.5802        0.6955  0.0000  0.6375\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5341\u001b[0m        \u001b[32m0.7517\u001b[0m                     \u001b[35m0.5522\u001b[0m        \u001b[31m0.7036\u001b[0m  0.0100  0.6344\n",
      "      2                     \u001b[36m0.5528\u001b[0m        \u001b[32m0.7046\u001b[0m                     0.5485        \u001b[31m0.6893\u001b[0m  0.0100  0.6378\n",
      "      3                     \u001b[36m0.5916\u001b[0m        \u001b[32m0.6767\u001b[0m                     \u001b[35m0.5578\u001b[0m        \u001b[31m0.6805\u001b[0m  0.0098  0.6392\n",
      "      4                     \u001b[36m0.6051\u001b[0m        \u001b[32m0.6663\u001b[0m                     \u001b[35m0.5765\u001b[0m        \u001b[31m0.6800\u001b[0m  0.0096  0.6378\n",
      "      5                     \u001b[36m0.6168\u001b[0m        \u001b[32m0.6600\u001b[0m                     0.5746        \u001b[31m0.6778\u001b[0m  0.0093  0.6368\n",
      "      6                     \u001b[36m0.6210\u001b[0m        \u001b[32m0.6578\u001b[0m                     0.5541        0.6945  0.0090  0.6371\n",
      "      7                     0.6145        \u001b[32m0.6536\u001b[0m                     0.5410        0.7062  0.0085  0.6378\n",
      "      8                     0.6187        \u001b[32m0.6527\u001b[0m                     0.5634        0.7024  0.0080  0.6367\n",
      "      9                     \u001b[36m0.6318\u001b[0m        \u001b[32m0.6482\u001b[0m                     0.5597        0.7013  0.0075  0.6395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     0.6276        0.6507                     0.5765        0.6948  0.0069  0.6376\n",
      "     11                     0.6285        \u001b[32m0.6459\u001b[0m                     0.5541        0.7094  0.0063  0.6372\n",
      "     12                     \u001b[36m0.6411\u001b[0m        \u001b[32m0.6354\u001b[0m                     0.5709        0.7038  0.0057  0.6376\n",
      "     13                     0.6360        0.6388                     0.5672        0.6979  0.0050  0.6375\n",
      "     14                     \u001b[36m0.6421\u001b[0m        \u001b[32m0.6262\u001b[0m                     0.5597        0.7181  0.0043  0.6366\n",
      "     15                     \u001b[36m0.6523\u001b[0m        0.6294                     0.5728        0.7120  0.0037  0.6359\n",
      "     16                     \u001b[36m0.6537\u001b[0m        0.6292                     0.5522        0.7092  0.0031  0.6371\n",
      "     17                     \u001b[36m0.6565\u001b[0m        \u001b[32m0.6233\u001b[0m                     0.5485        0.7125  0.0025  0.6361\n",
      "     18                     \u001b[36m0.6612\u001b[0m        \u001b[32m0.6190\u001b[0m                     0.5354        0.7116  0.0020  0.6364\n",
      "     19                     \u001b[36m0.6710\u001b[0m        \u001b[32m0.6106\u001b[0m                     0.5392        0.7189  0.0015  0.6364\n",
      "     20                     0.6687        0.6119                     0.5336        0.7174  0.0010  0.6359\n",
      "     21                     \u001b[36m0.6785\u001b[0m        \u001b[32m0.6068\u001b[0m                     0.5429        0.7251  0.0007  0.6546\n",
      "     22                     \u001b[36m0.6916\u001b[0m        \u001b[32m0.6019\u001b[0m                     0.5392        0.7221  0.0004  0.7281\n",
      "     23                     0.6841        \u001b[32m0.6005\u001b[0m                     0.5317        0.7216  0.0002  0.6822\n",
      "     24                     0.6804        \u001b[32m0.5985\u001b[0m                     0.5392        0.7217  0.0000  0.6377\n",
      "     25                     0.6636        0.6124                     0.5373        0.7218  0.0000  0.6565\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5150\u001b[0m        \u001b[32m0.7611\u001b[0m                     \u001b[35m0.5187\u001b[0m        \u001b[31m0.7262\u001b[0m  0.0100  0.7331\n",
      "      2                     \u001b[36m0.5790\u001b[0m        \u001b[32m0.6869\u001b[0m                     \u001b[35m0.5522\u001b[0m        \u001b[31m0.7173\u001b[0m  0.0100  0.6627\n",
      "      3                     \u001b[36m0.6000\u001b[0m        \u001b[32m0.6628\u001b[0m                     \u001b[35m0.5690\u001b[0m        \u001b[31m0.6918\u001b[0m  0.0098  0.6402\n",
      "      4                     \u001b[36m0.6089\u001b[0m        \u001b[32m0.6586\u001b[0m                     0.5616        0.7030  0.0096  0.6358\n",
      "      5                     \u001b[36m0.6126\u001b[0m        \u001b[32m0.6578\u001b[0m                     0.5634        0.7102  0.0093  0.6350\n",
      "      6                     \u001b[36m0.6196\u001b[0m        \u001b[32m0.6526\u001b[0m                     \u001b[35m0.5728\u001b[0m        0.7242  0.0090  0.6396\n",
      "      7                     0.6136        0.6582                     0.5634        0.7367  0.0085  0.6363\n",
      "      8                     \u001b[36m0.6201\u001b[0m        0.6536                     0.5373        0.7312  0.0080  0.6359\n",
      "      9                     \u001b[36m0.6369\u001b[0m        \u001b[32m0.6433\u001b[0m                     0.5709        0.7165  0.0075  0.6383\n",
      "     10                     \u001b[36m0.6388\u001b[0m        \u001b[32m0.6359\u001b[0m                     0.5578        0.7334  0.0069  0.6361\n",
      "     11                     \u001b[36m0.6421\u001b[0m        0.6372                     0.5634        0.7309  0.0063  0.6372\n",
      "     12                     \u001b[36m0.6477\u001b[0m        \u001b[32m0.6247\u001b[0m                     0.5578        0.7377  0.0057  0.6360\n",
      "     13                     \u001b[36m0.6519\u001b[0m        0.6262                     0.5653        0.7338  0.0050  0.6383\n",
      "     14                     0.6458        0.6280                     0.5672        0.7427  0.0043  0.6351\n",
      "     15                     \u001b[36m0.6547\u001b[0m        \u001b[32m0.6211\u001b[0m                     0.5522        0.7319  0.0037  0.6364\n",
      "     16                     \u001b[36m0.6636\u001b[0m        \u001b[32m0.6134\u001b[0m                     0.5448        0.7400  0.0031  0.6373\n",
      "     17                     \u001b[36m0.6771\u001b[0m        \u001b[32m0.6107\u001b[0m                     0.5597        0.7344  0.0025  0.6353\n",
      "     18                     0.6701        \u001b[32m0.6100\u001b[0m                     0.5504        0.7341  0.0020  0.6342\n",
      "     19                     0.6593        0.6118                     0.5634        0.7364  0.0015  0.6367\n",
      "     20                     0.6701        \u001b[32m0.5984\u001b[0m                     0.5560        0.7370  0.0010  0.6384\n",
      "     21                     0.6720        \u001b[32m0.5978\u001b[0m                     0.5672        0.7384  0.0007  0.6353\n",
      "     22                     \u001b[36m0.6855\u001b[0m        \u001b[32m0.5949\u001b[0m                     0.5634        0.7405  0.0004  0.6369\n",
      "     23                     0.6724        0.6018                     0.5560        0.7407  0.0002  0.6379\n",
      "     24                     0.6818        \u001b[32m0.5911\u001b[0m                     0.5578        0.7419  0.0000  0.6369\n",
      "     25                     0.6715        0.5985                     0.5578        0.7425  0.0000  0.6362\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5215\u001b[0m        \u001b[32m0.7616\u001b[0m                     \u001b[35m0.5541\u001b[0m        \u001b[31m0.6989\u001b[0m  0.0100  0.6365\n",
      "      2                     \u001b[36m0.5425\u001b[0m        \u001b[32m0.6955\u001b[0m                     \u001b[35m0.5709\u001b[0m        \u001b[31m0.6779\u001b[0m  0.0100  0.6381\n",
      "      3                     \u001b[36m0.5780\u001b[0m        \u001b[32m0.6766\u001b[0m                     \u001b[35m0.6138\u001b[0m        \u001b[31m0.6673\u001b[0m  0.0098  0.6368\n",
      "      4                     \u001b[36m0.5818\u001b[0m        \u001b[32m0.6712\u001b[0m                     0.5989        0.6739  0.0096  0.6379\n",
      "      5                     \u001b[36m0.6061\u001b[0m        \u001b[32m0.6619\u001b[0m                     0.5560        0.6804  0.0093  0.6383\n",
      "      6                     0.6056        \u001b[32m0.6572\u001b[0m                     0.5690        0.6809  0.0090  0.6367\n",
      "      7                     0.6023        0.6591                     0.5858        0.6809  0.0085  0.6382\n",
      "      8                     \u001b[36m0.6098\u001b[0m        \u001b[32m0.6528\u001b[0m                     0.5728        0.6892  0.0080  0.6388\n",
      "      9                     \u001b[36m0.6332\u001b[0m        \u001b[32m0.6508\u001b[0m                     0.5840        0.7009  0.0075  0.6372\n",
      "     10                     0.6271        \u001b[32m0.6431\u001b[0m                     0.5578        0.6968  0.0069  0.6351\n",
      "     11                     0.6313        \u001b[32m0.6412\u001b[0m                     0.5336        0.6992  0.0063  0.6366\n",
      "     12                     \u001b[36m0.6374\u001b[0m        \u001b[32m0.6270\u001b[0m                     0.5728        0.6877  0.0057  0.6382\n",
      "     13                     \u001b[36m0.6439\u001b[0m        \u001b[32m0.6211\u001b[0m                     0.5914        0.6922  0.0050  0.6379\n",
      "     14                     \u001b[36m0.6486\u001b[0m        0.6225                     0.5672        0.6984  0.0043  0.6358\n",
      "     15                     0.6453        \u001b[32m0.6196\u001b[0m                     0.5709        0.7042  0.0037  0.6381\n",
      "     16                     \u001b[36m0.6654\u001b[0m        \u001b[32m0.6106\u001b[0m                     0.5728        0.7003  0.0031  0.6364\n",
      "     17                     0.6579        0.6177                     0.5672        0.6965  0.0025  0.6377\n",
      "     18                     \u001b[36m0.6710\u001b[0m        \u001b[32m0.6094\u001b[0m                     0.5840        0.6985  0.0020  0.6372\n",
      "     19                     \u001b[36m0.6790\u001b[0m        \u001b[32m0.5990\u001b[0m                     0.5802        0.6976  0.0015  0.6397\n",
      "     20                     \u001b[36m0.6822\u001b[0m        \u001b[32m0.5937\u001b[0m                     0.5728        0.6931  0.0010  0.6366\n",
      "     21                     0.6706        0.5939                     0.5746        0.6948  0.0007  0.6378\n",
      "     22                     0.6766        \u001b[32m0.5927\u001b[0m                     0.5802        0.6958  0.0004  0.6381\n",
      "     23                     \u001b[36m0.6846\u001b[0m        \u001b[32m0.5902\u001b[0m                     0.5746        0.6966  0.0002  0.6373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24                     \u001b[36m0.6879\u001b[0m        \u001b[32m0.5888\u001b[0m                     0.5728        0.6970  0.0000  0.6367\n",
      "     25                     \u001b[36m0.6925\u001b[0m        \u001b[32m0.5882\u001b[0m                     0.5765        0.6975  0.0000  0.6367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7631\u001b[0m        \u001b[32m0.5114\u001b[0m                     \u001b[35m0.8060\u001b[0m        \u001b[31m0.4618\u001b[0m  0.0100  0.6381\n",
      "      2                     \u001b[36m0.8215\u001b[0m        \u001b[32m0.4005\u001b[0m                     \u001b[35m0.8209\u001b[0m        \u001b[31m0.4152\u001b[0m  0.0100  0.6406\n",
      "      3                     \u001b[36m0.8369\u001b[0m        \u001b[32m0.3748\u001b[0m                     \u001b[35m0.8507\u001b[0m        \u001b[31m0.3608\u001b[0m  0.0098  0.6365\n",
      "      4                     \u001b[36m0.8523\u001b[0m        \u001b[32m0.3543\u001b[0m                     \u001b[35m0.8638\u001b[0m        \u001b[31m0.3395\u001b[0m  0.0096  0.6380\n",
      "      5                     \u001b[36m0.8650\u001b[0m        \u001b[32m0.3325\u001b[0m                     \u001b[35m0.8731\u001b[0m        \u001b[31m0.3209\u001b[0m  0.0093  0.6357\n",
      "      6                     \u001b[36m0.8673\u001b[0m        \u001b[32m0.3297\u001b[0m                     0.8675        0.3620  0.0090  0.6353\n",
      "      7                     \u001b[36m0.8678\u001b[0m        \u001b[32m0.3240\u001b[0m                     0.8675        0.3228  0.0085  0.6353\n",
      "      8                     \u001b[36m0.8748\u001b[0m        \u001b[32m0.3156\u001b[0m                     0.8713        0.3342  0.0080  0.6374\n",
      "      9                     \u001b[36m0.8780\u001b[0m        \u001b[32m0.3065\u001b[0m                     \u001b[35m0.8806\u001b[0m        0.3248  0.0075  0.6356\n",
      "     10                     \u001b[36m0.8897\u001b[0m        \u001b[32m0.2936\u001b[0m                     0.8750        \u001b[31m0.3099\u001b[0m  0.0069  0.6368\n",
      "     11                     0.8822        \u001b[32m0.2934\u001b[0m                     0.8787        0.3122  0.0063  0.6364\n",
      "     12                     0.8827        0.2936                     0.8769        0.3164  0.0057  0.6361\n",
      "     13                     \u001b[36m0.8916\u001b[0m        \u001b[32m0.2834\u001b[0m                     0.8694        \u001b[31m0.3093\u001b[0m  0.0050  0.6373\n",
      "     14                     0.8846        0.2846                     0.8787        \u001b[31m0.3044\u001b[0m  0.0043  0.6361\n",
      "     15                     0.8822        \u001b[32m0.2746\u001b[0m                     0.8657        \u001b[31m0.3018\u001b[0m  0.0037  0.6371\n",
      "     16                     \u001b[36m0.8986\u001b[0m        \u001b[32m0.2697\u001b[0m                     \u001b[35m0.8825\u001b[0m        0.3099  0.0031  0.6353\n",
      "     17                     0.8972        \u001b[32m0.2648\u001b[0m                     0.8750        0.3090  0.0025  0.6363\n",
      "     18                     0.8916        \u001b[32m0.2635\u001b[0m                     0.8713        \u001b[31m0.2996\u001b[0m  0.0020  0.6399\n",
      "     19                     \u001b[36m0.9028\u001b[0m        \u001b[32m0.2533\u001b[0m                     \u001b[35m0.8843\u001b[0m        \u001b[31m0.2972\u001b[0m  0.0015  0.6359\n",
      "     20                     0.9028        0.2571                     0.8750        0.3002  0.0010  0.6344\n",
      "     21                     0.9014        \u001b[32m0.2459\u001b[0m                     0.8769        0.3027  0.0007  0.6372\n",
      "     22                     \u001b[36m0.9168\u001b[0m        \u001b[32m0.2346\u001b[0m                     0.8769        0.3021  0.0004  0.6378\n",
      "     23                     0.9005        0.2455                     0.8769        0.3020  0.0002  0.6368\n",
      "     24                     0.9009        0.2471                     0.8750        0.3021  0.0000  0.6410\n",
      "     25                     0.9051        0.2514                     0.8769        0.3021  0.0000  0.6386\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7435\u001b[0m        \u001b[32m0.5015\u001b[0m                     \u001b[35m0.8694\u001b[0m        \u001b[31m0.3420\u001b[0m  0.0100  0.6353\n",
      "      2                     \u001b[36m0.8252\u001b[0m        \u001b[32m0.3988\u001b[0m                     \u001b[35m0.8862\u001b[0m        \u001b[31m0.3073\u001b[0m  0.0100  0.6364\n",
      "      3                     \u001b[36m0.8411\u001b[0m        \u001b[32m0.3583\u001b[0m                     \u001b[35m0.8918\u001b[0m        0.3335  0.0098  0.6386\n",
      "      4                     \u001b[36m0.8495\u001b[0m        \u001b[32m0.3537\u001b[0m                     0.8526        0.4128  0.0096  0.6381\n",
      "      5                     \u001b[36m0.8612\u001b[0m        \u001b[32m0.3372\u001b[0m                     0.8899        \u001b[31m0.2984\u001b[0m  0.0093  0.6374\n",
      "      6                     \u001b[36m0.8696\u001b[0m        \u001b[32m0.3324\u001b[0m                     0.8843        0.3080  0.0090  0.6372\n",
      "      7                     0.8682        \u001b[32m0.3223\u001b[0m                     0.8657        0.3644  0.0085  0.6391\n",
      "      8                     \u001b[36m0.8771\u001b[0m        \u001b[32m0.3213\u001b[0m                     \u001b[35m0.9049\u001b[0m        0.3088  0.0080  0.6370\n",
      "      9                     0.8752        \u001b[32m0.3053\u001b[0m                     0.8862        0.3361  0.0075  0.6365\n",
      "     10                     \u001b[36m0.8780\u001b[0m        0.3086                     0.8918        0.3430  0.0069  0.6392\n",
      "     11                     0.8743        \u001b[32m0.2993\u001b[0m                     0.8955        0.3152  0.0063  0.6379\n",
      "     12                     \u001b[36m0.8846\u001b[0m        \u001b[32m0.2809\u001b[0m                     0.9011        0.2998  0.0057  0.6377\n",
      "     13                     \u001b[36m0.8907\u001b[0m        \u001b[32m0.2686\u001b[0m                     0.8955        \u001b[31m0.2968\u001b[0m  0.0050  0.6368\n",
      "     14                     0.8869        0.2728                     0.9011        \u001b[31m0.2904\u001b[0m  0.0043  0.6393\n",
      "     15                     \u001b[36m0.8958\u001b[0m        \u001b[32m0.2503\u001b[0m                     0.8190        0.4548  0.0037  0.6384\n",
      "     16                     \u001b[36m0.9023\u001b[0m        0.2618                     0.8862        0.3120  0.0031  0.6381\n",
      "     17                     0.9019        \u001b[32m0.2497\u001b[0m                     0.9049        \u001b[31m0.2888\u001b[0m  0.0025  0.6404\n",
      "     18                     0.9009        \u001b[32m0.2388\u001b[0m                     0.8769        0.2955  0.0020  0.6373\n",
      "     19                     \u001b[36m0.9051\u001b[0m        0.2407                     0.8918        \u001b[31m0.2843\u001b[0m  0.0015  0.6362\n",
      "     20                     \u001b[36m0.9093\u001b[0m        \u001b[32m0.2349\u001b[0m                     0.8899        0.2873  0.0010  0.6373\n",
      "     21                     0.8991        0.2451                     0.8881        0.2896  0.0007  0.6396\n",
      "     22                     0.9037        0.2474                     0.8937        0.2869  0.0004  0.6367\n",
      "     23                     \u001b[36m0.9164\u001b[0m        \u001b[32m0.2282\u001b[0m                     0.8899        0.2847  0.0002  0.6377\n",
      "     24                     0.9136        \u001b[32m0.2246\u001b[0m                     0.8918        0.2846  0.0000  0.6393\n",
      "     25                     0.9028        0.2349                     0.8899        0.2847  0.0000  0.6383\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7491\u001b[0m        \u001b[32m0.5235\u001b[0m                     \u001b[35m0.8451\u001b[0m        \u001b[31m0.3425\u001b[0m  0.0100  0.6353\n",
      "      2                     \u001b[36m0.8220\u001b[0m        \u001b[32m0.3969\u001b[0m                     \u001b[35m0.8750\u001b[0m        \u001b[31m0.3095\u001b[0m  0.0100  0.6398\n",
      "      3                     \u001b[36m0.8388\u001b[0m        \u001b[32m0.3700\u001b[0m                     \u001b[35m0.8862\u001b[0m        \u001b[31m0.2969\u001b[0m  0.0098  0.6370\n",
      "      4                     \u001b[36m0.8528\u001b[0m        \u001b[32m0.3487\u001b[0m                     0.8769        0.3211  0.0096  0.6358\n",
      "      5                     \u001b[36m0.8650\u001b[0m        \u001b[32m0.3328\u001b[0m                     0.8582        0.3597  0.0093  0.6371\n",
      "      6                     \u001b[36m0.8706\u001b[0m        \u001b[32m0.3256\u001b[0m                     0.8806        0.3016  0.0090  0.6390\n",
      "      7                     \u001b[36m0.8766\u001b[0m        \u001b[32m0.2992\u001b[0m                     0.8862        \u001b[31m0.2782\u001b[0m  0.0085  0.6365\n",
      "      8                     \u001b[36m0.8888\u001b[0m        \u001b[32m0.2839\u001b[0m                     0.8731        0.3234  0.0080  0.6382\n",
      "      9                     0.8808        0.3063                     \u001b[35m0.8955\u001b[0m        0.2800  0.0075  0.6388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     0.8869        \u001b[32m0.2818\u001b[0m                     0.8843        0.2837  0.0069  0.6368\n",
      "     11                     0.8794        \u001b[32m0.2755\u001b[0m                     \u001b[35m0.8993\u001b[0m        \u001b[31m0.2769\u001b[0m  0.0063  0.6365\n",
      "     12                     \u001b[36m0.8925\u001b[0m        \u001b[32m0.2684\u001b[0m                     \u001b[35m0.9030\u001b[0m        \u001b[31m0.2527\u001b[0m  0.0057  0.6373\n",
      "     13                     0.8879        \u001b[32m0.2638\u001b[0m                     0.8918        0.2622  0.0050  0.6397\n",
      "     14                     \u001b[36m0.8995\u001b[0m        \u001b[32m0.2545\u001b[0m                     \u001b[35m0.9104\u001b[0m        \u001b[31m0.2350\u001b[0m  0.0043  0.6373\n",
      "     15                     \u001b[36m0.9065\u001b[0m        \u001b[32m0.2455\u001b[0m                     0.9030        0.2362  0.0037  0.6374\n",
      "     16                     \u001b[36m0.9079\u001b[0m        \u001b[32m0.2426\u001b[0m                     0.9030        0.2364  0.0031  0.6395\n",
      "     17                     0.9042        \u001b[32m0.2346\u001b[0m                     0.9030        0.2526  0.0025  0.6365\n",
      "     18                     0.9075        \u001b[32m0.2264\u001b[0m                     0.9011        0.2451  0.0020  0.6379\n",
      "     19                     \u001b[36m0.9112\u001b[0m        \u001b[32m0.2230\u001b[0m                     0.9011        0.2439  0.0015  0.6354\n",
      "     20                     \u001b[36m0.9121\u001b[0m        \u001b[32m0.2220\u001b[0m                     0.9067        0.2371  0.0010  0.6399\n",
      "     21                     \u001b[36m0.9206\u001b[0m        \u001b[32m0.2121\u001b[0m                     0.8974        0.2458  0.0007  0.6363\n",
      "     22                     0.9164        0.2123                     0.8993        0.2403  0.0004  0.6374\n",
      "     23                     \u001b[36m0.9210\u001b[0m        \u001b[32m0.2084\u001b[0m                     0.9049        0.2378  0.0002  0.6391\n",
      "     24                     0.9159        0.2113                     0.9049        0.2373  0.0000  0.6377\n",
      "     25                     \u001b[36m0.9243\u001b[0m        \u001b[32m0.2081\u001b[0m                     0.9067        0.2368  0.0000  0.6369\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7537\u001b[0m        \u001b[32m0.5241\u001b[0m                     \u001b[35m0.8321\u001b[0m        \u001b[31m0.3718\u001b[0m  0.0100  0.6376\n",
      "      2                     \u001b[36m0.8318\u001b[0m        \u001b[32m0.3960\u001b[0m                     \u001b[35m0.8470\u001b[0m        \u001b[31m0.3208\u001b[0m  0.0100  0.6370\n",
      "      3                     \u001b[36m0.8435\u001b[0m        \u001b[32m0.3660\u001b[0m                     \u001b[35m0.8750\u001b[0m        \u001b[31m0.3045\u001b[0m  0.0098  0.6374\n",
      "      4                     \u001b[36m0.8486\u001b[0m        \u001b[32m0.3593\u001b[0m                     \u001b[35m0.8881\u001b[0m        \u001b[31m0.2733\u001b[0m  0.0096  0.6354\n",
      "      5                     \u001b[36m0.8598\u001b[0m        \u001b[32m0.3371\u001b[0m                     \u001b[35m0.9011\u001b[0m        \u001b[31m0.2639\u001b[0m  0.0093  0.6396\n",
      "      6                     \u001b[36m0.8724\u001b[0m        \u001b[32m0.3244\u001b[0m                     0.8899        0.2846  0.0090  0.6364\n",
      "      7                     \u001b[36m0.8785\u001b[0m        \u001b[32m0.3107\u001b[0m                     0.8843        0.3131  0.0085  0.6362\n",
      "      8                     \u001b[36m0.8822\u001b[0m        \u001b[32m0.3103\u001b[0m                     0.8993        \u001b[31m0.2560\u001b[0m  0.0080  0.6389\n",
      "      9                     \u001b[36m0.8850\u001b[0m        \u001b[32m0.3006\u001b[0m                     0.8993        0.2598  0.0075  0.6376\n",
      "     10                     0.8813        0.3034                     \u001b[35m0.9086\u001b[0m        \u001b[31m0.2417\u001b[0m  0.0069  0.6369\n",
      "     11                     0.8850        \u001b[32m0.2899\u001b[0m                     0.8377        0.3918  0.0063  0.6361\n",
      "     12                     0.8799        0.2984                     0.8937        0.2931  0.0057  0.6395\n",
      "     13                     \u001b[36m0.8921\u001b[0m        \u001b[32m0.2827\u001b[0m                     0.9011        0.2538  0.0050  0.6365\n",
      "     14                     0.8888        \u001b[32m0.2771\u001b[0m                     0.9067        \u001b[31m0.2379\u001b[0m  0.0043  0.6368\n",
      "     15                     \u001b[36m0.8972\u001b[0m        \u001b[32m0.2642\u001b[0m                     0.8806        0.2911  0.0037  0.6398\n",
      "     16                     \u001b[36m0.9014\u001b[0m        \u001b[32m0.2565\u001b[0m                     \u001b[35m0.9104\u001b[0m        0.2590  0.0031  0.6369\n",
      "     17                     0.8977        0.2599                     0.8638        0.3226  0.0025  0.6375\n",
      "     18                     0.8935        \u001b[32m0.2546\u001b[0m                     0.9104        \u001b[31m0.2355\u001b[0m  0.0020  0.6363\n",
      "     19                     \u001b[36m0.9051\u001b[0m        \u001b[32m0.2477\u001b[0m                     0.9049        \u001b[31m0.2259\u001b[0m  0.0015  0.6388\n",
      "     20                     0.9023        \u001b[32m0.2459\u001b[0m                     0.9067        0.2377  0.0010  0.6369\n",
      "     21                     0.9037        0.2535                     0.9030        0.2462  0.0007  0.6370\n",
      "     22                     \u001b[36m0.9103\u001b[0m        \u001b[32m0.2325\u001b[0m                     0.9086        0.2337  0.0004  0.6388\n",
      "     23                     0.9084        0.2409                     0.9067        0.2347  0.0002  0.6363\n",
      "     24                     \u001b[36m0.9107\u001b[0m        0.2348                     0.9067        0.2340  0.0000  0.6366\n",
      "     25                     \u001b[36m0.9192\u001b[0m        \u001b[32m0.2295\u001b[0m                     0.9067        0.2333  0.0000  0.6382\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5052\u001b[0m                     \u001b[35m0.8563\u001b[0m        \u001b[31m0.3709\u001b[0m  0.0100  0.6343\n",
      "      2                     \u001b[36m0.8397\u001b[0m        \u001b[32m0.3711\u001b[0m                     0.8377        0.3978  0.0100  0.6373\n",
      "      3                     \u001b[36m0.8467\u001b[0m        \u001b[32m0.3510\u001b[0m                     0.8433        0.3757  0.0098  0.6360\n",
      "      4                     \u001b[36m0.8565\u001b[0m        \u001b[32m0.3365\u001b[0m                     \u001b[35m0.8601\u001b[0m        \u001b[31m0.3544\u001b[0m  0.0096  0.6383\n",
      "      5                     \u001b[36m0.8696\u001b[0m        \u001b[32m0.3155\u001b[0m                     0.8396        0.3853  0.0093  0.6354\n",
      "      6                     0.8636        0.3369                     \u001b[35m0.8657\u001b[0m        0.3558  0.0090  0.6363\n",
      "      7                     \u001b[36m0.8776\u001b[0m        \u001b[32m0.3063\u001b[0m                     \u001b[35m0.8769\u001b[0m        \u001b[31m0.3226\u001b[0m  0.0085  0.6383\n",
      "      8                     \u001b[36m0.8794\u001b[0m        \u001b[32m0.2964\u001b[0m                     \u001b[35m0.8825\u001b[0m        \u001b[31m0.3010\u001b[0m  0.0080  0.6360\n",
      "      9                     \u001b[36m0.8822\u001b[0m        \u001b[32m0.2948\u001b[0m                     0.8787        0.3162  0.0075  0.6373\n",
      "     10                     \u001b[36m0.8902\u001b[0m        \u001b[32m0.2812\u001b[0m                     \u001b[35m0.8862\u001b[0m        0.3054  0.0069  0.6363\n",
      "     11                     0.8832        0.2885                     0.8862        0.3199  0.0063  0.6412\n",
      "     12                     0.8897        \u001b[32m0.2772\u001b[0m                     0.8862        0.3028  0.0057  0.6370\n",
      "     13                     \u001b[36m0.8939\u001b[0m        \u001b[32m0.2639\u001b[0m                     \u001b[35m0.8881\u001b[0m        0.3074  0.0050  0.6374\n",
      "     14                     \u001b[36m0.8949\u001b[0m        0.2669                     0.8750        0.3228  0.0043  0.6385\n",
      "     15                     \u001b[36m0.8991\u001b[0m        \u001b[32m0.2506\u001b[0m                     0.8825        0.3048  0.0037  0.6375\n",
      "     16                     \u001b[36m0.8995\u001b[0m        0.2517                     \u001b[35m0.8937\u001b[0m        \u001b[31m0.2973\u001b[0m  0.0031  0.6373\n",
      "     17                     \u001b[36m0.9005\u001b[0m        \u001b[32m0.2471\u001b[0m                     0.8825        0.3026  0.0025  0.6413\n",
      "     18                     \u001b[36m0.9028\u001b[0m        \u001b[32m0.2446\u001b[0m                     0.8937        \u001b[31m0.2863\u001b[0m  0.0020  0.6403\n",
      "     19                     \u001b[36m0.9107\u001b[0m        \u001b[32m0.2353\u001b[0m                     0.8862        0.2870  0.0015  0.6372\n",
      "     20                     0.9079        0.2438                     0.8918        \u001b[31m0.2851\u001b[0m  0.0010  0.6373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21                     0.9000        0.2415                     0.8918        \u001b[31m0.2838\u001b[0m  0.0007  0.6383\n",
      "     22                     0.9093        \u001b[32m0.2327\u001b[0m                     0.8937        \u001b[31m0.2830\u001b[0m  0.0004  0.6373\n",
      "     23                     0.9103        \u001b[32m0.2273\u001b[0m                     0.8881        0.2834  0.0002  0.6370\n",
      "     24                     \u001b[36m0.9112\u001b[0m        0.2293                     0.8862        0.2839  0.0000  0.6377\n",
      "     25                     0.9079        0.2305                     0.8862        0.2840  0.0000  0.6363\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7818\u001b[0m        \u001b[32m0.4791\u001b[0m                     \u001b[35m0.8284\u001b[0m        \u001b[31m0.4216\u001b[0m  0.0100  0.6353\n",
      "      2                     \u001b[36m0.8514\u001b[0m        \u001b[32m0.3612\u001b[0m                     0.8116        \u001b[31m0.4177\u001b[0m  0.0100  0.6368\n",
      "      3                     \u001b[36m0.8519\u001b[0m        \u001b[32m0.3557\u001b[0m                     \u001b[35m0.8340\u001b[0m        0.4186  0.0098  0.6380\n",
      "      4                     \u001b[36m0.8584\u001b[0m        \u001b[32m0.3434\u001b[0m                     \u001b[35m0.8451\u001b[0m        \u001b[31m0.4141\u001b[0m  0.0096  0.6373\n",
      "      5                     \u001b[36m0.8682\u001b[0m        \u001b[32m0.3164\u001b[0m                     0.8284        0.4691  0.0093  0.6368\n",
      "      6                     \u001b[36m0.8715\u001b[0m        \u001b[32m0.3149\u001b[0m                     \u001b[35m0.8563\u001b[0m        0.4252  0.0090  0.6373\n",
      "      7                     \u001b[36m0.8757\u001b[0m        \u001b[32m0.3103\u001b[0m                     \u001b[35m0.8582\u001b[0m        0.4429  0.0085  0.6373\n",
      "      8                     \u001b[36m0.8785\u001b[0m        \u001b[32m0.3004\u001b[0m                     0.8190        0.4899  0.0080  0.6371\n",
      "      9                     \u001b[36m0.8822\u001b[0m        \u001b[32m0.2976\u001b[0m                     0.8284        0.4523  0.0075  0.6376\n",
      "     10                     \u001b[36m0.8930\u001b[0m        \u001b[32m0.2797\u001b[0m                     0.8321        0.4379  0.0069  0.6395\n",
      "     11                     \u001b[36m0.8967\u001b[0m        \u001b[32m0.2780\u001b[0m                     0.8302        0.4368  0.0063  0.6357\n",
      "     12                     0.8897        \u001b[32m0.2670\u001b[0m                     0.8396        0.4193  0.0057  0.6379\n",
      "     13                     0.8939        \u001b[32m0.2658\u001b[0m                     0.8377        0.4456  0.0050  0.6393\n",
      "     14                     \u001b[36m0.9000\u001b[0m        \u001b[32m0.2529\u001b[0m                     0.8507        0.4426  0.0043  0.6374\n",
      "     15                     \u001b[36m0.9009\u001b[0m        \u001b[32m0.2493\u001b[0m                     0.8433        0.4145  0.0037  0.6360\n",
      "     16                     \u001b[36m0.9131\u001b[0m        \u001b[32m0.2418\u001b[0m                     0.8433        0.4157  0.0031  0.6367\n",
      "     17                     0.9019        0.2564                     0.8545        \u001b[31m0.4068\u001b[0m  0.0025  0.6391\n",
      "     18                     0.9033        0.2485                     \u001b[35m0.8638\u001b[0m        \u001b[31m0.3943\u001b[0m  0.0020  0.6372\n",
      "     19                     0.9098        \u001b[32m0.2380\u001b[0m                     0.8545        0.3965  0.0015  0.6361\n",
      "     20                     \u001b[36m0.9136\u001b[0m        \u001b[32m0.2256\u001b[0m                     0.8545        \u001b[31m0.3916\u001b[0m  0.0010  0.6396\n",
      "     21                     0.9070        0.2333                     0.8582        0.3967  0.0007  0.6362\n",
      "     22                     0.9065        0.2264                     0.8545        0.3998  0.0004  0.6370\n",
      "     23                     0.9126        0.2266                     0.8582        0.3975  0.0002  0.6378\n",
      "     24                     0.9121        0.2304                     0.8563        0.3972  0.0000  0.6391\n",
      "     25                     0.9117        \u001b[32m0.2189\u001b[0m                     0.8582        0.3965  0.0000  0.6365\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7673\u001b[0m        \u001b[32m0.4945\u001b[0m                     \u001b[35m0.8321\u001b[0m        \u001b[31m0.4255\u001b[0m  0.0100  0.6353\n",
      "      2                     \u001b[36m0.8416\u001b[0m        \u001b[32m0.3784\u001b[0m                     \u001b[35m0.8619\u001b[0m        \u001b[31m0.3930\u001b[0m  0.0100  0.6384\n",
      "      3                     \u001b[36m0.8565\u001b[0m        \u001b[32m0.3433\u001b[0m                     0.8619        \u001b[31m0.3744\u001b[0m  0.0098  0.6362\n",
      "      4                     \u001b[36m0.8598\u001b[0m        \u001b[32m0.3336\u001b[0m                     0.8377        0.4003  0.0096  0.6358\n",
      "      5                     \u001b[36m0.8636\u001b[0m        \u001b[32m0.3183\u001b[0m                     0.8507        0.4014  0.0093  0.6383\n",
      "      6                     \u001b[36m0.8748\u001b[0m        \u001b[32m0.3002\u001b[0m                     0.8601        0.3810  0.0090  0.6362\n",
      "      7                     0.8692        0.3079                     \u001b[35m0.8750\u001b[0m        \u001b[31m0.3649\u001b[0m  0.0085  0.6372\n",
      "      8                     \u001b[36m0.8808\u001b[0m        \u001b[32m0.2936\u001b[0m                     \u001b[35m0.8769\u001b[0m        \u001b[31m0.3539\u001b[0m  0.0080  0.6361\n",
      "      9                     \u001b[36m0.8822\u001b[0m        \u001b[32m0.2906\u001b[0m                     0.8657        0.4027  0.0075  0.6392\n",
      "     10                     \u001b[36m0.8911\u001b[0m        \u001b[32m0.2652\u001b[0m                     0.8713        0.4123  0.0069  0.6364\n",
      "     11                     0.8902        0.2792                     \u001b[35m0.8862\u001b[0m        0.3606  0.0063  0.6377\n",
      "     12                     0.8911        0.2751                     0.8825        \u001b[31m0.3488\u001b[0m  0.0057  0.6395\n",
      "     13                     0.8874        0.2738                     0.8731        0.3572  0.0050  0.6367\n",
      "     14                     \u001b[36m0.9019\u001b[0m        \u001b[32m0.2546\u001b[0m                     0.8862        \u001b[31m0.3414\u001b[0m  0.0043  0.6368\n",
      "     15                     \u001b[36m0.9023\u001b[0m        \u001b[32m0.2443\u001b[0m                     0.8843        0.3415  0.0037  0.6375\n",
      "     16                     \u001b[36m0.9117\u001b[0m        \u001b[32m0.2376\u001b[0m                     0.8769        0.3574  0.0031  0.6397\n",
      "     17                     0.9117        0.2444                     0.8787        0.3710  0.0025  0.6367\n",
      "     18                     0.9079        0.2415                     0.8843        \u001b[31m0.3392\u001b[0m  0.0020  0.6368\n",
      "     19                     0.9093        \u001b[32m0.2346\u001b[0m                     0.8862        0.3431  0.0015  0.6387\n",
      "     20                     0.9019        0.2427                     \u001b[35m0.8937\u001b[0m        \u001b[31m0.3358\u001b[0m  0.0010  0.6377\n",
      "     21                     0.9112        \u001b[32m0.2274\u001b[0m                     0.8918        0.3384  0.0007  0.6375\n",
      "     22                     \u001b[36m0.9126\u001b[0m        \u001b[32m0.2243\u001b[0m                     0.8918        \u001b[31m0.3340\u001b[0m  0.0004  0.6389\n",
      "     23                     0.9121        \u001b[32m0.2203\u001b[0m                     0.8937        0.3342  0.0002  0.6377\n",
      "     24                     0.9103        0.2300                     \u001b[35m0.8955\u001b[0m        0.3342  0.0000  0.6373\n",
      "     25                     0.9112        0.2214                     0.8955        \u001b[31m0.3339\u001b[0m  0.0000  0.6370\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5114\u001b[0m                     \u001b[35m0.7985\u001b[0m        \u001b[31m0.5073\u001b[0m  0.0100  0.6362\n",
      "      2                     \u001b[36m0.8313\u001b[0m        \u001b[32m0.3818\u001b[0m                     \u001b[35m0.8321\u001b[0m        \u001b[31m0.3785\u001b[0m  0.0100  0.6369\n",
      "      3                     \u001b[36m0.8505\u001b[0m        \u001b[32m0.3588\u001b[0m                     \u001b[35m0.8470\u001b[0m        0.3875  0.0098  0.6370\n",
      "      4                     \u001b[36m0.8607\u001b[0m        \u001b[32m0.3354\u001b[0m                     0.8414        \u001b[31m0.3706\u001b[0m  0.0096  0.6380\n",
      "      5                     \u001b[36m0.8640\u001b[0m        \u001b[32m0.3318\u001b[0m                     0.8340        0.3948  0.0093  0.6357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6                     0.8640        \u001b[32m0.3306\u001b[0m                     \u001b[35m0.8675\u001b[0m        \u001b[31m0.3485\u001b[0m  0.0090  0.6378\n",
      "      7                     \u001b[36m0.8869\u001b[0m        \u001b[32m0.2948\u001b[0m                     0.8638        0.3923  0.0085  0.6363\n",
      "      8                     \u001b[36m0.8874\u001b[0m        \u001b[32m0.2881\u001b[0m                     0.8433        0.3770  0.0080  0.6382\n",
      "      9                     \u001b[36m0.8944\u001b[0m        \u001b[32m0.2803\u001b[0m                     0.8470        0.3908  0.0075  0.6345\n",
      "     10                     0.8916        \u001b[32m0.2760\u001b[0m                     \u001b[35m0.8750\u001b[0m        0.3512  0.0069  0.6375\n",
      "     11                     0.8893        0.2782                     0.8638        0.3770  0.0063  0.6383\n",
      "     12                     0.8930        \u001b[32m0.2758\u001b[0m                     \u001b[35m0.8787\u001b[0m        0.3574  0.0057  0.6366\n",
      "     13                     0.8911        \u001b[32m0.2579\u001b[0m                     0.8563        0.3650  0.0050  0.6365\n",
      "     14                     \u001b[36m0.9014\u001b[0m        0.2599                     \u001b[35m0.8843\u001b[0m        \u001b[31m0.3441\u001b[0m  0.0043  0.6363\n",
      "     15                     0.8967        \u001b[32m0.2527\u001b[0m                     0.8731        0.3570  0.0037  0.6398\n",
      "     16                     0.8986        \u001b[32m0.2498\u001b[0m                     0.8806        \u001b[31m0.3345\u001b[0m  0.0031  0.6359\n",
      "     17                     \u001b[36m0.9065\u001b[0m        \u001b[32m0.2394\u001b[0m                     0.8825        \u001b[31m0.3330\u001b[0m  0.0025  0.6365\n",
      "     18                     0.9009        0.2410                     0.8750        0.3354  0.0020  0.6410\n",
      "     19                     \u001b[36m0.9112\u001b[0m        \u001b[32m0.2288\u001b[0m                     0.8825        \u001b[31m0.3288\u001b[0m  0.0015  0.6367\n",
      "     20                     0.9107        0.2326                     0.8825        \u001b[31m0.3277\u001b[0m  0.0010  0.6363\n",
      "     21                     \u001b[36m0.9131\u001b[0m        \u001b[32m0.2245\u001b[0m                     \u001b[35m0.8843\u001b[0m        0.3286  0.0007  0.6380\n",
      "     22                     \u001b[36m0.9164\u001b[0m        \u001b[32m0.2200\u001b[0m                     0.8806        0.3279  0.0004  0.6373\n",
      "     23                     0.9140        0.2227                     0.8825        \u001b[31m0.3275\u001b[0m  0.0002  0.6377\n",
      "     24                     0.9145        0.2221                     0.8806        0.3276  0.0000  0.6374\n",
      "     25                     0.9145        0.2206                     0.8806        0.3276  0.0000  0.6383\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7762\u001b[0m        \u001b[32m0.4785\u001b[0m                     \u001b[35m0.8470\u001b[0m        \u001b[31m0.3764\u001b[0m  0.0100  0.6338\n",
      "      2                     \u001b[36m0.8477\u001b[0m        \u001b[32m0.3719\u001b[0m                     \u001b[35m0.8526\u001b[0m        \u001b[31m0.3554\u001b[0m  0.0100  0.6379\n",
      "      3                     \u001b[36m0.8612\u001b[0m        \u001b[32m0.3413\u001b[0m                     \u001b[35m0.8582\u001b[0m        0.3580  0.0098  0.6383\n",
      "      4                     0.8612        \u001b[32m0.3368\u001b[0m                     \u001b[35m0.8619\u001b[0m        \u001b[31m0.3490\u001b[0m  0.0096  0.6376\n",
      "      5                     \u001b[36m0.8664\u001b[0m        \u001b[32m0.3243\u001b[0m                     0.8619        \u001b[31m0.3250\u001b[0m  0.0093  0.6374\n",
      "      6                     \u001b[36m0.8678\u001b[0m        \u001b[32m0.3234\u001b[0m                     0.8619        0.3301  0.0090  0.6371\n",
      "      7                     \u001b[36m0.8799\u001b[0m        \u001b[32m0.3042\u001b[0m                     0.8414        0.3636  0.0085  0.6371\n",
      "      8                     0.8799        \u001b[32m0.3017\u001b[0m                     \u001b[35m0.8731\u001b[0m        0.3261  0.0080  0.6375\n",
      "      9                     \u001b[36m0.8832\u001b[0m        \u001b[32m0.2942\u001b[0m                     0.8694        0.3423  0.0075  0.6381\n",
      "     10                     0.8762        0.2948                     \u001b[35m0.8806\u001b[0m        0.3299  0.0069  0.6410\n",
      "     11                     \u001b[36m0.8855\u001b[0m        \u001b[32m0.2861\u001b[0m                     \u001b[35m0.8881\u001b[0m        \u001b[31m0.3039\u001b[0m  0.0063  0.6359\n",
      "     12                     \u001b[36m0.8916\u001b[0m        \u001b[32m0.2728\u001b[0m                     \u001b[35m0.8899\u001b[0m        \u001b[31m0.3008\u001b[0m  0.0057  0.6368\n",
      "     13                     \u001b[36m0.8977\u001b[0m        \u001b[32m0.2636\u001b[0m                     0.8713        0.3189  0.0050  0.6374\n",
      "     14                     \u001b[36m0.9005\u001b[0m        \u001b[32m0.2554\u001b[0m                     0.8787        0.3064  0.0043  0.6398\n",
      "     15                     0.8967        0.2567                     0.8731        0.3235  0.0037  0.6360\n",
      "     16                     0.8995        \u001b[32m0.2538\u001b[0m                     0.8881        \u001b[31m0.2991\u001b[0m  0.0031  0.6370\n",
      "     17                     \u001b[36m0.9033\u001b[0m        \u001b[32m0.2407\u001b[0m                     0.8769        0.3075  0.0025  0.6366\n",
      "     18                     \u001b[36m0.9093\u001b[0m        \u001b[32m0.2362\u001b[0m                     \u001b[35m0.8937\u001b[0m        \u001b[31m0.2950\u001b[0m  0.0020  0.6365\n",
      "     19                     \u001b[36m0.9182\u001b[0m        \u001b[32m0.2254\u001b[0m                     0.8769        0.3000  0.0015  0.6365\n",
      "     20                     0.9126        0.2289                     0.8825        0.2997  0.0010  0.6374\n",
      "     21                     0.9182        \u001b[32m0.2217\u001b[0m                     0.8787        0.2994  0.0007  0.6393\n",
      "     22                     0.9093        0.2261                     0.8769        0.3007  0.0004  0.6373\n",
      "     23                     0.9117        0.2232                     0.8843        0.2978  0.0002  0.6370\n",
      "     24                     \u001b[36m0.9187\u001b[0m        \u001b[32m0.2177\u001b[0m                     0.8862        0.2975  0.0000  0.6383\n",
      "     25                     0.9131        0.2234                     0.8918        0.2974  0.0000  0.6371\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.7668\u001b[0m        \u001b[32m0.4980\u001b[0m                     \u001b[35m0.8060\u001b[0m        \u001b[31m0.5092\u001b[0m  0.0100  0.6354\n",
      "      2                     \u001b[36m0.8425\u001b[0m        \u001b[32m0.3786\u001b[0m                     \u001b[35m0.8358\u001b[0m        \u001b[31m0.3840\u001b[0m  0.0100  0.6394\n",
      "      3                     \u001b[36m0.8472\u001b[0m        \u001b[32m0.3553\u001b[0m                     0.8358        0.3939  0.0098  0.6379\n",
      "      4                     \u001b[36m0.8481\u001b[0m        \u001b[32m0.3546\u001b[0m                     \u001b[35m0.8657\u001b[0m        \u001b[31m0.3246\u001b[0m  0.0096  0.6353\n",
      "      5                     \u001b[36m0.8743\u001b[0m        \u001b[32m0.3201\u001b[0m                     0.8619        0.3544  0.0093  0.6370\n",
      "      6                     \u001b[36m0.8836\u001b[0m        \u001b[32m0.2960\u001b[0m                     \u001b[35m0.8713\u001b[0m        0.3456  0.0090  0.6381\n",
      "      7                     0.8776        0.3003                     \u001b[35m0.8731\u001b[0m        0.3507  0.0085  0.6380\n",
      "      8                     \u001b[36m0.8846\u001b[0m        \u001b[32m0.2955\u001b[0m                     0.8638        0.3405  0.0080  0.6378\n",
      "      9                     0.8841        \u001b[32m0.2918\u001b[0m                     \u001b[35m0.8787\u001b[0m        \u001b[31m0.3095\u001b[0m  0.0075  0.6383\n",
      "     10                     \u001b[36m0.8902\u001b[0m        \u001b[32m0.2734\u001b[0m                     0.8713        0.3343  0.0069  0.6374\n",
      "     11                     \u001b[36m0.8916\u001b[0m        0.2804                     \u001b[35m0.8806\u001b[0m        0.3101  0.0063  0.6370\n",
      "     12                     \u001b[36m0.8967\u001b[0m        \u001b[32m0.2576\u001b[0m                     \u001b[35m0.8843\u001b[0m        \u001b[31m0.3024\u001b[0m  0.0057  0.6383\n",
      "     13                     0.8893        0.2735                     0.8675        0.3199  0.0050  0.6393\n",
      "     14                     0.8916        0.2684                     0.8806        \u001b[31m0.3003\u001b[0m  0.0043  0.6370\n",
      "     15                     0.8921        0.2587                     0.8750        0.3112  0.0037  0.6377\n",
      "     16                     \u001b[36m0.9028\u001b[0m        \u001b[32m0.2457\u001b[0m                     0.8806        0.3134  0.0031  0.7560\n",
      "     17                     \u001b[36m0.9061\u001b[0m        0.2501                     0.8675        0.3042  0.0025  0.6842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18                     0.9047        0.2524                     0.8731        \u001b[31m0.2912\u001b[0m  0.0020  0.6361\n",
      "     19                     \u001b[36m0.9117\u001b[0m        \u001b[32m0.2354\u001b[0m                     0.8806        0.2914  0.0015  0.6361\n",
      "     20                     \u001b[36m0.9131\u001b[0m        \u001b[32m0.2317\u001b[0m                     0.8806        \u001b[31m0.2892\u001b[0m  0.0010  0.6352\n",
      "     21                     0.9056        0.2340                     \u001b[35m0.8862\u001b[0m        0.2920  0.0007  0.6360\n",
      "     22                     0.9061        \u001b[32m0.2312\u001b[0m                     0.8806        0.2920  0.0004  0.6362\n",
      "     23                     0.9112        0.2377                     0.8862        0.2901  0.0002  0.6362\n",
      "     24                     0.9131        \u001b[32m0.2179\u001b[0m                     \u001b[35m0.8881\u001b[0m        0.2901  0.0000  0.6363\n",
      "     25                     0.9070        0.2329                     0.8881        0.2901  0.0000  0.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5869\u001b[0m        \u001b[32m0.7076\u001b[0m                     \u001b[35m0.6269\u001b[0m        \u001b[31m0.6322\u001b[0m  0.0100  0.6330\n",
      "      2                     \u001b[36m0.7150\u001b[0m        \u001b[32m0.5719\u001b[0m                     \u001b[35m0.7257\u001b[0m        \u001b[31m0.5510\u001b[0m  0.0100  0.6408\n",
      "      3                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5247\u001b[0m                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5424\u001b[0m  0.0098  0.6372\n",
      "      4                     \u001b[36m0.7654\u001b[0m        \u001b[32m0.4947\u001b[0m                     \u001b[35m0.7631\u001b[0m        \u001b[31m0.4981\u001b[0m  0.0096  0.6357\n",
      "      5                     \u001b[36m0.7907\u001b[0m        \u001b[32m0.4583\u001b[0m                     0.7481        0.5321  0.0093  0.6350\n",
      "      6                     0.7855        0.4655                     \u001b[35m0.7799\u001b[0m        0.5019  0.0090  0.6348\n",
      "      7                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4397\u001b[0m                     0.7761        \u001b[31m0.4880\u001b[0m  0.0085  0.6357\n",
      "      8                     \u001b[36m0.8047\u001b[0m        0.4411                     \u001b[35m0.7892\u001b[0m        \u001b[31m0.4578\u001b[0m  0.0080  0.6353\n",
      "      9                     0.8000        \u001b[32m0.4303\u001b[0m                     0.7892        0.4619  0.0075  0.6351\n",
      "     10                     \u001b[36m0.8089\u001b[0m        \u001b[32m0.4122\u001b[0m                     0.7761        0.4899  0.0069  0.6368\n",
      "     11                     0.8028        0.4187                     0.7780        0.4684  0.0063  0.6350\n",
      "     12                     \u001b[36m0.8252\u001b[0m        \u001b[32m0.3895\u001b[0m                     0.7873        0.4779  0.0057  0.6351\n",
      "     13                     0.8234        0.3979                     \u001b[35m0.7985\u001b[0m        0.4706  0.0050  0.6347\n",
      "     14                     \u001b[36m0.8355\u001b[0m        \u001b[32m0.3733\u001b[0m                     0.7985        0.4774  0.0043  0.6342\n",
      "     15                     0.8271        0.3799                     0.7929        0.4654  0.0037  0.6370\n",
      "     16                     \u001b[36m0.8402\u001b[0m        \u001b[32m0.3685\u001b[0m                     \u001b[35m0.8004\u001b[0m        0.4682  0.0031  0.6354\n",
      "     17                     \u001b[36m0.8439\u001b[0m        \u001b[32m0.3591\u001b[0m                     0.7929        0.4646  0.0025  0.6357\n",
      "     18                     0.8439        0.3602                     0.7929        0.4645  0.0020  0.6357\n",
      "     19                     0.8350        0.3634                     0.7948        0.4601  0.0015  0.6355\n",
      "     20                     \u001b[36m0.8449\u001b[0m        \u001b[32m0.3490\u001b[0m                     \u001b[35m0.8172\u001b[0m        \u001b[31m0.4506\u001b[0m  0.0010  0.6355\n",
      "     21                     \u001b[36m0.8477\u001b[0m        0.3551                     0.8060        0.4507  0.0007  0.6354\n",
      "     22                     \u001b[36m0.8495\u001b[0m        0.3490                     0.8097        0.4532  0.0004  0.6356\n",
      "     23                     0.8458        0.3506                     0.8097        0.4539  0.0002  0.6355\n",
      "     24                     \u001b[36m0.8533\u001b[0m        \u001b[32m0.3388\u001b[0m                     0.8078        0.4542  0.0000  0.6358\n",
      "     25                     0.8467        0.3430                     0.8078        0.4545  0.0000  0.6359\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5991\u001b[0m        \u001b[32m0.7007\u001b[0m                     \u001b[35m0.6679\u001b[0m        \u001b[31m0.6154\u001b[0m  0.0100  0.6343\n",
      "      2                     \u001b[36m0.7332\u001b[0m        \u001b[32m0.5575\u001b[0m                     \u001b[35m0.7369\u001b[0m        \u001b[31m0.5268\u001b[0m  0.0100  0.6359\n",
      "      3                     \u001b[36m0.7664\u001b[0m        \u001b[32m0.4926\u001b[0m                     \u001b[35m0.7612\u001b[0m        \u001b[31m0.5182\u001b[0m  0.0098  0.6354\n",
      "      4                     \u001b[36m0.7687\u001b[0m        \u001b[32m0.4751\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.4866\u001b[0m  0.0096  0.6356\n",
      "      5                     \u001b[36m0.7813\u001b[0m        \u001b[32m0.4690\u001b[0m                     \u001b[35m0.7817\u001b[0m        0.4938  0.0093  0.6364\n",
      "      6                     \u001b[36m0.8009\u001b[0m        \u001b[32m0.4482\u001b[0m                     0.7799        0.5045  0.0090  0.6355\n",
      "      7                     0.8005        \u001b[32m0.4397\u001b[0m                     0.7631        \u001b[31m0.4812\u001b[0m  0.0085  0.6360\n",
      "      8                     \u001b[36m0.8028\u001b[0m        \u001b[32m0.4339\u001b[0m                     0.7649        0.5162  0.0080  0.6361\n",
      "      9                     0.7888        0.4453                     0.7687        0.5111  0.0075  0.6355\n",
      "     10                     \u001b[36m0.8065\u001b[0m        \u001b[32m0.4225\u001b[0m                     0.7761        0.5159  0.0069  0.6348\n",
      "     11                     \u001b[36m0.8131\u001b[0m        \u001b[32m0.4213\u001b[0m                     0.7761        \u001b[31m0.4660\u001b[0m  0.0063  0.6358\n",
      "     12                     0.8131        \u001b[32m0.4090\u001b[0m                     \u001b[35m0.7836\u001b[0m        0.4688  0.0057  0.6361\n",
      "     13                     \u001b[36m0.8248\u001b[0m        \u001b[32m0.4054\u001b[0m                     0.7836        0.4780  0.0050  0.6355\n",
      "     14                     0.8224        \u001b[32m0.3871\u001b[0m                     \u001b[35m0.7929\u001b[0m        \u001b[31m0.4599\u001b[0m  0.0043  0.6350\n",
      "     15                     \u001b[36m0.8280\u001b[0m        0.3879                     0.7761        0.4725  0.0037  0.6360\n",
      "     16                     \u001b[36m0.8350\u001b[0m        \u001b[32m0.3833\u001b[0m                     \u001b[35m0.7948\u001b[0m        0.4696  0.0031  0.6352\n",
      "     17                     0.8341        \u001b[32m0.3735\u001b[0m                     0.7929        0.4623  0.0025  0.6363\n",
      "     18                     \u001b[36m0.8430\u001b[0m        \u001b[32m0.3600\u001b[0m                     0.7948        0.4705  0.0020  0.6359\n",
      "     19                     0.8346        0.3623                     0.7873        0.4633  0.0015  0.6376\n",
      "     20                     \u001b[36m0.8472\u001b[0m        \u001b[32m0.3587\u001b[0m                     0.7910        0.4676  0.0010  0.6360\n",
      "     21                     0.8444        \u001b[32m0.3575\u001b[0m                     \u001b[35m0.7985\u001b[0m        0.4642  0.0007  0.6362\n",
      "     22                     0.8472        0.3577                     0.7985        0.4651  0.0004  0.6371\n",
      "     23                     0.8463        \u001b[32m0.3526\u001b[0m                     0.7966        0.4640  0.0002  0.6371\n",
      "     24                     \u001b[36m0.8500\u001b[0m        \u001b[32m0.3523\u001b[0m                     0.7929        0.4639  0.0000  0.6359\n",
      "     25                     0.8477        0.3576                     0.7948        0.4638  0.0000  0.6394\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5650\u001b[0m        \u001b[32m0.7169\u001b[0m                     \u001b[35m0.6884\u001b[0m        \u001b[31m0.6000\u001b[0m  0.0100  0.6357\n",
      "      2                     \u001b[36m0.7014\u001b[0m        \u001b[32m0.5804\u001b[0m                     \u001b[35m0.7593\u001b[0m        \u001b[31m0.5260\u001b[0m  0.0100  0.6514\n",
      "      3                     \u001b[36m0.7388\u001b[0m        \u001b[32m0.5243\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.4851\u001b[0m  0.0098  0.6692\n",
      "      4                     \u001b[36m0.7701\u001b[0m        \u001b[32m0.4928\u001b[0m                     0.7668        0.5033  0.0096  0.6697\n",
      "      5                     \u001b[36m0.7841\u001b[0m        \u001b[32m0.4621\u001b[0m                     \u001b[35m0.7854\u001b[0m        0.5005  0.0093  0.6728\n",
      "      6                     \u001b[36m0.7850\u001b[0m        \u001b[32m0.4581\u001b[0m                     \u001b[35m0.7985\u001b[0m        \u001b[31m0.4572\u001b[0m  0.0090  0.6791\n",
      "      7                     \u001b[36m0.8042\u001b[0m        \u001b[32m0.4469\u001b[0m                     0.7910        0.4607  0.0085  0.6379\n",
      "      8                     0.7991        \u001b[32m0.4376\u001b[0m                     0.7668        0.4776  0.0080  0.6353\n",
      "      9                     \u001b[36m0.8051\u001b[0m        \u001b[32m0.4219\u001b[0m                     \u001b[35m0.8004\u001b[0m        \u001b[31m0.4527\u001b[0m  0.0075  0.6358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                     \u001b[36m0.8093\u001b[0m        \u001b[32m0.4187\u001b[0m                     0.7780        0.4892  0.0069  0.6355\n",
      "     11                     \u001b[36m0.8182\u001b[0m        \u001b[32m0.4012\u001b[0m                     \u001b[35m0.8078\u001b[0m        0.4533  0.0063  0.6359\n",
      "     12                     \u001b[36m0.8210\u001b[0m        0.4050                     0.7948        0.4582  0.0057  0.6356\n",
      "     13                     \u001b[36m0.8280\u001b[0m        \u001b[32m0.3860\u001b[0m                     0.7910        0.4609  0.0050  0.6353\n",
      "     14                     0.8201        0.3915                     0.7929        0.4802  0.0043  0.6353\n",
      "     15                     \u001b[36m0.8290\u001b[0m        0.3920                     0.7873        0.4638  0.0037  0.6351\n",
      "     16                     \u001b[36m0.8336\u001b[0m        \u001b[32m0.3806\u001b[0m                     0.7948        0.4582  0.0031  0.6364\n",
      "     17                     \u001b[36m0.8514\u001b[0m        \u001b[32m0.3685\u001b[0m                     0.7780        0.4567  0.0025  0.6360\n",
      "     18                     0.8336        0.3787                     \u001b[35m0.8097\u001b[0m        0.4554  0.0020  0.6345\n",
      "     19                     0.8402        \u001b[32m0.3654\u001b[0m                     \u001b[35m0.8209\u001b[0m        \u001b[31m0.4420\u001b[0m  0.0015  0.6353\n",
      "     20                     0.8486        \u001b[32m0.3562\u001b[0m                     0.8078        0.4495  0.0010  0.6357\n",
      "     21                     0.8495        0.3575                     0.8060        0.4522  0.0007  0.6348\n",
      "     22                     0.8495        \u001b[32m0.3522\u001b[0m                     0.8041        0.4502  0.0004  0.6353\n",
      "     23                     0.8514        \u001b[32m0.3489\u001b[0m                     0.8097        0.4490  0.0002  0.6355\n",
      "     24                     0.8477        0.3503                     0.7966        0.4486  0.0000  0.6357\n",
      "     25                     0.8495        0.3519                     0.7929        0.4488  0.0000  0.6358\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6075\u001b[0m        \u001b[32m0.6777\u001b[0m                     \u001b[35m0.6940\u001b[0m        \u001b[31m0.6089\u001b[0m  0.0100  0.6319\n",
      "      2                     \u001b[36m0.7150\u001b[0m        \u001b[32m0.5623\u001b[0m                     \u001b[35m0.7351\u001b[0m        \u001b[31m0.5604\u001b[0m  0.0100  0.6384\n",
      "      3                     \u001b[36m0.7584\u001b[0m        \u001b[32m0.5083\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.4960\u001b[0m  0.0098  0.6398\n",
      "      4                     \u001b[36m0.7804\u001b[0m        \u001b[32m0.4669\u001b[0m                     0.7631        \u001b[31m0.4951\u001b[0m  0.0096  0.6352\n",
      "      5                     \u001b[36m0.7879\u001b[0m        \u001b[32m0.4582\u001b[0m                     0.7519        0.5416  0.0093  0.6351\n",
      "      6                     \u001b[36m0.7953\u001b[0m        \u001b[32m0.4376\u001b[0m                     0.7612        0.5165  0.0090  0.6347\n",
      "      7                     \u001b[36m0.7991\u001b[0m        0.4435                     \u001b[35m0.7892\u001b[0m        \u001b[31m0.4879\u001b[0m  0.0085  0.6352\n",
      "      8                     \u001b[36m0.8070\u001b[0m        \u001b[32m0.4304\u001b[0m                     0.7743        0.5095  0.0080  0.6347\n",
      "      9                     0.8028        0.4331                     \u001b[35m0.8022\u001b[0m        \u001b[31m0.4598\u001b[0m  0.0075  0.6340\n",
      "     10                     \u001b[36m0.8164\u001b[0m        \u001b[32m0.4155\u001b[0m                     0.7873        0.4866  0.0069  0.6343\n",
      "     11                     \u001b[36m0.8210\u001b[0m        \u001b[32m0.4072\u001b[0m                     0.7892        0.4801  0.0063  0.6343\n",
      "     12                     0.8210        \u001b[32m0.3968\u001b[0m                     \u001b[35m0.8041\u001b[0m        \u001b[31m0.4519\u001b[0m  0.0057  0.6350\n",
      "     13                     \u001b[36m0.8234\u001b[0m        0.3974                     0.7948        0.4641  0.0050  0.6353\n",
      "     14                     \u001b[36m0.8271\u001b[0m        \u001b[32m0.3947\u001b[0m                     0.8004        0.4619  0.0043  0.6331\n",
      "     15                     0.8234        \u001b[32m0.3881\u001b[0m                     0.7761        0.4750  0.0037  0.6345\n",
      "     16                     \u001b[36m0.8346\u001b[0m        \u001b[32m0.3760\u001b[0m                     0.7985        0.4588  0.0031  0.6352\n",
      "     17                     \u001b[36m0.8355\u001b[0m        \u001b[32m0.3730\u001b[0m                     0.7966        0.4578  0.0025  0.6366\n",
      "     18                     \u001b[36m0.8444\u001b[0m        \u001b[32m0.3672\u001b[0m                     0.7985        0.4558  0.0020  0.6343\n",
      "     19                     \u001b[36m0.8477\u001b[0m        \u001b[32m0.3543\u001b[0m                     0.8041        \u001b[31m0.4511\u001b[0m  0.0015  0.6350\n",
      "     20                     0.8463        0.3562                     0.8004        0.4573  0.0010  0.6350\n",
      "     21                     0.8393        0.3555                     0.7985        0.4615  0.0007  0.6356\n",
      "     22                     0.8439        \u001b[32m0.3487\u001b[0m                     \u001b[35m0.8097\u001b[0m        0.4587  0.0004  0.6350\n",
      "     23                     \u001b[36m0.8505\u001b[0m        \u001b[32m0.3462\u001b[0m                     0.8041        0.4581  0.0002  0.6351\n",
      "     24                     0.8435        0.3480                     0.8022        0.4586  0.0000  0.6347\n",
      "     25                     \u001b[36m0.8537\u001b[0m        0.3520                     0.8004        0.4591  0.0000  0.6367\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5780\u001b[0m        \u001b[32m0.7128\u001b[0m                     \u001b[35m0.6679\u001b[0m        \u001b[31m0.6038\u001b[0m  0.0100  0.6341\n",
      "      2                     \u001b[36m0.6818\u001b[0m        \u001b[32m0.6033\u001b[0m                     \u001b[35m0.7201\u001b[0m        \u001b[31m0.5560\u001b[0m  0.0100  0.6393\n",
      "      3                     \u001b[36m0.7458\u001b[0m        \u001b[32m0.5315\u001b[0m                     \u001b[35m0.7761\u001b[0m        \u001b[31m0.4666\u001b[0m  0.0098  0.6393\n",
      "      4                     \u001b[36m0.7640\u001b[0m        \u001b[32m0.5012\u001b[0m                     \u001b[35m0.7948\u001b[0m        \u001b[31m0.4402\u001b[0m  0.0096  0.6350\n",
      "      5                     \u001b[36m0.7832\u001b[0m        \u001b[32m0.4663\u001b[0m                     \u001b[35m0.8060\u001b[0m        \u001b[31m0.4303\u001b[0m  0.0093  0.6350\n",
      "      6                     \u001b[36m0.7846\u001b[0m        \u001b[32m0.4595\u001b[0m                     0.8041        0.4306  0.0090  0.6352\n",
      "      7                     \u001b[36m0.7893\u001b[0m        \u001b[32m0.4570\u001b[0m                     0.8060        0.4363  0.0085  0.6352\n",
      "      8                     \u001b[36m0.7949\u001b[0m        \u001b[32m0.4530\u001b[0m                     0.8022        0.4368  0.0080  0.6351\n",
      "      9                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4370\u001b[0m                     0.7892        0.4457  0.0075  0.6344\n",
      "     10                     \u001b[36m0.8047\u001b[0m        \u001b[32m0.4293\u001b[0m                     0.7910        0.4520  0.0069  0.6355\n",
      "     11                     \u001b[36m0.8136\u001b[0m        \u001b[32m0.4207\u001b[0m                     \u001b[35m0.8134\u001b[0m        0.4350  0.0063  0.6355\n",
      "     12                     0.8112        0.4231                     0.8134        \u001b[31m0.4179\u001b[0m  0.0057  0.6351\n",
      "     13                     \u001b[36m0.8173\u001b[0m        \u001b[32m0.4032\u001b[0m                     0.8097        0.4191  0.0050  0.6357\n",
      "     14                     0.8173        \u001b[32m0.3988\u001b[0m                     \u001b[35m0.8209\u001b[0m        0.4225  0.0043  0.6352\n",
      "     15                     \u001b[36m0.8178\u001b[0m        0.4010                     0.8190        \u001b[31m0.4061\u001b[0m  0.0037  0.6362\n",
      "     16                     \u001b[36m0.8271\u001b[0m        \u001b[32m0.3914\u001b[0m                     0.8078        0.4155  0.0031  0.6349\n",
      "     17                     0.8266        \u001b[32m0.3779\u001b[0m                     \u001b[35m0.8228\u001b[0m        \u001b[31m0.4041\u001b[0m  0.0025  0.6360\n",
      "     18                     \u001b[36m0.8402\u001b[0m        \u001b[32m0.3717\u001b[0m                     0.8060        0.4128  0.0020  0.6356\n",
      "     19                     0.8308        0.3746                     0.8172        \u001b[31m0.4025\u001b[0m  0.0015  0.6357\n",
      "     20                     \u001b[36m0.8444\u001b[0m        0.3728                     0.8134        0.4054  0.0010  0.6356\n",
      "     21                     \u001b[36m0.8481\u001b[0m        \u001b[32m0.3572\u001b[0m                     0.8078        0.4037  0.0007  0.6348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22                     0.8369        0.3619                     0.8116        \u001b[31m0.4022\u001b[0m  0.0004  0.6353\n",
      "     23                     0.8453        \u001b[32m0.3571\u001b[0m                     0.8060        \u001b[31m0.4017\u001b[0m  0.0002  0.6348\n",
      "     24                     0.8416        0.3587                     0.8078        0.4018  0.0000  0.6351\n",
      "     25                     0.8388        \u001b[32m0.3534\u001b[0m                     0.8078        0.4018  0.0000  0.6356\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6042\u001b[0m        \u001b[32m0.6901\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5538\u001b[0m  0.0100  0.6331\n",
      "      2                     \u001b[36m0.7430\u001b[0m        \u001b[32m0.5416\u001b[0m                     \u001b[35m0.7761\u001b[0m        \u001b[31m0.5066\u001b[0m  0.0100  0.6388\n",
      "      3                     \u001b[36m0.7720\u001b[0m        \u001b[32m0.4911\u001b[0m                     0.7687        \u001b[31m0.4790\u001b[0m  0.0098  0.6396\n",
      "      4                     \u001b[36m0.7729\u001b[0m        \u001b[32m0.4766\u001b[0m                     \u001b[35m0.7892\u001b[0m        \u001b[31m0.4457\u001b[0m  0.0096  0.6350\n",
      "      5                     \u001b[36m0.7963\u001b[0m        \u001b[32m0.4553\u001b[0m                     \u001b[35m0.8041\u001b[0m        \u001b[31m0.4431\u001b[0m  0.0093  0.6347\n",
      "      6                     0.7883        0.4614                     0.7929        0.4592  0.0090  0.6344\n",
      "      7                     \u001b[36m0.8056\u001b[0m        \u001b[32m0.4443\u001b[0m                     \u001b[35m0.8116\u001b[0m        \u001b[31m0.4136\u001b[0m  0.0085  0.6354\n",
      "      8                     \u001b[36m0.8136\u001b[0m        \u001b[32m0.4275\u001b[0m                     \u001b[35m0.8209\u001b[0m        0.4218  0.0080  0.6351\n",
      "      9                     0.8047        0.4304                     0.7836        0.4691  0.0075  0.6353\n",
      "     10                     0.7981        0.4406                     0.8116        0.4154  0.0069  0.6347\n",
      "     11                     0.8079        \u001b[32m0.4234\u001b[0m                     0.8078        \u001b[31m0.4097\u001b[0m  0.0063  0.6353\n",
      "     12                     \u001b[36m0.8220\u001b[0m        \u001b[32m0.4060\u001b[0m                     0.8060        0.4133  0.0057  0.6354\n",
      "     13                     0.8079        0.4069                     0.8060        0.4467  0.0050  0.6349\n",
      "     14                     0.8164        \u001b[32m0.3996\u001b[0m                     0.8116        0.4105  0.0043  0.6339\n",
      "     15                     \u001b[36m0.8257\u001b[0m        \u001b[32m0.3907\u001b[0m                     0.8172        0.4315  0.0037  0.6352\n",
      "     16                     0.8252        \u001b[32m0.3857\u001b[0m                     0.8116        0.4191  0.0031  0.6353\n",
      "     17                     \u001b[36m0.8322\u001b[0m        \u001b[32m0.3799\u001b[0m                     0.8190        \u001b[31m0.4083\u001b[0m  0.0025  0.6367\n",
      "     18                     \u001b[36m0.8327\u001b[0m        \u001b[32m0.3729\u001b[0m                     0.8116        0.4297  0.0020  0.6366\n",
      "     19                     0.8266        0.3773                     0.8153        \u001b[31m0.4082\u001b[0m  0.0015  0.6353\n",
      "     20                     \u001b[36m0.8393\u001b[0m        \u001b[32m0.3659\u001b[0m                     0.8116        0.4105  0.0010  0.6359\n",
      "     21                     0.8350        \u001b[32m0.3594\u001b[0m                     0.8134        0.4096  0.0007  0.6353\n",
      "     22                     0.8393        0.3703                     0.8078        0.4118  0.0004  0.6361\n",
      "     23                     \u001b[36m0.8407\u001b[0m        0.3616                     0.8060        0.4094  0.0002  0.6365\n",
      "     24                     \u001b[36m0.8421\u001b[0m        \u001b[32m0.3526\u001b[0m                     0.8041        0.4090  0.0000  0.6356\n",
      "     25                     0.8374        0.3571                     0.8041        0.4088  0.0000  0.6352\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6121\u001b[0m        \u001b[32m0.6804\u001b[0m                     \u001b[35m0.6996\u001b[0m        \u001b[31m0.5995\u001b[0m  0.0100  0.6335\n",
      "      2                     \u001b[36m0.7271\u001b[0m        \u001b[32m0.5549\u001b[0m                     \u001b[35m0.7071\u001b[0m        \u001b[31m0.5576\u001b[0m  0.0100  0.6411\n",
      "      3                     \u001b[36m0.7509\u001b[0m        \u001b[32m0.5056\u001b[0m                     \u001b[35m0.7612\u001b[0m        \u001b[31m0.5175\u001b[0m  0.0098  0.6380\n",
      "      4                     \u001b[36m0.7776\u001b[0m        \u001b[32m0.4756\u001b[0m                     \u001b[35m0.7817\u001b[0m        \u001b[31m0.4857\u001b[0m  0.0096  0.6353\n",
      "      5                     \u001b[36m0.7836\u001b[0m        \u001b[32m0.4742\u001b[0m                     0.7668        0.5184  0.0093  0.6352\n",
      "      6                     0.7790        \u001b[32m0.4636\u001b[0m                     0.7687        0.4969  0.0090  0.6345\n",
      "      7                     \u001b[36m0.7958\u001b[0m        \u001b[32m0.4351\u001b[0m                     \u001b[35m0.7854\u001b[0m        0.4978  0.0085  0.6346\n",
      "      8                     \u001b[36m0.8112\u001b[0m        \u001b[32m0.4258\u001b[0m                     0.7799        0.4893  0.0080  0.6342\n",
      "      9                     0.8009        0.4305                     0.7817        0.4888  0.0075  0.6355\n",
      "     10                     0.8093        \u001b[32m0.4256\u001b[0m                     0.7817        \u001b[31m0.4705\u001b[0m  0.0069  0.6344\n",
      "     11                     \u001b[36m0.8121\u001b[0m        \u001b[32m0.4174\u001b[0m                     0.7575        0.5430  0.0063  0.6347\n",
      "     12                     0.8112        \u001b[32m0.4072\u001b[0m                     0.7687        0.4787  0.0057  0.6349\n",
      "     13                     \u001b[36m0.8294\u001b[0m        \u001b[32m0.3992\u001b[0m                     0.7817        0.4905  0.0050  0.6346\n",
      "     14                     0.8224        \u001b[32m0.3963\u001b[0m                     \u001b[35m0.7948\u001b[0m        0.4765  0.0043  0.6360\n",
      "     15                     0.8229        \u001b[32m0.3871\u001b[0m                     0.7929        \u001b[31m0.4669\u001b[0m  0.0037  0.6362\n",
      "     16                     \u001b[36m0.8379\u001b[0m        \u001b[32m0.3734\u001b[0m                     0.7929        0.4716  0.0031  0.6357\n",
      "     17                     0.8299        0.3749                     0.7854        0.4707  0.0025  0.6362\n",
      "     18                     \u001b[36m0.8411\u001b[0m        \u001b[32m0.3643\u001b[0m                     0.7854        0.4936  0.0020  0.6353\n",
      "     19                     \u001b[36m0.8421\u001b[0m        \u001b[32m0.3552\u001b[0m                     0.7873        0.4728  0.0015  0.6345\n",
      "     20                     0.8397        0.3591                     \u001b[35m0.7985\u001b[0m        \u001b[31m0.4605\u001b[0m  0.0010  0.6364\n",
      "     21                     \u001b[36m0.8472\u001b[0m        \u001b[32m0.3491\u001b[0m                     \u001b[35m0.8022\u001b[0m        \u001b[31m0.4576\u001b[0m  0.0007  0.6351\n",
      "     22                     0.8453        0.3558                     0.8004        0.4632  0.0004  0.6354\n",
      "     23                     \u001b[36m0.8575\u001b[0m        \u001b[32m0.3441\u001b[0m                     0.8004        0.4587  0.0002  0.6356\n",
      "     24                     0.8467        \u001b[32m0.3438\u001b[0m                     0.8004        \u001b[31m0.4567\u001b[0m  0.0000  0.6357\n",
      "     25                     0.8491        \u001b[32m0.3398\u001b[0m                     0.7948        \u001b[31m0.4556\u001b[0m  0.0000  0.6357\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5995\u001b[0m        \u001b[32m0.6876\u001b[0m                     \u001b[35m0.6642\u001b[0m        \u001b[31m0.6106\u001b[0m  0.0100  0.6339\n",
      "      2                     \u001b[36m0.7252\u001b[0m        \u001b[32m0.5665\u001b[0m                     \u001b[35m0.7313\u001b[0m        \u001b[31m0.5281\u001b[0m  0.0100  0.6388\n",
      "      3                     \u001b[36m0.7467\u001b[0m        \u001b[32m0.5128\u001b[0m                     \u001b[35m0.7481\u001b[0m        0.5294  0.0098  0.6393\n",
      "      4                     \u001b[36m0.7664\u001b[0m        \u001b[32m0.4899\u001b[0m                     \u001b[35m0.7705\u001b[0m        \u001b[31m0.4701\u001b[0m  0.0096  0.6347\n",
      "      5                     \u001b[36m0.7818\u001b[0m        \u001b[32m0.4733\u001b[0m                     \u001b[35m0.7780\u001b[0m        \u001b[31m0.4499\u001b[0m  0.0093  0.6350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6                     \u001b[36m0.7916\u001b[0m        \u001b[32m0.4620\u001b[0m                     \u001b[35m0.8060\u001b[0m        \u001b[31m0.4459\u001b[0m  0.0090  0.6343\n",
      "      7                     \u001b[36m0.7977\u001b[0m        \u001b[32m0.4511\u001b[0m                     0.7892        \u001b[31m0.4427\u001b[0m  0.0085  0.6343\n",
      "      8                     0.7958        \u001b[32m0.4355\u001b[0m                     0.7388        0.5374  0.0080  0.6346\n",
      "      9                     \u001b[36m0.7991\u001b[0m        0.4356                     0.7631        0.4845  0.0075  0.6355\n",
      "     10                     \u001b[36m0.8089\u001b[0m        \u001b[32m0.4222\u001b[0m                     0.7388        0.5414  0.0069  0.6360\n",
      "     11                     0.8065        0.4338                     0.7910        0.4728  0.0063  0.6349\n",
      "     12                     \u001b[36m0.8164\u001b[0m        \u001b[32m0.4092\u001b[0m                     0.7799        0.4555  0.0057  0.6341\n",
      "     13                     0.8150        0.4123                     0.7724        0.4711  0.0050  0.6358\n",
      "     14                     \u001b[36m0.8262\u001b[0m        \u001b[32m0.3949\u001b[0m                     0.7761        0.4729  0.0043  0.6348\n",
      "     15                     0.8224        0.3991                     0.7799        0.4571  0.0037  0.6355\n",
      "     16                     \u001b[36m0.8290\u001b[0m        \u001b[32m0.3933\u001b[0m                     0.7948        0.4475  0.0031  0.6344\n",
      "     17                     0.8276        \u001b[32m0.3807\u001b[0m                     0.7836        0.4651  0.0025  0.6339\n",
      "     18                     \u001b[36m0.8435\u001b[0m        \u001b[32m0.3752\u001b[0m                     0.7799        0.4644  0.0020  0.6350\n",
      "     19                     0.8388        \u001b[32m0.3697\u001b[0m                     0.7910        0.4584  0.0015  0.6347\n",
      "     20                     0.8360        \u001b[32m0.3690\u001b[0m                     0.7873        0.4600  0.0010  0.6354\n",
      "     21                     0.8322        0.3699                     0.7836        0.4535  0.0007  0.6346\n",
      "     22                     0.8411        \u001b[32m0.3615\u001b[0m                     0.7873        0.4604  0.0004  0.6343\n",
      "     23                     0.8402        0.3619                     0.7854        0.4594  0.0002  0.6344\n",
      "     24                     0.8430        0.3643                     0.7873        0.4582  0.0000  0.6350\n",
      "     25                     \u001b[36m0.8481\u001b[0m        0.3636                     0.7892        0.4569  0.0000  0.6349\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5458\u001b[0m        \u001b[32m0.7485\u001b[0m                     \u001b[35m0.6269\u001b[0m        \u001b[31m0.6490\u001b[0m  0.0100  0.6331\n",
      "      2                     \u001b[36m0.6953\u001b[0m        \u001b[32m0.5965\u001b[0m                     \u001b[35m0.7239\u001b[0m        \u001b[31m0.5434\u001b[0m  0.0100  0.6390\n",
      "      3                     \u001b[36m0.7472\u001b[0m        \u001b[32m0.5254\u001b[0m                     \u001b[35m0.7724\u001b[0m        \u001b[31m0.5009\u001b[0m  0.0098  0.6395\n",
      "      4                     \u001b[36m0.7645\u001b[0m        \u001b[32m0.4926\u001b[0m                     0.7687        \u001b[31m0.4705\u001b[0m  0.0096  0.6347\n",
      "      5                     \u001b[36m0.7771\u001b[0m        \u001b[32m0.4683\u001b[0m                     \u001b[35m0.7966\u001b[0m        \u001b[31m0.4673\u001b[0m  0.0093  0.6352\n",
      "      6                     \u001b[36m0.7869\u001b[0m        \u001b[32m0.4581\u001b[0m                     \u001b[35m0.8060\u001b[0m        0.4721  0.0090  0.6347\n",
      "      7                     0.7860        0.4600                     0.7705        0.4857  0.0085  0.6356\n",
      "      8                     \u001b[36m0.7897\u001b[0m        \u001b[32m0.4408\u001b[0m                     0.7892        \u001b[31m0.4646\u001b[0m  0.0080  0.6356\n",
      "      9                     \u001b[36m0.8103\u001b[0m        \u001b[32m0.4242\u001b[0m                     0.7966        \u001b[31m0.4511\u001b[0m  0.0075  0.6346\n",
      "     10                     0.8075        \u001b[32m0.4209\u001b[0m                     0.8004        \u001b[31m0.4400\u001b[0m  0.0069  0.6341\n",
      "     11                     0.8019        0.4241                     0.7873        0.4632  0.0063  0.6346\n",
      "     12                     0.8019        0.4240                     \u001b[35m0.8116\u001b[0m        0.4551  0.0057  0.6353\n",
      "     13                     \u001b[36m0.8107\u001b[0m        \u001b[32m0.4050\u001b[0m                     0.8004        0.4460  0.0050  0.6351\n",
      "     14                     \u001b[36m0.8243\u001b[0m        \u001b[32m0.3977\u001b[0m                     0.7929        0.4646  0.0043  0.6348\n",
      "     15                     \u001b[36m0.8360\u001b[0m        \u001b[32m0.3835\u001b[0m                     0.8041        0.4413  0.0037  0.6355\n",
      "     16                     0.8224        0.3887                     0.7985        \u001b[31m0.4363\u001b[0m  0.0031  0.6355\n",
      "     17                     0.8304        0.3873                     0.8041        0.4478  0.0025  0.6353\n",
      "     18                     \u001b[36m0.8364\u001b[0m        \u001b[32m0.3783\u001b[0m                     0.7966        0.4511  0.0020  0.6344\n",
      "     19                     0.8327        \u001b[32m0.3730\u001b[0m                     0.7985        0.4400  0.0015  0.6344\n",
      "     20                     0.8360        0.3752                     0.8022        0.4430  0.0010  0.6355\n",
      "     21                     0.8341        \u001b[32m0.3721\u001b[0m                     0.7985        0.4430  0.0007  0.6356\n",
      "     22                     0.8332        \u001b[32m0.3659\u001b[0m                     0.8060        0.4399  0.0004  0.6347\n",
      "     23                     \u001b[36m0.8379\u001b[0m        0.3717                     0.8078        0.4398  0.0002  0.6341\n",
      "     24                     0.8355        0.3665                     0.8060        0.4396  0.0000  0.6345\n",
      "     25                     0.8318        \u001b[32m0.3569\u001b[0m                     0.8041        0.4399  0.0000  0.6342\n",
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5850\u001b[0m        \u001b[32m0.7052\u001b[0m                     \u001b[35m0.6567\u001b[0m        \u001b[31m0.6317\u001b[0m  0.0100  0.6334\n",
      "      2                     \u001b[36m0.7178\u001b[0m        \u001b[32m0.5678\u001b[0m                     \u001b[35m0.7463\u001b[0m        \u001b[31m0.5529\u001b[0m  0.0100  0.6385\n",
      "      3                     \u001b[36m0.7453\u001b[0m        \u001b[32m0.5252\u001b[0m                     \u001b[35m0.7575\u001b[0m        \u001b[31m0.5094\u001b[0m  0.0098  0.6402\n",
      "      4                     \u001b[36m0.7682\u001b[0m        \u001b[32m0.4903\u001b[0m                     0.7500        \u001b[31m0.4980\u001b[0m  0.0096  0.6351\n",
      "      5                     \u001b[36m0.7776\u001b[0m        \u001b[32m0.4653\u001b[0m                     \u001b[35m0.8022\u001b[0m        \u001b[31m0.4770\u001b[0m  0.0093  0.6357\n",
      "      6                     \u001b[36m0.7860\u001b[0m        \u001b[32m0.4570\u001b[0m                     0.7780        0.4978  0.0090  0.6356\n",
      "      7                     \u001b[36m0.8051\u001b[0m        \u001b[32m0.4324\u001b[0m                     0.7799        \u001b[31m0.4765\u001b[0m  0.0085  0.6360\n",
      "      8                     \u001b[36m0.8117\u001b[0m        \u001b[32m0.4298\u001b[0m                     0.7817        0.4772  0.0080  0.6347\n",
      "      9                     0.8070        0.4308                     \u001b[35m0.8041\u001b[0m        \u001b[31m0.4655\u001b[0m  0.0075  0.6358\n",
      "     10                     0.8037        \u001b[32m0.4200\u001b[0m                     0.7817        \u001b[31m0.4623\u001b[0m  0.0069  0.6355\n",
      "     11                     \u001b[36m0.8168\u001b[0m        \u001b[32m0.4161\u001b[0m                     0.7817        0.4704  0.0063  0.6396\n",
      "     12                     \u001b[36m0.8350\u001b[0m        \u001b[32m0.3919\u001b[0m                     0.7910        \u001b[31m0.4620\u001b[0m  0.0057  0.6349\n",
      "     13                     0.8192        0.3944                     0.7929        0.4621  0.0050  0.6357\n",
      "     14                     \u001b[36m0.8388\u001b[0m        \u001b[32m0.3831\u001b[0m                     0.7854        0.4630  0.0043  0.6353\n",
      "     15                     0.8271        0.3892                     0.7966        \u001b[31m0.4480\u001b[0m  0.0037  0.6358\n",
      "     16                     0.8346        \u001b[32m0.3744\u001b[0m                     0.7873        0.4486  0.0031  0.6375\n",
      "     17                     0.8350        \u001b[32m0.3648\u001b[0m                     \u001b[35m0.8097\u001b[0m        \u001b[31m0.4320\u001b[0m  0.0025  0.6354\n",
      "     18                     \u001b[36m0.8416\u001b[0m        \u001b[32m0.3595\u001b[0m                     \u001b[35m0.8172\u001b[0m        0.4337  0.0020  0.6359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19                     0.8416        0.3662                     0.8060        0.4360  0.0015  0.6354\n",
      "     20                     0.8379        0.3700                     0.8041        \u001b[31m0.4291\u001b[0m  0.0010  0.6358\n",
      "     21                     \u001b[36m0.8430\u001b[0m        0.3608                     0.8116        \u001b[31m0.4278\u001b[0m  0.0007  0.6354\n",
      "     22                     \u001b[36m0.8453\u001b[0m        \u001b[32m0.3528\u001b[0m                     0.8116        0.4307  0.0004  0.6351\n",
      "     23                     \u001b[36m0.8458\u001b[0m        0.3552                     0.8097        0.4301  0.0002  0.6361\n",
      "     24                     \u001b[36m0.8463\u001b[0m        0.3533                     0.8078        0.4303  0.0000  0.6359\n",
      "     25                     \u001b[36m0.8486\u001b[0m        \u001b[32m0.3462\u001b[0m                     0.8078        0.4303  0.0000  0.6354\n"
     ]
    }
   ],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "preprocessing = \"medium\"\n",
    "model_name = \"eegnet\"\n",
    "model_folder = \"ModelComparision_EqualSamples\"\n",
    "n_epochs = 25\n",
    "n_splits = 10\n",
    "lr = 0.01\n",
    "\n",
    "for model_name in [\"shallow\", \"deep\", \"eegnet\"]:\n",
    "    for task in [\"N170\", \"N400\", \"P3\", \"N2pc\", \"MMN\", \"ERN\", \"LRP\"]:\n",
    "        df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "        # lowest number of samples is P3 condition 0 with 1338 samples\n",
    "        df = df.groupby([\"condition\"]).sample(1338)\n",
    "        data, labels = DataLoader.create_data_labels(df)\n",
    "        Training.run_exp(data, labels, task, preprocessing, model_folder, model_name, \n",
    "                lr, n_epochs, n_splits, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43866ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Model</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Validation Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N170</td>\n",
       "      <td>eegnet</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.716418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N400</td>\n",
       "      <td>eegnet</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.772388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3</td>\n",
       "      <td>eegnet</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.710821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N2pc</td>\n",
       "      <td>eegnet</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.652985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MMN</td>\n",
       "      <td>eegnet</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.600746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>P3</td>\n",
       "      <td>deep</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.740672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>N2pc</td>\n",
       "      <td>deep</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.654851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MMN</td>\n",
       "      <td>deep</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.56903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ERN</td>\n",
       "      <td>deep</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.880597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LRP</td>\n",
       "      <td>deep</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.817164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Task   Model Preprocessing Validation Balanced Accuracy\n",
       "0   N170  eegnet        medium                     0.716418\n",
       "1   N400  eegnet        medium                     0.772388\n",
       "2     P3  eegnet        medium                     0.710821\n",
       "3   N2pc  eegnet        medium                     0.652985\n",
       "4    MMN  eegnet        medium                     0.600746\n",
       "..   ...     ...           ...                          ...\n",
       "16    P3    deep        medium                     0.740672\n",
       "17  N2pc    deep        medium                     0.654851\n",
       "18   MMN    deep        medium                      0.56903\n",
       "19   ERN    deep        medium                     0.880597\n",
       "20   LRP    deep        medium                     0.817164\n",
       "\n",
       "[105 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Results\n",
    "results = []\n",
    "for model_name in [\"eegnet\",\"shallow\",\"deep\"]:\n",
    "    for preprocessing in [\"medium\"]:\n",
    "        for task in [\"N170\", \"N400\", \"P3\", \"N2pc\", \"MMN\", \"ERN\", \"LRP\"]:\n",
    "            df = Training.load_exp(model_folder, model_name, task, preprocessing, n_splits)\n",
    "            df[\"Model\"] = model_name\n",
    "            df[\"Preprocessing\"] = preprocessing\n",
    "            df[\"Task\"] = task\n",
    "            results.append(df.iloc[-1])\n",
    "df_results = pd.concat(results, axis=1)\n",
    "df_results = df_results.transpose().reset_index()\n",
    "# Put results in correct dataframe for seaborn plot\n",
    "df_task = df_results[[\"Task\", \"Model\", \"Preprocessing\",\"valid_balanced_accuracy\"]]\n",
    "df_task = df_task.rename(columns={\"valid_balanced_accuracy\": \"Validation Balanced Accuracy\"})\n",
    "list = []\n",
    "for i in range(3,8):\n",
    "    list.append(df_task.iloc[:,[0,1,2,i]])\n",
    "df_task = pd.concat(list, axis=0)\n",
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eaf76770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACtwAAAXICAYAAAAzivgvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAD2EAAA9hAHVrK90AAEAAElEQVR4nOzde5zVdZ0/8Pf3zF2GYbgpSCiKioiSabqaknax1M0So8xMu/3W1dIsyTubWpbttq6Vbqkl7aa7tW0mrpaulYlgXkMFEckbgjDD/SLIbWa+vz98zCzjGQ7DcM6cmTPP5+MxD+d8zvf9+b6+LguTvOYzSZqmaQAAAAAAAAAAAAAAHcoUOwAAAAAAAAAAAAAA9GQKtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkEN5sQMAPVeapsWOAAAAAAAAAAAAQIlLkqTYEXZI4RbIacWK9cWOAAAAAAAAAAAAQIkaMqS22BE6JVPsAAAAAAAAAAAAAADQkyncAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA7lxQ4AAAAAAAAAALArWlqaY86c2TFjxkMxb97cSNM0kiSJsWPHxYQJx8chh4yPTKas2DEBAOjFkjRN02KHAHqmNE1jxYr1xY4BAAAAAAAAsF3r1q2N66//bsyfP2+714wZMzYmT74s6uoGdGMyAAA6Y8iQ2kiSpNgxdihT7AAAAAAAAAAAAF3R2NgQU6ZckrNsGxExf/68mDLlkmhsbOimZAAAlBqFWwAAAAAAAACg10nTNG677eZYtmxpp65ftmxpTJ16S/hBwAAAdEV5sQMAAAAAAAAAAOysWbOejDlznt2pmdmzn4lZs56Kww8/okCp+paWluaYM2d2zJjxUMybNzfSNI0kSWLs2HExYcLxccgh4yOTKSt2TACAvFC4BQAAAAAAAAB6nUcemdHFuYcVbvNg3bq1cf3134358+dlvTdz5vSYOXN6jBkzNiZPvizq6gYUISEAQH5lih0AAAAAAAAAAGBnLVz4WpfmFi3q2hz/p7GxIaZMuaTDsu225s+fF1OmXBKNjQ3dlAwAoHCccAsAAAAAAAAA9DoNDYu7NLdkSdfmeEuapnHbbTfHsmVLO3X9smVLY+rUW+Lyy6+KJEkKnK7vaGlpjjlzZseMGQ/FvHlzI03TSJIkxo4dFxMmHB+HHDI+MpmyYscEgJKicAsAAAAAAAAA9DrNzc3dOsdbZs16MubMeXanZmbPfiZmzXoqDj/8iAKl6lvWrVsb11//3Q5PGJ45c3rMnDk9xowZG5MnXxZ1dQOKkBAASlOm2AEAAAAAAAAAAOgdHnlkRhfnHs5zkr6psbEhpky5pMOy7bbmz58XU6ZcEo2NDd2UDABKn8ItAAAAAAAAAACdsnDha12aW7Soa3P8nzRN47bbbo5ly5Z26vply5bG1Km3RJqmBU4GAH2Dwi0AAAAAAAAAAJ3S0LC4S3NLlnRtjv8za9aTMWfOszs1M3v2MzFr1lMFSgQAfYvCLQAAAAAAAAAAndLc3Nytc/yfRx6Z0cW5h/OcBAD6JoVbAAAAAAAAAADo4RYufK1Lc4sWdW0OAGhP4RYAAAAAAAAAAHq4hobFXZpbsqRrcwBAe+XFDgAAAAAAAAAA0JfV1lZFeXlZsWMUXH39bsWO0ClNTc2xfv3mYsfI0tzc3K1zAEB7CrcAAAAAAAAAAEVUXl4WFRWlX7jtC88IAJSuTLEDAAAAAAAAAAAAAEBP5oRbAAAAAAAAACAiImprq6K8vPRPIa2v363YEdopK3NeGgBAT6dwCwAAAAAAAABERER5eVlUVJR+4bYvPCMAAPmlcAsAAAAAAAAA0MM0b9kUG5cvLnaMvFq/+OViR8hSM3RElFVWFzsGANALKNwCAAAAAAAAAPQwG5cvjvn/cV2xY+RVT3yeMWdeHrUjRhc7BgDQC2SKHQAAAAAAAAAAAAAAejIn3AIAAAAAAAAA27Vp89ZY1Lim2DHy6sXXlhc7QjujRw6OTMaZacVQVT80a62sLBP19bsVIU3h9IbnaWpqjvXrNxc7BgBsl8ItAAAAAAAAALBdixrXxFX/+kCxY+RVT3ue2771yaip6h2F20wS0ZJ2ba4nylRUZq9lkshkyoqQpnAqKkrreQCgGHrHV2sAAAAAAAAAABTd0H4VXZrbvYtzAAA9hcItAAAAAAAAAACdMqw2+0TYQs4BAPQUCrcAAAAAAAAAAHTKu4bXdmnu0C7OAQD0FOXFDgAAAAAAAAAAQO8wdmhNHDC4Jv66cmOnZw4YXBNjh9YUMFV+bdq6OV5fvaTYMfLqpWWvFjtCO+8YuGdUV1QVOwYA7BSFWwAAAAAAAAAAOiVJkph40OD4yVONsWpj0w6vH1RTHhMPGhxJknRDuvx4ffWSuPa33y92jLzqac8z5W+/Gvvtvk+xYwDATskUOwAAAAAAAAAAAL3H4N0q4vyj9oxR9blPKB1VXxXnH7VnDN6topuSAQAUjhNuAQAAAAAAAADYKbWVZXHukcPjpZWbYlbD+nhl1cZIIyKJiH0H1cRhw2tjv8HVkelFJ9sCAOSicAsAAAAAAAAAwE7LJEkcMKQmDhhSU+woAAAFlyl2AAAAAAAAAAAAAADoyRRuAQAAAAAAAAAAACAHhVsAAAAAAAAAAAAAyEHhFgAAAAAAAADohZJungMAoC9TuAUAAAAAAAAAep2y6gFdmiuvrs9vEAAA+gSFWwAAAAAAAACg1ymvGditc1B0DnUGgKJSuAUAAAAAAAAAep2aQaO7NFc9aN88J4HuUV5X1aW5igFdmwMA2lO4BQAAAAAAAAB6ncoBI6OybsTOzdSNiMoBIwuUCAqrsr66S3MVXZwDANpTuAUAAAAAAAAAep0kSaL/XkdHWWVtp64vq6yN/nsdHUmSFDgZFMZu+wzo2tyors0BAO0p3AIAAAAAAAAAvVJ5VV0MOvCUqKjdI+d1FbV7xKADT4nyqrpuSgb5Vz2if1QP71zBvG1meG1Uj+hfoEQA0LeUFzsAAAAAAAAAAEBXZSpqYuABJ8WWNxpi08qXYssbjW3vVfYfFtWD94vK/sMjSZxJRu+WJEkMPHJ4LPvjgmhev3WH15fVVsTAI4c71RkA8kThFgAAAAAAAADo1ZIkE1V1I6KqbkSxo0BBlfevij1OHB0rpi+MLcvf3O51lUN3iyHH7RVl1apBAJAv/lQFAAAAAAAAAIBeoqy6PHY/YZ/YvHRDbHhlTWxeuiEi0ohIomqPftFv3/qo2qNfJBkn2wJAPincAgAAAAAAAABAL5JkkqgeXhvVw2uLHQUA+oxMsQMAAAAAAAAAAAAAQE+mcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5KNwCAAAAAAAAAAAAQA4KtwAAAAAAAAAAAACQg8ItAAAAAAAAAAAAAOSgcAsAAAAAAAAAAAAAOSjcAgAAAAAAAAAAAEAOCrcAAAAAAAAAAAAAkIPCLQAAAAAAAAAAAADkoHALAAAAAAAAAAAAADko3AIAAAAAAAAAAABADgq3AAAAAAAAAAAAAJCDwi0AAAAAAAAAAAAA5KBwCwAAAAAAAAAAAAA5lFzhdv78+cWOAAAAAAAAAAAAAEAJKS92gHz72Mc+FgcffHCcdtpp8ZGPfCTq6uqKHanX2rx5czzwwAPxxBNPxLPPPhsrV66MtWvXRnl5eQwYMCD23XffOOyww+KEE06IAw88sNhxt2vRokUxY8aMeOyxx+Lll1+ONWvWxNq1a6OmpiYGDhwYw4YNiyOOOCKOPvroOPzwwyNJkmJHBgAAAAAAAAAAAHqQkivcRkTMnTs35s6dG9/97nfjgx/8YEycODGOPfZYRcpO2rp1a/zkJz+Jn//857F69eoO39+4cWM0NjbGn//857jpppvib/7mb+KSSy6Jgw8+uAiJOzZ//vz48Y9/HP/7v/8bLS0tWe9v3bo11q1bF6+99lo8/vjjcdNNN8UBBxwQ5513Xpx00kl+vQAAAAAAAAAAAAAREZEpdoBCSdM0tmzZEvfdd1+cc845cfzxx8cNN9wQCxYsKHa0Hm3BggXxiU98In7wgx90WLbdnscffzw++clPxk033RRpmhYwYefcfvvt8fGPfzzuu+++Dsu22/PXv/41vva1r8V5550Xb7zxRgETAgAAAAAAAAAAAL1FyRZukySJJEkiTdNI0zSWLl0at956a5x00knx6U9/Ou68887YsGFDsWP2KC+//HJ85jOfiXnz5nVpvrm5OW688ca48sori1q6veqqq+Laa6+NrVu3dnmPP/3pTzFp0qRYtmxZHpMBAAAAAAAAAAAAvVF5sQMUWpIkbZ+3lkCffvrpePrpp+Paa6+NE088MSZOnBhHHnlksSL2CKtWrYovfOELsXz58g7fHzlyZLzrXe+KPfbYI9auXRsvv/xyzJo1q8Ni7Z133hnDhw+PCy64oNCxs9x6663xy1/+crvvjxo1Kg477LAYMmRIbNy4MRYsWBBPPPFEbN68OevaBQsWxHnnnRd33HFH1NTUFDI2AAAAAAAAAAAAvURLS3PMmTM7Zsx4KObNmxtpmkaSJDF27LiYMOH4OOSQ8ZHJlBU7JnlWcoXbD33oQ/GnP/2p7XTTbQu3rZ+3lkQ3btwY06ZNi2nTpsWIESPitNNOi1NPPTX23HPP7g9eZJdddlk0NjZmrb/jHe+Ia665Jo455ph2/y4j3iqkXn/99fHAAw9kzf3oRz+K97znPXH44YcXLPPbzZ07N/7lX/6lw/fGjRsXV1xxRbz73e/Oem/dunVxyy23xNSpU6OlpaXde88991x897vfjWuuuaYgmQEAAAAAAAAAAOg91q1bG9df/92YPz/7J8nPnDk9Zs6cHmPGjI3Jky+LuroBRUhIoWSKHSDffvjDH8bMmTNjypQpMW7cuEjTNOsU1iRJ2j5a33/99dfjxhtvjA9+8IPx+c9/Pu69994OTz0tRb///e9j+vTpWeuHHnpo3H333XHsscdmlW0j3jot9sYbb4zJkydnvdfS0hLf+ta3sgqshfS9732vwxN3P/jBD8YvfvGLDsu2ERF1dXVx8cUXx0033RQVFRVZ7//3f/93vPTSS3nPCwAAAAAAAAAApaClpTmeffbpuOmmG+LLX/5/8aUvfTG+/OX/FzfddEM8++zT0dLSXOyIkBeNjQ0xZcolHZZttzV//ryYMuWSaGxs6KZkdIeSK9xGRAwYMCA+85nPxJ133hn33ntvfP7zn4/Bgwd3qnzb0tISjz32WFx88cVx7LHHxlVXXRXPPvtskZ6k8Jqbm+P666/PWh8+fHj86Ec/itra2h3ucc4558RnP/vZrPV58+bF/fffn5ecO/LSSy/Fo48+mrU+ZsyYuP7666OqqmqHe3zgAx+Iyy+/PGu9ubk5pk6dmpecAAAAAAAAAABQStatWxvXXDMlrrvumpg5c3qsXLkiVq1aGStXroiZM6fHddddE9dcMyXWrVtb7KiwS9I0jdtuuzmWLVvaqeuXLVsaU6fe0uEhkvROJVm43dZ+++0Xl156aTz88MNx8803x4c+9KEoLy/PWb5tfe+NN96IX/3qV/GpT30qTj755Ljtttti+fLlRXqSwpg+fXq8+uqrWeuXXXZZDB48uNP7XHbZZXHQQQdlrd922227lK+zHnzwwQ7XL7300qiuru70PmeccUaMGTMma/2hhx7q1tN6AQAAAAAAAACgp3PaJ33JrFlPxpw5O3d45+zZz8SsWU8VKBHdreQLt60ymUwcf/zx8cMf/jBmzpwZU6ZMiXHjxnXq1Ns0TeOVV16Jf/7nf473ve99ce6558bvf//7aGpqKtLT5M9///d/Z62NGjUqPvzhD+/UPplMJr7yla9krT/33HMxf/78LufrrKeffjprbejQofGe97xnp/bJZDLxsY99LGt95cqV8eKLL3Y5HwAAAAAAAAAAlBKnfdLXPPLIjC7OPZznJBRLnyncbmvAgAHxmc98Ju68886499574/Of/3wMHjy4U+XbpqammD59enzlK1+JY489Nr7zne/ECy+8UKQn2TUbNmyIGTOyfxM47bTTIkmSnd7vuOOOi9133z1r/d577+1Svp2xYsWKrLUDDzywS8/xrne9q8P1pUs798UBAAAAAAAAAACUOqd90tcsXPhal+YWLeraHD1Pnyzcbmu//faLSy+9NB5++OG4+eab40Mf+lCUl5fnLN+2vrdmzZq4/fbbY+LEiTFx4sS44447Ys2aNcV5kC544oknYuvWrVnr73//+7u0X+spwm83ffr0Lu23M9auXZu1Vltb26W9Bg4c2OH6qlWrurQfAAAAAAAAAACUGqd90tc0NCzu0tySJV2bo+fp84XbVq1l0R/+8Icxc+bMmDJlShx88MFt5dpty7dvP/U2TdOYN29efPvb344JEybEhRdeGNOnT4+WlpYiPtGOPfroo1lr9fX1sf/++3d5z8MPPzxrbf78+QUvq9bU1GStrV69ukt7rV+/vsP1qqqqLu0HAAAAAAAAAAClxmmf9DXNzc3dOkfPo3DbgQEDBsRnPvOZ+PWvfx3/+7//G1/5yldi//33z1m+jYhI0zS2bt0aDzzwQJx77rlx3HHHxQ033BCvvdYz/5CYN29e1tohhxyyS3tub37OnDm7tO+OjBw5Mmvt+eef79JvVi+88EKH6yNGjNjpvQAAAAAAAAAAoBQ57RPoaxRud2DvvfeOL33pS3HPPffEb3/727j44ovj6KOPjvLy8rbibZqmHZ56u3z58rj11lvjpJNOii984Qvxhz/8oV1Zt9heeumlrLXRo0fv0p577bVXlJWVZa3Pnz9/l/bdkcMOOyxrbd26dfHwwzt/BP0999yTtVZTUxMHHnhgl7IBAAAAAAAAAECpcdon0Nco3O6E0aNHxxe/+MW4+uqr48tf/nLU1tZGRLQr2ba+3rZ829LSEo8++mhccMEFccIJJ8R//ud/xtatW4v5KLF69epYtWpV1vqoUaN2ad+KiooYPnx41vqiRYt2ad8d+du//dsOi77/8i//Eps3b+70Pn/84x/j8ccfz1o/8cQTo7KycpcyAgAAAAAAAAAAAL2Twm0nzZkzJ/7xH/8xTjzxxDjxxBPjBz/4QWzYsCGraPt2bz/19vXXX49vfetbceKJJ8ZDDz3UzU/xf5YuXdrh+h577LHLew8dOjRrbfHiwh4Fv8cee8SnPvWprPW//vWvcckll8SWLVt2uMdzzz0Xl156adZ6RUVFnHvuuXnJCQAAAAAAAAAAAPQ+5cUO0JO9/PLLce+998bvfve7WLhwYUREW7l2W9sWbd/+fut7b79m8eLFcd5558WnPvWpmDJlSoensxbSypUrO1wfMmTILu/d0R7bu18+XXTRRfHkk0/GX//613br999/fyxZsiSuuOKKeNe73pU1t3nz5vjFL34RN9xwQ2zatCnr/UsuuWSXT/4FAAAAAAAAAAAAei+F27dZvXp13HvvvTFt2rR4/vnnI6J9ibajU2xbr0mSJN7znvfExz/+8Vi3bl3cfffd8cwzz2TNtX6epmn88pe/jGXLlsWNN94YmUz3HTi8YsWKDtcHDBiwy3v3798/a+2NN97Y5X13pLa2Nm677bY499xzY+7cue3emz17dnzqU5+KffbZJ971rnfFkCFDYuvWrfH666/HY4891mG+JEniK1/5Spx99tkFzw4AAAAAAAAAALW1VVFe3r0H9xVDff1uxY6wQ01NzbF+/eZixwB6EIXbiNi6dWs8+OCDMW3atJgxY0Y0Nzd3umQbEbHnnnvGxIkT47TTTosRI0a0vX/GGWfEggUL4le/+lXcddddsXr16kiSpK2c2/r5gw8+GN///vfjoosuKuyDbmP9+vUdrvfr12+X995tt+w/ENetW7fL+3bG7rvvHr/4xS/i5ptvjp/+9KexZcuWdu+/+uqr8eqrr+5wn5EjR8ZVV10VEyZMKFTUXqM3fIEDAAAAAAAA5EdZWfcdFAWwrbKyjI5CvPXvIZPpuKtUSioqen6puKws0yfKz3QPv7+Vhj5duH366adj2rRpcf/997cVQjtbtK2oqIgPfOADMWnSpDjmmGO2e+2oUaPikksuiQsvvDCmTp0aN910U7S0tLS7R5qmMXXq1Jg0aVLstddeeXzC7du6dWuH69XV1bu8d0d7bN7cfd/tUVVVFRdeeGFMnDgxLrjggnjhhRd2av7v/u7v4sILL4yKiooCJew9kiTpFV/gAAAAAAAAAAC9WyaTRCajo0DP4dck+aSDVRr6XOH29ddfj2nTpsX//M//xKJFiyKi8yXbiIj9998/Jk2aFB/72Meivr6+0/etqqqK8847L/bdd9+48MIL2510GxHR3Nwc//3f/x2TJ0/u4pPtnLef/NqqvHzXf0mUlWX/5tDU1LTL+3ZWY2Nj3HTTTXHPPffEpk2bdnr+Jz/5STz99NPxta99Ld797ncXICEAAAAAAAAAAADQm/SJwu369evjvvvui2nTpsWsWbMiYudKtrW1tXHyySfHpEmTYvz48buU5cMf/nCccsopcc8992Td97HHHtulvXfG9k647agsu7M6Ku22tLRES0tLZDKF/fEjd955Z3zzm9/sUtF2W0899VSceeaZMXHixLj66qvzcvIvAAAAAAAAAAAA0DuVbOG2paUlZsyYEdOmTYsHH3yw7UTX1hLt9kq2217z7ne/OyZNmhQnnnhiXguXp512Wtxzzz1tr1tPu33ttdfydo8d2VHJeFc0Nzd3eL9Cl21/8IMfxI9+9KMO3+vfv3+cdtppcfzxx8fo0aNj4MCBsWHDhli6dGk89thjce+998acOXOy5u6666548cUX42c/+1nU1dUVND8AAAAAAAAAALzdps1bY1HjmmLHyKsXX1te7AjtjBxWH9VVFcWOAfRwJVe4nTdvXkybNi1++9vfxsqVKyNi506zHTJkSEycODE+/vGPx6hRowqS8YADDuhwfePGjQW5X0c6OoU2ouOy7M7qaI/Kyspd3jeX3/zmN9st206cODEuvfTSGDhwYFamgQMHxoEHHhif+9zn4oEHHoh/+Id/iDVr1rS77rnnnosLLrggfvrTn0ZFRd/6gzVN02hqail2DAAAAAAAAKCblJVlIpPZ/gFWAIXS0pJGc7OOQke/Dy9qXBNX/esDRUpUGD3tea758odi/72Htlvza5J82rp113t5pay8PJPzENWeouQKtxMnTmw7MbbVjk6zLS8vj/e+970xadKkOO6446KsrKygGQcMGNDh+uDBgwt6321trwDb1NS0y3t3tEchC7crVqyI73znOx2+d+GFF8aXvvSlTu3zoQ99KMaOHRtf+MIXYuHChe3ee+yxx+JnP/tZnHPOObuct7dZs+bNYkcAAAAAAAAAukl9/W6RyRT278wBOtLc3KKjEH4f7kn8miSf/FrKbciQ2mJH6JRMsQMUSpIkbR9vl6ZppGkae++9d0yePDkeeuih+NGPfhTvf//7C162jYh44403Osw7evTogt+7VW1tx79A33xz1/8fu6M9qqurd3nf7fnJT37S4b/T0047rdNl21YjR46Mm2++Ofr375/13o9//ONYtWpVl3MCAAAAAAAAAAAAvVPJFm7frrVkW11dHRMnTow77rgj7r///vi7v/u7GDJkSLdmWbRoUbtMrR8TJkzotgz19fUdrq9fv36X996wYUPW2sCBA3d5345s2bIl7r777qz1+vr6uPLKK7u05+jRo+OCCy7IWn/zzTdj2rRpXdoTAAAAAAAAAAAA6L3Kix2g0NI0jYiI8ePHx6RJk+Jv//Zvo1+/fkXN9Nprr8Whhx4aw4cPj2HDhsXw4cNj+PDhcdRRR3Vbhu0VbtesWbPLe69evTprbdCgQbu8b0fmzp3b4f3OPPPM7Z7i2xlnnHFG3HLLLbFy5cp26/fff3984Qtf6PK+AAAAAAAAAAAAvKW2tirKywv/U+mLrb5+t2JH2KGmpuZYv35zsWP0aCVbuE3TNAYOHBgf/ehHY9KkSbH//vsXO1KbU045JU455ZSiZhgxYkSH6ytWrNjlvTvao1CnCM+ZM6fD9eOPP36X9q2srIwJEyZknWj7/PPPx5YtW6KysnKX9gcAAAAAAAAAgN4tiYi0i3PwlvLysqioKP3CbV94xr6g5Aq3SZLEMcccE5MmTYoPfOADUVFRUexIPdIee+wRFRUVsXXr1nbrDQ0Nu7z3kiVLstb22muvXd63I42NjVlrSZLEQQcdtMt7H3rooVmF261bt8aSJUti1KhRu7w/AAAAAAAAAAD0VmXVA6J505qdniuvrs97FoDuUHKF2z/96U8xbNiwYsfo8TKZTOy9997x0ksvtVtfsGDBLu27atWqeOONN7LW99tvv13ad3s6uteAAQOivHzXf2lv71TedevW7fLeAAAAAAAAAADQm5XXDOxa4bZmYP7DAHSDTLED5JuybeeNGzcua23+/Pm7tOcLL7zQ4XqhCrdVVVUF2Tcioqys42O8k8Sx9gAAAAAAAAAA9G01g0Z3aa560L55TgLQPUruhNsd2bJlSzzxxBNxxBFH7FRZ89prr41XXnklDj/88DjuuOPi4IMPLmDK7jF+/Pi4++67263Nmzcvtm7dGhUVFV3a89lnn81aq6urK1jhdvDgwVlr69atizfffDN22223Xdp75cqVHa5v7+RbAAAAAAAAAADoKyoHjIzKuhGxZd3izs/UjYjKASMLmIpSsGnr5nh99ZJix8irl5a9WuwI7bxj4J5RXVG4wy5LVZ8p3D7xxBPxs5/9LP785z/Hli1b4te//nWHJ7xuz/z58+PJJ5+MRx99NG666aY46KCD4qyzzopTTz21cKEL7Oijj85a27hxY8yaNSv+5m/+pkt7zpw5M2vtb/7mb7Z7Wuyu2n333bPWWlpaYvbs2XHUUUft0t7PP/981lomk+mw5AsAAAAAAAAAAH1JkiTRf6+jY81f74/mLet3eH1ZZW303+toP12aHXp99ZK49rffL3aMvOppzzPlb78a++2+T7Fj9DqZYgcotEWLFsWZZ54Zn/3sZ+Ohhx6KzZs3R0TEq6/uXGN88eLFkSRJpGkaaZrG3Llz4/LLL48zzjgjXnnllUJEL7jRo0fHiBEjstZ/+9vfdmm/hoaGePrpp7PW3/ve93Zpv8448sgjO1y///77d2nfNE3j4YcfzlofP358VFZW7tLeAAAAAAAAAABQCsqr6mLQgadERe0eOa+rqN0jBh14SpRX1XVTMoD8K+nC7f333x+nnnpqzJo1q60o22pnCrfNzc2xdOnSiHjrOzNav8siTdN4+umnY+LEifHQQw/lNXt3OeWUU7LW7rnnnli1atVO7/Xzn/88mpub263V1NTEySef3OV8OzJy5MjYd999s9bvvvvuWLlyZZf3vffee+P111/PWj/++OO7vCcAAAAAAAAAAJSaTEVNDDzgpKjf/8NRPWh0ZCr6tX1UDxod9ft/OAYecFJkKmqKHRVgl5Rs4fb3v/99TJ48OTZs2BBpmrYrykZELFiwoNN7NTQ0ZBVJW/dLkiQ2b94cF1xwQfz+97/PV/xuM2nSpMhk2v8yePPNN+Pb3/72Tu3z/PPPx+233561fvLJJ0dtbe0uZdyRiRMnZq29+eabce2113ZpvxUrVsQ///M/Z62Xl5cXtDwMAAAAAAAAAAC9UZJkoqpuRAzY57gYOv70to8B+xwXVXUjIklKtqYG9CEl+TvZX//617jooouiubm5rRTberpt6+c7c8Lt2rVro7a2NuuU3FZJksTWrVvj4osvjpdffjlvz9EdRo4cGR/5yEey1u+99964+eabO7XH8uXL47zzzoutW7e2W6+oqIjzzjsvLzlzOeuss2Lo0KFZ67/73e/ihhtu2Km91q5dG1/+8pejsbEx671PfOITsffee3c5JwAAAAAAAAAAANA7lVzhNk3TuPLKK2Pr1q3tTrRtLdpmMpk4/vjj4+yzz+70nuPGjYunnnoq7rvvvrjoooti3333zSreJkkSmzZtiq9//evR1NSUt+fpDl/5yleipib7yPYbbrghrrvuuqwi7baee+65OP300zssqJ555pkxcuTITmW48cYbY8yYMVkfl1122Q5na2pqYvLkyR2+d/PNN8cFF1wQS5cu3eE+zzzzTHzyk5+MZ555Juu9+vr6OP/883e4BwAAAAAAAAAAAFB6Sq5we99998WcOXPalW0j3irinnjiiXH//ffHzTffHKeeeupO773PPvvEOeecE/fee29cc801sdtuu2Vd88ILL8Rdd93V1fhFMXLkyO0WVv/t3/4tTjnllLj99tvj5Zdfjo0bN8aKFSvi8ccfj0svvTQ+9alPxeLFi7PmDjjggPja175W6OhtJk6cGJ/5zGc6fO+BBx6IE044IS6//PL44x//GEuXLo2tW7fG+vXrY8GCBXHnnXfGOeecE6effnosWLAga76ioiJuuummGDJkSIGfAgAAAAAAAAAAAOiJyosdIN9+8YtftHvdeqrtlClT4swzz8zLPZIkidNPPz3Gjx8fX/ziF2P16tVt62maxtSpU+MTn/hEXu7VXc4666yYO3duh2XhV199Na699tpO7zVw4MD4/ve/H9XV1fmMuENXXHFFNDY2xh/+8Ies9zZv3hy/+c1v4je/+c1O7VleXh7f+c534ogjjshXTAAAAAAAAAAAAKCXKakTbpcuXRpPPvlk2+m2aZpGkiTx5S9/OW9l222NHTs2vve972WtL1iwIJ566qm836/Qrr322jjttNN2aY8hQ4bEz3/+8xg9enSeUnVeWVlZ3HjjjXHuuedmnXDcFYMHD46f/exn8dGPfjQP6QAAAAAAAAAAAIDeqqQKt88880zW2t577x3nnXdewe55zDHHxAknnBBpmrZbf+yxxwp2z0IpLy+P6667Lr75zW9G//79d3r+fe97X9x9991xwAEHFCBd52Qymfja174WP/nJT2LcuHFd2qO8vDxOPfXUuOuuu+LII4/Mc0IAAAAAAAAAAACgtykvdoB8mjNnTtvnrafbnnnmmZHJFLZXfPbZZ8cDDzzQbu0vf/lLQe9ZSKeffnp8+MMfjjvuuCPuuuuueP3117d7bWVlZUyYMCHOPvvsOOqoo7oxZW4TJkyICRMmxJ/+9Ke466674vHHH481a9bknBk9enQcc8wxcdZZZ8Vee+3VPUEBAAAAAAAAAACAHq+kCrfLli3LWjv66KMLft9DDz00ampqYtOmTZEkSaRpGkuWLCn4fQupvr4+zj///Dj//PNj4cKF8cILL8SSJUvizTffjOrq6hgwYECMGjUqDjnkkKisrNzl+11wwQVxwQUX5CF5e+973/vife97X7S0tMQLL7wQr7/+eqxevTrWrFkTFRUVMXDgwBg0aFCMHTs2dt9997zfHwAAAAAAAAAAAOj9Sqpwu27duqy1Pffcs+D3LS8vj+HDh8err77atraj01R7k7322qvXn/iayWTioIMOioMOOqjYUQAAAAAAAAAAAIBeJlPsAPn05ptvZq2Vl3dPp7i2tjbSNG17vWHDhm65LwAAAAAAAAAAAACFVVKF2379+mWtLV++vFvuvWbNmkiSpO11RUVFt9wXAAAAAAAAAAAAgMIqqcJtfX191tqrr75a8Ps2NTXFsmXL2q0NHDiw4PcFAAAAAAAAAAAAoPBKqnC73377Za09+OCDBb/vE088EZs2bYqIiDRNI0mSeMc73lHw+wIAAAAAAAAAAADdIOnmOXqckircHnTQQW2fJ0kSaZrGfffdF+vXry/ofX/zm99krY0bN66g9wQAAAAAAAAAAAC6R3ldVZfmKgZ0bY6ep7zYAfLp8MMPj379+sWbb77ZtrZmzZq48cYb4/LLLy/IPZ9++un47W9/G0nSvoZ+1FFHFeR+AAB0rKWlOebMmR0zZjwU8+bNbfvJA2PHjosJE46PQw4ZH5lMWbFjAgAAAAAAANALVdZXR9PazTs9V1FfXYA0FENJFW4rKyvj+OOPbyvAtp5ye/vtt8e4cePiox/9aF7v19DQEJMnT24rc7Sqq6uLY445Jq/3AgBg+9atWxvXX//dmD9/XtZ7M2dOj5kzp8eYMWNj8uTLoq5uQBESAgAAAAAAANCb7bbPgHjztbU7PzfK31GXikyxA+TbZz/72XavkySJlpaWuOKKK+IXv/hF3u7z17/+NT73uc/FkiVL2sq2rcXbT37yk1FeXlJdZgCAHquxsSGmTLmkw7LttubPnxdTplwSjY0N3ZQMAAAAAAAAgFJRPaJ/VA+v3bmZ4bVRPaJ/gRLR3UquFTp+/Pg4+uij49FHH2074TZJkmhqaopvfvOb8Yc//CEuuOCCOPTQQ7u0//Lly+OOO+6IqVOnRlNTU7uTbSPeOt3285//fB6eBAB6rpaW5pgzZ3bMmPFQzJs3t+3P27Fjx8WECcfHIYeMj0ymrNgx6QPSNI3bbrs5li1b2qnrly1bGlOn3hKXX35V1tdxAAAAAAAAALA9SZLEwCOHx7I/Lojm9Vt3eH1ZbUUMPHK4v5suISVXuI2IuOqqq+LUU0+NzZs3t/1ibS3f/vnPf44///nPsc8++8QJJ5wQ48aNi7Fjx8awYcOioqKi3T5pmsbKlSvjr3/9a7zwwgvxyCOPxGOPPRYtLS2Rpmnbvq3XJkkSX//612PQoEHd+8AA0I3WrVsb11//3Q5PE505c3rMnDk9xowZG5MnXxZ1dX4sAoU1a9aTMWfOszs1M3v2MzFr1lNx+OFHFCgVAAAAAAAAAKWovH9V7HHi6FgxfWFsWf7mdq+rHLpbDDluryirLsmKZp9Vkv/XHDVqVFx++eVx1VXtTy5rLd1GRLzyyitx6623tpurqKiI2trayGQysWHDhti0aVPW3m8v2m6798c+9rH4xCc+ke/HAYAeo7GxIb7znat3eJro/PnzYsqUS+KKK66OYcOGd1M6+qJHHpnRxbmHFW4BAAAAAAAA2Gll1eWx+wn7xOalG2LDK2ti89INEZFGRBJVe/SLfvvWR9Ue/SLJONm21JRk4TYi4vTTT4+VK1fGD3/4w6xTaFu1lmdbbdmyJVatWpVz346Od07TNN7//vfHt7/97TwkB4CeKU3TuO22m3dYtm21bNnSmDr1lrj88qv8eAQKZuHC17o0t2hR1+agJ2lpaY45c2bHjBkPxbx5c9v+987YseNiwoTj45BDxkcmU1bsmAAAAAAAAFBykkwS1cNro3p4bbGj0I1KtnAbEfGlL30phg0bFt/85jdj06ZNHZ5KuytaC7uf/vSn48orr4yyMn+ZDUDpmjXryZgz59mdmpk9+5mYNespJ4lSMA0Ni7s0t2RJ1+agp1i3bm1cf/13Y/78eVnvzZw5PWbOnB5jxoyNyZMvi7q6AUVICAAAAAAAAFBaMsUOUGinnXZa3HnnnXHcccdFmqZZp9rmsr1rW/d5xzveEbfcckt84xvfULYFoOQ98siMLs49nOck8H+am5u7dQ56gsbGhpgy5ZIOy7bbmj9/XkyZckk0NjZ0UzIAAAAAAACA0lXyhduIiNGjR8ctt9wSv/zlL+PUU0+N6urqttJsrgJu6wm4216bpmkcfPDBce2118b9998fxx13XHc9BgAU1cKFr3VpbtGirs0BkC1N07jttptj2bKlnbp+2bKlMXXqLTv1jYcAAAAAAAAAZCsvdoDudOihh8ahhx4aV199dTz11FPx5JNPxnPPPRcLFy6MhoaGaGpqyprp379/jBgxIsaMGRPvfOc7Y8KECTFy5MgipAeA4mpoWNyluSVLujYHQLZZs56MOXOe3amZ2bOfiVmznorDDz+iQKkAAAAAAAAASl+fKty2qq6ujmOPPTaOPfbYtrU0TePNN9+MjRs3RlNTU1RWVka/fv2iqqqqiEkBoOdobm7u1jkAsj3yyIwuzj2scAsAAAAAAACwC/pk4bYjSZJEv379ol+/fsWOAgAA0KGFC1/r0tyiRV2bAwAAAAAAAOAtmWIHAAAAoHMaGhZ3aW7Jkq7NAQAAAAAAAPAWhVsAAIBeorm5uVvnAAAAAAAAAHiLwi0AAAAAAAAAAAAA5KBwWwBr1qyJ9evXFzsGAAAAAAAAAAAAAHmgcJsnTU1N8eSTT8a3vvWteN/73hevv/56sSMBAAAAAAAAAAAAkAflxQ7QXZYtWxbPP/98LF++PDZs2BBbtmyJ5ubmaGlpiTRNdzifpmk0NzdHU1NTbN26NTZv3hwbN26MdevWxYoVK+Kll16KTZs2dcOTAAAAAAAAAAAAANCdSr5we+edd8avf/3rePbZZztVrO2Kt++bJElB7gMAAABAz9DS0hxz5syOGTMeinnz5kaappEkSYwdOy4mTDg+DjlkfGQyZcWOCQAAAAAA5EnJFm4XL14cF198cTz99NMRkV2KzbfWkm2h7wMAAD2JshEAfdG6dWvj+uu/G/Pnz8t6b+bM6TFz5vQYM2ZsTJ58WdTVDShCQgAAAAAAIN9KsnC7atWqOOuss6KhoaGtAOvUWQAAyC9lIwD6osbGhvjOd66OZcuW5rxu/vx5MWXKJXHFFVfHsGHDuykdAAAAAABQKJliByiEq6++OpYsWRIRbxVtd1S2TdO07WNHtr22o4+IiJqamthtt912/UEAAKCHamxsiClTLumwbLut1rJRY2NDNyUDgMJJ0zRuu+3mHZZtWy1btjSmTr3FT0QCAAAAAIASUHIn3M6ePTseeOCBtpLt20+43dFfcOR6f9vybuuPym3954EHHhgHHnhgHHnkkfHhD39Y4RYAgJLV1bLR5Zdf5SdPABRAS0tzzJkzO2bMeCjmzZvb9t8qxo4dFxMmHB+HHDI+MpmyYscsCbNmPRlz5jy7UzOzZz8Ts2Y9FYcffkSBUgEAAAAAAN2h5Aq3P//5z9s+b/0LptbPBwwYEMccc0wceOCBMWDAgKisrIw//elPbQXdNE2jrKwsrrrqqqioqIimpqbYvHlzrFixIhYtWhRPP/10u5Nzt3XaaafFWWed1X0PCgBQILW1VVFeXvqlnPr6nv8NUk1NzbF+/eZix8iibATQc6xbtzauv/67HZ44PnPm9Jg5c3qMGTM2Jk++LOrqBhQhYWl55JEZXZx72J+BAAAAAADQy5VU4XbLli3x4IMPtpVhtz2B9qyzzoqvfe1rWSfPvuMd74gHHnig7XVLS0sMHTo03ve+93V4j1mzZsUNN9wQTz75ZNuJt2maxj/+4z/G+PHj453vfGfhHhAAoBuUl5dFRUXpF277wjMWirIR0BGnrHa/xsaG+M53rt7hiePz58+LKVMuiSuuuDqGDRveTelK08KFr3VpbtGirs0BAAAAAAA9R0kVbmfPnh1vvvlmu1NtkySJiRMnxpVXXtnhzGGHHRY1NTWxadOmtrU//vGP2y3cHnbYYXH77bfHrbfeGjfccENEvFXsbWpqiksvvTTuvvvuqKqqyvOTAb2BggEAfYWyEfB2Tlntfmmaxm233bzDsm2rZcuWxtSpt8Tll1+V9VN76LyGhsVdmluypGtzAAAAAABAz5EpdoB8ev7557PWampq4oorrtjuTFlZWRx88MFtxbg0TeOxxx7b4b3OOeecuPTSSyNN07a11157LX70ox91LTzQq61btzauuWZKXHfdNTFz5vRYuXJFrFq1MlauXBEzZ06P6667Jq65ZkqsW7e22FEBYJcpGwHbamxsiClTLumwbLut1lNWGxsbuilZaZs168mYM+fZnZqZPfuZmDXrqQIl6huam5u7dQ4AAAAAAOg5SuqE24ULF7Z93lqgPemkk6K2tjbn3Dvf+c548skn214vXrw4GhsbY9iwYTnnPve5z8VTTz0Vf/jDH9rKuv/+7/8en/70p2OPPfbYtYcBeg0/xhWAvqbUyka1tVVRXl76p9DX1+9W7Ag71NTUHOvXby52DHaCU1aL55FHZnRx7uE4/PAj8pwGAAAAAACg9JVU4Xb58uVZa+95z3t2OHfggQdmrc2ZM2eHhduIiClTpsRDDz3UVh7YvHlz/Nu//VtceumlnUgM9HYKBuwqJa+eRdFr+zZt3hqLGtcUO0Zevfha9teOxTRyWH1UV1UUO0afVF5eFhUVpf97cV94RrrfrpyyqvS5axYufK1Lc4sWdW0OAAAAAACgryupwu2GDRuy1g466KAdzu23335Za88991yccMIJO5wdNmxYnHzyyfE///M/bafc/uY3v4nJkydHeXlJ/esFOqBgwK5S8qK3WNS4Jq761weKHSOvetrzXPPlD8X+ew8tdgyAneKU1eJpaFjcpbklS7o2BwAAAAAA0NeVVCN08+bsE+kGDx68w7l99903MplMpGnatjZ//vxO37e1cNtq3bp18dhjj8Wxxx7b6T2A3knBAIB82X1QbdZaWVmm15wQ3Vk98XnKyjLFjgC9llNWi6f1J+101xwAAAAAAEBfV1KF27Ky7NPzamuziwtvV1lZGcOGDYuGhoa2U2pfffXVTt/3iCOOyPrR8I8//rjCLfQBCgYA5EtlZfaX5plMEplMaZ0Q7cRrKC1OWQUAAAAAAKCvKKnCbXV1ddba1q1bo6qqaoezI0eOjCVLlrQVZ19//fXYunVrVFRU7HC2X79+MXz48GhoaGhbmzt37k4kB3orBQMAKE2btm6O11cvKXaMvHppWee/qbA7vGPgnlFdseP/rUbP5pRVAAAAAAAA+oqSKtzW1dVlrW3YsKHThdvHH3+87XVLS0ssWLAg9t9//07du76+vq2wm6ZpLFq0qPPBgV5LwYBCUPLqHopeQC6vr14S1/72+8WOkVc97Xmm/O1XY7/d9yl2DAAAAAAAAIBOKanC7bBhw7LWFi5cGIMGDdrh7MiRI7PWXn755U4Xbt9+Eu7KlSs7NQcAb6fk1T0UvQAAAAAAAAAA6KySKtzus092aeb555+PQw89dIezHRVu582bFyeeeGKn7r1+/fpIkqTt9ebNmzs1BwAA29O8ZVNsXL642DHyav3il4sdIUu/4ftEkskUOwYAAAAAAAAAPVhJFW7HjRuXtfb73/8+Pv3pT+9wtqOy7tNPP92p+zY1NcXrr7/ebq26urpTswAAsD0bly+O+f9xXbFj5FVPfJ5Dv3pTlFX6+h2gWGprq6K8vKzYMQquvn63YkfYoaam5li/3jeRAwAAAABAR0qqcLv//vvHwIEDY82aNZEkSaRpGo899lg8/fTT8a53vSvn7L777htlZWXR0tLSNvvMM8/E+vXro7a2NufsM888E5s3b253wu2AAQPy8kwAAAAApay8vCwqKkq/cNsXnhEAAAAAAEpZSf3c1CRJ4vjjj480Tdtep2kaX/3qV2PRokU5Z6uqqmK//fZrm42I2Lp1a/zHf/zHDu97++23t32epmkkSRJDhw7t4lMAAAAAAAAAAAAA0JOUVOE2IuLjH/94u9dJksTSpUvjtNNOi1/96lfR1NS03dn3vve97ebSNI2bb745Xnjhhe3OTJs2Lf73f/+33em2ERGHHHJIF58AAAB6tkyy42vyOQcAAAAAAAAAxVZyhdt3v/vdcfDBB7e9bj1x9o033oirrroqPvjBD8Y///M/d3ji7SmnnNLudZIksXHjxjjrrLPiN7/5Tbuy7vr16+P73/9+XHnllVll24iIww47LI9PBQAAPcfQfhVdmtu9i3MAAAAAAAAAUGzlxQ5QCJdffnl85jOfiYhoK8O2nljb2NgYt912W+y///4xcuTIdnMHHHBAHHroofHss89GRPuy7pVXXhnXXXdd7L333tHc3Bwvv/xybN26te2abdXV1cUHPvCBbnhSAADofsNqK2Pp+q1dmgOAzti0dXO8vnpJsWPk1UvLXi12hHbeMXDPqK6oKnYMAAAAAADoNUqycHv44YfHueeeGz/+8Y/blWFbS7cREXvttVeHsxdffHGceeaZHc698cYb8dxzz7W7vvW9bf85adKkqKryFxYAAJSmdw2vjWcbN+z03KHDawuQBoBS9PrqJXHtb79f7Bh51dOeZ8rffjX2232fYscAAAAAAIBeI1PsAIVy4YUXxplnntlWsH277RVuDz/88PjEJz7RrkQb8Vax9u0l3G1Pz23958iRI+P888/P56MAAECPMnZoTRwwuGanZg4YXBNjh+7cDAAAAAAAAAD0FCVbuI2I+Id/+If47ne/GwMGDGhXvK2pqYnBgwdvd+6qq66K97znPW2l24josHi77Z5pmkZtbW3ccMMNUVOjSAAAQOlKkiQmHjQ4BtV07gdmDKopj4kHDW73DWwAAAAAAAAA0JuUdOE2IuLUU0+NP/7xj3HRRRe1nWo7cuTInDPl5eXx4x//OM4444x2Rdu327Z4O3z48Ljjjjti3LhxeX4CAAC2r6sFTsXPXTV4t4o4/6g9Y1R9Vc7rRtVXxflH7RmDd6vopmQAAAAAAAAAkH+dO5Kql+vXr1+cc845cc4558SLL74YDQ0NO5ypqqqKq666Kj74wQ/GjTfeGM8880yH1w0ePDg+9alPxf/7f//PybYAAN2srHpANG9as9Nz5dX1ec/SF9VWlsW5Rw6Pl1ZuilkN6+OVVRsjjbfqzPsOqonDhtfGfoOrI+NkWwAAAAAAAAB6uT5RuN3W/vvvH/vvv3+nrz/mmGPimGOOiSVLlsTTTz8dDQ0NkaZpDB48OEaPHh3jx4/3o3Ehj2prq6K8vKzYMbpFff1uxY6wQ01NzbF+/eZixwDYrvKagV0r3NYMzH+YPiqTJHHAkJo4YIhvPusWSUSkXZwDAAAAAAAAoMv6XOG2q/bcc8/Yc889ix0DSl55eVlUVPSNwm1feU6AQqoZNDo2r351p+eqB+1bgDRQeOV1VdG0due/GaZiQFUB0gAAAAAAAAD0HSVXuP3zn/8cV199dUyaNClOPfXU2H333YsdCQCAAqkcMDIq60bElnWLOz9TNyIqB4wsYCoonMr66q4VbuurC5AGSouftlE8ZWWZYkcAAAAAAADYoZIr3F533XWxcOHCuOGGG+KHP/xhHHvssfGJT3wi3ve+90Um4y9wAABKSZIk0X+vo2PNX++P5i3rd3h9WWVt9N/r6EiSpBvSQf7tts+AePO1tTs/N2pAAdJAafHTNgAAAAAAAMilpBqoM2fOjBdffDGSJIk0TaOpqSkeeuihuPLKK2PTpk3FjgcAQAGUV9XFoANPiYraPXJeV1G7Rww68JQor6rrpmSQf9Uj+kf18NqdmxleG9Uj+hcoEQAAAAAAAEDfUFIn3P7ud79r+7y1dJskSUyaNCl2261n/bhEoPM2bd4aixrXFDtG3r342vJiR2hn5LD6qK6qKHYMgC7JVNTEwANOii1vNMSmlS/Fljca296r7D8sqgfvF5X9h0eSlNT3m9EHJUkSA48cHsv+uCCa12/d4fVltRUx8MjhTnUGAAAAAAAA2EUlVbj9y1/+0uFfJH/gAx8oQhogXxY1romr/vWBYsfIu572TNd8+UOx/95Dix2DniyJiLSLc9ANkiQTVXUjoqpuRLGjQEGV96+KPU4cHSumL4wty9/c7nWVQ3eLIcftFWXVJfU/+wCKy9fEAAAAAADQZ5XU37wuW7asw/X999+/m5MAQOkpr6uKprWbd3quYkBVAdIA9G1l1eWx+wn7xOalG2LDK2ti89IN8VYDLImqPfpFv33ro2qPfpFkNLx6o9raqigvLyt2jIKrr+9ZP4mmrCz7FPTmLZti4/LFRUhTWOsXv1zsCO30G75PJJnecQq9r4kBAAAAAKDvKqnC7fZUV1cXOwIA9HqV9dVdKxfU+3MYoBCSTBLVw2ujenhtsaOQZ+XlZVFRUfqF297wjBuXL475/3FdsWPkXU97pkO/elOUVfaOrxl9TQwAAAAAAH1X7zg+pJP22WefSNPsn+u3eHHpnUYDAN1tt30GdG1uVNfmAACgp/E1MQAAAAAA9F0lVbg94YQTOlz/4x//2M1JAKD0VI/ov9OnKFYPr43qEf0LlAgAALqXr4kBAAAAAKDvKqnC7VlnnRVDhgxpe50kSaRpGj/5yU9i1apVRUwGAL1fkiQx8MjhUVZb0anry2orYuCRwyNJkgInAwCA7uFrYgAAAAAA6LvKix0gn2pra+Nf//Vf44tf/GJs2LChbX316tVx1llnxb/927/F0KFDi5gQAHq38v5VsceJo2PF9IWxZfmb272ucuhuMeS4vaKsuqS+1ACAotm0eWssalxT7Bh59eJry4sdoZ3RIwdHJlNS35dMgfiaGAAAAAAA+qaS+y/+73znO+OOO+6ICy64IBYtWtR2gsjLL78cp5xySpx99tlxxhlnxMCBA4ucFAB6p7Lq8tj9hH1i89INseGVNbF56YaISCMiiao9+kW/feujao9+kWSc4gUA+bKocU1c9a8PFDtGXvW057ntW5+MmiqFWzrH18QAAAAAAND3lFzhNiLiwAMPjLvvvju+973vxX/9139FmqYREbFmzZq48cYb46abbop999033vWud8X48eNj6NChUVdXFwMGDIiampq8ZNhzzz3zsg8A9ERJJonq4bVRPby22FEAAKAofE0MAAAAAAB9S8kVbv/lX/6l7fP+/fvH0UcfHY888kjbSbdpmkaapvHSSy/Fyy+/HL/+9a/zniFJknj++efzvi8AAAAAAAAAAAAA3a/kCre33nprW7l2W2maRpIkWcVbAAAAAAAAAAAAAMil5Aq3rToq07aubVu87Y77AgAAAAAAAAAAANB7lWzhtlCFWgAAAAAAAAAAAOgJWlqaY86c2TFjxkMxb97ctp8EP3bsuJgw4fg45JDxkcmUFTsmlISSLdw6aRYAAAAAAAAAAIBStW7d2rj++u/G/Pnzst6bOXN6zJw5PcaMGRuTJ18WdXUDipAQSkvJFW6POOKIYkcAAAAAAAAAAACAgmlsbIjvfOfqWLZsac7r5s+fF1OmXBJXXHF1DBs2vJvSQWkqucLt7bffXuwIQJ+SRERXTtRO8h0EAAAAAAAAAIA+IE3TuO22m3dYtm21bNnSmDr1lrj88qsiSXRWoKsyxQ4A0JuVVXftuP3y6vr8BgEAAAAAAAAAoE+YNevJmDPn2Z2amT37mZg166kCJYK+QeEWYBeU1wzs1jkAAAAAAAAAAPq2Rx6Z0cW5h/OcBPoWhVuAXVAzaHSX5qoH7ZvnJAAAAAAAAAAA9AULF77WpblFi7o2B7xF4RZgF1QOGBmVdSN2bqZuRFQOGFmgRAAAAAAAAAAAlLKGhsVdmluypGtzwFsUbgF2QZIk0X+vo6OssrZT15dV1kb/vY6OJEkKnAwAAAAAAAAAgFLU3NzcrXPAWxRuAXZReVVdDDrwlKio3SPndRW1e8SgA0+J8qq6bkoGAACUqkwXv4evq3MAAAAAAAB9XXmxAwCUgkxFTQw84KTY8kZDbFr5Umx5o7Htvcr+w6J68H5R2X94JInvcwAAAHbd0H4VsXT91p2e271fRQHSAAAAAAAAlL6SK9wuWbKk2BEiImLPPfcsdgSgmyVJJqrqRkRV3YhiRwEAAErcsNrKLhVuh9VWFiANAAAAAABA6Su5wu373//+SJLi/nzEJEni+eefL2oGAAAAoHS9a3htPNu4YafnDh1eW4A0AAAAAAAApa/kCrcREWmaFjsCAAAAQMGMHVoTBwyuib+u3NjpmQMG18TYoTUFTAUAAAAAAFC6MsUOUAhJkhTtAwAAAKDQkiSJiQcNjkE1nfte6kE15THxoMH+2wUAAAAAAEAXlWThtlDSNO3wAwAAAKC7Dd6tIs4/as8YVV+V87pR9VVx/lF7xuDdKropGQAAAAAAQOnp3DEovVA+i7Ctp79sewrMtvsr3QIAAADFUFtZFuceOTxeWrkpZjWsj1dWbYw0IpKI2HdQTRw2vDb2G1wdGSfbAgAAAAAA7JKSK9weccQRedtry5YtsWnTpli5cmWsWrUqWlpaIuKt4m1r+TZN09hrr73in/7pn2Lo0KF5uzcAAAD0fElEdOWbUJU/8ymTJHHAkJo4YEhNsaMAAAAAAACUrJIr3N5+++0F2XfLli0xf/78+POf/xx33313vPLKK23F20WLFsVXv/rV+PnPfx577bVXQe4PAAAAPU1Z9YBo3rRmp+fKq+vzngUAAAAAAAAKKVPsAL1FZWVlHHLIIfH3f//38dvf/ja+8Y1vRGVlZUS8dcptY2NjfPazn40VK1YUOSkAAAB0j/Kagd06BwAAAAAAAMWicNsFSZLEpz/96bjllluisrIykuStH4XZ0NAQF110UZHTAQAAQPeoGTS6S3PVg/bNcxIAAAAAAAAoLIXbXXDUUUfF17/+9UjTtK10++STT8a0adOKGwwAAAC6QeWAkVFZN2LnZupGROWAkQVKBAAAAAAAAIWhcLuLPvOZz8R+++0XEW+dfJumafzoRz8qcioAAAAovCRJov9eR0dZZW2nri+rrI3+ex3d9k2rAAAAAAAA0FuUFztAb5ckSZx99tnxjW98o+0vDBctWhSPPvpoHH300UVOBwAAAIVVXlUXgw48Jda88mBsXb90u9dV1O4R9fu+PzIVNd2YDgAAAAAAdmz3QdkHS5SVZaK+frcipCmsnvZMZWXODKX3ULjNgxNOOCG+8Y1vtFt76KGHFG4BAADoEzIVNTHwgJNiyxsNsWnlS7Hljca29yr7D4vqwftFZf/hkST+oxkAAAAAAD1PZWV2jS6TSSKTKStCmsKqqCi9Z4LuonCbBwMHDow999wzGhoa2k65nTNnTpFTAQAAQPdJkkxU1Y2IqroRxY4CAAAAAAAAeedomTwZPHhwpGkaERFpmsbChQuLnAgAAAAAAAAAAACAfFC4zZPm5uZ2r994440iJQEAAAAAAAAAAAAgn8qLHaBUNDQ0RJIkba9bWlqKmAYAAAAAAAAAAICuat6yKTYuX1zsGHm3fvHLxY7QTr/h+0SScW4ovYPCbR68+OKLsXr16naF29ra2iImAgAAAAAAAAAAoKs2Ll8c8//jumLHyLue9kyHfvWmKKusLnYM6BTV8Dy4/fbb2z5P0zQiIt7xjncUKw4AAAAAAAAAAAAAeaRwu4seffTR+PWvf93udNskSeLggw8uYioAAAAAAAAAAAAA8kXhdhfcf//9cf7550dLS0vWe8cee2wREgEAAAAAAAAAAACQb+XFDtDTpWkaTU1NsXHjxnjjjTdi2bJlMXfu3Lj33nvj2WefjTRN251uGxFRX18fxx13XJESAwAAAAAAAAAAAJBPJVe4HTt2bLfd6+1l29bXZ599dpSXl9y/WgAAAAAAAAAAAIA+qeRaoWmadtu93n6ybZIkseeee8bnP//5bssAAAAAAAAAAAAAQGGVXOE2IrsIWyjbnnCbpmlUV1fHDTfcENXV1d1yfwAAAAAAAAAAAAAKL1PsAL3ZtmXbIUOGxC233BLjx48vcioAAAAAAAAAAAAA8qkkT7gtpDRN272uq6uLM844I84555zo169fkVIBAAAAAAAAAAAAUCglV7jdc889875nkiRRVlYWlZWVUVtbG0OGDIl99903jjnmmHj3u98dZWVleb8nAAAAAAAAAAAAAD1DyRVuH3zwwWJHAAAAAAAAAAAAAKCEZIodAAAAAAAAAAAAAAB6MoVbAAAAAAAAAAAAAMhB4RYAAAAAAAAAAAAAclC47aQVK1ZEc3NzsWMAAAAAAAAAAAAA0M3Kix2gOzU1NcVf/vKXeOSRR+ILX/hC1NfXd3r2uuuuiwcffDDGjx8fxx13XJx22mk7NQ9AYbS0NMecObNjxoyHYt68uZGmaSRJEmPHjosJE46PQw4ZH5lMWbFjAgAAAAAAAADkRSaJaEm7Ngd0XZ8o3K5ZsybuuOOO+I//+I9Ys2ZNRES8973vjXe/+92d3uP111+PjRs3xhNPPBFPPPFE/OAHP4iTTz45Lrroohg6dGiBkgOQy7p1a+P6678b8+fPy3pv5szpMXPm9BgzZmxMnnxZ1NUNKEJCAAAAAAAAAID8GtqvIpau37rTc7v3qyhAGug7MsUOUGi/+93v4sMf/nD867/+a6xevTrS9K1q/6uvvrpT+yxevDiSJIk0TSNN09i8eXNMmzYtPvKRj8Rdd91ViOgA5NDY2BBTplzSYdl2W/Pnz4spUy6JxsaGbkoGAAAAAAAAAFA4w2oru3UOeEtJF26//e1vx+TJk2Pt2rVtP2K81YIFCzq9z6ZNm2LFihUREZEkSdtHmqaxdu3auOKKK+KGG27Id3wAtiNN07jttptj2bKlnbp+2bKlMXXqLW3fdAEAAAAAAAAA0Fu9a3htl+YO7eIc8JaSLdz+0z/9U9x+++1tRdtty7YRO3fC7eLFiztc37Z4e+utt8b3vve9XcoMQOfMmvVkzJnz7E7NzJ79TMya9VSBEgEAAAAAAAAAdI+xQ2vigME1OzVzwOCaGDt052aA9kqycPuHP/whpk6d2mHRtrUguzMn3JaXl8fxxx8fQ4YMiTRNs05IbN1z6tSp8eCDD+bjEQDI4ZFHZnRx7uE8JwEAAAAAAAAA6F5JksTEgwbHoJryTl0/qKY8Jh40OKtLB+yckivcrl+/Pq655pq219uWY9M0jZEjR8Yll1wSN998c6f33HvvvePmm2+OGTNmxH/913/FJz/5ySgrK2t3TWvpdsqUKbF69epdfxAAtmvhwte6NLdoUdfmAAAAAAAAAAB6ksG7VcT5R+0Zo+qrcl43qr4qzj9qzxi8W0U3JYPS1bmKey/y61//OpYvX97Wxm8twtbU1MTFF18cZ5xxxi419d/5znfGO9/5zvjsZz8bX/3qV+PFF19st9/q1avj9ttvj6985Su7/CwAdKyhYXGX5pYs6docAAAAAAAAAEBPU1tZFuceOTxeWrkpZjWsj1dWbYw0IpKI2HdQTRw2vDb2G1wdGSfbQl6UXOH2l7/8ZbsCbJqmUVdXFz/96U9j/PjxebvP6NGj4xe/+EWcffbZ8fzzz0eSJG3l3v/8z/+Mc845J6qrq/N2PwD+T3Nzc7fOAQAAAAAAAAD0RJkkiQOG1MQBQ2qKHQVKXqbYAfLp5ZdfjgULFrS9TtM0kiSJ73znO3kt27aqra2NH/zgB1FT0/43q7Vr18YjjzyS9/sBAAAAAAAAAAAA0P1KqnD77LPPtn3eWrY98sgj44Mf/GDB7jly5Mg4/fTTI03TdutPPPFEwe4JAAAAAAAAAAAAQPcpqcLt888/n7X2iU98ouD3nTRpUtbatuVfAAAAAAAAAAAAAHqvkircrl69OmvtsMMOK/h999tvv6ivr4+IiCRJIk3TWL58ecHvCwAAAAAAAAAAAEDhlVThdu3atVlrQ4YM6ZZ777777pGmadvrNWvWdMt9AQAAAAAAAAAAACiskircNjU1Za1t2bKlW+5dWVnZ7vXmzZu75b4AAAAAAAAAAAAAFFZJFW779++ftbZixYpuufeKFSsiSZK219XV1d1yXwAAAAAAAAAAAAAKq6QKt4MHD85ae+655wp+3/Xr12cVe4cMGVLw+wIAAAAAAAAAAABQeCVVuB0zZkzW2oMPPljw+z700EPR1NQUERFpmkaSJDFq1KiC3xcAAAAAAAAAAACAwiupwu0hhxzS9nmSJJGmafz+97+P1157raD3veOOO3JmAQAAAAAAAAAAAKD3KqnC7cEHHxx77LFHu7Wmpqb41re+VbB73nnnnfHMM89EkiTt1idMmFCwewIAAAAAAAAAAADQfUqqcBsRcfLJJ0eaphHxf6fcPvLII/HNb34z7/d69NFH45prrskq2+61114xfvz4vN8PAAAAAAAAAAAAgO5XcoXbz372s1FeXt72urV0+4tf/CIuuuiiWLNmTV7u88tf/jK+9KUvxZYtW9rW0jSNJEnis5/9bF7uAQAAAAAAAAAAAEDxlVzhdtiwYfHxj3+87ZTb1hJsmqZx3333xUc+8pG49dZbY+nSpTu995YtW+K+++6LT3/603HNNdfExo0b251umyRJjBo1Kj75yU/m7XkAAAAAAAAAAAAAKK7yHV/S+1x88cXx0EMPxbJly9rKtq3/XLFiRdxwww3xgx/8IMaPHx/jxo2LsWPHxrBhw6K2tjZqa2sjSZLYsGFDbNiwIVatWhXz58+P+fPnx1/+8pdYv359RPxfkbdVmqZRXl4e3/72t9udsAsAAAAAAORXS0tzzJkzO2bMeCjmzZvb9t/sx44dFxMmHB+HHDI+MpmyYscEAAAAoISUZDO0trY2brjhhvjiF78YmzZtale6jXirHNvc3BzPPPNMPPPMM53et/XU3IjIKtsmSRKXX355HHbYYXl7DgAAAAAAoL1169bG9dd/N+bPn5f13syZ02PmzOkxZszYmDz5sqirG1CEhAAAAACUokyxAxTKYYcdFjfeeGNUV1dHRPuCbJIkbSXcnfloneuobDt58uQ488wzu/05AQAAAACgr2hsbIgpUy7psGy7rfnz58WUKZdEY2NDNyUDAAAAoNSV5Am3rY499tj41a9+FV/5ylfi1VdfbVeUjYis1zsrTdPo379/fPOb34yTTjppl/YCKLbdB9VmrZWVZaK+frcipCmcnvY8ZWUl+70vAAAAAHmVpmncdtvNsWzZ0k5dv2zZ0pg69Za4/PKrdvnvAwAAAACgpAu3ERH7779/3H333XH77bfHrbfeGmvXro2IXSvbpmkamUwmPvKRj8TFF18cu+++e77iAhRNZWX2HwmZTBKZTFkR0hRORUVpPQ8AAABAXzFr1pMxZ86zOzUze/YzMWvWU3H44UcUKBUAAAAAfUXJF24jIiorK+OLX/xifPrTn4777rsvpk2bFrNmzYqmpqasa7ct4qZpmvX+8OHD46STTopPfvKTMWrUqELGBgAAAADolJaW5pgzZ3bMmPFQzJs3N9I0jSRJYuzYcTFhwvFxyCHjS+6baul7HnlkRhfnHla4BQAAAGCX9YnCbauampo47bTT4rTTTouNGzfGrFmzYu7cufHaa6/F66+/Hm+88UZs3LgxmpqaorKyMvr16xdDhw6NESNGxJgxY+LQQw+N0aNHF/sxAAAAAADarFu3Nq6//rsxf/68rPdmzpweM2dOjzFjxsbkyZdFXd2AIiSE/Fi48LUuzS1a1LU5AAAAANhWnyrcbqumpiaOOeaYOOaYY4odBQAAAACgSxobG+I737k6li1bmvO6+fPnxZQpl8QVV1wdw4YN76Z0kF8NDYu7NLdkSdfmAAAAAGBbfbZwC8CONW/ZFBuXl9ZfSKxf/HKxI7TTb/g+kWQyxY4BAABAL5Smadx22807LNu2WrZsaUydektcfvlVkSRJgdNB/jU3N3frHAAAAABsS+EWgO3auHxxzP+P64odI6962vMc+tWboqyyutgxAAAA6IVmzXoy5sx5dqdmZs9+JmbNeioOP/yIAqUCAAAAAChNffJIvVdffTWampp2auZHP/pR/MM//ENMmzYtli9fXqBkAAAAAACd88gjM7o493CekwAAAAAAlL4+c8LtokWL4t///d/jwQcfjIaGhrjrrrviwAMP7PT8k08+GY899lj8+te/jrKysvjABz4QZ555Zhx55JEFTA0AAAAA0LGFC1/r0tyiRV2bAwAAAADoy0q+cLtu3bq49tpr47e//W20tLREmqaRJEksWLBgpwq3ixcvjjRNIyKiqakpHnjggXjggQfi5JNPjiuvvDIGDRpUqEcAAAAAAMjS0LC4S3NLlnRtDgAAAACgL8sUO0Ah/eUvf4lTTjkl7rnnnmhubm4rzEZEvPrqq53eJ03TWLJkSSRJ0vaRpmmkaRq/+93v4iMf+Ug899xzhXgEAAAAAIAONTc3d+scAAAAAEBfVrKF21mzZsXf/d3fxdKlS9tOtU2SpO39BQsWdHqvpUuXRlNTU7u1bYu3q1atis997nPx9NNP5ys+AAAAAAAAAAAAAD1ESRZulyxZEuecc068+eabWUXb1pLszpxw29jYGBHRdqrttlr3X79+fZx77rmxdOnS/DwEAAAAAAAAAAAAAD1CSRZuv/GNb8T69evbiratJdnWwuz+++8fH/jABzq936GHHhqPP/54/PSnP43TTz896urqsoq3ERFr166NSy+9ND8PAQAAAAAAAAAAAECPUHKF24cffjhmzpzZ4am2hx56aNx+++1xzz33xN///d/v1L4DBgyIY489Nq655pr405/+FOecc06UlZW1u0dExOOPPx733Xdffh4GAAAAAAAAAAAAgKIrL3aAfPvP//zPdq/TNI0kSeILX/hCfP3rX49MZtc7xrvttltcdNFF8e53vzsuvPDC2LRpU7v73XbbbXHSSSft8n0AAAAAAKCQamurory8bMcX9nL19bsVO0KnNDU1x/r1m4sdAwAAAIAOlFThdtWqVfHwww+3nTbbWrY9/fTT45JLLsn7/d773vfGNddcE5dcckkkSdJ2ku7cuXPjueeei4MPPjjv9wQAAAAAgHwpLy+LiorSL9z2hWcEAAAAoLB2/bjXHuTZZ5+NlpaWdmu77757XH755QW750c/+tH/z96dh2lZl/3jP6/ZGIZhk01AUFFUMBX1IXFLUsvEDUmlJ838ZlnmkmUZllpuaU/a8kjmU6mlpSUSmuLegoobpiEgYKIIsiOyb8PM9fvD34yM9wDDzNxzz9y8XscxB3Of1/05P+c14nXo8OYzceihh0aaprXqzz//fNb2BAAAAAAAAAAAAKD55F3gtlr16baf//zno02bNlnd95xzzsmovfzyy1ndEwAAAAAAAAAAAIDmkVeB24ULF2bUjjzyyKzvO2TIkCgpKYmIiCRJIk3TmDNnTtb3BQAAAAAAAAAAACD7inI9QFNasWJFRq1v375Z37dNmzbRq1eveOedd2pqy5cvz/q+AAAAAADQ1NZXbIh335+f6zGa1JuL3871CBl26dwrSouz+xP6AAAAAGg6eRW4XbNmTUatTZvm+WZVhw4dIk3TSJIkIiJWrVrVLPsCAAAAAE2nvLxNFBUV5nqMrOvUqSzXI9TLpk2VsXr1hlyPscN59/35cd34n+d6jCbVEu/nihMuiT27757rMQAAAACop7wK3JaVZX6T+L333oudd94563uvXLmyJmwbEVFcXJz1PQEAAACAplVUVBjFxfkfuN0R7hEAAAAAoCkV5HqAptShQ4eM2ty5c7O+b5qmsXjx4m3OAgAAAAAAAAAAAEDrk1eB2379+mXU/vGPf2R931dffTXWrl0bER+Eb5MkiV122SXr+wLsqAqSbb+nKdcBAAAAAAAAAAA7trwK3A4YMKDm8yRJIk3TePTRR2Pjxo1Z3fehhx7a6iwANK1u7YobtK57A9cBAAAAAAAAAAA7tqJcD9CUBg8eHG3atKkVsF24cGHcfvvtcf7552dlz1mzZsWYMWMiSWofmzh48OCs7AdAxM7lJbFodUWD1gEAAMD2Wr+hIuYuXJ7rMZrUf95ZkusRMvTZuVOUtvGXZQEAAACAlimvArdlZWVx2GGHxT/+8Y9IkqTmlNtf/epXMWjQoDj00EObdL9Vq1bFpZdeGps2baoVuC0tLY2jjjqqSfcC4EMH9iyPyQvXbPe6QT3LszANAAAA+W7uwuXxg18+kesxmlRLvJ+rL/h09N+1W67HAAAAAACoU0GuB2hqZ511Vq3XSZLExo0b44ILLoinn366yfZZunRpfOUrX4kZM2bUhG3TNI0kSeKUU06J0tLSJtsLgNoGdGsbe3Vpu11r9urSNgZ02741AAAAAAAAAAAAEXkYuD388MNj3333rXldHYJdu3ZtfPWrX40rrrgiFi1a1OD+FRUVMWbMmDjhhBNi8uTJGdfbtGkT5513XoP7A7BtSZLEqQO7xE5t63dQ+05ti+LUgV1qnUYOAAAAAAAAAABQX/VLKrUyP/zhD+Nzn/tcVFVV1YSrkiSJNE1j7NixMW7cuDj00EPjuOOOi4EDB0b//v2jpKRki/3eeeedmDFjRkycODEef/zxWLlyZaRpWtM34sNg79e//vXo1atX9m8SYAfXpaw4LhzSK+56dVHMXr5hi+/brVObOPvAHlFeUtiM0wEAAAAAAAAAAPkkLwO3++23X5x//vkxevToWqcZVoduKysrY+LEiTFx4sSIiCgsLIzOnTtHeXl5lJeXR0FBQaxZsybWrFkTy5cvj/Xr19f0+GjQdvPehx12mNNtAZpReUlhfO3jPePN99bHKwtWx1vL1kUaEUlE9NupbRzUszz27FIaBU62BQAAAAAAAAAAGiEvA7cRERdeeGEsWrQoxowZk3EKbfXn1TZt2hRLliyJJUuWRMSHwdy61PXjyNM0jQMOOCBuueWWpr4NALahIElir65tY6+ubXM9CgAAAADZlERE3d+63/Y6AAAAAGikglwPkE3XXHNNfP3rX68JyX70tNu6PiJqn2Jb1/XNpWkaQ4cOjTvuuCPKysqa4a4AAAAAAGDHU9ShTYPWFXds2DoAAAAA2FxeB26TJImLL7447rjjjujfv3+kabrFk2s3X7OlcO3m0jSNdu3axZVXXhm33XZbtGvXrilHBwAAAAAANlPSqbRB64obuA4AAAAANleU6wGaw5AhQ+LBBx+M8ePHx5///Od4+eWXa51iWx+bB3W7desWp512WpxzzjnRsWPHrMwMAAAAAAB8qGz3jrH2nRXbv24338cHAAAAoPF2iMBtxAfB2hNPPDFOPPHEmDdvXjz77LPx0ksvxbRp02LevHlRUVGxxbXt27ePvffeOw444IA44ogj4pBDDomCgrw+HBgAAAAAAFqU0t7to7RneaxfsLr+a3qWR2nv9lmcCgAAAIAdxQ4TuN1c7969Y+TIkTFy5MiIiKiqqopFixbFqlWrYt26dVFRURElJSXRrl276N69e7Rv75txAAAAAABkqqqqjClTXotnnvlnTJ8+LdI0jSRJYsCAfePII4fGfvvtHwUFhbkeMy8kSRKdP94zFv9tdlSu3vIhGtUKy4uj88d71vsn3QEAAADA1uyQgduPKigoiJ49e0bPnj1zPQoAAAAAAK3EypUr4uabb4yZM6dnXHv22Qnx7LMTYu+9B8Sll46KDh065mDC/FPUvk30+MwesXTCnNi4ZO0W31fSrSy6HtU3Ckv9MQgAAAAATaMg1wPkqw0bNuR6BAAAAAAgrzX01E6nfTaFhQsXxBVXXFZn2HZzM2dOjyuuuCwWLlzQTJPlv8LSouj+qd2j2zG7RdnunaKwrDgKy4qisKw4ynbvFN2O2S26f2p3YVsAAAAAmpTvNjWhioqKePTRR+Pee++NI444Ii644IJcjwQAAAAA5KnC0o5RuX75dq8rKu3U5LPsaNI0jdtvvy0WL15Ur/cvXrwo7rjj/+Lyy38QSSLw3BSSgiRKe5ZHac/yXI8CAAAAwA5C4LYJzJ07N/785z/H2LFjY/ny5RERcfjhh+d2KAAAAAAgrxW17dywwG3bzk0/zA7mlVcmxZQpk7drzWuv/TteeeXlOPjgwVmaCgAAAADIJoHbBkrTNP75z3/GvffeG88++2ykaRppmkZEOKEAAAAAAMi6tjvtERvef3u715Xu1C8L0+xYJk58poHrnha4BQAAAIBWSuB2O7333nsxZsyYuO+++2LBggUREbWCttWfAwAAAABkU0nHPlHSoXdsXDmv/ms69I6Sjn2yONWOYc6cdxq0bu7chq0DAAAAAHJP4LaeJk2aFPfee288+eSTsWnTplrBWifaAgAAAADNLUmSaN/30Fj+xmNRuXH1Nt9fWFIe7fse6vuZTWDBgvqHnDc3f37D1gEAAAAAuSdwuxWrV6+OBx98MO69996YNWtWRNQ+zXZzaZr6RjUAAAAA0KyK2nSInfY5KZa/9feoWL1oi+8rLu8RnfodHQXFbZtxuvxVWVnZrOsAAAAAgNwTuK3D9OnT4957742HHnoo1q9fv8XTbDcP39ZVBwAAAADItoLittF5r+Nj46oFsf69N2PjqoU110ra7xylXfaMkvY9I0kKcjglAAAAAEDrJnD7/9u4cWM88sgj8ac//SkmT54cEbHVoO2WQrYHHnhgfPazn41hw4Y10+QAAAAAwI4uSQqiTYfe0aZD71yPAgAAAACQl3b4wO2cOXPi3nvvjXHjxsWKFSsiovbJtdXqOs22uta1a9c45ZRT4rOf/Wz069evOccHAAAAAAAAAAAAIMt2yMBtmqbxt7/9Le699954/vnnI03T7T7NtqioKI488sg47bTTYujQoVFYWNis9wAAAAAAAAAAAABA89ihArdLliyJMWPGxH333ReLFi2KiK2fZrt5vbq2++67x4gRI+LUU0+Nrl27NtfoAAAAAAAAAAAAAOTIDhG4ffHFF+Oee+6Jv/3tb1FZWbnV02w/Wk/TNNq2bRvHH398fPazn42DDz64+QYHAAAAAAAAAAAAIOfyNnC7atWqGDduXPzpT3+Kt99+OyLqPs22up4kSSRJUvN59a/XXXddDBs2LMrKypr9HgAAAAAAAAAAAADIvbwL3E6bNi3uvffeGD9+fKxfv36rp9lWv64O2NbltNNOy+7AAAAAAAAAAAAAALRoeRG43bhxYzz88MNx7733xtSpUyNiy6fZVquuV79vwIABsXTp0liyZEkzTAwAAAAAAAAAAABAa9GqA7ezZ8+OP/3pTzFu3LhYuXJlRNQdtN38NNvN39OlS5c48cQTY8SIEbH33nvH5z//eYFbAAAAAAAAAAAAAGppdYHbqqqq+Nvf/hb33ntvPP/88xHxYYA2ou4TbZMkqXlPcXFxDB06NE499dQ46qijorCwsHkGBwAAAAAAAAAAAKBVajWB20WLFsWYMWNizJgxsXjx4oio+zTbj6p+z8CBA+PUU0+Nk046KTp16pT1eQEAAAAAAAAAAADIDy0+cPv888/HvffeG3//+9+jsrJym6fZRnwYsu3SpUucdNJJMWLEiNhrr72aZV4AAAAAAAAAAAAA8kuLDtx+5jOfiXfeeScitn2abfX14uLi+OQnPxnDhw+Po446KgoLC5tnWAAAAAAAAAAAAADyUosO3M6ePTuSJIk0TesM2m5+2u2gQYPipJNOihNPPDE6duzYnGMCAAAAAAAAAAAAkMdadOC2WnXYdvOAbUTEgAED4vjjj49hw4bFLrvskovRAAAAAAAAAAAAAMhzrSJwGxE1p9wecMAB8ZnPfCaOOeaY6NOnT67HAgAAAAAAAAAAACDPtZrAbZIkkaZpzJgxI4qLi2PFihUxePDgGDRoUJSVleV6PAAAAAAAAAAAAADyVKsJ3KZpGhER69evj5dffjlefvnluO2226KwsDCGDBkSxx13XBxzzDGx00475XhSAAAAAAAAAAAAAPJJqwncJklS83l1+DYiYtOmTTFx4sSYOHFi/PCHP4yDDz44TjjhhBg2bFi0b98+F6PmjQ0bNsQTTzwRL730UkyePDnee++9WLFiRRQVFUXHjh2jX79+cdBBB8WnPvWp2GeffXI97lYtXbo0nnrqqXjxxRfjzTffjIULF8batWujTZs20blz5+jVq1cMHjw4jjjiiDjooINyPS4AAAAAAAAAAADQgrTowO1Pf/rT+Mtf/hLPPfdcVFVV1YRuNw/fRnwYwK2srIxJkybFpEmT4kc/+lEcffTRMXz48DjyyCOjoKCg2edvrSoqKuI3v/lN3HXXXfH+++/XeX3dunWxcOHCeO6552L06NFxyCGHxGWXXRYf+9jHcjDxlr3zzjvxi1/8Ih5//PHYtGlTxvVNmzbFmjVr4t13342XXnopfvnLX8YBBxwQF1xwQRx11FE5mBgAAAAAdkzddyrPqBUWFkSnTmU5mCZ7Wtr9FBb63jkAAAAA1EeLDtwOGzYshg0bFgsXLoz7778/xo0bF/PmzYuI2qHbuk6/3bBhQzz22GPx2GOPRZcuXeKUU06J4cOHR//+/Zv3JlqZ2bNnxyWXXBLTp0/frnUvvvhinHHGGfH1r389LrjggoxQdHNL0zRuv/32+PnPfx4VFRXbtXby5Mlx3nnnxZlnnhmjRo2KkpKSLE0JAAAAAFQrKcn8dnVBQRIFBYU5mCZ7iovz634AAAAAYEfRKv7q+s477xwXXnhh/O1vf4s777wzTjjhhCgpKYk0TWsCttWSJKn5qL6+dOnSuOOOO+Lkk0+OESNGxB//+MdYvnx5bm6mBZs1a1acddZZ2x22rVZZWRm33HJLfP/738/459KcKioq4jvf+U785Cc/2e6w7eb++Mc/xgUXXFDnybgAAAAAAAAAAADAjqNFn3Bbl0MPPTQOPfTQWLlyZfz1r3+NsWPH1gREP3qqal0n377++usxffr0uPHGG2Po0KExfPjwGDp0aBQW7tinCixbtiy+9KUvxZIlS+q83qdPnzjwwAOjR48esWLFipg1a1a88sordQZrx44dGz179oyLLroo22NnSNM0vve978VDDz1U5/XCwsIYNGhQ7LPPPlFeXh7vv/9+TJ06NV5//fU63//000/HVVddFT/60Y+yOTYAAAAAAAAAAADQgrW6wG21Dh06xFlnnVVzIuuYMWNi/PjxsWLFiojYcvi2+tTbioqKeOqpp+Kpp56Kzp07x4knnhjvv/9+s99HSzFq1KhYuHBhRn2XXXaJq6++Og4//PCMr+ns2bPj5ptvjieeeCJj3a233hqHHXZYHHzwwVmbuS633357/PWvf63z2ogRI+Liiy+Onj17Zlz7z3/+Ez/60Y/iueeey7g2duzYOP744+PII49s8nkBAAAAAAAAAACAlq/VBm43N2DAgLjqqqti1KhR8fjjj8df/vKXeOGFFyJN03qderts2bK4++67m3XmluTJJ5+MCRMmZNQHDRoUt99+e5SXl9e5brfddotbbrklfv3rX8fNN99c61pVVVVce+218Ze//CUKCgqyMvdHzZw5M37+859n1IuLi+P666+PU045ZYtr+/fvH3fccUdcf/31df5euO666+Kxxx7L+P0EAAAAAGRP5cb1sW7JvFyP0aRWz5uV6xFqaddz90ia6Xu4AAAAANCa5UXgtlpJSUmcdNJJcdJJJ8W8efPi/vvvjwceeCAWLFgQEVs+9Tbiw/DtRz377LNx2GGHNVtotLlVVlZmhGUjInr27Bm33nrrFsO2mzvvvPNi6dKl8fvf/75Wffr06fHYY4/FsGHDmmzerfmf//mfqKioqFVLkiR+8pOfxPHHH7/N9UmSxBVXXBFz5szJCCDPnj07XnjhhTj00EObdGYAAAAAYMvWLZkXM/94Q67HaFIt7X4GXTI6CktKcz0GAAAAALR4+ZkijYjevXvHN77xjfj73/8ev/nNb+K4446LoqKiSNO0znBtkiQ1Adzqk3HTNI2vfOUr8YlPfCJuvPHGmDZtWnPfRtZNmDAh3n777Yz6qFGjokuXLvXuM2rUqBg4cGBG/fbbb2/UfPX1r3/9K5599tmM+rnnnluvsO3mvvvd79YZsH7ggQcaOh4AAAAAAAAAAADQiuXVCbd1SZIkjjzyyDjyyCNj+fLl8cADD8Rf/vKXeOONN2qu16U6lJumac3prb///e+jX79+ccopp8SJJ54YvXr1arb7yJYxY8Zk1Hbbbbc47rjjtqtPQUFBXHzxxfG1r32tVn3q1Kkxc+bM2HvvvRs157bcc889GbU+ffrERRddtN299thjjxg8eHC8+OKLteqTJk1q8HwAAAAAAACQT6qqKmPKlNfimWf+GdOnT6s51GjAgH3jyCOHxn777R8FBYW5HhMAAKDJ5O0Jt3Xp1KlTnHPOOfHXv/417rvvvjjjjDOiXbt2tU693fyk281fV79n1qxZ8bOf/SyOPfbY+MIXvhBjxoyJVatW5eqWGmXNmjXxzDPPZNRHjBixxSDy1hx11FHRvXv3jPrDDz/coPnqa+XKlfHEE09k1M8///woLW3Yj0KrK3A8b968eP/99xvUDwAAAAAAAPLFypUr4uqrr4gbbrg6nn12Qrz33tJYtuy9eO+9pfHssxPihhuujquvviJWrlyR61EBAACaTN6fcLsl+++/f+y///7xve99Lx577LEYO3ZsvPzyyzV/8/KjgdPq19XB2zRN4+WXX46XX345rrvuujjqqKPi5JNPjqFDh0ZRUev4sr700ktRUVGRUT/66KMb1K+goCCGDh0a9913X636hAkT4tJLL21Qz/p49tlnY+PGjbVqXbp0iZNPPrnBPT/xiU/EmWeeGZ07d46ddtopOnfuHJ07d25wgBcAAAAAAADywcKFC+JHP/phLF68aKvvmzlzelxxxWXxve/9MHbeuWczTQcAAJA9rSMZmkWlpaUxfPjwGD58eMyZMyfuv//+eOCBB2Lx4sUREbWCt9Vh3M1fR0Rs2LAhnnzyyXjyySejQ4cOMWzYsPjCF74Q/fr1a96b2U7PP/98Rq1Tp07Rv3//Bvc8+OCDMwK3M2fOjGXLlsVOO+3U4L5bM2HChIzascceG8XFxQ3u2adPn7jqqqsaMxYAAAAAAADklTRN4/bbb9tm2Lba4sWL4o47/i8uv/wHDfoJmwAAAC1JQa4HaEn69u0b3/rWt+Kf//xn3HbbbXHsscdGYWFhTbC2rlNvqz+qT71dsWJF/OlPf4pHHnkkF7ewXaZPn55R22+//RrVc0vrp0yZ0qi+W/Ovf/0rozZ06NCs7QcAAAAAAAA7oldemRRTpkzerjWvvfbveOWVl7M0EQAAQPMRuK1DQUFBDB06NEaPHh1PP/10fOc734l+/frVhGo3t3kYt7X9rcw333wzo7bHHns0qmffvn2jsLAwoz5z5sxG9d2S999/P+bOnZtR/9jHPpaV/QAAAAAAAGBHNXHiMw1c93QTTwIAAND8BG63Yaeddopzzz03xo8fH/fee2989rOfjbZt29aEb1tbyLba+++/H8uWLcuo77bbbo3qW1xcHD179syo1xWKbQozZszIqHXp0iW6d++elf0AAAAAAABgRzVnzjsNWjd3bsPWAQAAtCRFuR6gNTnwwAPjwAMPjCuuuCLGjx8fY8eOjVdffTUiotUFbxctWlRnvUePHo3u3a1bt3j33Xdr1ebNm9fovnWpK8jbu3fvOt87a9aseOqpp+Kll16KN998M5YvXx5JkkTXrl2jR48eceihh8axxx4b++yzT1ZmBQAAAAAAgNZswYKG/Znf/PnZ+bNCAACA5iRw2wBt27aN0047LU477bR4++23Y8yYMfHXv/41li5dmuvR6u29996rs961a9dG966rx5b2a6w5c+Zk1D56uu2UKVPipz/9aTz33HN19pg7d27MnTs3Xn755bjlllti8ODBcfnll8e+++6blZkBAAAAAACgNaqsrGzWdQAAAC1JQa4HaO123333uOyyy2LChAkxevToGDp0aBQWFuZ6rG3aUji4Y8eOje7dvn37jNqqVasa3bcuixcvzqh16dIlIj74H/ef/OQncfrpp28xbFuXSZMmxWmnnRY33XRTpGnaZLMCAAAAAAAAAAAArZMTbptIYWFhHHvssXHsscfGkiVLsnaia1NZvXp1nfV27do1undZWVlGbeXKlY3uW5fly5dn1EpLS6OioiIuvvji+Pvf/96gvlVVVfGb3/wm5s+fHzfeeGOUlJQ0ctLWq1OnzH+e2VRY6O8BALlTWFjQ7M+9lsizGMgVz+EPeA4DueI5/AHPYSCXPIuBfOb51nr4b2IgV/z38Ac8h4Fc8RzeNoHbLOjWrVt069Yt12NsVUVFRZ310tLSRveuq8eGDRsa3bcudQVu27RpE5dffnmdYdtu3brFIYccEt26dYskSWLx4sXx0ksv1XlSbkTE+PHjo7i4OH784x839eitQpIkUVzc8k9sBmgqBQVJFBR47gHkiucwQG55DgPknmcxkM/8mRMA2+K/hwFyy3N42wRud1AbN26ss15U1PjfEoWFmf/Sbdq0qdF967Ju3bqM2mOPPRZz5sypVdtnn33isssui8MOOyySJKl1LU3TmDhxYvz4xz+ON954I6PfAw88EAcffHCcccYZTTs8AAAAAAAAAAAA0Co4g3wHtaUTbusKy26vukK7VVVVUVVV1ejeH1XXfXw0bHv66afH2LFj4/DDD88I20Z8cIrrEUccEX/5y1/ihBNOqHOf66+/PpYsWdI0QwMAAAAAAAAAAACtisDtDqqu4GnEB6e9NlZlZWWd+xUUNP1vt22dnHv66afHddddV6+Te4uLi+Pmm2+OT37ykxnX1q9fH3fccUeD5wQAAAAAAAAAAABar22nEMlLWwqg1hWW3V519SgpKWl03+211157xVVXXbVda5IkiRtuuCFOOOGEeO+992pd+9Of/hQXXHBBlJeXN+WYLVqaprFpU9OfTLw1hYUFUVBQdyAcINuqqtKorGze515L5FkM5Irn8Ac8h4Fc8Rz+gOcwkEuexUA+q6ho/J9D0jz8NzGQK/57+AOew0Cu5PI5XFRUsMVDRFsSgdsd1JYCsNs6MbY+6uqRrcBtcXHxFq994xvfaNC+nTt3jjPPPDP+93//t1Z97dq18cILL8Sxxx673T1bs+XL1zbrfp06lUVBQWGz7glQrbKyqtmfey2RZzGQK57DH/AcBnLFc/gDnsNALnkWA/nM86318N/EQK747+EPeA4DuZLL53DXrq3jEMyCXA9AbmzplNa1axv/L0xdPUpLSxvdty5bCtTuvPPOccwxxzS478iRI+tMzL/44osN7gkAAAAAAAAAAAC0TgK3O6hOnTrVWV+9enWje69Zsyaj1rlz50b3rUuHDh3qrA8ZMqRRR0x37do1dt9994z6a6+91uCeAAAAAAAAAAAAQOskcLuD2lLgdvny5Y3u/f7772fUdtppp0b3rcuWgrwDBgxodO+BAwdm1JYuXdrovgAAAAAAAAAAAEDrInC7g+rdu3ed9aYIlNbVo2vXro3uW5ct9d1SoHh71BXmrStMDAAAAADsWAoa+MO1GroOAAAAAMg9gdsdVI8ePaK4uDijvmDBgkb3nj9/fkatb9++je5bly31LS0tbXTvdu3aZdTWrl3b6L4AAAAAQOvWrV3m91bro3sD1wEAAAAAuSdwu4MqKCiIXXfdNaM+e/bsRvVdtmxZrFq1KqO+5557Nqrvluy222511levXt3o3mvWrMmotW3bttF9AQAAAIDWbefykmZdBwAAAADknsDtDmzffffNqM2cObNRPWfMmFFnPVuB24EDB9ZZnzNnTqN7r1ixIqPWuXPnRvcFAAAAAFq3A3uWN2jdoAauAwAAAAByryjXA5A7+++/fzz44IO1atOnT4+KioooLm7YjzabPHlyRq1Dhw5ZC9x27949evXqFfPnz69Vf+211xrd+4033sio9evXr9F9AQAAAIDWbUC3trFXl7bxxnvr6r1mry5tY0A3P0EL8llVVWVMmfJaPPPMP2P69GmRpmkkSRIDBuwbRx45NPbbb/8oKCjM9ZgAAABAAwnc7sAOPfTQjNq6devilVdeiUMOOaRBPZ999tmM2iGHHBKFhdn7BtJhhx0W999/f63av/71r1i1alW0b9++QT3XrFkT//nPfzLqWzpRFwAAAADYcSRJEqcO7BK/eXlhLFu3aZvv36ltUZw6sEskSdIM0wG5sHLlirj55htj5szpGdeefXZCPPvshNh77wFx6aWjokOHjjmYEAAAAGisglwPQO7sscce0bt374z6+PHjG9RvwYIF8eqrr2bUP/GJTzSoX30dffTRGbWNGzdmnN67PR555JGorKzMqGf7XgAAAACA1qFLWXFcOKRX7NapzVbft1unNnHhkF7RpaxhP1UMaPkWLlwQV1xxWZ1h283NnDk9rrjisli4cEEzTQYAAAA0JSfc7uBOOumkuO2222rVHnroobjkkktip5122q5ed911V0ZItW3btjFs2LBGz7k1n/jEJ6Jr166xdOnSWvXf/OY3cfrpp0ebNlv/hvdHVVVVxV133ZVR79atWxx44IGNmhUAAAAAyB/lJYXxtY/3jDffWx+vLFgdby1bF2lEJBHRb6e2cVDP8tizS2kUONkW8laapnH77bfF4sWL6vX+xYsXxR13/F9cfvkPnHpNRESUl7eJoqLs/aTIlqRTp7Jcj7BNmzZVxurVG3I9BgAA0EIJ3O7gTjvttPj1r38dVVVVNbW1a9fG9ddfHzfffHO9+7z++utx9913Z9SHDRsW5eXlTTLrlhQXF8fnPve5GD16dK36woUL46c//Wlcfvnl29Xv17/+dbzxxhsZ9c997nNRWLhjfMMDAAAAAKifgiSJvbq2jb26ts31KEAOvPLKpJgyZfJ2rXnttX/HK6+8HAcfPDhLU9GaFBUVRnHxjvHnTzvKfQIAAPmrINcDkFt9+vSJE088MaP+8MMPZ5x8uyVLliyJ888/PyoqKmrVi4uL4/zzz2+SObfl//2//1fniby/+93v4k9/+lO9+/ztb3/LCO5GRHTs2DHOPPPMRs0IAAAAAADkl4kTn2nguqebeBIAAAAg21r0CbfHHHNMrkdokCRJ4qmnnsr1GPV28cUXx5NPPhnr1q2rVf/Zz34W77//fnz729+O4uLiOtdOnTo1Lr744li4cGHGtTPPPDP69OlTrxluueWWOoOup556atx4443bXF9eXh7f//7349JLL8249oMf/CDefPPN+Na3vhVlZXX/qJqqqqq455574oYbbohNmzZlXL/wwgujc+fO9bgTAAAAAABgRzFnzjsNWjd3bsPWAQAAALnTogO38+bNiyRJIk3TXI+yXZIkyfUI26VPnz5x6aWXxnXXXZdx7Xe/+11MmDAhzjzzzDjssMOiV69esWbNmpg1a1b85S9/ifHjx2ecbBsRsddee8U3v/nN5hi/xoknnhiTJk2q80Tbu+++Ox577LE4/fTT45Of/GTsscceUVRUFIsWLYrnn38+7rvvvpg6dWqdfY888sj4whe+kO3xAQAAAACAVmbBgnkNWjd/fsPWAQAAALnTogO31VpTgLW1hYOrfeELX4hp06bFuHHjMq69/fbbdYZxt6Rz587x85//PEpLS5tyxHq56qqrYuXKlfHII49kXFuyZEnceuutceutt9a73z777BM33XRTq/o9CAAAAAAANI/KyspmXceOYX3Fhnj3/fm5HqPJvbn47VyPUMsunXtFaXGbXI8BAAC0Iq0icEvzuO666yJJkvjLX/7S4B5du3aNO++8M/bYY48mnKz+CgsL4+abb46ePXvGHXfc0agA9H/913/FL3/5y+jUqVPTDQgAAAAAAABb8e778+O68T/P9RhNrqXd0xUnXBJ7dt8912MAAACtSEGuB8i2NE0zPurznq29f2trWrOioqK44YYb4pprron27dtv9/pPfvKT8eCDD8Zee+2Vhenqr6CgIC677LK4/fbbY/fdt/9/ksvKyuKSSy6Ju+66S9gWAAAAAAAAAAAAaPkn3DZliDVJkoy+RUVFsfvuu8fuu+8e7du3j/Ly8igrK4v169fH6tWrY8WKFfGf//wn3nnnnaiqqqrpU90rTdNIkiQ+9alPRYcOHZps1lwaOXJkHHfccfGHP/whxo0bF+++++4W31tSUhJHHnlknH322TFkyJBmnHLbDj/88Bg/fnw89thjcf/998ekSZOioqJii+/v06dPnHjiifGFL3whunTp0oyTAgAAAAAAAAAAAC1Ziw7czpgxo8Frq6qq4sorr4yxY8dmBG27du0aJ554YgwbNiwGDBgQxcXF2+y3du3aeOmll+L++++Pf/7zn7Fp06aa4G2apjF16tQYPXp0DBw4sMEztySdOnWKCy+8MC688MKYM2dOzJgxI+bPnx9r166N0tLS6NixY+y2226x3377RUlJSaP3u+iii+Kiiy5qgslrKywsjBNOOCFOOOGEWLt2bUybNi1mz54dy5cvj4qKiigvL4/u3bvHvvvuG3369Gny/QEAAAAAAAAAAIDWr0UHbhsqTdP45je/GU888UStk2jbtGkTF1xwQZx77rlRWFi4XT3Lyspi6NChMXTo0Jg9e3Z873vfi1deeaUmdDt//vz44he/GHfeeWd87GMfy8Zt5Uzfvn2jb9++uR6j0crKymLw4MExePDgXI8CAAAAAAAAAAAAtCIFuR4gG37zm9/E448/HmmaRsQHYdudd945xo0bF+edd952h20/arfddos//vGPceaZZ9bsERGxatWquOCCC2L58uWN6g8AAAAAAAAAAABAy5F3gds33ngjbrnllpqTZ9M0ja5du8Zdd90V/fr1a7J9kiSJK6+8Mk4++eRI07TmJN3FixfHzTff3GT7AAAAAAAAAAAAAJBbeRe4/elPfxoVFRURETVB2CuvvDL69u2blf2uuuqq6N69e0RETcB33LhxsWjRoqzsBwAAAAAAAAAAAEDzyqvA7cKFC2PChAk1wdckSeLggw+O4447Lmt7lpeXx1e+8pVI07SmVllZGePHj8/angAAAAAAAAAAAAA0n7wK3E6YMKFW8DUi4uSTT876vscff3wUFNT+Ur7wwgtZ3xcAAAAAAAAAAACA7MurwO3rr7+eURs8eHDW9+3atWt07949IqLmdN0333wz6/sCAAAAAAAAAAAAkH15FbhduHBhRq1Hjx7NsnfXrl1rna77/vvvN8u+AAAAAAAAAAAAAGRXXgVu161bl1Grqqpqlr1XrVoVSZLUvN60aVOz7AsAAAAAAAAAAABAduVV4LZdu3YZtUWLFmV9302bNmXs06FDh6zvCwAAAAAAAAAAAED25VXgtnPnzhm1F154Iev7Pv3007F+/fqIiEjTNCIievTokfV9AQAAAAAAAAAAAMi+vArc7rPPPrVep2ka48aNy/q+d999d63XSZLEfvvtl/V9AQAAAAAAAAAAAMi+olwP0JQOPPDAms+TJIk0TWPatGkxZsyYOP3007Oy5wMPPBDPP/98JElSq3744YdnZT8AAAAAAIB8VV7eJoqKCnM9RrPo1Kks1yNs06ZNlbF69YZcjwEAAAAtQl4Fbvfbb7/o27dvzJ07NyI+DN3eeOONsddee8UBBxzQpPu98MIL8cMf/jAjbNuxY8f45Cc/2aR7AQAAAAAA5LuiosIoLt4xArc7yn0CAABAvijI9QBN7bTTTos0TWteJ0kSa9asiXPPPTcmTJjQZPs89NBDcf7558f69etrammaRpIkcdZZZ0VxcXGT7QUAAAAAAAAAAABA7uRd4Pbss8+O7t2717yuDsGuXr06vva1r8Wll14as2bNanD/qVOnxte//vW47LLLYt26dRmn23br1i3OPffcBvcHAAAAAAAAAAAAoGUpyvUATa20tDS+//3vxze+8Y1aYdgkSSJN03jkkUfikUceiQEDBsTRRx8dAwYMiL333jt69uwZhYW1f3RPZWVlLFmyJKZNmxbTpk2Lv/3tb/HGG29ExIdB3mppmkZhYWHceOON0bZt2+a5WQAAAAAAAAAAAACyLu8CtxERxx13XJx99tlx11131Rm6jYh4/fXXY/r06bXWlZSURLt27aKgoCBWr14dGzZsqHW9em11r4/6xje+EYcddlhT3goAAAAAAMAObf2Gipi7cHmux2hy/3lnSa5HqKXPzp2itE1xrsegpUsiIt3mu+peBwAA0MrlZeA2IuLyyy+PdevWxZgxY2rCsZufSpumaa0AbUTEhg0bMkK2H/XRoG11j2984xtx3nnnNdX4AAAAAAAARMTchcvjB798ItdjNLmWdk9XX/Dp6L9rt1yPQQtX1KFNbFqx9T9PrUtxxzZZmAYAAKB55W3gNkmSuPbaa6NPnz5xyy23REVFRcZpt42VpmmUl5fHtddeG8cff3yj+wEAAAAAAAD1V1VVGVOmvBbPPPPPmD59Ws0BPAMG7BtHHjk09ttv/ygoKMz1mHmjpFNpwwK3nUqzMA0AAEDzytvAbbXzzjsvPvGJT8SPf/zjeP755yOi8WHb6v9RP+GEE+Lb3/529OzZsylGBQAAAAAAAOpp5coVcfPNN8bMmdMzrj377IR49tkJsffeA+LSS0dFhw4dczBh/inbvWOsfWfF9q/bzdcfAABo/QpyPUBz2GeffeLOO++MP/3pT3HyySdHWVlZpGla87Etm7+3bdu2ccYZZ8S4cePi5ptvFrYFAAAAAACAZrZw4YK44orL6gzbbm7mzOlxxRWXxcKFC5ppsvxW2rt9lPYs3741PcujtHf7LE0EAADQfPL+hNvNDRo0KAYNGhQbNmyISZMmxcsvvxxTpkyJOXPmxIIFC2LTpk213p8kSXTv3j1222232HfffePQQw+NwYMHR2mpH3kCAAAAAAAAuZCmadx++22xePGier1/8eJFcccd/xeXX/6DRv8kzB1dkiTR+eM9Y/HfZkfl6optvr+wvDg6f7ynrzsAAJAXdqjAbbU2bdrEEUccEUcccURNLU3TWLNmTaxbty6qqqqibdu20a5duygsLMzhpAAAAAAAAMDmXnllUkyZMnm71rz22r/jlVdejoMPHpylqXYcRe3bRI/P7BFLJ8yJjUvWbvF9Jd3KoutRfaOwdIf8I2kAACAP+b+b/1+SJFFeXh7l5dv3I1AAAAAAAACA5jNx4jMNXPe0wG0TKSwtiu6f2j02LFoTa95aHhsWrYmINCKSaNOjXbTr1yna9GgXSYGTbQEAgPwhcAsAAAAAAAC0GnPmvNOgdXPnNmwddUsKkijtWR6lPR1oBAAA7BgKcj0AAAAAAAAAQH0tWDCvQevmz2/YOgAAAIgQuAUAAAAAAABakcrKymZdBwAAABERRbkeIFeqqqpi6tSpMX369Jg1a1YsWrQo3n///diwYUNUVFTEgAED4vrrr89Yd9ttt8UJJ5wQffr0ycHUAAAAAAAAAAAAADS3HS5wO3HixBg7dmw888wzsXr16ozraZpGRETbtm0zrr333nvx85//PH7xi1/Ef/3Xf8Ull1wSBx98cNZnBgAAAAAAAAAAACB3dpjA7YQJE+JnP/tZzJw5MyI+DNbWJUmSOuvz5s2rWTtp0qQ466yz4lOf+lRce+210bFjx6YfGgAAAAAAAAAAAICcK8j1ANm2bt26+Pa3vx1f+9rXYubMmZGmaaRpGkmS1PmxNfPnz4+IDwO5aZrGk08+GZ/97GdjxowZWb8XAAAAAAAAAAAAAJpfXgduFy9eHJ/73Odi/PjxGUHbj9raibfVqgO3EVHTJ03TePfdd+OLX/xivPXWW006PwAAAAAAAAAAAAC5l7eB22XLlsXZZ59dc6ptXUHb6hBufcK2EbUDt9Xrq/uuWLEivvzlL8eyZcua7B4AAAAAAAAAAAAAyL28DNxWVlbGN77xjZg9e3atk2gjPgzZtmnTJg4//PD48pe/HNdcc029+u6///6x88471/TavG9ExIIFC+LHP/5x098QAAAAAAAAAAAAADlTlOsBsuGOO+6ISZMm1TrRtjocu88++8S5554bxx13XJSUlNRcv+qqq7bZ9+STT44TTjgh/vznP8cvfvGLWLFiRU3f6l//+te/xsiRI+Oggw7Kyr0BAAAAAADQUiQRUb+fpJi5DgAAAGhN8u6E2/feey9++ctf1grbVp9qe/7558fYsWPjpJNOqhW23R6FhYXx+c9/Pv7617/Gxz72sZqw7eZ+97vfNeYWAAAAAAAAaAUKSzs2aF1RaaemHQQAAADIurwL3P7+97+P9evX17yuDsReeuml8Y1vfCMKCwubZJ8ePXrEXXfdFQMGDKipVZ9y+49//COWLVvWJPsAAAAAAADQMhW17dys6wAAAIDcyavAbZqm8cADD9ScOFsdtj355JPjK1/5SpPvV1ZWFj//+c+juLi4Vn3Tpk3x9NNPN/l+AAAAAAAAtBxtd9qjQetKd+rXxJMAAAAA2ZZXgdupU6fG4sWLa9U6dOgQo0aNytqeu+66a5x22mmRpmnGLAAAAAAAAOSvko59oqRD7+1b06F3lHTsk6WJAAAAgGzJq8DtK6+8UvN59em2xx13XHTunN0fyzNixIiMmsAtAAAAAABAfkuSJNr3PTQKS8rr9f7CkvJo3/fQmp/WCAAAALQeeRW4feuttzJqn/rUp7K+78CBA6OsrCwiPvjGSpqmsWTJkqzvCwAAAAAAQG4VtekQO+1zUhSX99jq+4rLe8RO+5wURW06NNNkAAAAQFMqyvUATWnRokUZtd133z3r+xYUFETv3r3jzTffrKmtWrUq6/sCAAAAAACQewXFbaPzXsfHxlULYv17b8bGVQtrrpW03zlKu+wZJe17RpLk1Vk4AAAAsEPJq8Dt2rVrM2rdu3dvlr3LysoiTdOaHwFU1ywAAAAAAADkpyQpiDYdekebDr1zPQoAAACQBXn112gLCwszahs3bmyWvZcvX14Tto2IKCrKqywzAAAAAAAAAAAAwA4rrwK37dq1y6gtWbIk6/umaZqxT3l5edb3BQAAAAAAAAAAACD78ipw27Nnz4zav/71r6zvO2XKlFi7dm1EfBC+3dIsAAAAAAAAAAAAALQ+eRW43WOPPTJqTz31VNb3feyxx2q9TpIk+vXrl/V9AQAAAAAAAAAAAMi+vArcHnzwwTWfJ0kSaZrGhAkT4vXXX8/ankuXLo177703kiSpVT/ggAOyticAAAAAAAAAAAAAzaco1wM0pf79+8cuu+wS8+bNq6mlaRpXXXVV3HPPPVFSUtLke1599dWxbt26jMDt0KFDm3wvAAAAAAAAyIbuO5Vn1AoLC6JTp7IcTJM9Le1+Cgvz6nwkAACAvJZXgduIiOHDh8fo0aMjSZKaU26nTZsW3/3ud+Omm26KwsLCJtvrpptuiieffLImbJumaSRJEgceeGD06tWryfYBAAAAAACAbCopyfxjw4KCJAoKmu7P1lqC4uL8uh8AAACaT979lcmzzjoryso+/Jup1aHbxx57LL70pS/FsmXLGr3Hhg0b4tvf/nbcfvvtGSfbRkR88YtfbPQeAAAAAAAAAAAAALQMeRe47dSpU1x44YWRpmlEfHjqbJqm8eKLL8anPvWpuPXWW2Pp0qXb3buioiLuueeeOP7442P8+PE1e2y+z/777x/HHXdck90PAAAAAAAAAAAAALmV+bNh8sA555wT//jHP2LSpEk1YdvqX9esWRO33HJLjB49Ovbff//Yd999o2/fvhk9KisrY+7cuTF//vx4++23Y+LEifH888/HmjVraoK2Hz3dtk2bNnHttdc2yz0CAAAAAAAAAAAA0DzyMnBbUFAQP//5z+Nzn/tcvPvuu7VCtxEfnEabpmlMnjw5Jk+eXLNu81NxJ0+eHJ/+9Kdr9f1o0HbzIG+SJPHDH/4w9tprr+a4RQAAAAAAAMiqyo3rY92Sebkeo0mtnjcr1yPU0q7n7pEU5N0PJQUAAMhLeRm4jYjo0qVL/P73v4//9//+X7zzzju1TqPdPDC7JXVd++iJtpuHbS+//PIYPnx40wwPAAAAAAAAObZuybyY+ccbcj1Gk2pp9zPoktFRWFKa6zEAAACoh7z+65K9evWKMWPGxJFHHrnFAG31x9aubek9aZpGu3bt4mc/+1mcffbZWbkHAAAAAAAAAAAAAHIrrwO3EREdOnSI3/zmN3H99ddH9+7dI03TbYZv6wrXbq66x7HHHhsPP/xwfOYzn8nW+AAAAAAAAAAAAADkWFGuB2gun/3sZ+Okk06KBx98MMaOHRuTJ0+uFbzdUsj2o+Hctm3bxmc+85n4/Oc/H/vtt19WZwYAAAAAAAAAAAAg93aYwG1ERElJSZx++ulx+umnx9KlS2PixIkxderUePPNN2PBggWxbNmyWLduXWzatClKSkqirKwsunXrFn369Il99tknDjrooBg8eHCUlJTk+lYAAAAAAAAAAAAAaCY7VOB2c127do1TTjklTjnllFyPAgAAAAAAAAAAAEALVpDrAQAAAAAAAAAAAACgJRO4BQAAAAAAAAAAAICtKMr1ALnw9ttvR58+faKoqP63f+utt8aCBQvi4IMPjsMPPzy6deuWxQkBAAAAAAAAAAAAaCl2mMDt3Llz4/e//338/e9/jwULFsS4ceNin332qff6SZMmxQsvvBD3339/FBYWxjHHHBNnnnlmfPzjH8/i1AAAAAAAAAAAAADkWt4HbleuXBnXXXddjB8/PqqqqiJN00iSJGbPnr1dgdt58+ZFmqYREbFp06Z44okn4oknnohhw4bF97///dhpp52ydQsAAAAAAAAAAAAA5FBBrgfIpn/9619x0kknxUMPPRSVlZU1gdmIiLfffrvefdI0jfnz50eSJDUfaZpGmqbxyCOPxIknnhhTp07Nxi0AAAAAAAAAAAAAkGN5G7h95ZVX4itf+UosWrSo5lTbJElqrs+ePbvevRYtWhSbNm2qVds8eLts2bI455xz4tVXX22q8QEAAAAAAAAAAABoIfIycDt//vw477zzYu3atRlB2+qQ7PaccLtw4cKIiJpTbTdX3X/16tXxta99LRYtWtQ0NwEAAAAAAAAAAABAi5CXgdurrroqVq9eXRO0rQ7JVgdm+/fvH8ccc0y9+w0aNChefPHF+O1vfxsjR46MDh06ZARvIyJWrFgR3/3ud5vmJgAAAAAAAAAAAABoEfIucPv000/Hs88+W+eptoMGDYq77747HnroofjqV7+6XX07duwYRxxxRFx99dXxj3/8I84777woLCystUdExIsvvhiPPvpo09wMAAAAAAAAAAAAADmXd4Hbe+65p9br6pNov/SlL8U999wTgwcPbvQeZWVl8a1vfStuvfXWKC0tzdjv9ttvb/QeAAAAAAAAAAAAALQMeRW4XbZsWTz99NM1p82maRpJksTIkSPjsssui4KCpr3dT3ziE3H11VfXhHqr9502bVpMnTq1SfcCAAAAAAAAAAAAIDfyKnA7efLkqKqqqlXr3r17XH755Vnb8+STT45DDz20JnRb7fnnn8/angAAAAAAAAAAAAA0n7wL3FarPt3285//fLRp0yar+55zzjkZtZdffjmrewIAAAAAAAAAAADQPPIqcLtw4cKM2pFHHpn1fYcMGRIlJSUREZEkSaRpGnPmzMn6vgAAAAAAAAAAAABkX14FblesWJFR69u3b9b3bdOmTfTq1atWbfny5VnfFwAAAAAAAAAAAIDsy6vA7Zo1azJqbdq0aZa9O3ToEGma1rxetWpVs+wLAAAAAAAAAAAAQHblVeC2rKwso/bee+81y94rV66MJElqXhcXFzfLvgAAAAAAALAjKUi2/Z6mXAcAAAAReRa47dChQ0Zt7ty5Wd83TdNYvHjxNmcBAAAAAAAAGqdbu4YdfNO9gesAAAAgIs8Ct/369cuo/eMf/8j6vq+++mqsXbs2Ij4I3yZJErvsskvW9wUAAAAAAIAdzc7lJc26DgAAACLyLHA7YMCAms+TJIk0TePRRx+NjRs3ZnXfhx56aKuzAAAAAAAAAE3jwJ7lDVo3qIHrAAAAICLPAreDBw+ONm3a1KotXLgwbr/99qztOWvWrBgzZkwkSZIxCwAAAAAAANC0BnRrG3t1abtda/bq0jYGdNu+NQAAALC5vArclpWVxWGHHRZpmkbEh6fc/upXv4rnn3++yfdbtWpVXHrppbFp06Za9dLS0jjqqKOafD8AAAAAAADY0SVJEqcO7BI7tS2q1/t3alsUpw7sknGADgAAAGyPvArcRkScddZZtV4nSRIbN26MCy64IJ5++ukm22fp0qXxla98JWbMmFHzP+dpmkaSJHHKKadEaWlpk+0FAAAAAAAAfKhLWXFcOKRX7NapzVbft1unNnHhkF7Rpay4mSYDAAAgX9Xvr322Iocffnjsu+++8frrr0fEhyHYtWvXxle/+tX47Gc/GxdddFH06NGjQf0rKirigQceiJtuuilWrlyZcb1NmzZx3nnnNeoeAAAAAAAAgK0rLymMr328Z7z53vp4ZcHqeGvZukgjIomIfju1jYN6lseeXUqjwMm2AAAANIG8C9xGRPzwhz+Mz33uc1FVVVVz+mySJJGmaYwdOzbGjRsXhx56aBx33HExcODA6N+/f5SUlGyx3zvvvBMzZsyIiRMnxuOPPx4rV66MNE1r+kZ8GOz9+te/Hr169cr+TQIAAAAAAMAOriBJYq+ubWOvrm1zPQoAAAB5Li8Dt/vtt1+cf/75MXr06JpAbMSHodvKysqYOHFiTJw4MSIiCgsLo3PnzlFeXh7l5eVRUFAQa9asiTVr1sTy5ctj/fr1NT0+GrTdvPdhhx3mdFsAAAAAAAAAAACAPJOXgduIiAsvvDAWLVoUY8aMyTiFtvrzaps2bYolS5bEkiVLIuLDYG5dPhq0re51wAEHxC233NLUtwEAAAAAAAAAAABAjhXkeoBsuuaaa+LrX/96TUj2o6fd1vURUfsU27quby5N0xg6dGjccccdUVZW1gx3BQAAAAAAAAAAAEBzyuvAbZIkcfHFF8cdd9wR/fv3jzRNt3hy7eZrthSu3VyaptGuXbu48sor47bbbot27do15egAAAAAAAAAAAAAtBBFuR6gOQwZMiQefPDBGD9+fPz5z3+Ol19+udYptvWxeVC3W7ducdppp8U555wTHTt2zMrMAAAAAAAAAAAAALQMO0TgNuKDYO2JJ54YJ554YsybNy+effbZeOmll2LatGkxb968qKio2OLa9u3bx9577x0HHHBAHHHEEXHIIYdEQUFeHw4MAAAAAAAAAAAAwP9vhwncbq53794xcuTIGDlyZEREVFVVxaJFi2LVqlWxbt26qKioiJKSkmjXrl1079492rdvn+OJAQAAAAAAAAAAAMiVHTJw+1EFBQXRs2fP6NmzZ65HAQAAAAAAAAAAAKCFKcj1AAAAAAAAAAAAAADQkgncAgAAAAAAAAAAAMBWCNwCAAAAAAAAAAAAwFYI3AIAAAAAAAAAAADAVgjcAgAAAAAAAAAAAMBWFOV6gOY0e/bseOutt+K9996LNWvWxMaNG6OysjKqqqoiTdMm3evCCy9s0n4AAAAAAAAAAAAA5EbeB26XLl0ad955Zzz11FMxZ86cZttX4BYAAAAAAAAAAAAgP+R14PbXv/513HbbbbFu3bomP8F2a5Ikaba9AAAAAAAAAAAAAMiuvAzcpmka3/rWt+Kxxx6rCdo2Vwi2OYO9AAAAAAAAAAAAAGRfXgZuf/nLX8ajjz4aEbWDtmmaOn0WAAAAAAAAAAAAgO2Sd4HbOXPmxG233VZn0La65hRaAAAAAAAAAAAAAOor7wK3v/3tb2PTpk2RJElNsHbzz3fZZZc44IADYtddd42ddtop2rRpE0VFefdlAAAAAAAAAAAAAKCJ5FXSdMOGDTF+/Piak2yrg7ZpmsY+++wTV1xxRfzXf/1XjqcEAAAAAAAAAAAAoDXJq8DtSy+9FGvWrKkJ3KZpGkmSxEEHHRS33357lJaW5nhCAAAAAAAAAAAAAFqbglwP0JT+/e9/Z9Tatm0bP/vZz4RtAQAAAAAAAAAAAGiQvArczp49u+bz6tNtTzvttOjevXvuhgIAAAAAAAAAAACgVcurwO3y5cszap/+9KebfxAAAAAAAAAAAAAA8kZeBW43btyYUevfv38OJgEAAAAAAAAAAAAgX+RV4LZdu3YZtfbt2+dgEgAAAAAAAAAAAADyRV4Fbrt06ZJRW7lyZQ4mAQAAAAAAAAAAACBf5FXgtl+/fhm1BQsW5GASAAAAAAAAAAAAAPJFXgVuBw8enFF78cUXczAJAAAAAAAAAAAAAPkirwK3++23X/To0SMiIpIkiTRN4+GHH87xVAAAAAAAAAAAAAC0ZnkVuE2SJEaOHBlpmtbUpk2bFk8//XQOpwIAAAAAAAAAAACgNcurwG1ExDnnnBNdu3aNiA9Pub3yyitj+fLluR0MAAAAAAAAAAAAgFYp7wK3ZWVlce2119aqLVq0KL7+9a8L3QIAAAAAAAAAAACw3fIucBsR8clPfjK++c1vRpqmkSRJJEkSr776apx44olxzz33xJo1a3I9IgAAAAAAAAAAAACtRFGuB8iW8847L0pKSuInP/lJVFVVRZqmsXTp0rj22mvjxz/+cRx00EHxsY99LPr27RtdunSJtm3bRlFR0305Bg8e3GS9AAAAAAAAAAAAAMidvAvcXnrppbVe9+rVK+bOnRtJkkRERJqmsWHDhnjhhRfihRdeyMoMSZLE66+/npXeAAAAAAAAAAAAADSvvAvcjh8/viZcu7k0TSNJklrBWwAAAAAAAAAAAADYlrwL3FarK1BbXds8eNsc+wIAAAAAAAAAAADQeuVt4DZbgVoAAAAAAAAAAAAAdix5Gbh1yiwAAAAAAAAAAAAATSXvArennnpqrkcAAAAAAAAAAAAAII/kXeD2hhtuyPUIAAAAAAAAAAAAAOSRglwPAAAAAAAAAAAAAAAtmcAtAAAAAAAAAAAAAGyFwC0AAAAAAAAAAAAAbIXALQAAAAAAAAAAAABshcAtAAAAAAAAAAAAAGyFwG2WpGma6xEAAAAAAAAAAAAAaAICt01o48aN8fjjj8cZZ5wRM2fOzPU4AAAAAAAAAAAAADSBolwP0JzWrl0bS5cujTVr1sSGDRuisrIy0jSt12m0VVVVUVVVFRUVFVFRUREbNmyIdevWxapVq2LJkiUxY8aM+Ne//hUbNmxohjsBAAAAAAAAAAAAoLnkfeD27bffjgceeCCeeOKJmD17dlb3qg7uJkmS1X0AAAAAAAAAAAAAaD55G7jduHFj/PKXv4zbb7+95iTbbEuSpFn2AQAAAAAAAAAAAKD55GXgtrKyMs4///x47rnnnDoLAAAAAAAAAAAAQKPkZeD2f//3f2PixIkRUTtom6ZpncHbzU+l3Vowd2un1350nYAvAAAAAAAAAAAAQH7Iu8Dt/Pnz484776wzaFtd21pwdkvXNl+/+fuSJIk0TSNN0+jdu3d8/OMfj1NPPTX23nvvprgdAAAAAAAAAAAAAHIs7wK3f/jDH2Ljxo01QdiIqPV5165dY++9945OnTpFSUlJTJs2Ld54442a9xQWFsbJJ58cERGbNm2KDRs2xNKlS2Pu3LmxZMmSmn6bh3eTJInRo0fHMccck4M7BgAAAAAAAAAAACCb8i5w+/DDD9eEYTc/fXb//fePUaNGxUEHHVTr/U8++WRcdNFFNa+rqqriv//7v2P//ffP6D1v3rx46KGH4s4774wVK1bUBG/TNI0f/OAHsf/++0e3bt2ye4MAAAAAAAAAAAAANKuCXA/QlGbMmBGLFy+ueV19+uygQYPiD3/4Q0bYNiLisMMOi6Ki2rnjJ554os7+vXv3jq997Wvx+OOPx1FHHVVzam5ExNKlS+Pyyy9vojsBAAAAAAAAAAAAoKXIq8Dt1KlTM2oFBQVxww03RElJSZ1r2rVrF3vttVdNODdN03juuee2uk+nTp3itttui2HDhtWsi4iYOHFiPPDAA42+DwAAAAAAAAAAAABajrwK3L711ls1n1cHYY844ojYfffdt7rugAMOqPV6xowZsWrVqq2uSZIkbrzxxujfv3/N6zRN4+abb45169Y18A4AAAAAAAAAAAAAaGnyKnC7ePHijNqxxx67zXUDBw6s9TpN05gyZco215WUlMQ111wTaZrW1JYuXRr33XdfPaYFAAAAAAAAAAAAoDXIq8DtypUrM2r77rvvNtftueeeGbWpU6fWa88DDzwwhgwZUnOibpqm8cc//rFeawEAAAAAAAAAAABo+fIqcLthw4aM2s4777zNdf3798+ozZgxo977Dh8+vNbruXPnxrRp0+q9HgAAAAAAAAAAAICWK68Ct1VVVRm19u3bb3NdeXl5dOnSJSKi5pTaWbNm1Xvfww8/PKM2ceLEeq8HAAAAAAAAAAAAoOXKq8BtaWlpg9f26dMn0jStef3OO+/Ue223bt2ic+fOtWqTJ09u8CwAAAAAAAAAAAAAtBx5FbgtLy/PqK1bt65ea/v06VPr9YYNG+Ldd9+t997du3ePNE1rTsidPXt2vdcCAAAAAAAAAAAA0HLlVeC2R48eGbUFCxbUa+1HA7cREbNmzar33m3btq31esmSJfVeCwAAAAAAAAAAAEDLlVeB27pCs2+88UaD186cObPee69du3arrwEAAAAAAAAAAABonfIqcDtw4MCM2j//+c96rd11110zaq+99lq99164cGEkSVLzuqioqN5rAQAAAAAAAAAAAGi58i5wW1paGhERSZJEmqbx1FNPxcKFC7e5ds8996z5vHrtpEmTorKycptrZ82aFStXrqxV69Chw3ZODwAAAAAAAAAAAEBLlFeB2zZt2sSQIUMiTdOa2saNG+M73/lObNy4catrO3ToEL17965VW7lyZTz++OPb3Hfs2LE1n1fv3blz5+0ZHQAAAAAAAAAAAIAWKq8CtxERJ510Us3n1SfVvvzyy/H5z38+Zs2atdW1hxxySE1gtnrtTTfdFKtXr97imhkzZsTdd98dSZLU2nfAgAGNvBMAAAAAAAAAAAAAWoK8C9wed9xx0aNHj5rX1cHZqVOnxsknnxzf+ta3YsKECVFRUZGxdvOwbrX58+fHWWedFXPmzMm49vzzz8eXvvSlOnsNGjSocTcCAAAAAAAAAAAAQItQlOsBmlpRUVFccsklcfnll9eEbat/raysjEcffTQeffTRuPrqq+OMM86otXbIkCHRq1evWLBgQUREzam1M2bMiOOPPz4OP/zw2HPPPaOysjJeffXVmDJlSk3/j85wzDHHNM8NAwAAAAAAAAAAAJBVeXfCbUTEqaeeGkOHDq0Vtk2SpObziIg+ffpkrEuSJC6++OKa91T/GhFRWVkZzzzzTNx5551x1113xWuvvZYRtq1+/elPfzq6deuW5bsEAAAAAAAAAAAAoDnkZeA2IuKmm26KAw44oM4TaCMi+vbtW+e64cOH11r30bBu9UdE1Nm3rKwsLrnkkia9FwAAAAAAAAAAAAByJ28Dt+Xl5fG73/0uRowYUSskGxFRVFQUvXr12uLaW2+9teb65qHa6uDtRwO4ER+ebnvVVVfVeXouAAAAAAAAAAAAAK1T3gZuIyLatm0bP/rRj+Kee+6JI444oiY826tXrzpPp63WpUuXuPPOO2OfffapFdT9qM2Dt0VFRXHDDTfEKaec0uT3AQAAAAAAAAAAAEDuFOV6gOZw0EEHxW9/+9tYtGhRPPXUU7F69eptrtl1113jvvvui9GjR8ddd90V69at2+J7Bw8eHN/73vdiwIABTTk2AAAAAAAAAAAAAC3ADhG4rdajR48488wz6/3+4uLi+OY3vxnnnntu/P3vf49XXnklFi5cGGmaRpcuXaJfv35x9NFHx5577pnFqQEAAAAAAAAAAADIpR0qcNtQHTp0iOHDh8fw4cNzPQoAAAAAAAAAAAAAzawg1wMAAAAAAAAAAAAAQEsmcAsAAAAAAAAAAAAAWyFwCwAAAAAAAAAAAABbIXALAAAAAAAAAAAAAFshcAsAAAAAAAAAAAAAW1GU6wG2Zv78+bkeocF69eqV6xEAAAAAAAAAAAAAaAItOnB79NFHR5IkuR5juyVJEq+//nquxwAAAAAAAAAAAACgCbTowG1ERJqmuR4BAAAAAAAAAAAAgB1Yiw/ctrYTbgWEAQAAAAAAAAAAAPJLQa4HAAAAAAAAAAAAAICWrMWfcBvh1FgAAAAAAAAAAAAAcqdFB24HDx6c6xEAAAAAAAAAAAAA2MG16MDt3XffnesRAAAAAAAAAAAAANjBFeR6AAAAAAAAAAAAAABoyQRuAQAAAAAAAAAAAGArBG4BAAAAAAAAAAAAYCsEbgEAAAAAAAAAAABgKwRuAQAAAAAAAAAAAGArBG6zJE3TXI8AAAAAAAAAAAAAQBMQuG1CGzdujMcffzzOOOOMmDlzZq7HAQAAAAAAAAAAAKAJFOV6gOa0du3aWLp0aaxZsyY2bNgQlZWVkaZpvU6jraqqiqqqqqioqIiKiorYsGFDrFu3LlatWhVLliyJGTNmxL/+9a/YsGFDM9wJAAAAAAAAAAAAAM0l7wO3b7/9djzwwAPxxBNPxOzZs7O6V3VwN0mSrO4DAAAAAAAAAAAAQPPJ28Dtxo0b45e//GXcfvvtNSfZZluSJM2yDwAAAAAAAAAAAADNJy8Dt5WVlXH++efHc88959RZAAAAAAAAAAAAABolLwO3//u//xsTJ06MiNpB2zRN6wzebn4q7daCuVs7vfaj6wR8AQAAAAAAAAAAAPJD3gVu58+fH3feeWedQdvq2taCs1u6tvn6zd+XJEmkaRppmkbv3r3j4x//eJx66qmx9957N8XtAAAAAAAAAAAAAJBjeRe4/cMf/hAbN26sCcJGRK3Pu3btGnvvvXd06tQpSkpKYtq0afHGG2/UvKewsDBOPvnkiIjYtGlTbNiwIZYuXRpz586NJUuW1PTbPLybJEmMHj06jjnmmBzcMQAAAAAAAAAAAADZlHeB24cffrgmDLv56bP7779/jBo1Kg466KBa73/yySfjoosuqnldVVUV//3f/x37779/Ru958+bFQw89FHfeeWesWLGiJnibpmn84Ac/iP333z+6deuW3RsEAAAAAAAAAAAAoFkV5HqApjRjxoxYvHhxzevq02cHDRoUf/jDHzLCthERhx12WBQV1c4dP/HEE3X27927d3zta1+Lxx9/PI466qiaU3MjIpYuXRqXX355E90JAAAAAAAAAAAAAC1FXgVup06dmlErKCiIG264IUpKSupc065du9hrr71qwrlpmsZzzz231X06deoUt912WwwbNqxmXUTExIkT44EHHmj0fQAAAAAAAAAAAADQcuRV4Patt96q+bw6CHvEEUfE7rvvvtV1BxxwQK3XM2bMiFWrVm11TZIkceONN0b//v1rXqdpGjfffHOsW7eugXcAAAAAAAAAAAAAQEuTV4HbxYsXZ9SOPfbYba4bOHBgrddpmsaUKVO2ua6kpCSuueaaSNO0prZ06dK477776jEtAAAAAAAAAAAAAK1BXgVuV65cmVHbd999t7luzz33zKhNnTq1XnseeOCBMWTIkJoTddM0jT/+8Y/1WgsAAAAAAAAAAABAy5dXgdsNGzZk1Hbeeedtruvfv39GbcaMGfXed/jw4bVez507N6ZNm1bv9QAAAAAAAAAAAAC0XHkVuK2qqsqotW/ffpvrysvLo0uXLhERNafUzpo1q977Hn744Rm1iRMn1ns9AAAAAAAAAAAAAC1XXgVuS0tLG7y2T58+kaZpzet33nmn3mu7desWnTt3rlWbPHlyg2cBAAAAAAAAAAAAoOXIq8BteXl5Rm3dunX1WtunT59arzds2BDvvvtuvffu3r17pGlac0Lu7Nmz670WAAAAAAAAAAAAgJYrrwK3PXr0yKgtWLCgXms/GriNiJg1a1a9927btm2t10uWLKn3WgAAAAAAAAAAAABarrwK3NYVmn3jjTcavHbmzJn13nvt2rVbfQ0AAAAAAAAAAABA65RXgduBAwdm1P75z3/Wa+2uu+6aUXvttdfqvffChQsjSZKa10VFRfVeCwAAAAAAAAAAAEDLlVep0IEDB0ZpaWls2LAhkiSJNE3jqaeeioULF8bOO++81bV77rlnzefVaydNmhSVlZVRWFi41bWzZs2KlStX1grcdujQoXE30wJs2LAhnnjiiXjppZdi8uTJ8d5778WKFSuiqKgoOnbsGP369YuDDjooPvWpT8U+++yT63Eb5Utf+lJMnDixVm17TjgGAAAAAAAAAAAA8ldenXDbpk2bGDJkSKRpWlPbuHFjfOc734mNGzdudW2HDh2id+/etWorV66Mxx9/fJv7jh07tubz6r07d+68PaO3KBUVFXHrrbfGUUcdFd/+9rfjvvvui5kzZ8bSpUujoqIi1q1bFwsXLoznnnsuRo8eHaecckqcffbZMXXq1FyP3iD33ntvRtgWAAAAAAAAAAAAoFpeBW4jIk466aSaz6tPqn355Zfj85//fMyaNWuraw855JCawGz12ptuuilWr169xTUzZsyIu+++u9bptkmSxIABAxp5J7kxe/bsOP300+MXv/hFvP/++/Ve9+KLL8YZZ5wRo0ePrhV4bunmzp0b//M//5PrMQAAAAAAAAAAAIAWLO8Ct8cdd1z06NGj5nV1cHbq1Klx8sknx7e+9a2YMGFCVFRUZKzdPKxbbf78+XHWWWfFnDlzMq49//zz8aUvfanOXoMGDWrcjeTArFmz4qyzzorp06c3aH1lZWXccsst8f3vf79VhG7TNI3LL7881q5dm+tRAAAAAAAAAAAAgBasKNcDNLWioqK45JJL4vLLL68J21b/WllZGY8++mg8+uijcfXVV8cZZ5xRa+2QIUOiV69esWDBgoiImlNrZ8yYEccff3wcfvjhseeee0ZlZWW8+uqrMWXKlJr+H53hmGOOaZ4bbiLLli2LL33pS7FkyZI6r/fp0ycOPPDA6NGjR6xYsSJmzZoVr7zySp3B2rFjx0bPnj3joosuyvbYjfL73/8+Jk2alOsxAAAAAAAAAAAAgBYu7wK3ERGnnnpqPP744/HPf/6zVug2Imo+79OnT8a6JEni4osvjlGjRmWsq6ysjGeeeSaeeeaZmj7Va6pVv//Tn/50dOvWLdu32aRGjRoVCxcuzKjvsssucfXVV8fhhx+eESyePXt23HzzzfHEE09krLv11lvjsMMOi4MPPjhrMzfGW2+9FT/72c9yPQYAAAAAAAAAAADQChTkeoBsuemmm+KAAw6o8wTaiIi+ffvWuW748OG11lX/Wv159UdE1Nm3rKwsLrnkkia9l2x78sknY8KECRn1QYMGxYMPPhhHHHFEnfe62267xS233BKXXnppxrWqqqq49tpro6qqKiszN0ZlZWWMGjUq1q9fn+tRAAAAAAAAAAAAgFYgbwO35eXl8bvf/S5GjBhRKyQbEVFUVBS9evXa4tpbb7215vrmQdPq4O1HA7gRH55ue9VVV9V5em5LVVlZGTfffHNGvWfPnnHrrbdGeXn5Nnucd9558cUvfjGjPn369HjssceaZM6m9Nvf/jYmT56c6zEAAAAAAAAAAACAViJvA7cREW3bto0f/ehHcc8999Q6pbVXr151ntharUuXLnHnnXfGPvvsUyuo+1GbB2+LiorihhtuiFNOOaXJ7yObJkyYEG+//XZGfdSoUdGlS5d69xk1alQMHDgwo3777bc3ar6m9sYbb8Qtt9yS6zEAAAAAAAAAAACAViSvA7fVDjrooPjtb38b//jHP+LKK6+MESNGbHPNrrvuGvfdd1989atfjdLS0prTbOv6GDx4cIwZMyaGDx+e/ZtpYmPGjMmo7bbbbnHcccdtV5+CgoK4+OKLM+pTp06NmTNnNni+plRRURHf/e53o6Kiolb9xBNPzNFEAAAAAAAAAAAAQGtQlOsBmlOPHj3izDPPrPf7i4uL45vf/Gace+658fe//z1eeeWVWLhwYaRpGl26dIl+/frF0UcfHXvuuWcWp86eNWvWxDPPPJNRHzFixFZPAN6So446Krp37x6LFy+uVX/44Ydj7733bvCcTeVXv/pVvP7667VqI0eOjEGDBsXDDz+co6kAAAAAAAAAAACAlm6HCtw2VIcOHWL48OGt8gTbrXnppZcyTnuNiDj66KMb1K+goCCGDh0a9913X636hAkT4tJLL21Qz6YyderU+L//+79atV69esVll10WTzzxRI6mAgAAAAAAAAAAAFqDglwPQO48//zzGbVOnTpF//79G9zz4IMPzqjNnDkzli1b1uCejbVx48YYNWpUbNq0qVb9+uuvj/Ly8hxNBQAAAAAAAAAAALQWArc7sOnTp2fU9ttvv0b13NL6KVOmNKpvY/ziF7+I//znP7VqI0eOjMMOOyxHEwEAAAAAAAAAAACticDtDuzNN9/MqO2xxx6N6tm3b98oLCzMqM+cObNRfRvq1VdfjTvuuKNWrXfv3nHZZZflZB4AAAAAAAAAAACg9RG43UG9//77sWzZsoz6brvt1qi+xcXF0bNnz4z63LlzG9W3IdavXx+jRo2KqqqqmlqSJHH99ddHeXl5s88DAAAAAAAAAAAAtE5FuR6gJVq6dGnMmjUrVqxYEZs2bYp27drFrrvuGrvuumskSZLr8ZrEokWL6qz36NGj0b27desW7777bq3avHnzGt13e910000xe/bsWrWRI0fGoYce2uyzAAAAAAAAAAAAAK2XwO3/b9GiRfGnP/0pHn744YywaLV27drFYYcdFqeeemp88pOfbOYJm9Z7771XZ71r166N7l1Xjy3tly0vvvhi/OEPf6hV6927d1x22WXNOgcAAAAAAAAAAADQ+uVF4Hbjxo2xYsWKaNu2bZSXl2/X2srKyvj1r38dt912W2zcuDHSNN3ie1evXh1PPvlkPPnkk7H77rvHpZdeGsccc0xjx8+JpUuX1lnv2LFjo3u3b98+o7Zq1apG962v1atXx+WXX17rn2WSJHH99ddHu3btmm0OAAAAAAAAAAAAID+02sDtlClTYuzYsfH888/HnDlzauodOnSIAw88ME4++eT4zGc+EwUFBVvssXr16vj6178ekyZNqglnJkmy1X2r3/fWW2/FhRdeGMOGDYtrr702ysrKmuCums/q1avrrDdFILWur8XKlSsb3be+fvzjH8e8efNq1T73uc/FoYce2mwz5JNOnZr393Zh4Zb/nQXItsLCgmZ/7rVEnsVArngOf8BzGMgVz+EPeA4DueRZ7DkM5Jbn8Ac8i4Fc8Rz+gOcwkCuew9vW6gK3s2fPjhtuuCGefvrpiIiME2lXrFgREyZMiAkTJsTo0aPjqquuiiFDhmT02bBhQ3zxi1+M119/PdI0rRW03dIpt0mSZLzvkUceibfffjt+97vfRYcOHZriFptFRUVFnfXS0tJG966rx4YNGxrdtz6eeeaZuO+++2rVdtlll/jOd77TLPvnmyRJori4MNdjADSbgoIkCgo89wByxXMYILc8hwFyz7MYILc8hwFyy3MYILc8h7etVf2ViPHjx8eIESPi6aefjjRNa4KyH/2ovvbWW2/FueeeG3fccUdGrx/+8Icxbdq0iPjwVNvNT7mt66P6PZu/L03TmD59elx44YVbDOq2RBs3bqyzXlTU+Ax2YWHmv3SbNm1qdN9tWblyZXz/+9+vVUuSJK6//vomObkXAAAAAAAAAAAA2DG1msDt/fffH9/5zndi7dq1tYK2ER+GYOsKzFZWVsZPfvKTuPvuu2t6TZo0KcaNG1dn0Paj/bbUe/PAb5qmMWnSpPj973/fbF+PxtrSCbd1hWW3V12h3aqqqqiqqmp076257rrrYtGiRbVqn//85+s84RgAAAAAAAAAAACgvhp/nGkz+Pe//x0/+MEPoqqqaoun0W5JdSD2xz/+cRxwwAGx//77x80331xzvTo0W/35zjvvHKeeemoMHjw4evToERUVFbFw4cJ49dVXY9y4cbF48eJaodvN9/jVr34Vp512WpSXl2frS9FktvQ1a4pTeisrK+vcr6Age/nup556Kh588MFatV122SW+/e1vZ21PAAAAAAAAAAAAYMfQ4gO3lZWV8d3vfjcqKyu3eiJtXaqvJ0kSmzZtiiuvvDJ+/OMfx7///e9a16pDt1/96lfjggsuiJKSklp99tlnnxg6dGhcfPHF8atf/Spuu+22mnk2D+yuXLky7r///jjnnHOa/OvQ1Oo6hTai7rDs9qqrx0e/pk1p2bJl8YMf/KBWLUmS+NGPfhRlZWVZ23dHkKZpbNqU3ZOJP6qwsCAKCrYcogfIpqqqNCorm/e51xJ5FgO54jn8Ac9hIFc8hz/gOQzkkmex5zCQW57DH/AsBnLFc/gDnsNAruTyOVxUVLDVg1dbihYfuB03bly88847tcK1m3/ep0+fGDlyZBx88MHRpUuXWL16dUyZMiX+/Oc/x+uvv17rH8Ibb7wR3/rWt2r1r+53zTXXxOmnn77VWQoLC+PCCy+MAQMGxEUXXVRrlupeDzzwQKsI3G4pALtp06ZG966rRzYDt1dffXUsXbq0Vu3MM8+MQw45JGt77kiWL1/brPt16lQWBQWFzbonQLXKyqpmf+61RJ7FQK54Dn/AcxjIFc/hD3gOA7nkWew5DOSW5/AHPIuBXPEc/oDnMJAruXwOd+1anpN9t1dBrgfYlnvvvbfm8+qAa/WJtuecc0488sgj8eUvfzkOPPDA6Nu3bwwcODBGjhwZY8eOjQsuuKDWabhpmsZbb72VEd4dPnz4NsO2mzvmmGPiK1/5Sk3vzYO3M2fOjAULFjTJvWdTeXndv0HXrm38vzB19SgtLW1037qMHz8+HnvssVq1Pn36xKWXXpqV/QAAAAAAAAAAAIAdT4sO3M6dOzemTZtWE5bd/NczzjgjRo0aFcXFxXWuTZIkLrroojjttNNqhW4/qrCwMC655JLtnu3LX/5ytGvXrs6+L7/88nb3a26dOnWqs7569epG916zZk1GrXPnzo3u+1FLliyJa665plYtSZL40Y9+FGVlZU2+HwAAAAAAAAAAALBjatGB20mTJtV8vnmotby8PL7zne/Uq8ell14abdu2zehTHdw95JBDokePHts9W/v27ePTn/50TZh3c9OmTdvufs1tS4Hb5cuXN7r3+++/n1HbaaedGt33o6688sqMec8666z4+Mc/3uR7AQAAAAAAAAAAADuuolwPsDVTpkyp9bo6JHvsscdGeXl5vXp07tw5jj766Bg/fnytE3KrNSacecghh8S4ceMy6rNnz25wz+bSu3fvOutLly5tdO+6enTt2rXRfTc3YcKE+Mc//lGrliRJFBcXxy233LJdvaZPn15nva4+7du3j3POOWe7+gMAAAAAAAAAAACtW4sO3L7zzjt11g866KDt6nPYYYfF+PHj67zWv3//7Z6r2sc+9rGMWpqmsXDhwgb3bC49evSI4uLiqKioqFVfsGBBo3vPnz8/o9a3b99G993ce++9l1FL0zTuuOOOJttj9OjRGbXevXsL3AIAAAAAAAAAAMAOpiDXA2zNwoULa51GW217w5t77LHHFq/16dNnu+eq9tFTW6tnXbZsWYN7NpeCgoLYddddM+qNPZ132bJlsWrVqoz6nnvu2ai+AAAAAAAAAAAAALnSogO377//fp31nXfeebv69OjRY4vXOnfuvF29NldWVlZnva7AaUu07777ZtRmzpzZqJ4zZsyosy5wCwAAAAAAAAAAALRWLTpwu379+jrr7du3364+W3v/9vbaXElJSZ31ioqKBvdsTvvvv39Gbfr06Y2af/LkyRm1Dh06CNwCAAAAAAAAAAAArVZRrgfYmg0bNtRZb9eu3Xb1KS0t3eK1oqKm+RKkaRpJkkRERGVlZZP0zLZDDz00o7Zu3bp45ZVX4pBDDmlQz2effTajdsghh0RhYWGD+m3JiBEjYsSIEU3S6y9/+UtcfvnlGfXGnvYLAAAAAAAAAAAA5IcWfcJtVVVVRHwQZt3c9oY3txaqbeogaGuyxx57RO/evTPq48ePb1C/BQsWxKuvvppR/8QnPtGgfgAAAAAAAAAAAAAtQYsO3G5JU51KS8RJJ52UUXvooYdi2bJl293rrrvuyjjdt23btjFs2LAGzwcAAAAAAAAAAACQa60ycEvTOe2006KgoPZvg7Vr18b111+/XX1ef/31uPvuuzPqw4YNi/Ly8kbNCAAAAAAAAAAAAJBLArc7uD59+sSJJ56YUX/44Yfjtttuq1ePJUuWxPnnnx8VFRW16sXFxXH++ec3yZwAAAAAAAAAAAAAuSJwS1x88cXRtm3bjPrPfvazuOGGGzKCtJubOnVqjBw5MhYuXJhx7cwzz4w+ffrUa4Zbbrkl9t5774yPUaNG1f9GAAAAAAAAAAAAALJA4Jbo06dPXHrppXVe+93vfhcnnXRS3H333TFr1qxYt25dLF26NF588cX47ne/G5/73Odi3rx5Gev22muv+OY3v5nt0QEAAAAAAAAAAACyrijXA9AyfOELX4hp06bFuHHjMq69/fbbcd1119W7V+fOnePnP/95lJaWNuWIAAAAAAAAAAAAADnhhFtqXHfddTFixIhG9ejatWvcddddscceezTRVAAAAAAAAAAAAAC5JXBLjaKiorjhhhvimmuuifbt22/3+k9+8pPx4IMPxl577ZWF6QAAAAAAAAAAAAByoyjXA9DyjBw5Mo477rj4wx/+EOPGjYt33313i+8tKSmJI488Ms4+++wYMmRIM04JAAAAAAAAAAAA0DwEbqlTp06d4sILL4wLL7ww5syZEzNmzIj58+fH2rVro7S0NDp27Bi77bZb7LffflFSUtLo/S666KK46KKLmmDy7TdixIgYMWJETvYGAAAAAAAAAAAAWr5WGbg9++yzW2SvfNW3b9/o27dvrscAAAAAAAAAAAAAyIlWE7hN07Tm10mTJjW6R2N71dUXAAAAAAAAAAAAgPzTagK3m2vKgKuwLAAAAAAAAAAAAABb0yoDt0mSbPeaLQVrG9KrPn0BAAAAAAAAAAAAyA+tInDb2FBsU/Vozr4AAAAAAAAAAAAAtAwtPnDrBFkAAAAAAAAAAAAAcqlFB25PPfXUXI8AAAAAAAAAAAAAwA6uRQdub7jhhlyPAAAAAAAAAAAAAMAOriDXAwAAAAAAAAAAAABASyZwCwAAAAAAAAAAAABbIXALAPD/sXff0VGV69vHr5l00iCk0BKaFAUBBenVAoiC2MDKOXb0h4hyjr0h6LHgsTdQOQpWBEQEQYogVar03lsggZDeZ79/+CYy7Emdlgzfz1qzyNx7P89z74xO1tq58gwAAAAAAAAAAAAAAABQCgK3AAAAAAAAAAAAAAAAAAAAQCkI3AIAAAAAAAAAAAAAAAAAAAClIHALAAAAAAAAAAAAAAAAAAAAlILALQAAAAAAAAAAAAAAAAAAAFAKArcAAAAAAAAAAAAAAAAAAABAKQjcAgAAAAAAAAAAAAAAAAAAAKUgcAsAAAAAAAAAAAAAAAAAAACUgsAtAAAAAAAAAAAAAAAAAAAAUAoCtwAAAAAAAAAAAAAAAAAAAEApCNwCAAAAAAAAAAAAAAAAAAAApSBwCwAAAAAAAAAAAAAAAAAAAJSCwC0AAAAAAAAAAAAAAAAAAABQCgK3AAAAAAAAAAAAAAAAAAAAQCkI3AIAAAAAAAAAAAAAAAAAAAClIHALAAAAAAAAAAAAAAAAAAAAlILALQAAAAAAAAAAAAAAAAAAAFAKArcAAAAAAAAAAAAAAAAAAABAKQjcAgAAAAAAAAAAAAAAAAAAAKUgcAsAAAAAAAAAAAAAAAAAAACUgsAtAAAAAAAAAAAAAAAAAAAAUAoCtwAAAAAAAAAAAAAAAAAAAEApCNwCAAAAAAAAAAAAAAAAAAAApSBwCwAAAAAAAAAAAAAAAAAAAJTC39sNeFJOTo4OHjyo06dPKzMzU7m5uSosLJTNZnP5WoMHD3b5nAAAAAAAAAAAAAAAAAAAAPA8nw/c5ubmatq0aZo/f77WrFmjwsJCj6xL4BYAAAAAAAAAAAAAAAAAAMA3+HTgdvbs2Ro/frwSExMlSYZheGRdi8XikXUAAAAAAAAAAAAAAAAAAADgfj4buH399dc1adIku5CtJ4Kwngr1AgAAAAAAAAAAAAAAAAAAwDN8MnD7zTff6PPPP5fEbrMAAAAAAAAAAAAAAAAAAABwjs8FbpOSkvTGG2/YBW0NwzA9BwAAAAAAAAAAAAAAAAAAAMrD5wK3n3/+ubKysmSxWIqDtUVhW8MwFBISohYtWqhhw4aKiopSYGCgAgICvNkyAAAAAAAAAAAAAAAAAAAAqjCfCtwWFBToxx9/LA7Ynh20jY2N1ejRo9W/f38FBQV5s00AAAAAAAAAAAAAAAAAAABUIz4VuF2/fr1SUlLsgrYWi0UXXHCBvvzyS0VFRXm5QwAAAAAAAAAAAAAAAAAAAFQ3Vm834Err16831fz9/fXuu+8StgUAAAAAAAAAAAAAAAAAAECl+FTgdv/+/cVfF+1uO2jQIDVp0sSLXQEAAAAAAAAAAAAAAAAAAKA686nA7alTp0y1AQMGeKETAAAAAAAAAAAAAAAAAAAA+AqfCtzm5uaaahdeeKEXOgEAAAAAAAAAAAAAAAAAAICv8KnAbUhIiKlWs2ZNzzcCAAAAAAAAAAAAAAAAAAAAn+FTgduoqChTLTMz0wudAAAAAAAAAAAAAAAAAAAAwFf4VOC2cePGptqJEye80AkAAAAAAAAAAAAAAAAAAAB8hU8Fbi+99FJTbe3atV7oBAAAAAAAAAAAAAAAAAAAAL7CpwK3l1xyiSIjI+1qc+fO9VI3AAAAAAAAAAAAAAAAAAAA8AU+Fbj19/fXDTfcIMMwZLFYZBiG/vjjD23atMnbrQEAAAAAAAAAAAAAAAAAAKCa8qnArSTdd999Cg8Pl6Ti0O3TTz+t3NxcL3cGAAAAAAAAAAAAAAAAAACA6sjnArdRUVF64oknZBhGcW3v3r3617/+pby8PC92BgAAAAAAAAAAAAAAAAAAgOrI5wK3knTTTTfptttuk2EYxbvcLliwQDfeeKOWLFni7fYAAAAAAAAAAAAAAAAAAABQjfh7uwF3ef7552W1WjVlypTi0O3u3bs1fPhwxcXFqXv37mrdurUSEhJUu3Zt1ahRQ35+fi5bv169ei6bCwAAAAAAAAAAAAAAAAAAAN7jc4Hb//73v8Vf16hRQw0bNtTBgweLQ7eGYSgxMVHTpk3TtGnT3NKDxWLRtm3b3DI3AAAAAAAAAAAAAAAAAAAAPMvnArcTJkyQxWIx1Q3DsKsbhuHJtgAAAAAAAAAAAAAAAAAAAFBN+VzgtoijQG1RzWKxOAzlumtdAAAAAAAAAAAAAAAAAAAAVF8+G7h1V6AWAAAAAAAAAAAAAAAAAAAA5xefDdyy0ywAAAAAAAAAAAAAAAAAAABcwecCt5dddpm3WwAAAAAAAAAAAAAAAAAAAIAP8bnA7eTJk73dAgAAAAAAAAAAAAAAAAAAAHyI1dsNAAAAAAAAAAAAAAAAAAAAAFUZgVsAAAAAAAAAAAAAAAAAAACgFARuAQAAAAAAAAAAAAAAAAAAgFIQuAUAAAAAAAAAAAAAAAAAAABKQeAWAAAAAAAAAAAAAAAAAAAAKIW/txvwtqysLB0+fFgnTpxQRkaGcnNzFRAQoJCQENWoUUOxsbGKj49XYGCgt1sFAAAAAAAAAAAAAAAAAACAF5x3gVvDMLRs2TItXLhQq1ev1oEDB2QYRqljLBaL6tSpow4dOqhz58664oorFBkZ6aGOAQAAAAAAAAAAAAAAAAAA4E3nTeC2oKBA33zzjSZNmqTjx49LUplB2yKGYejYsWOaNWuWZs2apbFjx+raa6/VAw88oAYNGrizbQAAAAAAAAAAAAAAAAAAAHiZ1dsNeMLGjRt1zTXX6JVXXtGxY8dkGIYMw5DFYqnQo2hcdna2fvjhBw0cOFCfffaZty8PAAAAAAAAAAAAAAAAAAAAbuTzgdtvvvlGt99+uw4dOmQK2VbUueHb7OxsjR8/XiNHjlRubq4bugcAAAAAAAAAAAAAAAAAAIC3+XTgdsKECXrppZdUUFAgSaWGbIt2rz33UZKzg7fz58/Xgw8+qMLCQpdfAwAAAAAAAAAAAAAAAAAAALzL39sNuMusWbP03//+V9LfQduiHW6Lvi4SGxurevXqKTw8XBEREbLZbEpPT1dGRoaOHTumkydPFp97bmi3KHS7cuVKvfDCCxo3bpy7Lw0AAAAAAAAAAAAAAAAAAAAe5JOB24MHD+r555+XZB+QLQrHWq1W9e7dW4MHD1a7du0UFxdX6nzJycnasmWL5syZo19//VU5OTmmEK9hGJo2bZquvPJK9e7d223XBgAAAAAAAAAAAAAAAAAAAM+yersBdxg7dqyys7OLg7BFDMNQnz59tGDBAn300Ufq169fmWFbSYqOjlbv3r31+uuva+nSpbr77ruLA7dn/2sYhsaMGaP8/Hz3XBgAAAAAAAAAAAAAAAAAAAA8zud2uF23bp2WLVtmCsJarVa98MILuuWWW5yaPzw8XI8//riuuOIKPfzww0pJSbE7npiYqBkzZmjIkCFOrQMAAAAAAAB4m2EYKijIV25utgoLC2Sz2WSz2SQZZY6trtLT/WW1Wuxq2Rk56texjpc6On8cPXJY/n72e0TkZ2VJF/bxUkfnh0NHjspi9bOrZWVn6IoGXb3U0fkj61SGDmQdsKvZbIby8gq805DbWWS1WosfgYHBCgwMksXik3vDAAAAAAAAH+RzgdvJkyfbPTcMQxaLRWPGjNHNN9/ssnXat2+viRMn6h//+IcyMzMl/R3unTRpEoFbAAAAAAAAVFt5eTnKyclSTk6WbLZCb7fjUTZbfvEf8xcpLCxQdGSQlzo6f+Tm5Cj/nLBzYX6hFBblpY7OD9k5OabAoy2/ULWDa3qnofOILb9Q2cq2qxmGocJCm5c68qzMzDRJFgUGBisoKFghIaGynhP+BgAAAAAAqEp86s+Gs7OztXjx4uIb4kVh25tvvtmlYdsirVq10gsvvCDDsN/R48CBA9q1a5fL1wMAAAAAAADcyWYr1JkzyTp9+oSystLPu7AtAMDTDOXlZSs9PUVJSceVlZVh+p0LAAAAAABAVeFTO9xu2LBBOTk5djtQREREaPTo0W5bc+DAgfruu++0du1au3UXL16s5s2bu21dAAAAAAAAwJVycnKUnHz8vA/ZFhYakuzDXn4Wi+rUDvdOQ+cRm80mm81+h1tZ/ORfM847DZ0n/vpv3n5HVav8FBcW652GziNW+amg4PzYzbY8DKNQaWmnZLPlKiEhXn5+7HYLAAAAAACqFp/a4XbTpk3FXxftbtu/f39FRka6dd2hQ4eaalu2bHHrmgAAAAAAAICr5Obm6tixI+d92BYA4H3Z2Vk6fPiwCgv5mQQAAAAAAKoWn9rh9tChQ6baVVdd5fZ1+/TpI39/fxUWFspiscgwDO3evdvt6wIAAAAAAADOKiws1KFDh8oRbLLIarXKavWz+6QnX+Po2gptNuUX5nuhm/NLYGCgJPvvv2ErVCFBcLfyDwzSud/3Qluh8sXOq+7mHxAoP6t5F1fDMBycXf0ZhvH/d7Iu1Lk7iZ8rOztbR44cUUJCgk//zAEAAAAAANWLTwVuT506Zao1btzY7euGhYUpOjpaJ06cKK6lpKS4fV0AAAAAAADAWcnJySooKCjhqEUhIaEKCgpRYGCwrFaf+sAsh/z8rDo325WTW6DspDTvNHQeiY6uJav1nOBnXq5yTid6qaPzQ42YuqZAY05+rnJST3qpo/NHrcgYBQcE2dUMQyos9O2ws2EYys/PU15etnJzc5Sfn3vOcclikbKyspSamqqaNWt6p1EAAAAAAIBz+FTgNjs721SLiYnxyNrR0dFKTEwsvjGZkZHhkXUBAAAAAACAysrLyyvxD8cDA4MVGVlbfn4+dQsRAOBlFotFgYFBCgwMUliYlJOTpbS00/9/59uiHX7/+l3LyZMnFR4e7sVuAQAAAAAA/uZTW1KEhISYajk5OR5ZOycnx24XgPNhtw8AAAAAAABUb2fOnCn+6HKb7e+P9w4ICFKtWjGEbQEAbhccXENRUXGyWv0k/bXDbdHPpsLCQqWmpnqzPQAAAAAAgGI+lQqtVauWqZaUlOSRtc9dJzQ01CPrAgAAAAAAAJWVmZlZ/LXxd95WkZFRslh86tYhAKAK8/cPUEREVPHzs/8IhE8UBAAAAAAAVYVP3TV3FLhdt26d29fdu3dv8V9YF/3Vdd26dd2+LgAAAAAAAFBZhYWFxZ8OZZyVtg0KCpG/f6C32gIAnKeCg2sU//w5e5fbrKwsu59TAAAAAAAA3uJTgdsWLVqYar/++qvb1z13DYvFosaNG7t9XQAAAAAAAKCysrKyir8+O8cUGBjshW4AAJCCg0OKvy762WQYhgoL8rzUEQAAAAAAwN98KnDbqVOn4q8tFosMw9CKFSu0ceNGt62Znp6uL7/8UhaLxa5+ySWXuG1NAAAAAAAAwFn2gdu/E7cEbgEA3hIYeHbg9u+fTTYCtwAAAAAAoArwqcBt3bp11bBhQ7uazWbTmDFjlJfnnpsxr7/+ulJSUkz1Xr16uWU9AAAAAAAAwBUKCwtNNYvFKn//AC90AwCAFBAQaNrgRJIMw+aFbgAAAAAAAOz5VOBWkoYOHVr8V89FN2W2b9+uUaNGyWZz7Q2Zjz/+WFOnTi1exzAMWSwWtW3bVvHx8S5dCwAAAAAAAHClkgK3joJOAAB4gsVikdVq/tUVgVsAAAAAAFAV+GTgNjw8vPi5xWKRYRj67bffdO+99+rEiRNOr5Gbm6uxY8fqnXfecfgLiLvvvtvpNQAAAAAAAAB3OvuP04s+tdtRyAkAAE8q+llU9LNJkgwXb6gCAAAAAABQGT53Bz00NFT33Xdf8S63RbvOGoahlStXauDAgZowYYJOnTpV4blzcnL0ww8/aNCgQfr666+L1zh7nXbt2qlv374uux4AAAAAAADAHc6+t1WE3W0BAN7GzyIAAAAAAFBV+Xu7AXe49957tXjxYq1fv744bFv0b1pamt566y29++676tKli9q2bavWrVsrPj5e4eHhCg8Pl5+fn9LT05Wenq4TJ05o69at2rx5s5YtW6aMjIziX0ace9MnODhY48aN88YlAwAAAAAAAAAA+AACtwAAAAAAoGryycCt1WrVG2+8oRtvvFGpqal2oVvpr907CgoKtGzZMi1btqzc85YUtDUMQ1arVWPHjlXTpk1ddyEAAAAAAAAAAAAAAAAAAADwOqu3G3CX+vXr67PPPlN4eLgk+5CsxWIpDuFW5FE07mxF9WeeeUbXXnutR68RAAAAAAAAAAAAAAAAAAAA7uezgVtJatWqlb777js1adKkeHfasxUFaMv7OJdhGAoPD9e7776r22+/3ROXBAAAAAAAAAAAAAAAAAAAAA/z6cCtJDVu3FjTp0/X8OHDFRgY6DB4W1FFc/Tr108///yzrrrqKqfnBAAAAAAAAAAAAAAAAAAAQNXk7+0GPCEoKEijRo3SsGHDNGXKFP300086cuSI3TmOdrCVZArohoWFacCAAbrtttvUsmVLt/UMAAAAAAAAAAAAAAAAAACAquG8CNwWiYqK0siRIzVy5Ejt2LFD69at05YtW3Tw4EElJiYqLS1NOTk5MgxDgYGBCg0NVVxcnOLj49WyZUtdeumluvTSS+Xvf1592wAAAAAAAAAAAAAAAAAAAM5r521ytGXLluxQCwAAAAAAAAAAAAAAAAAAgDJZvd0AAAAAAAAAAAAAAAAAAAAAUJURuAUAAAAAAAAAAAAAAAAAAABKQeAWAAAAAAAAAAAAAAAAAAAAKIW/txsAAAAAAAAAAHjebTddWeY5Pfv00/D/+7cHupESE4/rxhsHyjCMEs8ZNnSo/nHrLR7pp7q4YvD1plpcTIy+njjBC9047+G771PyyZOm+jc/z/RCNwAAAAAAAMDf2OEWAAAAAAAAAODQ2tXLVZCf75G15s+fV2rYFgAAAAAAAAC8qUrvcLtmzZoSj1122WUVHuNJJfUHAAAAAAAAANVFVmaGNm5cq/Ydurh9rfnz57p9DQAAAAAAAACorCoduL3zzjtlsVhMdYvFom3btlVojCeV1h8AAAAAAAAAVCerli92e+D26JGD2rNnt1vXAAAAAAAAAABnVOnAbZHKfIwYHz0GAAAAAAAAAM5bv3al8vLyFBgY6LY1li9d6La5AQAAAAAAAMAVrN5uoDwsFkvxozJjPPkAAAAAAAAAAF+SnZ2lP9evcusaK5cvduv8AAAAAAAAAOCsahG4BQAAAAAAAAB4jzsDsXt2bdeJxGNumx8AAAAAAAAAXKHKB24Nw7B7VGaMJx8AAAAAAAAAUJ3FJzQy1f5cv1o5OdluWW/5skWmmr+/v1vWAgAAAAAAAIDKqtJ3La+//nqPjAEAAAAAAAAA/KVLt8t1+NDndrXc3BytX7tKXbv3celatsJC/bFiianeqVMXLV++1KVrAQAAAAAAAIAzqnTg9j//+Y9HxgAAAAAAAAAA/tKle29N/XaS6RO9Vq1Y7PLA7dYtf+rMmdN2tSZNmqpZs+YEbgEAAAAAAABUKVZvNwAAAAAAAAAAqDqio+PUrPlFpvrGDauVlZXp0rVWLFtkqvXt29+lawAAAAAAAACAKxC4BQAAAAAAAADY6eJgJ9v8/HytXb3cZWvk5+dpzR/LTHUCtwAAAAAAAACqIn9vNwAAAAAAAAAAqFo6d+mlyZM+lM1ms6uvWrFYPXv3dckaG9b9Ydox9+KL26pevfoumb80hmFo/8FD+nPLZh09dlxp6elKTUuTzWZTcHCwYmrXVoN69dSqZQs1a9pUfn5+bu9px+7d2rpjh/YdOKDUtHRlZWcrOChIMdG1dUGTJup0aXvFxkS7vY9znUpO1o4tW3Vw/wElnTyhrIxM5ecXKCgoSOGREapTr54uaNFcLS68UEHBQR7vDwAAAAAAAPAUArcudPLkSe3atUshISFq3769t9sBAAAAAAAAgEqJrFlLF7Vupy2b1tvVN29ar4z0NIWFRzi9xoplv5lq/fpd7fS8pTlx8qR++GmWFixZorT09HKNqRkZqT49uuvGgQNVNy7Opf3k5OZqxuzZmvPrfB1LTCz1XIvFoo7tL9W9d9ypJo0aurSPcxUUFOi3Xxdo0bx52rNzV7nGBAUHq2OXzrrmhuvVsHEjt/YHAAAAAAAAeINPBW4nTJig/fv3Fz+3WCx65ZVX3Lrm8ePH9cQTT2jXrl1KTU2VJF111VUEbgEAAAAAAABUa1269TEFbgsLCrRm9XL1ucK5YGx2dpY2rF9lV/Pz89OVV17l1LwlycjI1Ieff64FS5aosLCwQmPPpKZqxs+zNXPOL7ru6qv1z9tuVVhoqNM9/b5ipd7/9FOdOn26XOcbhqE/1q7T+j83atgtt2jo9YPdsvPu8uVLNX78azp+/FiFxuXm5Gjpb4u1bPES9bi8j+645y6FRzgfzAYAAAAAAACqCp8K3C5evFgbNmyQ9NfNR08EbgMDA7V69WpZLBYZhiFJOnDggFvXBAAAAAAAAAB369i5hyZNfFcFBfl29VXLFzsduF3zxzLl5+XZr9exs2rWrOXUvI78uXmLXnvnHZ1MTnZqHpvNphmzZ2vV2rV6/vF/q3nTppWe5+NJ/9O0WbMqNT6/oECfTZmiXXv36rl/ja7UHI4UFOTrnXfe0tSp3zo1j2EY+n3hIm3dtEmPPv2kmjZr5qIOAQAAAAAAAO+yersBVysKvXpKZGRk8dcWi0WSlOzkjVsAAAAAAAAA8LbQ0DC1adfBVN+6ZYNSU1OcmnvFskWmWt++/Z2a05G5Cxfq3y+84HTY9mzHT5zQY888q/UbN1Vq/NsffVzpsO3Zlq5cqf9++JHT80hSXl6ennjiX06Hbc92KilZY596Vts2bXbZnAAAAAAAAIA3+dQOt96Qd84uDJKUkZHhhU4AAAAAAAAAwLW6du+j9WtX2tVsNptWr1qqq/oNqtScaalntHXzBrtaUFCwevbsU+k+Hflt2TKNf/+DEjdpCA4KUvfOndW9cyc1bthQtWvVks0wdDolRTv37NHSlSu1cs1aFRYWmsZm5+TouVde0ZvjxqplBXZw/Wb6dM2eP7/E482bNlXfPn10Sds2io6KktVqVfKpU9q4dasWLF6iLdu3250/d+HCcq9dEsMwNGbMc1q+fGmJ5zRs0lhde/ZQ63ZtVTs6WqGhoUpNTdWppCStX71WK39fqpMnTpjG5ebkaPzYlzXmjdcU36ih070CAAAAAAAA3kTg1kmbNpl3MbDZbF7oBAAAAAAAAABc69IOXRQUFKzc3By7+qrliysduF25YrEpxNr+si4KCQmpdJ/nOnj4cKlh255du+jBu+5WbEy06VhojRqKr19fV/bqpf0HD+rdCRO0aes203k5ubl64dXX9Ok7bys8LKzMnvYeOKAvvv7G4bGQ4GANv+suXdP3quJPUiuS0KCBEho00MB+/bRk+XK9O2GizqSmlrleeU2Z8oUWLnQcAg6PiNBdDz6gzt27mfqqHR2t2tHRan7hhbrptlv007QZ+vH7qco/Z5OK7Oxs/ffl/+jV995RUHCQy/oGAAAAAAAAPM3q7Qaqs8OHD2vcuHGmelg5bq4CAAAAAAAAQFUXHByiS9p3NtV37NiilJRTlZpz5bLfTLWu3a+o1Fwlefvjj5WTk+Pw2D9vvVUvPP64w7DtuRo3bKjxL72kfpdf7vB48qlT+uDTz8rV08eT/qf8ggJTvUZIiF578QVd26+vKdR6rl7duumd/7yi6Nq1y7VmWQ4fPapPP/3E4bEGCQl6+e031aVH9zL78g8I0A23DNETLz6noCBzqDbx+HH98PXXLukZAAAAAAAA8JYqvcPtjz/+qOnTp5f7/F27dplqw4YNc2VLMgxD+fn5SklJ0eHDh2UYRvHNxqLdEho0aODSNQEAAAAAAADAW7p276NVKxbb1QybTX+s+F39r7m+QnMlJZ3Q7l32u8WGhUeoTbsOzrZZbMXq1Q53pJWkO4cO0Z1Dh1RoPj8/P/374RHKy8/Tb0uXmY4vWLJENw4aqGZNmpQ4x9YdO7R+40aHx55+7FG1atmy3P00qFdPb4x5UQ88Nlp55+wmW1GfTfnK4Ry1oqL05JjnVTsmpkLztWrTRo89+7Reff5F0+7C836eo6sHDVJUtGvCwgAAAAAAAICnVenA7RVXXKHXX39dKSkpFRpXdCPPMAytWbPGHa2ZbhYWhW4tFosuueQSt6wJAAAAAAAAAJ7W7pKOqhEapqzMDLv6qhWLKxy4XbF0oeneaqcuPeXv77pb1VNnznRYb9msme4cUrGwbRGLxaJRDwzX1u07dDI52e6YYRia+uNMPf3YoyWOnzV3nsN67+7d1eWyyyrcT0KDBho2dIg+nTylwmOLHDl2TMtWrXJ47Iknn6lw2LZIm0va6coB/TV/9i929fy8PM37ebZu/adrN8kAAAAAAAAAPMXq7QZKEx4erkcffdQuQFvW41zlGVOZh8ViKX6c67rrrnP79wYAAAAAAAAAPME/IECXdexmqu/etU3JSScqNNeKZYtMtW49rqh0b+c6cfJkibvbPnj3XfLz86v03GFhobr7jtsdHvt9xQplZGQ6PJabm6vlf/zh8NidQ26udD9DBg9WrZo1Kz1+7kJz+FmSLrmkvTp36VrpeSVp0I03ymo1//rh90WLZLPZnJobAAAAAAAA8JYqHbiVpJtvvlmtW7eWJLuQa0mPc5VnTGUe565R9O+gQYOK+wUAAAAAAAAAX9ClWx9TzTAMrVqxpNxzHDl8QIcPHbCrRUfHqkVL191PXVZCsLVRQrxaX3ih0/P36tZNNSMjTPX8ggKt3rDe4ZhN27YpKzvbVL+gcWM1SkiodC9+fn66vGePSo9ftsrx9+qGG26q9JxFomNjdKGD++RnTqdo/569Ts8PAAAAAAAAeEOVD9xK0nPPPeftFkpVtOttz549NXbsWG+3AwAAAAAAAAAu1friSxQRWdNUX7VicbnnWPb7QlOta4/LHW6kUFmbt213WL+yVy+XzB8YEKBe3cy7/UrSxi1bSujJ8Y67XTt2dLqfy7t3r9S4k0nJOnz0qKnu7++vrl0dX19FtW7XxmF966ZNLpkfAAAAAAAA8LRqEbht27atBg8eXBxsLe1xrvKMqewjICBACQkJ6tu3r95++2198sknCgwM9MJ3CAAAAAAAAADcx+rnp05dzKHVfXt36UTisXLNsXL5b6Za1+6XO93b2Xbt2eOw3rJ5c5etcWEJc+3eu89hfeduxz01bdzY6V6aNm4sPz+/Co/btddxT/HxCQoNDXO2LUlSoyZNHNYPlPB9AgAAAAAAAKo6f283UF4vvviiRowYUeJxwzA0atQobdmyRRaLRYZhyGKxaMGCBS7rwWKxyM/PT/7+/goJCVGNGjVcNjcAAAAAAAAAVGVduvXW/LkzTfVVKxbruhtuK3Xsrp3blHQy0a4Wn9BYCQ0dhzIro6CgQEmnTpnqFotFzZs2ddk6LZs1c1g/evx4hepNGjV0upeAgAAl1K+v/YcOVWjcgUOHHdYbNIh3uqcicXXrOKwfPXzEZWsAAAAAAAAAnlRtArdBQUGqX79+qec42l22rDEAAAAAAAAAgLK1aNla0dGxSk4+aVdftWJJmYHbFcsWmmrderh2d9ukU6dks9lM9ciIcIW6cPOEOrGxDusZmZnKzslRSHBwcc1ms+lkcrLD82Ojo13ST4NKBG5L6mnp0iXq3PlSV7RVotOnzaFoAAAAAAAAoDqwersBV7NYLN5uAQAAAAAAAAB8jsViUaeuvUz1gwf26ujRkgOftsJCrVqxxDRXl+6uDdxmZGY6rIfWCHXpOgEBAQ43f5Ck7Oxs++c5OSosLHQ4h7+/a/bDqEyY+ExqqkvWroyMtHQZhuG19QEAAAAAAIDK8rnArWEY3KwDAAAAAAAAADfoWkJIdtXyxSWO2bx5vdJSz9jVmrdopZiYOBd2JuXl5Tmsu3J32yLhoY5DvLnn9JCTk+PwvBohIS7rpTLXl5ub67L1KyO/hNcKAAAAAAAAqMpc8yf0VcTw4cN16hQfRwUAAAAAAAAA7tC4STPVrddAx48dsauvWrFYNw4Z5nDMymW/mWrdelzh8t5sNpvDutXq+n0nCktY69xda0vqyZWf1BYSHFzhMQWFBS5bvzIKCgoUGBTk1R4AAAAAAACAivKpwG3Pnj293QIAAAAAAAAA+LQu3fpo+tTJdrWjRw7p0MF9SmjYxK6el5enNauX2dX8/P3Vqavr7+UGBAQ4rGdmZbl8rZLmDAmyD78GlxCGzcrOdlkvOZXYrbak71WjRo0VEREpm2FTfmG+s62VyB0haAAAAAAAAMDdfCpwCwAAAAAAAABwry7dzYFb6a9dbs8N3P65fpWyzwmntmnbQeHhkS7vK7RGDYd1Vwdu8/LzlZ/vOIwaFBRo97xGSIgsFosMw7CfIy9PBQUFph1xKyMru+LXV9KuuLfddqcGDRqsnPxcHU896WxrAAAAAAAAgE/hz8gBAAAAAAAAAOVWv36CGjZqaqqvXL7EVFux7DdTrVuPy93SV3RUlMN6alqa8koIyFbGyaQkh/WakRGmnWP9/PxUu4S+TqWkuKSf9IzMCo+pXctxT4mJx51tBwAAAAAAAPBZBG7d4Pfff9fJk/z1PwAAAAAAAADf1KVbH1PtROJRHdi/p/h5dnaWNqz/w+6coOBgtb+sq1t6CgkJUc3ICFO9sLBQ+w4ccNk6u/budVivW6eOw3qDenUd1nfv3eeSfg4cOlThMXXj4hzWDx484GQ3AAAAAAAAgO8icOsCOTk52rBhg95//30NHDhQDzzwgE6fPu3ttgAAAAAAAADALbp0NwduJemPlb8Xf7129XLl5+XZHe9wWTcFBQW7ra8mjRo7rO/cvcdhvTJ27XEcuG1Qr57DevOm5t2AJWl3CcHdisjOztbR4xXflbZJ40YO62vXrpHNZnOyKwAAAAAAAMA3+Xu7AU/IysrS4sWLtXnzZiUnJyszM1O5ubmy2Wyy2WwyDKPMOQzDUGFhoQoKCpSfn6/c3FxlZ2crLS1NWVlZdudZLBZ3Xg4AAAAAAAAAeFVMTJyaNb9Iu3dts6v/sXKJht52tyRp1fLFpnHdelzh1r5at2yp9Rs3mupLV67UdQOudnp+m82mpStXOjx28UUXOaxf0qaNvv9xpqm+fPVq3XX7bU718+eWLZUKyLa84AIF+Psrv6DArp6aekZbt25Ws5YtnepLkvbv2av5c35RdGysYmJjFB0bq+jYGEXVri0/Pz+n5wcAAAAAAAA8zacDt6dPn9Zrr72muXPnKu+cnRQqqzzhXAAAAAAAAADwdV269TEFbhOPH9XBA3sVHROnzZvW2x2LiKipi9u2d2tPndpfqi+/+85U/3PLFh05dqzEXWjLa93GjUo8edLhsUsuvthhvU2rVgoJDlZ2To5dff/Bg9qxe7daNmtW6X7mLlxYqXEhISG6+KKLtH7TJtOxb775Ss+PGVvpnorMmjZdK5cuM9Vbt22jZ152fn4AAAAAAADA06zebsBdfv/9d1199dX66aeflJubK8MwXPIoYrFYHD4AAAAAAAAA4HzQuWsvWa3mW8xr/limtauXq6Ag367eqWsvt+9s2rJ5c4ehWsMw9MU33zo1d2Fhob741vEcLZs1U706dRweCw4K0uU9ejg8NuX7qZXu58ixY1q1dl2lx1/Rq5fD+m+/LdSePbsrPa8kHTpwQH8sX+HwWNdePZ2aGwAAAAAAAPAWnwzcbtu2TSNGjFBqaqoMwygxHOvM42zsegsAAAAAAADgfFOzVpQuvKitqb7mj6VavfJ3U71b98s90ZYGXd3fYX3R0qVasnx5pef9bsaP2r5zl8NjgwcMKHXsdQOudrhhw8o1azR/8eIK92Kz2fTGe++roKCgwmOL9O7eTTUjI0x1wzA05oVnlZWVVal5CwsL9fmHH8tms5mOhYSEqGPXLpWaFwAAAAAAAPA2nwvcGoahp59+Wnl5eXbh2NJCsY52sC1rjbMfFovF7usOHTooOjraJdcDAAAAAAAAAFVV1+59TLXDhw5o08a1drWY2Dpq3rKVR3oa2K+fYmrXdnjs9ffe19oNf1Z4zrkLF2rS1187PNY4IUF9enQvdXzTxo3Vp7vjc97+6GNt2LSp3L0YhqEPPv1MW7ZvL/cYR4KDgnTbTTc5PHb40CG99cqrlQrdfjFhonZuc9zbtTdcr9CwsArPCQAAAAAAAFQFPhe4/fXXX7Vjxw5T0PbsUOy5j7OVdM7Z5zra6faWW27Rq6++qvnz52vy5MkEbgEAAAAAAAD4vMs695C/f4CpXlhYaPe8q4d2t5WkwMBAjXzgAYfHcnJy9OzLL2varFmmHh3Jy8vT5199pfHvf+Bwx1ar1apHH3pQ/v7+Zc5177A7FVqjhrmn3Fw9M+5lzVu0qMw5MrOy9No77+rHOXPKPLc8Bg8YoJYtL3R4bMufG/X8vx7X/r37yjVXVlaW3h//X82f/YvD41G1a2vA9ddVulcAAAAAAADA28q+C1jNfPPNN8VfF+04W/R1165d1bdvX7Vs2VKRkZEKDAzU999/r08++aQ4kOvn56cffvhBERERKiwsVE5Ojk6dOqXDhw9rw4YNWrBggdLT000f/5Wbm6vBgwd78lIBAAAAAAAAwKvCwsJ1cdv22rBuVanndevhucCtJHXteJluHDhQ02bNMh3LLyjQh599rl8WLNTgAQPUtVNHRdWsaXfOyaRkLV21UjN+nq3jJ06UuM7wf/5TrVq2LFdPcTExeuyhhzR2/HjTsdy8PL3+7ntasHiJrr/2GnW89FK7EO/plBQtWb5C30yfrlOnT5drvfLw8/PTmDEv67777lJaWqrp+NFDh/XMqMfUuXs39bi8j1q1uViBQUF255xMPKEVvy/V3J9mKfXMGYfr+Pv765EnH1dwcLDLegcAAAAAAAA8zacCt2lpaVq7dm1xGLYoRBsUFKQ333xTV155pWnMtddeq08++aT4uc1m0/bt23XDDTfYndelSxcNGTJEL7zwgiZOnKgJEyaooKCgeI0ff/xRl112mWkcAAAAAAAAAPiyrt37lBq4bdioqRrEN/JcQ//f8Lv+qdNnUvTb0mUOj+8/eFBvffSR3v74Y9WqWVNRNWvKMAydSknRmVRz+PRct9xwvW4cNLBCPfXu3k1Hjh3TpK+/dnh8/aZNWr9pk4ICAxUdXVthNUJ1OiVFp1JSHO6wGxsdreYXNNWyVX9UqI+zJSQ01BtvvKVHH31YWVmZpuOGYWjl0mVauXSZ/P39Vat2lCIiIlVQUKCU06eVVsb3ymK16u6HHlTzC8sXTAYAAAAAAACqKqu3G3ClDRs2qKCgoPh50Q63jzzyiMOwrSQ1a9ZMtWvXtqv99ttvJa4RHByshx9+WJMmTVJ4eLikv4O9r7zyihITE11wJQAAAAAAAABQPbS/rKuCgkreubSrh3e3LWK1WvX0o4/qxoGlh2INw9DplBTt2b9few8cKDNsG+Dvr0ceeED3DRtWqb7uGHKz7r79dlmtJd+ez83L09Fjx7Vzzx4lnTrlMGxbIyREY558UuFh4ZXq42xt27bThx9+otjYuFLPKygoUNKJk9q7e7cO7t9fZtg2ICBAjzz+L/Xp6/j+PAAAAAAAAFCd+FTgdseOHaZabGys7rrrrlLHtWnTpjicaxiGVq9eXeZaHTp00Pvvvy8/P7/iWmZmpl599dWKNw4AAAAAAAAA1VRwcIguad/J4TGLxaKu3fp4uKO/Wa1WPXTP3XrpqScVc87GC5VxUYsW+vi/b2rQ1f2dmuf2m2/Sf557TrVr1arU+Nq1aumNMS+q+QVNnerjbC1bXqTJk79Rv35Xu2S+C1o010tvvq5O3bu5ZD4AAAAAAADA23wqcHvs2LHir4sCtNddd50sFkup49q0aWP3PC0tTXv27ClzvY4dO+qee+6xC+vOmzdPmzdvrtwFAAAAAAAAAEA11KW7411sW1x4sWpHx3q4G7NunTrpfx9+oOF3/VN1YivWj8ViUdvWrfXKs8/qvddeVaOEBJf01OGSdvriww9059AhCg8PK9cYPz8/9b/iCk185221bN7cJX2cLTKypsaMeVkfffKpOnXvJj9//wrP0axFCz346CN6afzratSkict7BAAAAAAAALyl4nfLqrDk5GRT7dJLLy1zXHMHNya3bNmiCy64oMyxw4cP17fffqu0tLTi2qRJk/Tf//63zLEAAAAAAAAA4C1f/7DAZXNd1rGbS+cbPvz/NHz4/6kwL1c5pxNdMmdwUJBuvu463TRokLbv2qV1f27Ujt27dfT4cZ1OSVFObq6sVqtCa9RQdFSUGjdqqIuat1CXjpe5ZHdcR0JCQvTPW2/VbTfdpJVr1mjdnxu1c/duJZ8+pYzMLPlZrYqMjFTD+Aa65OKL1ad7D8XGRNvN8a8R/6d/jfg/l/Z14UWtNOrJx5WZkaE/167Tzu3bdWj/QSWdPKHM9Azl5ecrMDBQoaGhCo+IUEKjhmrSvJkubtdW9ePjnVr7vc8nuugqAAAAAAAAANfyqcBtTk6OqeYoTHuuZs2amWpbt27V4MGDyxwbEhKiG2+8UZ9//nnxLrfz589XRkaGwsLKtysBAAAAAAAAAMAzLBaLLmrRQhe1aOHtVooFBgSoV9eu6tW1q7dbsRMaFqZuvXupW+9e3m4FAAAAAAAA8Dqrtxtwpby8PFMtMjKyzHHx8fEKDAy0q+3evbvc6/bt29fueUFBgZYsWVLu8QAAAAAAAAAAAAAAAAAAAKi6fCpwGxAQYKrVqFGjzHEWi0X169cv/towDO3du7fc67Zq1Ur+/vabBa9du7bc4wEAAAAAAAAAAAAAAAAAAFB1+VTgNigoyFTLzs4u19iEhAQZhlH8PDk5WZmZmeUaGxAQYBfYlaSdO3eWaywAAAAAAAAAAAAAAAAAAACqNp8K3NauXdtUS09PL9fY+Ph4U60iu9xGREQUB3YNw9CRI0fKPRYAAAAAAAAAAAAAAAAAAABVl08FbuvUqWOq7du3r1xjGzRoYKpVJHB7rrS0tEqPBQAAAAAAAAAAAAAAAAAAQNXhU4HbZs2amWobNmwo19iEhARTbevWreVeOzU1VRaLpfh5QUFBuccCAAAAAAAAAAAAAAAAAACg6vKpwO3FF19s99wwDP3yyy/lGtu4cWPT2HXr1pVrbFZWlo4cOWJXCw0NLddYAAAAAAAAAAAAAAAAAAAAVG0+FbitV6+eGjZsKEnFu83u2bNHU6dOLXNsw4YNFRwcbDd2586dOnr0aJljlyxZIpvNZleLiIioUO8AAAAAAAAAAAAAAAAAAAComnwqcCtJV1xxhQzDkPRXcNYwDL300kuaN29eqeOsVqtat25dPFb6a5fb999/v9RxhYWFmjhxot0Yi8WiBg0aOHEVAAAAAAAAAAAAAAAAAAAAqCp8LnB7yy23FO9QK/0Vus3Pz9eoUaM0evRo7d27t8SxV155pd04wzA0c+ZMTZ8+vcQxL730krZt22a3piS1adPGiasAAAAAAAAAAAAAAAAAAABAVeFzgduEhAT179+/eKfaoh1nDcPQnDlzdO2112rIkCFau3ataew111wjq/Xvb4nFYpHNZtMzzzyjJ598Ups2bVJ2drYyMjL0+++/64477tD3339fPP/ZOnfu7N4LBQAAAAAAAAAAAAAAAAAAgEf4e7sBd/j3v/+tpUuXKjMzs3jn2bNDsZs3b1ZycrJpXExMjK6++mrNnj27+Pyzd7qdOXOmaUzROWfvcJuQkKAuXbq46eoAAAAAAAAAAAAAAAAAAADgST63w60k1atXT6+++qpdCLYoGFskISHB4djRo0crMDBQkkxhXUePs4O8Rc/vvvtud10aAAAAAAAAAAAAAAAAAAAAPMwnA7eSdOWVV2r8+PEKCgoyhW2lkgO39erV0zPPPFMcopXsd7E993H23BaLRV27dtXQoUPdd2EAAAAAAAAAAAAAAAAAAADwKJ8N3ErSgAEDNH36dHXt2rV4R1pJioyMVFhYWInjhg4dqgceeKD4/KJA7bm72557rGXLlnrjjTfceUkAAAAAAAAAAAAAAAAAAADwMJ8O3EpSkyZN9Pnnn+v777/XzTffrKioqBJ3tz3bo48+qrffflsRERF24dqzH9LfIdy+fftq8uTJioqKcuv1AAAAAAAAAAAAAAAAAAAAwLP8vd2Ap7Rp00Zt2rTR2LFjdfLkyXKN6d+/vzp27Kj//e9/mj17to4ePWp3vEaNGurRo4duu+02derUyR1tAwAAAAAAAAAAAAAAAAAAwMvOm8Dt2WJjY8t9blRUlB577DE99thjSkpK0vHjx4vrderUkb//efktBAAAAAAAAAAAAAAAAAAAvtlhQwAA4hhJREFUOG+QFq2AmJgYxcTEeLsNAAAAAAAAAAAAAAAAAAAAeJDV2w0AAAAAAAAAAAAAAAAAAAAAVRmBWwAAAAAAAAAAAAAAAAAAAKAUBG4BAAAAAAAAAAAAAAAAAACAUhC4BQAAAAAAAAAAAAAAAAAAAEpB4BYAAAAAAAAAAAAAAAAAAAAohb+3GyjN+++/7+0WKm3EiBHebgEAAAAAAAAAAAAAAAAAAAAuUOUDtxaLxdttVAqBWwAAAAAAAAAAAAAAAAAAAN9QpQO3RQzD8HYLFVJdQ8IAAAAAAAAAAAAAAAAAAAAwqxaB2+oUYK1u4WAAAAAAAAAAAAAAAAAAAACUrloEbp1RUgDWmRCvozmrUygYAAAAAAAAAAAAAAAAAAAA5VflA7eu3jH27GDsuXP7+fkpNDRUNWrUUHZ2tjIzM1VQUGAaf+4cFotFVqvV4RoAAAAAAAAAAAAAAAAAAACo3qp04PY///mPU+NXrFihWbNmOQzZRkVFqV+/fmrdurVatmypxo0bq0aNGqY5srKytHv3bm3dulWrVq3Sb7/9pvz8fFOo9tZbb9UTTzyhgIAAp3oGAAAAAAAAAAAAAAAAAABA1VKlA7fXX399pcfOnz9fc+fONYVtmzVrptGjR6tHjx7y8/Mrc54aNWqobdu2atu2rW677TalpKToq6++0ieffKKCggJZLBYZhqGvvvpK+/bt04cffqjg4OBK9w0AAAAAAAAAAAAAAAAAAICqxertBtxh/fr1GjVqlAoKCiT9vavtiBEjNGPGDPXu3btcYVtHatWqpREjRmjatGlKSEiQYRjFoduVK1fq3//+t8uuAwAAAAAAAAAAAAAAAAAAAN7nc4HbnJwcPfXUUyosLJSk4kDsc889pxEjRsjf3zWb+jZv3lxffvml4uPjJak4dLtgwQL98MMPLlkDAAAAAAAAAAAAAAAAAAAA3udzgduPPvpIBw8eLA7AWiwW3XHHHbrttttcvlZcXJzeeustWa1/fRuL1nznnXeKd9cFAAAAAAAAAAAAAAAAAABA9eZTgdvc3Fx98803slgsxbXY2Fg99thjbluzdevWuvHGG2UYRnEtOTlZ8+fPd9uaAAAAAAAAAAAAAAAAAAAA8ByfCtyuWLFCaWlpklS8u+11112nkJAQt6578803m2pLlixx65oAAAAAAAAAAAAAAAAAAADwDJ8K3K5fv95Uu/LKK92+7sUXX6zIyEhJksVikWEY2rZtm9vXBQAAAAAAAAAAAAAAAAAAgPv5VOD2wIEDplr9+vU9snZsbKwMwyh+fvz4cY+sCwAAAAAAAAAAAAAAAAAAAPfyqcBtWlqaqRYREeGRtf39/e2eZ2dne2RdAAAAAAAAAAAAAAAAAAAAuJdPBW7PDb1KUlJSkkfWTkxMlMViKX4eEhLikXUBAAAAAAAAAAAAAAAAAADgXuaEajUWGRlpqu3atUv16tVz67q7d+9WSkqKXeA2KirKrWsCAAAAAAAAVV1YWJD8/f283UaFnHWLr1hoqKHwiGDPN1ONZefk69DRFG+3AQAAAAAAAAAu41OB24YNG5pqc+bMUe/evd267vTp04u/NgxDFotFLVq0cOuaAAAAAAAAQFXn7++ngIDqFbgtSWCgT91KBQAAAAAAAABUkNXbDbhSmzZtir+2WCwyDEO//PKL9uzZ47Y19+/fr6+++spud1tJuvTSS922JgAAAAAAAAAAQGkyMjK83QIAAAAAAIBP8anAbadOnRQSEmJXy8/P11NPPaWcnByXr5eRkaHRo0crLy/Prm61WnX11Ve7fD0AAAAAAAAAAIDSHDiwXyNHDtfvv//m7VYAAAAAAAB8ik8FbmvUqKG+ffvKMAxJKt51dsuWLbr//vuVnZ3tsrVSU1N17733atu2bcXrGIYhi8Wi3r17Ky4uzmVrAQAAAAAAAAAAlCYnJ0effPKB7rrrNq1fv9bb7QAAAAAAAPgcf2834GoPPPCAZs+ercLCQkl/hW4Nw9CaNWs0aNAgvfjii+rWrZtTayxYsEAvvviiTp06ZTrm5+enxx57zKn5AQAAAAAAAF+Vk5uvw4lnvN0GXCi+Tk0FBwV4uw3gvLZs2RK9/fZ4JSYe93YrAAAAAAAAPsvnArdNmjTRLbfcoilTphSHbYv+PXz4sO699161bt1agwYN0hVXXKF69eqVa97Dhw9r4cKFmjZtmvbs2WPaRbdonX/84x9q2rSp264PAAAAAAAAqM4OJ57RCx/86u024EJj/q+vmjWM8XYbwHntySdHe7sFAAAAAAAAn+dzgVtJevzxx7VmzRrt2rXLLnQr/RWM3bx5s7Zs2aJXXnlFERERat68uerUqaPQ0FCFhobKarUqIyNDGRkZSkpK0vbt25WWllY8Xvo7aHu2Sy65RKNHc1MLAAAAAAAAAAAAAAAAAADAl/hk4DYwMFAffPCBhg0bpuPHj9uFY4sCuEXB2dTUVK1du7bU+YrOPXuOc4+3bt1aH3/8saxWq4uuAgAAAAAAAAAAAAAAAAAAAFWBz6ZDGzRooG+++UbNmzd3GJg9+1EUwC3pce75ZzMMQ71799akSZMUERHhyUsEAAAAAAAAAAAAAAAAAACAB/hs4FaS4uLi9MMPP+jee++Vn5+fKXhb5NxAbWkB2yKGYSg0NFRPP/20Pv74Y4WHh7vzUgAAAAAAAAAAAAAAAAAAAOAlPh24laSAgAD961//0rx583TzzTcrODi4eOfaiioaFx4errvuukvz58/XsGHD3NA1AAAAAAAAAAAAAAAAAAAAqgp/bzfgKfXr19fYsWP11FNPacGCBVqyZInWrVunxMTEco2PiopS586d1atXL/Xv319BQUFu7tj7cnNz9euvv2r16tXauHGjTp06pdTUVPn7+ysyMlJNmjTRpZdeqquuukotW7b0drulMgxDe/fu1d69e5WamqrU1FRJUq1atVSzZk1dcMEFatSokXebBAAAAAAAAAAAAAAAAAAAVdJ5E7gtUqNGDQ0aNEiDBg2SJCUlJengwYM6cuSI0tPTlZ2drcLCQoWEhCgsLEz169dXo0aNVLduXS937jn5+fmaOHGivvzyS6WkpDg8np2drcTERK1YsULvv/++OnXqpMcff1ytW7f2QsclW7Zsmb799lutWbNGZ86cKfXcuLg4de3aVcOGDdNFF13kmQYBAAAAAAAAVHsnTp7U3gMHdTIpSVnZ2TIMQzVCQhQbE60mjRqpblzcedFD0smT2r9nr86cPq3MzEwFBASoVlSUGjZprAYJCW5f35EzKWd0YO9enUxMVFZWtiwWKSw8XHF16+qC5s0UHBLi1vWTTp7Uof0HlHwySdnZ2ZIMBYeEKDomRh0ubq/GDRu7dX0AAAAAAAC4znkXuD1XTEyMYmJi1KFDB2+3UiUcOHBAo0aN0vbt2ys07o8//tCQIUP00EMP6f/+7/9ksVjc1GH5rFmzRmPGjNHu3bvLPebEiROaMWOGZsyYoZ49e+r5559XfHy8G7sEAAAAAAAAUF0dP3FCP/0yV0uWL9eJpKRSz42NiVGvrl113YCrXRp8rQo9pJ45o/mzf9HSRb/p5IkTJZ4XHRurK/r3Vf+B1xaHXDMzMnTvLbebznvv84mlrnnrtdfZPa/XoL7e/PjD4ucF+flasvA3LZo3T/v37JVhGA7n8fPzU+t2bXXl1f3VvlNHl93XPpl4QvPn/KJVy5Yr+eTJUs+tU6eOLr/8Kt100xDVq1e/3GvcdNNAJSYeL/WcV14Zo1deGWNXa9fuUr3//oRyrwMAAAAAAIC/nfeBW/xt7969+sc//qGkMm7MlqSwsFDvvfeejh07ppdfftkroVubzaZ33nlHEyZMkM1mq/Q8v//+u2644Qa98cYb6t27t+saBAAAAAAAAFCtZWRk6tMpkzVn/gIVFhaWa8zJpCRNnTlT02bN0rX9+uqeO+5QWGhote7BMAzNnjFT07/9TtlZWWWen3zypL77corm/TxbDzwyUu3aX1rptUuza/t2ffz2uzp+9FiZ5xYWFmrjuvXauG69mrVsoQcfHaW69etVeu3MjAx9+8Vk/fbr/HK/LomJifr668n67ruvNXjwDRo+fIRCQir/ugAAAAAAAMB9rN5uAFXD6dOndffdd5cYto2Pj9egQYN03333aciQIWrfvn2Jgdpp06bp/fffd2e7JXr55Zf18ccfOxW2LZKWlqbhw4dr7ty5LugMAAAAAAAAQHW3Y/du3TdqlGbNnVfuQOXZbDabfvplroY/Nlp79++vtj3kZGdr/NiX9dXnk8oVtj3bmdMpev3FlzRn5k+VWrs0Sxf9ppeeerZcYdtz7d6xU8//63Ht37uvUmvv3bVbTzz8iBb8MrdSr0thYaGmTZuqf/7zdu3evatSPQAAAAAAAMC92OEWkqQnn3xSiYmJpnqDBg00ZswYdevWzRSwPXDggN588039+uuvpnEffvihunbtqvbt27ut53N99tlnmjJlisNjNWrU0A033KCePXuqefPmioqKUn5+vpKTk7Vu3TrNmTNHy5YtM40zDENPPPGE6tWrpzZt2rj7EgAAAAAAAABUURs2bdKzL7+inNxcp+c6fuKEHn3mWb36wvO6qEULr/bw9jsf6uKLy3/vMz8/X+PHvqKtmzZVem3DMDR54mfKc8F1FFmzYqU+evtdGU5sxpCRnq7/vvwfvfnxBwoMDCz3uC0bN2n8S+OU64LrOXr0iB5++H6NH/+eWre+2On5AAAAAAAA4DoEbqH58+dryZIlpnq7du302WefKSwszOG4Ro0a6b333tOECRP05ptv2h2z2WwaO3aspk+fLqvV/RspHzlyRO+8847DY/369dMLL7yg2rVr29WDgoIUFhamRo0a6cYbb9SaNWs0evRonThxwu68nJwcPfHEE5o1a5b8/flfBgAAAAAAADjf7D1wQM+98h+HQVeLxaKuHTuqe+dOatWypWrVrCk/q1WnUlK078ABLfvjDy1eukz5BQV24zKzsvTCq6/pk7f+q6iaNb3Ww5NPjtaXX35run9akkkffVxi2DYmLla9r7pK7TtepujYGPn5++t0crK2bNyk3xcs0t7du+3O/37yV+VasyxpaWmmsK2/v7+69+mt9p06qvEFTRURGanMjEwlnzyhVctWaPH8BcrMyDDNlXzypH6Z+ZOuu/mmcq19cP8BvTnuZYdhW4vFovadOuqyLp3V/MILFVkzUn5+fkpJSdGhffu1df1GLVq4QPn5+XbjMjIy9Mwz/9KkSV8rKsrx6zJkyK1KT08vfj5p0kTTOT169NIFFzS3q9WtW69c1wUAAAAAAAAz0oPnucLCQlNYVpLq1q2rDz/8sMSw7dnuv/9+JScn64svvrCrb9++XXPnztWAAQNc1m9Jxo8f7/CG5u23367nnnvOtDuvI5dddpl++OEH3XLLLTp69KjdsX379mnq1Km69dZbXdYzAAAAAAAAgKovNzdXY157Xdk5OaZjTRs10pOjRqlJo4amY/Xq1FG9OnXUvXNn3TlkiN768CNt2LzZ7pzTKSl6+c039caYMaVuXODOHk6dOqXnn39K7733cZmbJ2xYs1a//brA4bFrrh+sIXfcpsCgIPseGjRQvQYNdNWAq7V4/gJ98cnE4nu5hmGUul55ZaSl2z1vc0k73f/Iw6odHW1Xr1mrpmrWqqkLWrTQwBuv15vjXtHuHTtN882Z+ZMG3nhDmd+PvNxcvf3Kq8rJNr8uDRs31kOjRymhUSPTsbg6dRRXp44GXX2d7r3nAb322stau3aN3TmnTp3Siy8+o7ff/tBhH0OG3Gb33HHgtrcGDBhY6jUAAAAAAACg/Kp04HbYsGEO6xaLxRTuLGuMJ5XWX1WzZMkS7d+/31R/8skny72jQdH5a9as0bZt2+zqn332mdsDt8nJyZo3b56pfumll+rpp58uV9i2SGxsrD766CPdcMMNKjhnt4cvv/ySwC0AAAAAAABwnpkydaqOHj9uqndq314vPvG4AgMDy5yjft26evWF5/X6e+9p4ZLf7Y79uXmLfv1tsfpfcbnXeli3bq3mzPlZ1147qMTxNptNX036n8Njd953jwZcV/JY6a/75n36XqW69evr9TFjlZ2VVWbPldHrysv1wCMjy7wvHFmzpp59eaz+/dDDOnnOp56lnUnVnp071fzCC0udY8Z33yvRwevSrkN7Pfr0k+V6XeLjE/T22+9r7NgXNW/eL3bH1q9fq7lzZxOaBQAAAAAAqCKqdOB29erVpptihmGUeqPM0RhPKqu/qmbq1KmmWqNGjdSvX78KzWO1WjVy5EgNHz7crr5lyxbt3LlTLVq0cKrP0sybN0+2sz4qrMjo0aPl71/x/8RbtGihm266Sd9++61dfd++fTp06JASEhIq3SsAAAAAAACA6uNMaqqm/zzbVG/aqJGef/zf5QpUFvH399cTI0fq2PFEbd+1y+7Y1Jk/qt/lfRzeW/ZUD19/PVnXXDOwxPvba1as1NFDh0313lddWWbY9mwtW12k+0eO0Duvvl7uMeWV0KiR7vm/h8p9jz4wKEi3/HOY3n3tDdOxLRs3lxq4TUtN1S8//WyqN2zcWKOefKKCr0uAnntujI4cOaytW7fYHfvmm8m6+uprq9XvHQAAAAAAAHxV6Z+HVEUYhlHhj5YqGuPJR3WTmZmppUuXmuo33HBDpW7e9erVS7Gxsab6zz+bbzq60qJFi0y1Zs2aqUOHDpWe87rrrnNY/+OPPyo9JwAAAAAAAIDqZd6iRcrJyTHVRz5wv4KDgio8n5+fnx68+y5T/cChw1q9fr1Xe9i3b69WrlxR4rjf5i8w1SIiI3XnffdUuIfO3bupY9cuFR5Xlmuuv04BAQEVGtOhcyeHY5JPnnBw9t+WLFioXAevy10PPqCg4Iq/Lv7+/nrkkdGm+v79+7RqVcmvCwAAAAAAADynWgRuLRZLhQOgRWM8+ahuVq9erfz8fFP98stL/uiy0litVvXu3dtUX7JkSaXmK6+dO3eaap06dXJqzrZt28pqNf/vceJE6TdZAQAAAAAAAPiORb+bNyxofWFLtS5l59OytGrZUo0bNjTVV6xe7fUeli5d7PD8tNQ0bd7wp6ne84rLVaNGjUr1cO0N11dqXEmCgoLUpWePCo8LCAhQvQYNTPWM9IxSxy1f8rup1uKiC9Xiosq/Lm3atFXTpheY6suWufceOwAAAAAAAMqnWgRu4R4rV6401WrWrKlmzZpVes727dubajt37tTp06crPWdp0tLSlJSUZKo3adLEqXn9/PwUGRlpqicnJzs1LwAAAAAAAIDq4XRKivYeOGCq9+za1em5L7vkElNty/btXu9h48Y/HZ67ffNm2Ww2cx9XVG7zBklq1rKF6ifEV3r8uRo2aVzh3W2L1IqKMtVyc3NLPP9MSooO7T9gqnfq1q1S65+tc2fza7tp059OzwsAAAAAAADn+Xu7gbIYhuGRMeej7Q5u4F588cVOzVnS+M2bN6tXr15Oze2IYRh64okndOLECZ04cUInT57UiRMnFB0d7fTcmZmZDtcDAAAAAAAA4Pu279rl8H5gMyf/2F+SmjZuZKodPHxEqWlpioyI8FoP+/fvU2rqGQXVCLGr79i6zXRuaGioGjgZmG150UU6euiwU3MUadS0aaXHhoSad+m1FRaWeP6enY5fl0ZNnX9dmjVrbqodOLBfqalnFBlZ0+n5AQAAAAAAUHlVOnC7cOFCj4w5X+3Zs8dUa+rETUlJSkhIkJ+fnwrPuRm5c+dOtwRuIyMjdffdd7t83iNHjigvL89Ur127tsvXAgAAAAAAAFD17D94yGF96cpV2rBps1Nzn3DwqV2GYejo8eN2gVtv9HD48CFd0KKFXf3okSOmcxs1bSqLxeJUD02bN9PCufOcmqNIZM2alR7r72/+VYmjHX2LHDpw0GF99YqV2rpxU7nXDQsOlb/Vz66WmHjcdN5fr8thArcAAAAAAABeVqUDt/Xr1/fImPNRSkqKTp8+bao3atTIqXkDAgJUt25dHTnnBuzhw67ZpcBTfv/9d4d1ZwPJAAAAAAAAAKqHkw4CqZI0/eef3bZmenqG13tIS0sz1U4cTzTVoqKd35wgJi7O6TmK1DhnV96KcBQcLu3Dzk6V8LrM/WlWpXsoS3p6qtvmBgAAAAAAQPlYvd0AvOPEiRMO63EuuMEZExNjqh09etTpeT3p+++/d1jv0KGDhzsBAAAAAAAA4A0pqZ4POKZlpHu9h1QHa2akp5tqNUJrOL2WK+YoEhxS+cBtRaV5478NB0FoAAAAAAAAeBaB2/PUqVOnHNajo6OdntvRHCWtVxUtWrRI27dvN9UvueQSlwSSAQAAAAAAAFR9ubm5Hl8zMzPL6z1kZJjDtbk5OaZaUFCw02vVqBHq9BxFHO1S6y5V5XUBAAAAAACAZxG4PU8lJyc7rEdGRjo9d3h4uKmW7mAHhKooKytL48aNc3hsyJAhHu4GAAAAAAAAgLfYbDaPr1lYWFjlepAkP38/Uy0vL8/ptfLz852ewxtshVXjdQEAAAAAAIBn+Xu7AXhHRkaGw3poqPM7CtSoYf4YsOrycVevvvqqjh49aqrHx8dr4MCBXujI+2rWdN3HupWHnx9/BwDAe/z8rB5/36uKeC8G4C28D/+F92EA3mSxWFz6PuTBDScBlwsICDDV6terqy8//NCneqgRl2DaHTYn334H17CwcJ3Otf8Us+ws+914KyM7K9PpObzB0etSp149vTXhowrNUzcyVsEBQXY1w3CqNTtWq2vf0wHgfMC9ib/w8wOAt/A+/BfehwF4C+/DZSNwe54qaeeA4GDnPwbM0Rze+Iitivr222/13XffOTz29NNPO7yJ6ussFosCAsy7VwCAr7JaLbJaed8DAG/hfRgAqgZCssBfwhxsTpCUfMrBmb7dgySFhoXp9Cn7dTNL2NShIjIzqmfgtkaY+XU5VcKnylWUq9+DeU8HgIrh3gQAeBfvwwDgXbwPl40/iThPlfRxX/7+zmew/fzM/9MVFBQ4Pa87LVy4UGPHjnV47Prrr9fll1/u4Y4AAAAAAAAAeFN07dqmWl5enpJPnz6vepCkuLp1TLVDBw46Pe/hg4ecnsMbohy8Lvl5eaZQMgAAAAAAAHwLgdvzVEk73DoKy1aUo9CuzWaTzWZzem53WLZsmR599FGHoeBmzZrp2Wef9UJXAAAAAAAAALwpoUF9h/U/N28+r3qQpGYtW5hqSSdOKCsry6l5D+zb59R4b6nXoIHD+rZNnn1dAAAAAAAA4FnOb2fqRsOGDfN2C5VisVj0xRdfeLuNUllK+BwpwzCcnruwsNDhelZr1ct3L1myRA8//LByc3NNx6KiovTRRx8pLCzMC50BAAAAAAAA8KbmF1zgsL52w5+6slcvp+b+c/MW/bFurerExqlObKzqxMUqLjZWwUFBHu8h4YKWqlevvurWrae6desqODjEdG7zCy801QzD0Ma169SlZ49KrV+Qn68tf26s1Fhva9LM8euyaf0Gde/T26m5161bqxUrlqpu3fqqV69eqa8LAAAAAAAAPKtKB25Xr15dYjC0qjIMo1r07GgXWslxWLaiHM0RGBjo9LyuNmfOHD3++OMOd/sNCwvTp59+qvj4eC90VnUYhqGCAs/uTOznZ5XVWvX/HwLgm2w2Q4WFVXNHdk/ivRiAt/A+/BfehwF4mwv+HhvwCU0aNlRUrVo6nZJiV1+yYoUe+Oc/VKtmzUrP/dXUqVq/aZOpPvnjj1SvTh0P9zDTrj5t2k+qHRtjV7ugRXNF1qyp1DNn7PtYuKjSgdt1f6xWelpapcZ6W0KjhqpZq5bOnPO6rFq+Qrffc5cinXhdJk36VGvXrjbVf/jhJ9Wv73hn3dLwng4AFcO9ib9wbwKAt/A+/BfehwF4izffh/39rdUjd+ntBsrDFbuuekJ1eMGLlBSALSgocHpuR3NUtcDtV199pXHjxslmM79BhIaGasKECWrVqpUXOqt6zpxx7mPhKqpmzRqyWv08uiYAFCkstHn8fa8q4r0YgLfwPvwX3ocBeJNhuPaGajW5rQc4ZLFY1LNLF/04Z45dPS8vT9//OFMP/PMflZp3644dDsO2FzRpYhe29UYPzZu3VP36DZSTb/+JYP7+/urT7yr9+N1Uu/qm9Ru0Y+s2tWx1UYXWL8jP17Rvv6t441WExWJRx25d9evPs+3q+Xl5+nn6j7r97n9Wat7Nmzc6DNs2b95CderUK/X92Wq1mu53E1YAgIrj3sRfuDcBwFt4H/4L78MAvMWb78PR0dXjU+it3m6gPCwWS7V4VCdhYY7/A83Kcv5/GEdzBAcHOz2vKxiGofHjx+ull15yGLYNDw/XpEmT1L59ey90BwAAAAAAAKAqubZfX4f3fn/46Sdt2Ly5wvPl5+fr/YmfOjzWt09vr/dwzTXXljjuin795Odn/wtfwzD0yTvvKSszs0I9fDd5ig4fOFihMVXNlf37OXxdZv84U1sdhJnLkp+frzfffN3hsf79S35dijj6VDtXfKIdAAAAAAAA/lYtArdwvZolfKRVRkaG03NnOri5WqtWLafndVZOTo5GjRqliRMnOjweHR2tyZMnq23bth7uDAAAAAAAAEBV1LhhQ/Xq2tVUt9lsGjd+vHbs3l3uuQzD0DufTNCuvXtNx6Jr19bAfv282kNMTKwGD76xxLHRsTG6+rpBpnrisWMa+/SzSktNLVcP3305WT9P/7HcPVdV8Y0aqlN38+ti2Gx697Xx2rurYq/L66//Rzt2bDcdi4mJ1XXX3VDmHMHBIaZaenpauXsAAAAAAABA2ap84NYwjGr1qC5KCtyeOXPG6blTUlJMtaioKKfndUZSUpLuuOMOzZ071+HxBg0a6Ouvv9aFF17o4c4AAAAAAAAAVGXD775LYaGhpvqZ1DSNfvY5zZo3r8ydRFPT0vTSG2/olwULHB6/547bFRgY6NUeHnxwhIKCgkqd46bbblVsnTqm+oG9+/Tvhx7Wb7/OV35+vsOxe3ft1ouPP6Ufv/+h1DWqkzvvuUehDl6XtNRUjX3qGS34Za5sZb0uqWf0zDNPaNasHx0ev//+h8p8XSQpIiLCVNu8ueI77QIAAAAAAKBk5s8YqkIWLlzo7RZ8Vv369R3Wk5OTnZ7b0RzR0dFOz1tZO3fu1PDhw3Xs2DGHx1u1aqVPPvlEMTExHu4MAAAAAAAAQFUXU7u2Hh/5sF587XXZbDa7Yzm5uXr7o4814+fZuqp3L7Vv105xMTEKCw1VRmam9h08qFVr1mreokVKL+HTxa7q3Vt9+/Txag9XX32NBgy4tszvRVBwkB4cNVLjnn1ehQUFdsfSUlM14d33NfnTz9WqzcWKjo1VYGCAUk6naPeOnUp0cH/Wz8/PFBS2WMpso8qIiq6t4Y8+ov++8qqMc16X3NxcffbBR5o762f16NNbbS65RLVjYhQaFqrMjEwdPnhAMzZu09xfZistzfFOtP36DdDVV5f9ukhSXFxdHTly2K62dOliffbZJ7ruuhtVs2ZNZWVlKSMjXfXqOf79AAAAAAAAAEpXpQO3JYVC4by4uDgFBASYdhs4fvy403M7CrYmJCQ4PW9lLF++XCNHjlRGCTeSe/Xqpbfffls1atTwcGcAAAAAAADnp/g6NTXm//p6uw24UHydmt5uwe26deqkUQ8O19sffWwKvErSwcOH9enkKfp08pQKzdvu4tYa9eBwr/bQvn0HPfHE0+U+v2XrVnpw1Eh9+N+3HfaRnZWltav+KHOeC1u3Vp16dfXbr/Pt6n5Wv3L3UhV06NxJ9/7fg/r0g49MoVtJOnrosL79YrK+/WJyhea99NIO+ve/K/C6tLxQ69atNtUnTZqoSZMmFj+Pi6ujadN+rlAvAAAAAAAA+EuVDtzCfaxWqxo2bKg9e/bY1Q8cOODUvKdPn1Z6erqpfsEFFzg1b2XMmTNHjz/+eIkfYTZ06FC98MIL8vOrXjdwAQAAAAAAqrPgoAA1a8gnDaH6ueaqqxQZHqHX331XmVlZTs/Xq2tXPfnoKAUGBHi1h5f+86aCgoIqNK5b714KDQvTB/99Sxlp5vvBZenUraseemyUpnw+yXTML6D6/dri8n59FR4RoY/ffldZmZlOz9enz5V67rmXFBgYWO4xAwcO1g8/fKvc3NxSzztxIlHp6ekKDw93tk0AAAAAAIDzjtXbDcB7WrVqZart3LnTqTl37NjhsO7pwO0PP/yg0aNHOwzbWiwWjR49Wi+99BJhWwAAAAAAAADl1r1zJ336zjvq3a2bLBZLpeaoVbOmnnnsMT3/+L8rFLZ1Ww8VCHWerV2H9nrjg/d0eb++8vcvX0i2ZlQtDX/0EY166gkFBgWpIL/AdE5wUHCl+vG2y7p01uvvv6POPSr/ukRF1daYMa9o7NhXK/y6NGgQr1Gj/l2ue9579+6uVH8AAAAAAADnu+r3p+JwmTZt2mjmzJl2te3btys/P18BlbjRK0kbN2401SIiIjwauJ02bZqeffZZGYZhOhYYGKjXXntNAwYM8Fg/AAAAAAAAAHxHbEy0nvv3vzTs8GHNnj9fy1au0omkpFLH+Pv768LmzdX/iivUp3u3Cu8oWxV7kKSatWrpvof/T0OH3aFVy5Zr68ZNOnLokFJOnVZubq4CAgJUq3ZtNWl2gdp3vEyXde1id+/Z0W6soWGhTvflLbVjYvTIE4/r6G2HtWjur1q9cpWST54sdYy/v79at75Y1157na68sq+CgoJVWGir1PoDBw5WvXr19dZbr+vAgf0lnrd37261a3dppdYAAAAAAAA4nxG4PY916dLFVMvOztb69evVqVOnSs25bNkyU61Tp04e20l2wYIFJYZtIyIi9MEHH6hjx44e6QUAAAAAAACA72oYH6+H7r5bD919t04mJWv/oYM6mZSkjMxMFRbaVKNGiMJDw1Svbh01a9q0UrvZVoceJCkiMlJ9rxmgvtdUbKODrIxM81w1a5Y57pufZ5Z5TkU8+OgjevDRR1w2X/34eN153z268757dCopSYcPHlLSyZPKOut1CQ0LU6tmF+niVm3sdrN1cGu7Qtq3v0xTpkzVvn17tXv3TiUnJyk/P1/BwcGKjKyp+vUbqEkTz34iHQAAAAAAgK8gcHsea9q0qerXr6+jR4/a1WfPnl2pwO3x48e1YcMGU71nz56V7rEiNm3apMcee0w2m/mv/2NiYvTpp5+qZcuWHukFAAAAAAAAUkFBobdbqDBHnwRvsxnKq4bX4k3ZOfnebsGjYmOiFRsTfd73UFEpp0+batGxMV7oxH1qx8Sodozja6obGavAgECHx5zVpElTNWnS1C1zAwAAAAAAnK8I3LpYXl6epk2bpp49e6p+/frebqdMAwcO1Mcff2xXmzVrlkaNGqWoqKgKzfXll1+qsND+Fw8hISEaMKBiuxpUxpkzZ/TII484/AiyuLg4TZ48WQ0bNnR7HwAAAAAAAPhbRob5Xk1V5+dnNYVuc3ILdCwpzTsNAVXE/Dm/KCYuTnXq1lVMXKzTn2qWl5urY0eOmOr1GzRwal4AAAAAAADAXQjcOsEwDKWlpSk5OVnbt2/X8uXLtWTJEqWkpGjGjBnebq9cbrrpJk2YMMFuV9isrCy9/PLLevPNN8s9z7Zt2zR58mRTfcCAAQoLC3NJr6UZO3asjh07ZqqHh4dr0qRJhG0BAAAAAAAAwAkzvv2+eEdaPz8/xcTF6YGRI9SydatKzbdm1R/KzzfvhNyk2QVO9QkAAAAAAAC4y3kRuD148KDmzZunLVu2KDk5WZmZmcrNzZXNZpPNZpNhGGXOYRiGCgsLVVBQoPz8fOXm5iovL8/heRZHnztXRcXHx+vaa6/VTz/9ZFf/+eef1axZMw0fPrzMOZKSkvTggw+abo4GBATowQcfdGm/jixfvlw///yzqW6xWPTWW2+paVM+NgsAAAAAAAAAnFEzqlZx4LawsFCJx45pzcpVlQrcFhQUaMa335nq0TExqscOtwAAAAAAAKiifDpwu2fPHr344otat26dXb08AdvzyciRIzV//nxlZ2fb1d966y2lpKToX//6lwICAhyO3bJli0aOHKnExETTsdtvv13x8fHl6uG9997T+++/b6pff/31evXVV0sdW9JOvHfccYd69OhRrvUBAAAAAAAAACVrcdFF2r9nr13tt/kLNOD661Q7OrpCc33xyUQdPXzEVO9+eW9nWgQAAAAAAADcymcDtz/88IPGjBmjgoIChwFbd+1CWx3DvPHx8Ro9erTGjRtnOva///1PS5Ys0e23366uXbuqXr16yszM1N69ezV9+nTNnj3b4cd+NW/eXI8++qjbe1+5cqW2bt3q8NjkyZM1efJkl67XsWNHl88JAAAAAAAAAFVdp65dNPenWXa17KwsvfLsCxr5xL/VsHGjMufISE/X/z6eoOVLfjcdC4+I0DXXD3ZRtwAAAAAAAIDr+WTgduXKlXr++edls9kkeSZc6641POXOO+/U1q1bNWPGDNOx/fv3OwzjlqRWrVp6++23FRwc7MoWHZo+fbrb1wAAAAAAAACA813L1q3Uul1bbflzo1392JEjevqRR9Whcyd17NpFjS9oqlpRUQoKClJeXp5SU1N1+OBBbVy7Xit/X6rMzEyH8//zgfsUFhbmiUsBAAAAAAAAKsXnArf5+fl67rnnZLPZikOwhmGUGoitaHD23F1sLRZLcS0kJET9+/dX3bp1K9O+V40bN04Wi8WpEGt0dLQmTZqkpk2burAzxwzD0PLly92+DgAAAAAAAABAuvf/HtQzj/5LmRkZdnWbzabVK1Zq9YqVlZr3xttuVddePV3RIgAAAAAAAOA2Phe4/emnn3TkyBGHYdtzg7KOlHWOxWKxC+UWnT9ixAh16tRJrVu3VkhISGXb9yp/f3/95z//Ubt27fTGG28oPT29QuP79OmjcePGKTo62k0d2ktMTNSpU6c8shYAAAAAAAAAnO/i6tbVC6++oleef0FnTqc4PZ+fv7/uvOdu9Rt4jQu6AwAAAAAAANzL5wK333//vd3zot1nAwICNHjwYPXt21ctW7ZUZGSkAgMDNXnyZL388svF5/n5+WnJkiWKjo5WYWGhcnJydOrUKR0+fFgbNmzQ7NmztX//ftNOuJs2bdKIESM8ealuM3ToUPXr109TpkzRjBkzdOTIkRLPDQwMVI8ePTRs2DB17tzZg11KSUlJHl0PAAAAAAAAAM538Y0a6rX33tG3X0zW4gULZdhslZqnxUUX6e6HHlBCo0aubRAAAAAAAABwE58K3CYnJ2vTpk2mHWgjIyM1ceJEtWnTxjSmd+/eevnll4uf22w2zZs3T7fffrv8/PwUGhqq0NBQJSQkqFu3bhoxYoSmT5+uV199Venp6cVB3aVLl2rixIm67777PHKt7lazZk2NGDFCI0aM0KFDh7Rjxw4dO3ZMWVlZCg4OVmRkpBo1aqSLL75YgYGBTq/38MMP6+GHH67QmDZt2mjnzp1Orw0AAAAAAAAAKL+IyEjdP3KEBt54vX5fuEirli5X4vHjZY6rWauW2ra/VH36XqUWF13ogU4BAAAAAAAA1/GpwO2ff/4pwzCKA7dFXz/11FMOw7aSFB8frzp16ujEiRPFtSVLluj2228vcZ0bbrhB7dq10z333KPExMTi0O17772nyy+/XE2bNnXthXlZQkKCEhISvN0GAAAAAAAAAKAKqVu/voYOu1NDh92p9LQ0Hdi3T8knk5WdnaWc7BwFBPgrOCREtWNiVD++gWLj4kyfHgcAAAAAAABUFz4VuHW022lCQoIGDx5c6ri2bdtq3rx5xcHZdevWyWazyWq1ljimSZMm+uSTTzR06FDl5ORIkvLy8vTyyy/r888/d+o6AAAAAAAAAACoTsIjInRxu3bebgMAAAAAAABwm5ITpdVQYmJi8ddFu9ted911ZY67+OKL7Z5nZWU5DO+eq3nz5nrkkUfsdtVduXKlVq5cWcHOAQAAAAAAAAAAAAAAAAAAUFX5VOD21KlTplq7cvxFfbNmzUy1LVu2lGvNO+64Q3Xq1LGrTZo0qVxjAQAAAAAAAAAAAAAAAAAAUPX5VOA2JyfHVGvatGmZ45o3b26qbd26tVxr+vv766abbire5dYwDC1fvtxh+BcAAAAAAAAAAAAAAAAAAADVj08FbvPz80218PDwMsfVrVtXISEhdrU9e/aUe90rr7zS7rnNZtPixYvLPR4AAAAAAAAAAAAAAAAAAABVl08FbgMDA021c4O0JWnQoIEkFe9Su3fv3nKv27x5cwUFBdnV1q9fX+7xAAAAAAAAAAAAAAAAAAAAqLp8KnAbHBxsqmVlZZVrbHx8vAzDKH5+5swZpaamlmus1WpV/fr1Jf0d2N29e3e5xgIAAAAAAAAAAAAAAAAAAKBq86nAbXR0tKl25syZco2Nj4831Sqyy21YWJhdYPfYsWPlHgsAAAAAAAAAAAAAAAAAAICqy6cCt3Xr1jXV9u3bV66xjgK3e/bsKffahYWFds/T09PLPRYAAAAAAAAAAAAAAAAAAABVl08Fblu0aGGqrVmzplxjExISTLUtW7aUe+2UlBRZLJbi5+cGcAEAAAAAAAAAAAAAAAAAAFA9+VTgtnXr1sVfWywWGYah2bNnq6CgoMyxTZo0MY0tb1g3NTVVx44ds6uFh4eXs2sAAAAAAAAAAAAAAAAAAABUZT4VuI2JiVGLFi1kGEZx7dixY5o4cWKZYxs0aKCwsDC72oEDB7R3794yxy5YsMBUi4iIKEfHAAAAAAAAAAAAAAAAAAAAqOp8KnArSVdddVXx10U71b733nv6/PPPyxzbtm1bu7CuJI0fP77UMTk5OZowYYIsFoskyTAMWSwWNWzYsBLdAwAAAAAAAAAAAAAAAAAAoKrxucDtkCFD5O/vX/zcYrHIZrPpjTfe0K233qqlS5eWOLZfv3524wzD0OLFi/X+++87PD8vL0+jR4/WwYMHTcfatm3rxFUAAAAAAAAAAAAAAAAAAACgqvC5wG1sbKxuvvnm4p1qi3acNQxDf/75p+6//3716NFDS5YsMY3t37+/AgICip8Xjfvggw9055136pdfftGePXu0Y8cOffPNNxo4cKAWLVpUfN7ZunXr5t4LBQAAAAAAAAAAAAAAAAAAgEf4l31K9TNy5EgtWLBAycnJslgsklT8r2EYSk5OVn5+vmlcRESEhgwZoq+++qo4RFv079q1a7V27Vq784tCthaLxe78Fi1aqF27du69SAAAAAAAAAAAAAAAAAAAAHiEz+1wK0m1atXS22+/reDg4BLPSUhIcFgfMWKEwsLCJMkUuj33cXbQ9mzDhw933cUAAAAAAAAAAAAAAAAAAADAq3wycCtJ7du318SJE1W7dm1TIFYqOXBbq1Ytvfbaa3Y7454drj37Ian4WNG511xzjfr37++mqwIAAAAAAAAAAAAAAAAAAICn+WzgVpI6dOigWbNm6eabb5afn19x8LZ27dql7n57xRVX6IUXXpCfn58kFQdqJRXvblvk7OBt165dNW7cOHdcCgAAAAAAAAAAAAAAAAAAALykSgduhw0bptmzZys/P7/Sc9SqVUtjx47VwoULNWrUKLVq1UqNGjUqc9zQoUM1ZcoU1a9f3y5ke/buttJfQVs/Pz/ddddd+uSTT0oN8gIAAAAAAAAAAAAAAAAAAKD68fd2A6VZvXq11qxZo8jISF1//fW66aab1LRp00rNFRcXp+HDh2v48OEqKCgo15h27dppzpw5mjFjhmbPnq1NmzYpJydHkmS1WtWwYUP16dNHQ4YMKVeIFwAAAAAAAAAAAAAAAAAAANVPlQ7cFjlz5oz+97//6X//+5/at2+vIUOGqH///goMDKzUfP7+5b/swMBADR06VEOHDpVhGEpJSZFhGKpVq5as1iq9QTAAAAAAAAAAAAAAAAAAAABcoFokRi0WiwzDkGEYWrdunZ544gn16NFDr7zyivbs2ePRPqKiolS7dm3CtgAAAAAAAAAAAAAAAAAAAOeJapMatVgsdsHb1NRUTZ48WQMHDtQtt9yiH3/8Ubm5ud5uEwAAAAAAAAAAAAAAAAAAAD6m2gRuJckwjOLg7dnh240bN+qpp55Sjx49NG7cOO3cudPbrQIAAAAAAAAAAAAAAAAAAMBHVOnAbZ8+feTn51ccrLVYLHbHzw3epqWl6auvvtLgwYM1ZMgQTZs2TTk5OV7qHgAAAAAAAAAAAAAAAAAAAL6gSgduP/roI/3+++964okn1Lx58+Jg7bkc7Xq7adMmPfvss+revbvGjBmjbdu2eeEKAAAAAAAAAAAAAAAAAAAAUN1V6cCtJEVFRemuu+7STz/9pGnTpum2225TREREmeFbSTIMQxkZGfr2229144036sYbb9TUqVOVlZXl6csAAAAAAAAAAAAAAAAAAABANVXlA7dna9WqlZ5//nktW7ZMb731lnr27Cmr1VruXW+3bt2q559/Xj169NDzzz+vzZs3e+EqAAAAAAAAAAAAAAAAAAAAUJ34e7uByggICNDVV1+tq6++WklJSZoxY4Z+/PFH7du3T5KKd7gtcvaOt4ZhKDMzU1OnTtXUqVPVsmVLDR06VNdee63CwsI8fi0AAAAAAACArwoLC5K/v5+326iQc24tSpJCQw2FRwR7vplqLDsnX4eOpni7DQAAAAAAAABwmWoZuD1bTEyM7r//ft1///3auHGjpk2bpl9++UXp6emS7MO3Z39dtCvu9u3bNWbMGL322mu65pprdPPNN6tt27aevQgAAAAAAADAB/n7+ykgoHoFbksSGFjtb6Wimvnim2/15Xff2dXiYmL09cQJXurIu1568hlt37LFrtbzisv14KOPlDhm26bNGvv0s6b6u59NUExcnMt7BAAAAAAAgG+zersBV2rbtq1eeuklLVu2TG+88Ya6du0qi8VSHK49m8Visdv5Njs7W9OmTdMtt9yiQYMG6auvvlJGRoanLwEAAAAAAAAAAAAAAAAAAABVjE8FbosEBQVp4MCB+vzzz7Vo0SKNHDlS8fHxMgzDFL4tCt4WBXMNw9CuXbs0btw49ejRQ0899ZTWr1/vpSsBAAAAAAAAAAAAAAAAAACAt/lk4PZsderU0UMPPaRff/1VU6ZM0fXXX6+QkJBSw7fS37ve/vjjj7r99ts1cOBATZ48WWlpad64DAAAAAAAAAAAAAAAAAAAAHiJv7cb8KQOHTqoQ4cOev755/XLL79oxowZWrt2rQzDKA7aSrL7uiiUu3v3br3yyisaP368+vXrpyFDhqhDhw4evwYAAAAAAACgOivMy1F20lFvtwEXCompL7/AYG+3AQAAAAAAAABudV4FbouEhITohhtu0A033KDDhw9r+vTpmjlzpo4dOyZJDsO3RTvi5ubmatasWZo1a5YaN26sIUOGaPDgwapZs6Y3LgUAAAAAAACoVrKTjmrnV//xdhtwoRa3P6Ww+k293QYAAAAAAAAAuJXV2w14W3x8vB555BEtWrRIkyZN0rXXXqvg4ODigG0Ri8VS/Cg6tm/fPr322mvq2bOnRo8erT/++MOLVwIAAAAAAAAAAAAAAAAAAAB3OC93uC1Jly5d1KVLF2VkZGjOnDmaPn26/vzzT0mOd72V/tr5Ni8vT3PmzNGcOXOUkJCgoUOH6u677/Z0+wAAAAAAAAAAAAAAAAAAAHCD836HW0fCwsI0ZMgQffvtt5o7d64eeOAB1atXz7TrrSTTrrcHDx7UG2+84aXOAQAAAAAAAAAAAAAAAAAA4GoEbsvQqFEjPfroo1q4cKGmTJmioUOHqlatWsUB26IAblHwFgAAAAAAAAAAAAAAAAAAAL7F39sNVCcdOnRQhw4d9OKLL2rdunVauHChFixYoMOHDxO2BQAAAAAAAAAAAAAAAAAA8FHscFsJOTk5OnPmjFJTU5WZmentdgAAAAAAAAAAAAAAAAAAAOBG7HBbTjk5OVq0aJHmzJmjpUuXKi8vz+64YRjscgsAAAAAAAAA55G8vDzt3rdPR48fV3p6hrJzchQQ4K+wsDDFRceoccME1Y6KcnsfhYWF2rv/gA4cPqSUM2dUUFCg4OBgRUdFqXHDhkpo0MCt659KStLRI0d1KilZmRkZysvLk9VqUWBgoMIjIlQ7OloJTRorLCzMrX14UlZmpvbs2q3UlBSlp6UpNzdXQcHBqhUVpXr16yu+YYKsfn7ebhMAAAAAAAAuROC2FDabTUuXLtVPP/2k3377TdnZ2ZL+CteejaAtAAAAAAAAAJwfsrOztWjpUi1YskTbdu5SQUFBqefXjYtT144dNeCqK9UoIcGlvRw7flxTZ/6k35YtU3pGRonnxcbE6MpePXX9NdcoqlYtp9e12WzasGat/li+QpvWb1DqmTPlGhffqKE6du2qK/r3VS0PBJFdLSMjQwvm/KLVK1bqwL79Mmy2Es8NqVFDl1zWQX2uulKt27Ut1/w//PC9xo9/1UH9Z9WpU6dCvX7wwTv65pvJDuqfqm3bdhWa66uvvtBHH71nV7vkkvZ6771PKjQPAAAAAABAdUfg1oGtW7dq5syZmj17tk6fPi3JPmTrKGBbdPzCCy/UzTff7JlGAQAAAAAAAAAeM2/RIk38crJSyhkwlaTjJ05o2qxZmjZrlrp37qSH7rlHcTExTvWRl5+vL775RlNn/qTCwsIyzz+ZlKSvf5imH2fP0f3/GKaB/ftXal3DMLRs8RJNnfKVkk6crPD4wwcO6vCBg/pp6g8aMPg63XTbLfIPCKhUL56Ul5urH77+VvPnzFFOdk65xmRnZWnFkt+1Ysnvatayhe4a/oAaX9C01DE9e/ZyGLhds2aVBg4cXKGe1679w2F9w4a1FQ7crly53FTr0aNXheYAAAAAAADwBQRu/7/jx49r1qxZmjlzpvbt2yep/CHb0NBQXXPNNRoyZIhat27tmYYBAAAAAAAAAB5hGIYmfPGlvv/xR6fmWbbqD23culVjn3pKF190UaXmOJOaqmdfeUXbd+6q8Nis7Gy9/fEnOnzsmB66++6Kjc3K0gfj/6v1q9dUeN1z5efna+bUH7Rz+3Y9+eILCgoOcnpOdzm4/4Def+NNHTl0qNJz7N6xU8+N/rdu+ceduvaG60s8LzY2Ts2atdDu3Tvt6qtXVyxwm5KSoj17djs8tm7dGv3zn/eWe6709HRt3rzRVO/encAtAAAAAAA4/5zXgduMjAzNmzdPM2fO1Nq1a2UYRpkhW+nvoG27du108803a8CAAQoJCfFIzwAAAAD+H3v3HR1F2f5//LPpCQESOqGFIr33XqQXu4g+FlRUQOwVRCwUwYKioCJiw8ojj0gvUqV36R1CJwklgfSy+/vDH3xZZrIku5vskrxf53gOue6Za64Z8GaZveYeAAAAIG/9NnOmy822V1y+nKA3x7ynLz/6UBFly+Zo39S0VL3y1ts6euyYSzX8b/YcVapQQb27ds3W9mmpqXr/7Xd1YO8+l457vX27duu3aT+q/1PZbwDNSzv/+UcfjRqjtNQ0l3NlZmbq52+/V2x0jB4bPDDL7dq1a29ouN2yZZOsVqt8fHyydazNmzfYfddxrd27dyo1NVWBgdlrct64cZ1hFeWqVaspIqJctvYHAAAAAADITwpcw63VatWqVas0a9YsLVu2TKmpqZKyv5pt0aJFdccdd+i+++5TtWrV8qZoAAAAAAAAAIBHRB0/rh9++dV0rHnjxurUrq2qVa6iUiVLKDgoSOkZGbpw8aIOHDqsv1as0PrNmw37JSQmatI33+i9N9/MUS1x8ZcUF3/JLlYuoqx6d+2qxg0aqHzZsvLz81PcpUvau/+AFi1bZnp8Sfp62jR1aN1aIdk47s/ffp9ls231WjXVqn07Vat+i0qVKaug4CD5WCxKSkpSzNloHT5wUGtWrNTB/ftN9188d5469+im8hUrZqOSvHP08BGHzbZVb7lFrTu2V90G9VWseHEFBAYq7sJFHTt6VBvWrNX61WuUmZFh2G/xvPkKLRyqvg89aJq3bdsO+vbbr+1ily7Fa//+vapVq062at+0aUOWY2lpadq1a4eaNGmWrVxr164yxNq165itfQEAAAAAAPKbAtNwu2vXLs2aNUvz58/XhQsXJN24yfbKNhaLRS1atNB9992nrl27KiAgIE9qBgAAAAAAAAB41v/mzFH6dY2TAQEBevPll9SmRQvD9r6+voooU0YRZcqoY9s2Wr95s0Z9+JFS/v/iD1ds2LxFUcePK9LJRtPAgAA98cjDuqNnT/n6+tqNlSxeXCVbt1L71q3099p1GjthgtLS7BtHL19O0IrVa3RflVoOj3Pi2HH9tWChIe4fEKBBzz+r1h3am+5XpGhRFSlaVNVqVFf323pr26bNmvzpZ7oUF2+3ndVq1ZoVK9XvkYezc9p5IiUlRRPeG2fabBtauLD6P/WE2nbqaBgrVaa0SpUprWatWuqufn319cTPtX/PXsN2M6f/rhp16qhsx26GsZo1a6lEiZI6dy7WLr5x4/psN9xu3rzR4fiWLZuy1XBrtVq1YcM6Q7xt2w7ZqgMAAAAAACC/yd77h25Sp0+f1ldffaVevXqpb9+++umnn3T+/HnZbLarjbRX/rvWlfHixYvrqaee0uLFi/XDDz+od+/eNNsCAAAAAAAAQAGRmZmpFavXGOIDHnrQtNnWTMumTfX688+bjq1YY8ydHUFBQXr/nbd1d58+hmbb67Vv3UpPD3jcdGzdpk03PNbcP2bKZrUa4gOfeybLZlszjZo11Wtvj5DFx/i1xJYNN64jL/05/XfFREcb4sWKF9fb779n2mx7vXIVKmj4mFFq1qqlYcxms+mbSV8oPT3dMGaxWNS6dVtD3NGqtdc6dixKMTHG2q+1bZv5qsfX27Nnl+Li4uxipUqVVs2ajpu0AQAAAAAA8qt813CbkJCgGTNm6OGHH1aXLl00YcIEHTly5IZNttL/rWbboUMHTZo0SStXrtRLL72kChUqeOBMAAAAAAAAAACeFHvuvJKSkw3xrh075ihP+9atVKdmTUP8n527nKrrmScGqF7t2tnevk+3boooU8YQ33fwoMP90lJTtWHNWkO8boP6atMx56ucVr3lFjVpblxZ9cypUznOlVsSExK0cM5cQ9zXz0+vjBiu8jlYkdjf31/Pvf6qKletahiLiY7W/LlzTPczW0F2164dSkpKuuExN21ab4gVK1bc7ue9e/dkK9fatasNsTZtst9kDQAAAAAAkN/ki4bbzMxMrVixQi+++KLatm2rESNGaPPmzbJardlqsrXZbCpbtqyeeeYZLVu2TF999ZW6dOlyw5UBAAAAAAAAAAD51/mLF9yWq12rlvLx8VHJ4sXVoE4d9eh8q9q2zN4qudeqVKGCenTunKN9LBaLOrUzrpoaFx+vxMTELPfbt3uPUlNSDPHed92Zo+Nfq06D+oZYRkaGkrPRAJoX/l663PSc7+h7jypXMzbO3oifn5+GvPKifP38DGMz/5hhuk+TJs0UFBRkF8vIyNC2bVtueLzrV8ItV6682rWzb+DNyMjQjh3/3DCXWcNt+/Y5b7QGAAAAAADIL4x3eG4iO3fu1KxZszR//nxdvHhR0r8NtFeYNdheYbPZ5Ofnp1tvvVV9+/ZV27ZtHW4PAAAAAAAAAChYggIDTePLV63Wnb175SjXHT176s5eveTv7+9STV06dHDqXnaNatVM47GxMQoNDTUdqxBZSc8PfU0xZ88q5my0os+e1eX4S6rbsEGOj39FiVIlTeOpqWkKDglxOq+7mK3o6x8QoJ633+Z0znIVKqhpyxbasHqNXTwq6qgOHz6kqlXtf28CAwPVrFkLrVq10i6+adMGtWnTLsvj/NuUu9Uu1qRJM9WtW1+zZv1hF9+6dZNatmydZa7Y2BgdOnTALlaoUCE1atQ0y30AAAAAAADyu5uu4fb06dOaPXu2Zs2apaioKEk5a7KVpEqVKqlv3766++67VaxYsVytFwAAAAAAAABwc6pQrpz8/PyUkZFhF//qhx8UVrSoOrZtk+1cAQEBbqmpfp3aTu1XsngJ03hycnKW+4QXK6aWOTjH7AgJNm+qzczMMI3npZSUFB3ct88Qb96qpUILF3Ypd5ce3Q0Nt5K0YcN6Q8OtJLVp086k4Xa9w2Ps3r1LSUn2KxY3adJc1avXMGy7davj1XLNVrdt2bKN/ExW6gUAAAAAACgoboo7IwkJCVqwYIFmz56tLVu2yGazZbvJVvq30TYwMFDdunVT37591bx589wuGQAAAAAAAABwkwsICFDThg21fvNmu3haWppGffSR/pg7V91v7aQWTZuqRB4t7hBZoaJT+xUqZN7ompaW6ko5OWK1WhV99myWY54WdfiIaR2169dzOXeNOrVNm7f37t1tun3r1u1ksVjsvgs5dixK0dFnVbp0GdN9Nm/eYPezxWJR48ZNFR4errCwMMXFxV0dO3Bgny5fvqzCWTQSr1tnbLht166D6bYAAAAAAAAFhVc33K5YsUKzZs3SsmXLlJaWJun/Vqm9vsnWZrPZxa5sV716dfXt21d33HGHihQpkkeVAwAAAAAAAADygwfuudvQcHvF7n37tPv/r4haNTJSzRo3UtOGDVW3Vi35+/u7vZagwECFhhZyal9fH1/TeGZmpislOZSQkKCTUcd05NAhHdi3X3t37tKl+Hjzja9pLPWU0ydPmsar3GJcgTan/P39VbFypI4cPGQXP3YsynT7YsWKq1atOtqzZ5ddfNOmDerT5w7TfTZtsm+4rVKlmsLDwyVJderU05o1q66OWa1Wbd++VW3bGpto09LStGXLJruYn5+fWrVy72rHAAAAAAAANxuvbrgdNGiQ4Qnu65tqr/x87XYhISHq1auX7rvvPtWvXz9viwYAAAAAAAAA5Bt1a9XSvbffrhmzZzvc7nBUlA5HRem3P2YqKDBQ9evUUfPGjdWiaRNFlDFfkTSnQkLMV6n1pITLl3X65CnFRkcr+my0Ys6eVczZaJ09fVoXL1zwdHk5cv7cOdN4RLlybslftlw5Q8NtTEx0ltu3bdvepOF2vWnDbUJCgmG13KZNm139dcOGTewabiVpy5bNpg23W7duVnJysl2sceOmKlQoNMtaAQAAAAAACgKvbri9IqvVbK/ErzTa1qtXT/fdd5969+7tlTceAQAAAAAAAAA3n0GPPapMa6Zmzp2Xre1TUlO1cetWbdy6VZOmTlXlihXVvk1rdW7fXuXKlnW6Dj8/z9/ST0tL09aNm7R+9Rod3LtPF86f93RJbpNwOcEQ8/PzU0BgoFvyhxQyfm+RmJiY5fZt2rTXlClf2MU2b94oq9UqHx8fu/jWrZsNqxU3adL86q+vbb69dh8z69atNsTMGnMBAAAAAAAKGs/fncumrFazLVKkiG6//Xb17dtXNWrU8GSJAAAAAAAAAIB8yGKx6JknnlCzho309Y8/6uixYzna/+jx4zp6/Lim/TZdzRs31uDHH1MFJ1ZNtdx4k1yTkZ6uhXPm6s//zlBigrExNbsCg4KUmpLixsrcJz0t1RALKVTIbfkLmeRKT0+3+/7jWlWrVlPZshE6c+b01Vh8fLwOHNivmjVr2W27adMGu599fX3VsGGjqz9Xq1ZdYWHhiou7eDV25MghxcXFKSwszG7ftWvXGGpp27a945MDAAAAAAAoAG6ahtvrV7Nt1qyZ+vbtqx49eiggIMCTpQEAAAAAAAAACoAWTZuoRdMm2r5rl5asXKnV6zfo0uXL2d7fZrNpw5Yt2rJ9u554+CH1veOOXKzWfWKjo/XBu6N18vjxHO9r8fFRpcqRql2vnho3byqbTRozfEQuVJk7rnwn4Q4ZGZmGmL+/v2mz7RVt2rTTjBnT7WKbNq2/YcNtrVp1FBLyfw2+FotFjRs31bJlf12N2Ww2bdu2WZ06dbkaO3r0iM6cOWWXq2bN2ipVqrSDMwMAAAAAACgYboqG2ys3tIoXL64777xTffv2VWRkpGeLAgAAAAAAAAAUSA3q1lWDunX1wqBB2r1vvzZs2azN/2zX4aNHs9WgmZGRocnffa+gwEDd1qNHHlTsvDOnTuvd14cpPi7uhtuGFQtXRPnyiihfXuUrVFClKpUVWbWKgoKCrm6zZ8fOXKzWNf4BgYZYUlKS2/Inm+S69tqYadOmvaHhduPG9Xr44ceu/nz27BmdPGnfDN2kSTNDrqZNm9s13ErSli32Dbfr1q027MfqtgAAAAAAAP/y+oZbi8WiNm3aqG/fvurcubP8/Ly+ZAAAAAAAAABAAeDr66v6dWqrfp3aevIRKf7SJf2za5e2bt+urdt36PTZsw73/2zK16pXu7YiK1bMo4pzJiMjQ5M+Gp9ls21k1Spq1qqVataupciqVRRSqJDpdtdKT093c5XuE1o41BDLzMhQakqqAoOMzbg5lZiYYIgVusE1a9SoiQoVKqTExMSrsV27dig5OVnBwcGSjKvbSuYNt2axbds22/28dq2x4bZdu44OawQAAAAAACgovLp79emnn9a9996riIgIT5cCAAAAAAAAAIBDRYsUUYfWrdWhdWtJ0vGTJ/X32nVauHSpzkRHG7a3Wq36fdYsvfrss3ldarYsmjNPRw4eMsRDChXSU88OUYu2bXKcMzU11R2l5YpixUuYxk+fPKnK1aq6nP/ksROGWNmyjr//8PPzU4sWre1Wpk1PT9e2bVvUunVbSdKWLZvs9gkMDFTduvUNucqVK6+yZcvpzJlTV2PHjkXp3LlYlShRUgkJCdq5c/t19ZVT1arVbnxyAAAAAAAABYCPpwtw5LnnnqPZFgAAAAAAAABwU6pYvrweuq+vfvjic9135x2m26zfvCWPq8q+RXPnGmIWi0UvDx/mVLOtJF04d840brM5lc6tylUobxo/csjYdJxTqSmpOn3ypCEeEVHuhvu2adPOENu4cf3VX//zz1a7sXr1GiggIMA0V9OmxlVurzTsbtiwTpmZmXZj7dt3uGF9AAAAAAAABYVXN9wCAAAAAAAAAOBpcfHx2rF7j+YuWqzTZ87keH9fX18NfPRRNahb1zT3pcuX3VGmW506cVKx0TGGeKOmTVW7fj2n80YdOWoat9msTud0l8gqleXr62uI796+w+Xcu3fulNVqPMcaNWrdcN9WrdoY6tq4cZ0k6fjxYzp3LtZurEmT5lnmatIk64bbtWtXGcbatqXhFgAAAAAA4Ao/TxcAAAAAAAAAAIC3yczM1Ktvva2jx4/bNcQ+PeBx3XPbbU7lbNOiubbv2mWIJycnq0jhwk7XmhvMVmOVpJp1azud05qZqR1bt5mPmTSj5rWg4GBVq1FD+/fssYtvXr9Bl+IvqUjRIk7nXr5osWncbMXZ6xUpUlT16jWwW8n2+PFjOnv2jLZtM66Q7ChnkybNZbFYZLtmSeGtWzfLarVqw4Z1huPWr9/whvUBAAAAAAAUFKxwCwAAAAAAAADAdXx9fZWYnGxYfXbtxo1uP1ZISIjbc7oqNSXFNB4YFOR0zpVLl+nihQumYxkZmU7ndaeWbVsbYunp6Vowe7bTOU9EHdO2TZsN8Yhy5VSlStVs5WjTpr0htnHjersmXEkKDQ1V9eo1s8wTHh6uKlWq2cXOnj2jJUsWKy7uol28deu2piv+AgAAAAAAFFQ03AIAAAAAAAAAYKJx/XqG2D87d2nfwYNO5ftnp3F12xLFi6twaKhT+XJT4SLmq7lGHT7iVL7oM2f0y3c/ZDmenp7mVF53a9upo2lT8ZwZf+iwE7/vGRkZ+uKTCcrMNDYU337HXdmvq61Zw+06Q8Nto0ZNbtgka7YC7pQpnxti7dp1zHZ9AAAAAAAABQENtwAAAAAAAAAAmOjZpYssFosh/uHEibqckJCjXNt27tS6TZsM8RZNmjhdX24qE1HWNL7271U6Hxubo1ynT57Ue2++rYTrVgu+VlqqdzTchhYurO59ehvimZmZGj/qPZ04djzbuTLS0zXxg49Mm5TDwsPV57Y7sp2rQoWKqlixkl1s7drVio2NsYs1aWJspr1e06bNDbGzZ8/Y/RwQEKjmzVtmuz4AAAAAAICCgIZbAAAAAAAAAABMVCxfXk0bNTTEo46f0Csj3lLUiRPZyrNx61a9PXacbDabXdzHx0d39OrpjlLdrnTZsoooX94QT01J0fvvjspW0216errmz5qt4S++rJjoaIfbJiUlOV2ru93Vr69KlS5tiF+8cEEjh76h1ctX3DDH6ZMnNebNt7Rx7TrT8YefeFyhOVzZuE0b+1Vu09KMTcpNmhibaa/XoEFj+fn5OdymadPmCg4OzlF9AAAAAAAA+Z3jOyoAAAAAAAAAABRgQwYM0MBdLyn1uubGQ0ePauCLL6lN8+Zq17qVqletqmJhYQoICFBySorOnT+v/YcOafmq1dq0bZtp7t7duqpqZGQenIVzut/WW999+ZUhfiLqmF4d8pxu7d5NjZs3U/mKFVUotJAyMzIUHx+vE8eOa+fWbVq3arXi4+KydaxL2dwuLwQFB+vZ117RqDeGG1beTbh8WZ+P/0QLZs9Rm44dVK9BA4WXKK7AwEDFXbyoY0eOauOatVq7arUyMzJM83e/rbdad2hvOuZI27bt9euvP2Y5Xrx4cVWuXOWGeUJCQlS7dl3t2PFPltu0a9chx/UBAAAAAADkdzTcAgAAAAAAAMgzwSXLqcaDwzxdBtwouGQ5T5eQqyqUK6chTwzQx198aRjLyMjQyrVrtXLt2hznrVGtmgY/9pg7Ssw1XXp014rFS3T08GHDWHJSkubN/FPzZv6Zo5zdb+utE8eOa8+OnXbxE8eOu1Kq21WrUV0vDH1dn4x9X+kmK8keOXhIRw4eynHedp066pEnBjhVU9269VW0aFHFx8ebjjdu3CzbuZo0aZZlw62Pj4/atGnnTIkAAAAAAAD5Gg23AAAAAAAAAPKMb0CQQstV9XQZQI707tZN6ekZmjR1qmw2m8v5Gtarq3eHDlVgYKAbqss9Pr6+evXtN/X2q68rNjrGpVyhhQvr8cED1ap9O/30zXeGhtvd23e4lD83NGrWVMNHj9Sn4z7QxQsXXMrl6+enu+/vp7vvv8/5HL6+atmyjRYtmm863qRJ9htumzZtru+++9p0rE6duipWrLhTNQIAAAAAAORnPp4uAAAAAAAAAAAAb3dn7176ZMxoVapQwekchQuHatBjj+rDd99VaKFCbqwu94QXK6aRH32ohk2aOLW/j4+POnXrog+/mKhW7f9dNbVW3TqG7Y5HRenMqVMu1ZobatSupfcnfapbu3eTj49zX6nUrldXYz4Z71Kz7RVt2rTPcqxp0+bZzlOnTj0FB4eYjrVt2yHHdQEAAAAAABQErHALAAAAAAAAAEA21KtdW19P+ETrN2/WX8tXaPM//yg5JcXhPj4+Pqp5yy3q3L69unbqqEIh5k2O3iwsPEyvvTNC27ds1aK587Trn+3KyMjIcnsfHx9VrlpV9Ro31K3duqpk6dJ24w2aNFaRokV1KT7+asxms2nezFl64pmnc+08nFW4SBE9+ewQ3dH3Xi1ZsFAb1qxVzNmzDvcpElZUTZo3163du6pajRpuq6Vly1by9/dXenq6XTwiopzKlCmb7Tx+fn5q0KCh1q9faxhr146GWwAAAAAAADM03AIAAAAAAAAAkE2+vr5q06KF2rRoIZvNplNnzujosWO6dPmyEpOSlZqaqsDAABUpXFgRZcqqWpXKCgkOzvFx+j9wv/o/cL/b6i5TupSW/jnTEA8pXTFb+1ssFjVs2kQNmzZRWlqajh46rLNnzigxIUGpKSkKCSmk0CKFFRYepirVqinYQWOxn5+fvvp5Wo7qf2vcmBxtL0m169fTr3Nn5Xi/rJQqU1r/eay//vNYf50/d07HjkbpXEyMkhKTZLNZFRwSorDwcFWMrKQyERFOr4jrSEhIIS1fvs4tuT766DO35AEAAAAAACgoaLgFAAAAAAAAkCsyMjI9XUKOWSzGmNVqU9pNeC6elJySfuON8gGLxaLyEREqHxHh6VLyVEBAgGrUrqUatWt5uhSPKV6ihIqXKOHpMgAAAAAAAJCHaLgFAAAAAAAAkCsSElI9XUKO+fr6GJpuU1IzdDr2kmcKAgAAAAAAAAB4Bfe/zwgAAAAAAAAAAAAAAAAAAADIR2i4BQAAAAAAAAAAAAAAAAAAAByg4RYAAAAAAAAAAAAAAAAAAABwgIZbAAAAAAAAAAAAAAAAAAAAwAEabgEAAAAAAAAAAAAAAAAAAAAHaLgFAAAAAAAAAAAAAAAAAAAAHKDhFgAAAAAAAAAAAAAAAAAAAHCAhlsAAAAAAAAAAAAAAAAAAADAARpuAQAAAAAAAAAAAAAAAAAAAAdouAUAAAAAAAAAAAAAAAAAAAAcoOEWAAAAAAAAAAAAAAAAAAAAcICGWwAAAAAAAAAAAAAAAAAAAMABGm4BAAAAAAAAAAAAAAAAAAAAB2i4BQAAAAAAAAAAAAAAAAAAAByg4RYAAAAAAAAAAAAAAAAAAABwgIZbAAAAAAAAAAAAAAAAAAAAwAEabgEAAAAAAAAAAAAAAAAAAAAHaLgFAAAAAAAAAAAAAAAAAAAAHKDhFgAAAAAAAAAAAAAAAAAAAHCAhlsAAAAAAAAAAAAAAAAAAADAARpuAQAAAAAAAAAAAAAAAAAAAAdouAUAAAAAAAAAAAAAAAAAAAAcoOEWAAAAAAAAAAAAAAAAAAAAcICGWwAAAAAAAAAAAAAAAAAAAMABGm4BAAAAAAAAAAAAAAAAAAAAB2i4BQAAAAAAAAAAAAAAAAAAAByg4RYAAAAAAAAAAAAAAAAAAABwgIZbAAAAAAAAAAAAAAAAAAAAwAEabgEAAAAAAAAAAAAAAAAAAAAHaLgFAAAAAAAAAAAAAAAAAAAAHKDhFgAAAAAAAAAAAAAAAAAAAHCAhlsAAAAAAAAAAAAAAAAAAADAARpuAQAAAAAAAAAAAAAAAAAAAAdouAUAAAAAAAAAAAAAAAAAAAAcoOEWAAAAAAAAAAAAAAAAAAAAcICGWwAAAAAAAAAAAAAAAAAAAMABGm4BAAAAAAAAAAAAAAAAAAAAB2i4BQAAAAAAAAAAAAAAAAAAABzw83QBAAAAAAAAAPKn0NBA+fn5erqMHLFYjLFChWwqXCQo74u5iSWnpOv4qYueLgMAAAAAAAAA3IaGWwAAAAAAAAC5ws/PV/7+N1fDbVYCAriVCvd4afib2r57t12sW6dOev355zxUEQAAAAAAAIDs8PF0AQAAAAAAAAAAAAAAAAAAAIA3o+EWAAAAAAAAAAAAAAAAAAAAcICGWwAAAAAAAAAAAAAAAAAAAMABP08XAAAAAAAAAKDgSElP1cmLpz1dBtyofHiEgvwDPV0GAAAAAAAAAOQqGm4BAAAAAAAA5JmTF09r9LwJni4DbvRm7xdUrVRlT5cBAAAAAAAAALnKx9MFAAAAAAAAAAAAAAAAAAAAAN6MhlsAAAAAAAAAAAAAAAAAAADAARpuAQAAAAAAAAAAAAAAAAAAAAdouAUAAAAAAAAAAAAAAAAAAAAcoOEWAAAAAAAAAAAAAAAAAAAAcICGWwAAAAAAAAAAAAAAAAAAAMABP08XAAAAAAAAAADAzer0mTM6cPiILsRdVHJysgIDA1WmVGnVrH6LShQrlmd1RMfE6HDUMcXExiopOVk2m00hwcEqVbKEqkRGqmzp0nlSR2xMjI4fjdK5mFglJydLsikoOFglSpZUxchIlSqTN3VIUmZmpg4fOKiTx48r4fJl2Ww2FQoNVanSpVWl+i0KDQ3Ns1oAAAAAAABw86PhFgAAAAAAAACAHLgYF6dZCxbor+UrdDYmxnQbi8WiOjVr6P6771arZs1ypY4z0dGavWChVq5Zo+jYWIfblipZUh1at9YdvXq6vfk25my0/pq/QOtXr9G5LK7HFSVKllSLtm3UrXcvp5tvH+hzh93PxYoX1+c/fHv15/i4OM36fYb+XrpciQkJpjksFotq1KmtW7t1VZsO7eXj6+tULQAAAAAAACg4aLgFAAAAAAAAACAbrFarpv/5p375fYaSkpMdbmuz2bRr7z69OeY9dW7fXs899ZRCQwu5pY6EhERN/elHzf9riTIzM7O1T0xsrH6fNUv/mzNHfbp304CHHlJoIdfqSUxI0G8//Kjli//Kdh3nYmM1b+afWjBrtjr37K77H3lYIS7Wca2tmzbry08mKOHSZYfb2Ww27du1W/t27dac/83U408PUs06td1WBwAAAAAAAPIfH08XAAAAAAAAAACAt0tITNSwkaM0ddqPN2y2vd7Sv//WoJdf1oWLF12uY9/Bg3ryhRc0Z+GibDe5XstqtWr2goUa9NLLOnz0qNN1HD5wUK8/+7yWLFjodB1/zVugYc+/pGNHnK/jWiuXLtP4UWNu2Gx7vRPHjmn0G29q0Zx5bqkDAAAAAAAA+RMNtwAAAAAAAAAAOJCSmqrho0dr8z//OJ3jTHS0Xn93pBJz2Kx7rW07dujlN0co5tw5p3NcW8+Lw9/Url07c7zvru07NGrYcJ2Pdb2OmLNnNXLYcB3ct9+lPNu3bNWUTyfKarU6tX9mZqa+/2qKFs6Z61IdAAAAAAAAyL/8PF0AAAAAAAAAAADebMKXk7Vr7z7TsXJly6pn1y5q0biJSpUsIR8fH8WcO6fN27Zp4ZKlOnr8+NVtj0RFOV3D4agojXhvrFJSUw1jFotFrZs3V9uWLVSnZk2Fh4XJ18dH5y9e1JGoKK3esEErVq1WekaG3X6JSUkaOvRlTZv2m4oXL56tOo4djdL40WOUmkUdTVo0V7NWLVW9Vi0VDSsqX19fXbx4UcePHNWm9Ru07u9VyriujqTERH08ZqzGfjZBYeFh2b8o/19KSoo+H/+Jodm2Tv366tS9q26pUUPhxYvpcvwlHTt6VGtW/q21f6+SzaQ5d9qUqSpfoYLqNmyQ4zoAAAAAAACQv9FwCwAAAAAAAABAFtZu3KS/VqwwHbv/7rvU/4EHFODvbxePrFBBkRUq6K7evfXfP//Utz//4vTKq5KUmpqqd9//QMkpKYaxqpGRGvrCC6oSWckwFlGmjCLKlFHbli318H336ZMvvtS2nfYr2p4/f15vvTVMEydOlo+P45fipaWmasJ745SSbKyjUuXKevrlF1QxMtIwVrpMGZUuU0bNWrfS3ff309RJX2j3jh1228RdvKiJH36k4aNH3rCO6yUlJtr9XKhQIT329CC16dDeLl6sRHEVK1FcjZo1VY/b+ujzjz7W2TNn7Lax2Wz66tOJ+vDLSQoKCspRHQAAAAAAAMjfcnbXCgAAAAAAAACAAsJms+nbn34yHRv8+GN68pFHDM221/L19dUD99yjYS+8kOMm0mv99PvvOnVdY6gktWjSRJM+eN+02fZ65cqW1bi331Ln65pQJWnLls2aP3/uDXPMnP5fQ4OqJDVs2kQjx39g2mx7vTIRZTV05Ntq07GDYWzPjp1atWz5DXM4UqhQIQ0b/a6h2fZ61WpU11vvv6eI8uUMY+diY7V47nyX6gAAAAAAAED+Q8MtAAAAAAAAAAAm1m/erKPHjxviHdu21b23357tPLe2b6cH+97rVA1x8fH6Y+48Q7xqZKTeeu1VBQQEZDuXn5+fXn/uOdWqXt0w9ssvP8pms2W576X4eC2YbWzKrVS5sl4Y+nqO6xj84vOqVsNYx9w//nRYx408N/Q1Vb3llmxtG16smF4a/ob8TZqm5838Uxnp6U7XAQAAAAAAgPyHhlsAAAAAAAAAAEz8tXyFIRYUGKghTwzIca7/3HuvykdE5Hi/RcuWKSUlxRB/buBTCgoMzHE+X19fDX78MUP8yJHDWrdubZb7rVyyVKkmdTw2eKACg5yr4+EnHjfETx4/ru1btuY4nyR16tZV9Rs1zNE+5SqU1+0mzdCX4uP1z1bn6gAAAAAAAED+RMMtAAAAAAAAAADXSUtL0/rNmw3xNi1bqFhYWI7zBfj76/YePXK837K/VxlidWvVVN1atXKc64o6NWuqcqVKhviqVSuy3GfNyr8NsRq1a6lGbefrqF6rlipEGuvYvH5DjnNZfHx01/33OVVHt9495evnZ4ivX7XGqXwAAAAAAADIn2i4BQAAAAAAAADgOvsOHlRqWpohfmu7dk7nvLV9O/n6+mZ7+wsXL+pwVJQh3r51a6druKJZo0aG2Pbt/5huG3fxoo4fNdbRok0bl+to0LixIbZ/z94c56nXoL5KlirlVA1FihZVo6ZNDPE9O3Y6lQ8AAAAAAAD5Ew23AAAAAAAAAABcZ/f+/abxWtWrO50zPCxMlSpUyPb2ew8ckM1mM8RvqVLF6RquqFo50hA7evSI4uPjDPFD+83riKzqeh2VTOo4deKELl+6lKM8jZo3c6mO6rVqGmIXL1xQbEyMS3kBAAAAAACQfxjfkQQAAAAAAAAAQAF38tRpQ6x4sWIqWqSIS3mrRkbqiMmqtWaOHjtuGl+1br22ubj6anRsrCFms9l04sRxVatRwy5+POqYaY6Na9dp9/YdLtVxLtbY0Gqz2XT29BkVzsG1rlKtqkt1ZNU8fPb0aadXzgUAAAAAAED+QsMtAAAAAAAAAADXORMdbYiVLe1642WlCuWzvW2MSVOsJP0xd67LdWTlksnKsuezqGPh7Dm5VkfC5cs52r5iZKRLxytVpoxpPOYsK9wCAAAAAADgXz6eLgAAAAAAAAAAAG+TkJBgiAUHB7ucNzQ0NNvbXoyPd/l4ORVvcsxLHqjD7PpnxdfPT0Eu/t4EB4eYxpMSs18HAAAAAAAA8jcabgEAAAAAAAAAuE5KWqohFhLkesNtTnKkphpryG0JCcaVZT1RR1JCYra3DQkxb5bNiZBC5jlSU/L+3AEAAAAAAOCdaLgFAAAAAAAAACCPBAUFZntbq9Wai5WYy8zMNNaR6YE6rMY6suIf4O/68TLMj+fr5+tybgAAAAAAAOQPfp4uAAAAAAAAAAAAbxMcGGSIJSYluZw3OSUl29v6+xsbSctFlNW0L75wuY4rQkpXlMVisYulpNuv6mpWR5mICH0y5Uu31eGKlOTsX9OsJCeb/94GuWFVYwAAAAAAAOQPrHALAAAAAAAAAMB1QkMLGWLuaLhNTMx+jtBCxhpiz513uYacCjG5FufPncvzOrKSnJwsm83mWo4sfm+LhBV1KS8AAAAAAADyDxpuAQAAAAAAAAC4TkSZMobYyTOnXc574eLFbG9bonhxQywtLU3nLlxwuY6cKGZSR3pami6cz/vmXzM2q1UXz7t2TU6dPGkaLxNR1qW8AAAAAAAAyD9ouAUAAAAAAAAA4DoVy5c3xC5fTtCZ6GiX8h48ciQHNZQzjf+zc6dLNeRUhMm1kKQ9O/K2DkeicnBdTfc/fNQQ8/HxUbkszh0AAAAAAAAFj5+nCwAAAAAAAAAAwNvUq13bNL59126VLV3aqZxWq1X7Dx3K9vbVq1UzjW/e9o+6dOjgVA1X/LNzlzZs2ayK1WoqIqKcypaNUNmyZRUUFGzYtsot5nXs2LpNbTt1dKmOPTt2atvmzSpZurRKliqlUmVKq0TJUgoMCsxRnoP796tx82ZO17Fv125DrHK1qgoKNl4PAAAAAAAAFEw03AIAAAAAAAAAcJ3qVauqcOFQXb6cYBdftGypenS+1amcm//5R3Hx8dnevkqlSioWHq4LFy/axVeuXauBj/ZXeFiYU3VI0s+//66tO3ZImmUX/9//Zqt4qZJ2sYqRlRQWHq646+pYv2atHhzwmIq6UMfM6f/Vru07DPEJU79S6TJlsp1n7Yq/dd9DD8piseS4hnMxsdpjsmpwg8aNcpwLAAAAAAAA+ZePpwsAAAAAAAAAAMDb+Pj46NZ27QzxHbv3aN/Bg07lnL1gYY62t1gsat+qlSGelpam//45y2SP7Nm9b9//b7a1V716TZUrV960juZtWhvi6WlpmvvHn07XcWDvPtNm28iqVXLUbCtJMdHR2r5lq1N1LJg9RzabzRBv26mTU/kAAAAAAACQP9FwCwAAAAAAAACAiT7dupuumPrhxIlKT0/PUa6/167Tuk2bcl5D926mNcyYPVvbTFZlvZH09HRN+nqq6Vjv3n2y3K9LD/NrMe/PWdpt0rx7Ixnp6fr+qymmY+1vdW4F4e++/Eopyck52ufwgYNaOHuOIV6nfn2VLRfhVB0AAAAAAADIn2i4BQAAAAAAAADARJXISurQpo0hHnX8hMaM/1gZGRnZynPg8GF9/OUXTtVQuVIldWhtXF3WarVq9Ecf5Wi1XZvNpk+/mqIDhw8bxkqWLKU777wny30rRFZSi7bGOmxWqz57/yMdPpCzOr79crKOHjLWUax4cXXu2T3bua4VEx2tSR99rIxsNkOfPX1Gn4wdJ6vVahi77+H/OFUDAAAAAAAA8i8abgEAAAAAAAAAyMKTDz+sQiEhhviq9es1bNRoxcSec7j/qnXr9MqIt3T5coLTNQx6/DGFFipkiMfFX9LLb47QnEWLlJmZ6TBH/KVLGvnhh1qwZInp+ODBzygwMNBhjocHDFAhkzouxcdr1LDhWrJgoaw3qOPypUv6dNwHWr7YvI5+jzykgIAAhzkc2bJho95/Z6TOx8Y63G77lq16d+gwnTf5/Wvdob2q16rldA0AAAAAAADIn/w8XQAAAAAAAAAAAN6qTOlSemHwII0Z/7FhbOv27Xr82WfVo0tndWzTRuUjIhQSEqILFy5q5949Wrxsubbu2GG3T0BAgNLS0nJUQ8nixfXac8/qnfc/MKzGmpKaqglfTtbMufPUtWMHNWnYUKVLllRooUJKSEzUkWPHtH7TZi1atkyXE8ybfnv27K1evfrcsI5iJYpr0IvP6+P3xsl2XR2pqan65vMvtXDOXLXr1FH1GzVS8ZIlVSi0kBITEnXiWJS2btyslUuWKjGLOtp16qj2nW/N1jVxZNf2HXp58DPq1LWLWrRto4jy5RQcEqK4ixd1aP8BrVq2XNs2bTbdt1Tp0hrw9CCXawAAAAAAAED+Q8MtAAAAAAAAAAAO3Nqunc5Gx+ibn34yjCWnpGjm3HmaOXfeDfMULhyqu3r11rTp03NcQ5sWLfTC4EGa8OVkQ9OtJB07cUJTf/xJU3801uhIkyZN9frrb2R7+6YtW+iJIYM19fMvDU23knTq+An99sOP+u2HH3NUR+369TRgyNM52uda5SqU16kTJ6/+nJqSooVz5mrhnLnZzhFWLFxDR76tEJNVfAEAAAAAAAAabgEAAAAAAADkmfLhEXqz9wueLgNuVD48wtMl5In/3HuPgoIC9dX3PygjIyPH+wcFBmrUsGE6fuqU0zX07tpVRQsX0QeffabEpCSn81zRoXVrjRw7XoGBgTna79bu3VS4SBFNnvCZkhITXa6jRds2GvLyi/L393c6x2333K0jBw9p8bz5Tu1fMTJSL74xVGUiyjpdAwAAAAAAAPI3Gm4BAAAAAAAA5Jkg/0BVK1XZ02UATrm7Tx/VqFZNn07+SoejorK9X9nSpfXWq6+qerWqLjXcSlLbli1Uveqn+ur777Vy7VrZbLYc5wgPC9PTjz+uW9u3U0BAgFN1NGvVUlWqVdVP336nDaudq6NoWJgefnKA2nRo71QN13t00FMqV7GCfvnuB6WmpGRrH/+AAPW4rY/uffABp68FAAAAAAAACgYabgEAAAAAAAAAyKY6NWtq8sfjtXzVas376y/t2L07y2bTksWL67YePXTP7bcpKIeryDpSqmQJjXj1FT1y4oTm/fWXVq9br+jYWIf7+Pn5qVb16urRubM6tW2T41VtzRQvWVLPv/6aTv3nhJYtXKyN69brXEyMw318/fx0S43q6ti1i1q1a6sAN14Xi8Wibr17qUGTxvrfL79p/eo1Sk9LM902vFgxte3UUd1v663iJUq4rQYAAAAAAADkXzTcAgAAAAAAAACQAz4+Purcob06d2ivywkJ2nvggM6cjVZCYqJ8fX1VLDxMVSMjVSUyUhaLxW7f3l27qnfXrm6po1KFCnr68cf19OOPKyb2nI4eP6aY2FglJCYqM9OqkJBgFS4UqoiyZXRL1aoK8Pd3y3GvV65CBT385AA9/OQAnY+N1YljxxUbE6Oka+ooFBqq0mXLqnK1qvLPpTquKF2mjJ5+6QU9/vQgHdi7T6dPnlRSYpICAwNVNKyoKlerpojy5Qy/NwAAAAAAAIAjNNwCAAAAAAAAAOCkwqGhat64safLUKmSJVSqpOdXai1esqSKlyzp6TIkSUFBQarfqKHqN2ro6VIAAAAAAACQD9BwCwAAAAAAACBXZGRkerqEHDNb8NJqtSntJjwXT0pOSfd0CQAAAAAAAADgVjTcAgAAAAAAAMgVCQmpni4hx3x9fQxNtympGTode8kzBQEAAAAAAAAAvIKPpwsAAAAAAAAAAAAAAAAAAAAAvBkNtwAAAAAAAAAAAAAAAAAAAIADNNwCAAAAAAAAAAAAAAAAAAAADtBwCwAAAAAAAAAAAAAAAAAAADhAwy0AAAAAAAAAAAAAAAAAAADgAA23AAAAAAAAAAAAAAAAAAAAgAM03AIAAAAAAAAAAAAAAAAAAAAO0HALAAAAAAAAAAAAAAAAAAAAOODn6QIAAAAAAAAAAACy69e5szxdAgAAAAAAAAogVrgFAAAAAAAAAAAAAAAAAAAAHKDhFgAAAAAAAAAAAAAAAAAAAHCAhlsAAAAAAAAAAAAAAAAAAADAARpuAQAAAAAAAAAAAAAAAAAAAAdouAUAAAAAAAAAAAAAAAAAAAAcoOEWAAAAAAAAAAAAAAAAAAAAcICGWwAAAAAAAAAAAAAAAAAAAMABGm4BAAAAAAAAAAAAAAAAAAAAB2i4BQAAAAAAAAAAAAAAAAAAAByg4RYAAAAAAAAAAAAAAAAAAABwgIZbAAAAAAAAAAAAeAmbpwsAAAAAAAAwRcMtAAAAAAAAUABZLBZDzGajyQkA4Fn8XQQAAAAAALwVDbcAAAAAAABAAeTj83+3Bq/03lqtVg9VAwDAv678XXTtcyEWH77OAgAAAAAAnscdCgAAAAAAAKAA8vX1NcSs1kxWFgQAeIzVapXVmmmIWyx8nQUAAAAAADyPOxQAAAAAAABAAeTv73/11/+3iqBNaWkpHqkHAIC0tBRdee7DfoVb40MiAAAAAAAAeY2GWwAAAAAAAKAAKlSo0NVfW67pakpNpeEWAOAZKSlJV3997d9Nvv6BnigHAAAAAADADg23AAAAAAAAQAEUHBwsH58rtwf/r6kpJSVRNpvVM0UBAAqszMyMqw23Fsv/Ndz6+fnJx8fPk6UBAAAAAABIouEWAAAAAAAAKJAsFsvVVW7/bWz6N261Zury5TjZrrzTGwCAXGazWRUff17Sv3/3XLO4rQoXLmy32i0AAAAAAICn0HALAAAAAAAAFFBFihS5+utrm5mSki7r8uWLrHQLAMh1GRlpunAhRmlpKVdjFsv/fX117d9VAAAAAAAAnsQ7eAAAAAAAAIACqnDhwgoODlZycrKuXzwwKemyUlKSFBRUSIGBQQoICGKFQQCAy2w2mzIy0pWWlqK0tBSlpibbjfv4WK7+nVS0aFGFhIRISsz7QgEAAAAAAK5Dwy0AAAAAAABQQFksFpUpU0ZHjx41HbdaM5WUdElJSZckWeTj4yMfH9983Xhrdm6ZVquUke6BagqWc+dSJNlff5s1U5mpaZ4pqIBIjD2j6697pjVTtvRUzxRUgFzMiJWvj68hbrPZPFBN7rPZbLJarbLZrA5XUL8yDfv6+qp06dJ5VB0AAAAAAMCN0XALAAAAAAAAFGBBQUGKiIjQqVOnb7ClTVZrpqzWzDypy3OMDbc2m02y5ffz9ry0NJsM199mldVBYx5cZ01LlaHR2WaVMrnuuS1Dacq0+JiM5M+G25zw9fVVhQoV5OtrbEgGAAAAAADwFLM7OQAAAAAAAAAKkKJFi6pMmTKeLgMAAPn5+atSpUoKDg72dCkAAAAAAAB2WOEWAAAAAAAAgAoXLqLixa26dOmC0gvwq+R9fS2yWOxX+0xJy9DZ85c9VFHBERlRTD4+9tc+My1dGXHRHqqoYAguXVGW61ZZTU1PV3RCjIcqKjgiipZWoJ/91zQ2m02ZmQV1hVuLChUqrIiIMgoMDPB0MQAAAAAAAAY03AIAAAAAAACQJPn7B6hYsdJKSUlSamqSUlNT/n21PAAAucTPL0CBgUEKDi4sPz8/+fr6erokAAAAAAAAUzTcAgAAAAAAALjKYrEoOLiQgoMLyWazKT09TWlpycrMzJTVapXVapWUf1dfDAjwM6yymp6ZonPxBXfV37xSo0qQ/HztV1pNt2UoNeGChyoqGIIrVZfFx77BMV0ZOp8S55mCCpDyJSIUHBxsF7NabUpLy/BQRbnNIh8fiywWH/n4+CogIFABAUHy8fG58a4AAAAAAABegIZbAAAAAAAAAKYsFsv/b4gK9HQpeSYsLET+/vbNhwePxWrRxi0eqqjguO+29goO9LeLJZw6rEt7l3uoooKhYvd75BsQZBc7FHNUS9ev9VBFBUerBs0VWSrSLpaenqm4uCTPFAQAAAAAAACHeGwYAAAAAAAAAAAAAAAAAAAAcICGWwAAAAAAAAAAAAAAAAAAAMABGm4BAAAAAAAAAAAAAAAAAAAAB2i4BQAAAAAAAAAAAAAAAAAAAByg4RYAAAAAAAAAAAAAAAAAAABwwM/TBcB7paamavHixdq4caO2b9+u8+fPKz4+Xn5+fipatKiqVKmixo0bq2vXrqpZs6any83S5cuXtXDhQm3atEm7d+/WhQsXdOnSJQUEBCg8PFzVqlVTs2bN1L17d1WsWNHT5QIAAAAAAAAAAAAAAAAAAC9Dwy0M0tPT9fXXX2vatGm6ePGi6XhycrLOnj2rtWvXatKkSWrRooVee+011a1b1wMVm0tMTNTEiRM1ffp0JSUlGcYzMjKUlJSkU6dOaeXKlRo/frw6d+6s1157TZUqVfJAxQAAAAAAAAAAAAAAAAAAwBv5eLoAeJeoqCj17dtXn376qWmzbVY2bNig++67T5MmTZLNZsvFCrNnx44duv322/Xdd9+ZNtuasdlsWrJkiW677Tb997//zeUKAQAAAAAAAAAAAAAAAADAzYKGW1x1+PBhPfTQQ9q7d69T+2dmZmrixIkaPny4R5tuN2/erP79++vkyZNO7Z+amqoRI0Zo0qRJbq4MAAAAAAAAAAAAAAAAAADcjPw8XQC8w4ULF/T4448rNjbWdLxChQpq1KiRSpcurfj4eB0+fFhbt241baz93//+p7Jly+rZZ5/N7bINoqKi9NRTT2W5qm316tVVt25dlShRQufOndP+/fu1e/du020nTpyoiIgI3X333blZMgAAAAAAAAAAAAAAAAAA8HI03EKSNHToUJ09e9YQL1++vN599121adNGFovFbiwqKkrjx4/X4sWLDft98cUXat26tZo0aZJrNV8vLS1NL7zwghITEw1jtWrV0siRI1W/fn3D2J49e/Tee+9p06ZNhrFRo0apSZMmqlSpUq7UDAAAAAAAAAAAAAAAAAAAvJ+PpwuA5/31119auXKlId6wYUPNmjVLbdu2NTTbSlJkZKQmTpyol19+2TBmtVo1atQoWa3WXKnZzC+//KK9e/ca4l26dNF///tf02ZbSapdu7a+//57Pfjgg4axpKQkjR071u21AgAAAAAAAAAAAAAAAACAmwcNtwVcZmamxo8fb4iXLVtWX3zxhUJDQ2+Y46mnnlL//v0N8b1792rhwoVuqfNGEhIS9OWXXxritWvX1kcffaSAgACH+/v5+WnEiBHq1q2bYWz58uXavn2722oFAAAAAAAAAAAAAAAAAAA3FxpuC7iVK1fq6NGjhvjQoUNVvHjxbOcZOnSoateubYh/8803LtWXXTNnzlRcXJxdzGKx6N1331VwcHC2clgsFo0bN06lSpUyjOXVeQAAAAAAAAAAAAAAAAAAAO9Dw20B9/vvvxtikZGR6t69e47y+Pj46LnnnjPEd+3apf379ztdX3aZnUerVq1Uv379HOUpVKiQBg4caIgvW7ZMFy9edLo+AAAAAAAAAAAAAAAAAABw86LhtgBLTEzUqlWrDPG7775bFoslx/k6dOhgujrs3Llznaovu44cOWLa1HvPPfc4le/OO++Uv7+/XSw9PV2LFi1yKh8AAAAAAAAAAAAAAAAAALi50XBbgG3cuFHp6emG+K233upUPh8fH3Xs2NEQX7lypVP5smvt2rWGmJ+fn9q3b+9UvtDQUDVv3twQz+3zAAAAAAAAAAAAAAAAAAAA3omG2wJs3bp1hlhYWJhuueUWp3M2adLEENu/f78uXLjgdM4bMTuPGjVqqEiRIk7nNDuPDRs2yGq1Op0TAAAAAAAAAAAAAAAAAADcnGi4LcD27t1riNWrV8+lnFntv3PnTpfyOpJX55GYmKijR4+6lBcAAAAAAAAAAAAAAAAAANx8aLgtwA4dOmSIVa1a1aWcFStWlK+vryG+f/9+l/JmJSkpSadPnzbEXT2PypUrm8b37dvnUl4AAAAAAAAAAAAAAAAAAHDzoeG2gLp48aIuXLhgiEdGRrqU19/fX2XLljXET5w44VLerBw5ckQ2m80Qd/U8IiIi5OfnZ4jn1nkAAAAAAAAAAAAAAAAAAADvRcNtARUdHW0aL126tMu5S5YsaYidOnXK5bxmcus8fH19FR4ebojn1nkAAAAAAAAAAAAAAAAAAADvRcNtAXX+/HnTeIkSJVzObZYjq+O5Kqu8Zk2/OWWWI7fOAwAAAAAAAAAAAAAAAAAAeC8abguoc+fOmcaLFi3qcu7ChQsbYpcvX3Y5r5mszqNIkSIu5w4NDTXEcus8AAAAAAAAAAAAAAAAAACA97LYbDabp4tA3vv55581cuRIQ3zNmjUur3I7atQo/fTTT3axwoULa/PmzS7lNfPBBx/om2++sYsFBgZqx44dLuceNGiQli9fbherWbOmZs2a5XLum4WnpgeLxWKog4kq91lkfu2R+7junsO1zxpzcd5jHvYsrr1ncN2zxjyc95iHPYtr7xlc96wxD3sGc7HncN09h2tvjnnYM5iHPYfr7jlc+6wxF+c95mHP4tp7Btc9a8zDeY952LO49p7hbdf9+nq8kZ+nC4BnpKenm8aDgoJczm2WIzU11eW8ZtLS0gyx4OBgt+TOy/PwVt4yiVksFnlHJQWPt/wZKGi47p7Dtc8ac7Fn8GfSc7j2nsF1zxrzsGfwZ9JzuPaewXXPGvOw5/Dn0jO47p7DtTfHPOw5/Jn0DK6753Dts8Zc7Bn8mfQcrr1ncN2zxjzsGfyZ9ByuvWdw3W/Mx9MFwDPMGlUlyc/P9R5sX19fQywjI8PlvGbMzsPs+M7Iy/MAAAAAAAAAAAAAAAAAAADei4bbAiqrFW7d0axq1rRrtVpltVpdzn09swZYdzXcmp0HDbcAAAAAAAAAAAAAAAAAABQ8NNwWUFkt/2yz2VzOnZmZaXo8Hx/3/3EzOw93nINkfh7uauYFAAAAAAAAAAAAAAAAAAA3DxpuCyiz1Vsl8ybTnDLLERAQ4HJeM2bn4Y5zyCpPbp0HAAAAAAAAAAAAAAAAAADwXjTcFlBZNY5mZGS4nNssR241qprldcc5ZJWHhlsAAAAAAAAAAAAAAAAAAAoeGm4LqNDQUNN4UlKSy7nNcgQFBbmc14zZebjjHLLKk1vnAQAAAAAAAAAAAAAAAAAAvBcNtwVUWFiYaTwhIcHl3ImJiYZYeHi4y3nNFC1a1BDLyMhQamqqy7nz8jwAAAAAAAAAAAAAAAAAAID3ouG2gMqq4TYuLs7l3BcvXjTEihUr5nJeM1k1wN5s5wEAAAAAAAAAAAAAAAAAALwXDbcFVLly5Uzj586dczm3WY4SJUq4nNdMVucRGxvrcu68PA8AAAAAAAAAAAAAAAAAAOC9aLgtoEqXLi1/f39D/MyZMy7nPn36tCFWsWJFl/OaKV++vGnc1fO4dOmSEhISDPHcOg8AAAAAAAAAAAAAAAAAAOC9aLgtoHx8fFSpUiVDPCoqyqW8Fy5c0OXLlw3xatWquZQ3K6VLl1ZISIgh7up5HDt2zDSeW+cBAAAAAAAAAAAAAAAAAAC8Fw23BVidOnUMsf3797uUc9++fabx3GpUtVgsql27tiGeG+dhsVhouAUAAAAAAAAAAAAAAAAAoACi4bYAq1+/viG2d+9epaenO51z+/bthliRIkVytVHV7Dx27tzpUk6z87jlllsUGhrqUl4AAAAAAAAAAAAAAAAAAHDzoeG2AGvVqpUhlpycrK1btzqdc/Xq1YZYixYt5Ovr63TOGzE7j6ioKJ04ccLpnGvWrDHEWrdu7XQ+AAAAAAAAAAAAAAAAAABw86LhtgCrWrWqypUrZ4jPmzfPqXxnzpzRtm3bDPH27ds7lS+7mjdvrqCgIEN8wYIFTuXbtm2bTp8+bYjn9nkAAAAAAAAAAAAAAAAAAADvRMNtAXfbbbcZYnPmzNGFCxdynGvatGnKzMy0iwUHB6tXr15O15cdQUFB6tq1qyH+66+/Ki0tLcf5vvvuO0MsIiLCdCVdAAAAAAAAAAAAAAAAAACQ/9FwW8Dde++98vGx/2OQlJSkMWPG5CjPnj179OOPPxrivXr1UmhoqEs1Zsd9991niJ0+fVoTJ07MUZ4VK1Zo0aJFhrjZdQIAAAAAAAAAAAAAAAAAAAUDHYQFXIUKFdSnTx9DfO7cuZo8eXK2csTGxmrw4MFKT0+3i/v7+2vw4MFuqfNGmjdvriZNmhjiU6ZM0ezZs7OV49ChQ3r55ZcN8bCwMD3yyCMu1wgAAAAAAAAAAAAAAAAAAG5ONNxCzz33nIKDgw3xTz75RGPHjjU00l5r165d6tevn86ePWsYe/DBB1WhQoVs1TBx4kTVqFHD8N/QoUOzfR6vvPKK6Sq0w4YN09SpU2Wz2bLcd9WqVXrwwQeVkJBgGBsyZIgKFy6c7ToAAAAAAAAAAAAAAAAAAED+QsMtVKFCBdOVXSXp+++/12233aYff/xRhw8fVnJyss6dO6cNGzbo9ddf1/33369Tp04Z9qtevbpefPHF3C7dTuPGjdW/f39DPCMjQx9++KHuuece/fHHHzp27JhSU1MVHR2tlStXasiQIXrqqacUFxdn2Ld169Z66KGH8qB6AAAAAAAAAAAAAAAAAADgrfw8XQC8w8MPP6zdu3dr5syZhrGjR49q9OjR2c4VHh6uCRMmKCgoyJ0lZsvLL7+sffv2ad26dYax3bt3a9iwYdnOVbFiRX344Yemq+YCAAAAAAAAAAAAAAAAAICCg05CXDV69GjdfffdLuUoUaKEpk2bpqpVq7qpqpzx9/fXF198oXbt2rmUJzIyUj/99JNKlCjhpsoAAAAAAAAAAAAAAAAAAMDNioZbXOXn56exY8dq5MiRKly4cI7379Spk2bNmqXq1avnQnXZFxISoq+++krPP/+8AgMDc7z/Pffco//9738qXbp0LlQHAAAAAAAAAAAAAAAAAABuNn6eLgDep1+/furevbt++uknzZw5UydPnsxy24CAALVr106PPPKIWrZsmYdVOubr66unn35ad999t3744QfNmTNHsbGxWW4fEhKirl276pFHHlHdunXzsFIAAAAAAAAAAAAAAAAAAODtLDabzebpIuDdjh8/rn379un06dNKSkpSUFCQihYtqsjISNWrV08BAQGeLjFbDh06pAMHDig6OlrJyckKCQlRWFiYqlatqtq1a8vX19fTJQIAAAAAAAAAAAAAAAAAAC9Ewy0AAAAAAAAAAAAAAAAAAADggI+nCwAAAAAAAAAAAAAAAAAAAAC8GQ23AAAAAAAAAAAAAAAAAAAAgAM03AIAAAAAAAAAAAAAAAAAAAAO0HALAAAAAAAAAAAAAAAAAAAAOEDDLQAAAAAAAAAAAAAAAAAAAOAADbcAAAAAAAAAAAAAAAAAAACAAzTcAgAAAAAAAAAAAAAAAAAAAA7QcAsAAAAAAAAAAAAAAAAAAAA4QMMtAAAAAAAAAAAAAAAAAAAA4AANtwAAAAAAAAAAAAAAAAAAAIADNNwCAAAAAAAAAAAAAAAAAAAADtBwCwAAAAAAAAAAAAAAAAAAADhAwy0AAAAAAAAAAAAAAAAAAADgAA23AAAAAAAAAAAAAAAAAAAAgAM03AIAAAAAAAAAAAAAAAAAAAAO0HALAAAAAAAAAAAAAAAAAAAAOEDDLQAAAAAAAAAAAAAAAAAAAOAADbcAAAAAAAAAAAAAAAAAAACAAzTcAgAAAAAAAAAAAAAAAAAAAA7QcAsAAAAAAAAAAAAAAAAAAAA4QMMtAAAAAAAAAAAAAAAAAAAA4AANtwAAAAAAAAAAAAAAAAAAAIADNNwCAAAAAAAAAAAAAAAAAAAADtBwCwAAAAAAAAAAAAAAAAAAADhAwy0AAAAAAAAAAAAAAAAAAADgAA23AAAAAAAAAAAAAAAAAAAAgAM03AIAAAAAAAAAAAAAAAAAAAAO0HALAAAAAAAAAAAAAAAAAAAAOODn6QIAAK75448/NGzYMEP8lltu0R9//KGAgAC3H/P333/Xm2++aYgvXbpU5cuXd/vx0tPT1adPH0VFRUmSypUrp2XLlrn9ON4kIyNDK1as0Lp167Rt2zbFxMQoPj5eFotFRYoUUeXKlVW/fn116dJFjRo18nS5QIFWEObhK7788ktNmDDBLjZt2jS1aNHCbcfwlvnPW+oAgLySlpamFStWaOPGjdq2bZvOnTunuLg4Wa1WFSlSRBUrVlSdOnXUsWNHtWzZUn5+3FICYC+/fS6Ojo7Wnj17dPHiRcXHxystLU1FixZVWFiYypUrp9q1a8vX19elYwCAu0ycOFGTJk1yuM2ECRPUs2fPXK3jpZde0rx587Icv9E9hKFDh2rmzJkOj/H777+rfv36TteYHf369dM///yT5Xh2/p7J6lz69u2r0aNHu1qiqVdeeUVz5syxixWEe+kA3GPDhg3auHGjW3JZLBb5+voqICBAQUFBCg8PV/HixVWxYkWVLVvWLcfI6u++Nm3a6Ntvv3XLMa73ySefaPLkyYb4/v37c+V4AHJfVnPfXXfdlavft13r5MmTN/wMnBM+Pj4KDAxUQECAihQpohIlSqhs2bKKjIzkPgbgJnw7AgD51MGDB/Xll1/q+eef93QpLvvhhx+uNtu6qkaNGm7Jk11jx47V3Xffne3tbTabfvnlF3399dc6c+aM6TaxsbGKjY3Vxo0bNXXqVNWpU0cvv/yy2rRp466yAbhBfpqHJWnfvn36/PPPcy2/t8x/3lIHgJvXhg0b9Mgjj7iU48oXU35+fgoODlaRIkWufjFVs2ZNNWrUSPXr15ePj+svLkpJSdF3332nn3/+WbGxsabbnDt3TufOndPWrVv1448/qkyZMurfv78eeuihXGmgA5C/3Eyfiw8ePKgff/xRq1ev1qlTpxxuGxoaqmbNmunee+9V586dZbFY8qhKAHDO4sWLc7XhNiUlRcuXL8+1/FcsXrw4VxtuT58+re3bt+da/t9//129e/dWq1atcu0YALzTyZMn1blzZ48d/5lnntGzzz6b5fjGjRtv+PCGOxQtWlSNGzdW586d1adPHwUHB7s1/5o1azRjxgzde++9bs0LIH/Kau5r3rx5njXcnjp1Kk/m38DAQNWpU0edOnXSHXfcodKlSzuVx11/n/n5+cnX11fBwcEKDQ1ViRIlVL58eVWvXl0NGzZUkyZNWPQBXsv1b2YAAF7r66+/1r59+zxdhkv27dunTz/91NNl5Ilz586pf//+GjlyZJZNXmZ2796txx9/XG+99ZbS09NzsUIAOZUf5mHp35XGhw4dmmtzjLfMf95SBwDYbDZlZGQoJSVFFy9e1LFjx7R161b9+eefGjdunPr166f27dtrwoQJunDhgtPH2bNnj+6++25NmDAhy2ZbM2fPntX777+v2267TTt27HD6+AAKDm//XHz69Gk99dRT6tOnj6ZPn37DZltJSkhI0PLlyzVkyBDdfvvtWrt2bR5UCgDOW7FihVJTU3Mt/8qVK5WUlJRr+a9YvHhxruZfuHChbDZbrh7jzTffzJNrBQDeKD4+XsuXL9ebb76pW2+9VdOnT3f7Md5//33FxMS4PS8A3MxSU1O1detWjR8/Xl26dNF7772nlJQUj9WTkZGh1NRUxcXF6eTJk/rnn380d+5cffzxx3rkkUfUqlUrjR49WqdPn/ZYjUBWaLgFgHwsPT1db7zxhjIzMz1dilPOnz+vIUOGKC0tzdOl5LqYmBg9/PDD2rBhg9M5pk+frkGDBhWI6wXcLG72efiKzz//XHv37s2V3N4y/3lLHQCQXbGxsfryyy/Vs2dPzZ49O8f7r1+/Xg888IAOHz7sdA1RUVF68MEHtXDhQqdzACgYvPlz8aJFi3THHXdo5cqVTuc4cOCABgwYoC+++CLXm7QAwFlJSUlatWpVruVfsGBBruW+1rFjx3L1IY68OI+TJ0/qk08+yfXjAIC3u3Dhgt566y299NJLysjIcFveS5cu6Z133nFbPgDIb9LS0vTDDz+ob9++OVqIIS9dunRJP/74o3r27Klvv/3W0+UAdmi4BYB8bvfu3frmm288XUaOJSQkaODAgTp58qSnS8l1qampeuqpp3TkyBHT8ZIlS6p3794aMGCAHnjgAbVu3TrL1yesXr1ab731Vm6WCyCHbtZ5+IqdO3fq66+/zpXc3jL/eUsdAOCMuLg4vfrqqzl67diOHTs0ePDgLFcwKFGihHr06KHHHntMTz75pO68805VrFjRdNu0tDS98sorWrdunVP1Ayg4vPFz8YIFC/TCCy/o0qVLLueyWq369NNPNWbMGDdUBgC5Y9GiRbmSNzk52aUHF3Iqtx74OnXqVJ69weGnn37S1q1b8+RYAODt5s2bp2HDhrk159KlSzVv3jy35gSA/ObAgQN69NFHlZCQ4OlSspSSkqL3339fw4YN4yFneA3zb4kBAPnKpEmT1KVLF1WpUsXTpWTLpUuXNGDAAO3cudPTpbikTJkyatu27Q23GzdunOnKkeHh4XrrrbfUvXt3+fr62o1FR0fr888/N33VzsyZM9WuXTv17t3b+eIBuNXNNg9fkZaWptdff92tqwtcy1vmP2+pAwBcMXHiRJUoUUL333+/w+1SU1P16quvmr7CtmzZsnr11VfVs2dP+fgYn9HeuHGjxo4dqz179tjF09PT9dprr2nOnDkKCwtz6TwA5G/e9Ll4y5Yteu2112S1Wg1jFotF3bp1U+fOndWoUSMVK1ZM/v7+unDhgnbv3q1ly5Zp9uzZSk9PN+z7448/KjIyUg899FBenAYA5Mjy5cuVlpamgIAAt+ZdsWKF6efL3LJ48WK98MILbs+bV6v0Sv8+qDF8+HDNmjXL7b8fAOBO06ZNU4sWLXK8X1pamhISEnT+/Hnt3r1bGzZs0Pz587N8+Hf27Nlq2bKl7rnnHldLvmr06NFq1aqVihUr5racAJBXnnnmGT377LM53i8zM1OXL1/WpUuXtH//fm3btk2zZ8/OciXbQ4cOadSoUXr//fddLTlX/fHHHypRooRefvllT5cC0HALAAVBamqqhg8frp9//tn0i3NvcurUKQ0aNEgHDhzIlfz79+93a774+Hjdc889OnHihF08JCREkydPVqlSpRzuv3v3bv3666+GeMWKFfXTTz+pdOnSpvuVLl1aI0eOVNOmTTV06FDDaznHjh2rTp06KSQkJIdnBCA33Ezz8LU++eQTl1417oi3zH/eUgeA/C+nN0gzMjKUmZmppKQkxcXF6cSJE1q3bp3++OMPxcXFme4zduxYtWrVSpUqVcoy7xdffKGoqChDvH79+poyZYrCw8Oz3Ld58+b67bff9OKLL2rp0qV2YzExMZoyZYpee+21bJ0fgILJWz4XZ2Zm6u2331ZaWpphrFq1avr4449Vo0YNw1jZsmVVtmxZdenSRUOGDNErr7xiujrhuHHj1KFDB1WoUCFX6geA7PLz87N7iPby5ctat26dOnTo4Nbj5Gaj6vXnIEmHDx/WoUOHVK1aNbceKy8bbiXpyJEjmjRpkl566aU8PS4A73LXXXdp3Lhxni7D7QICAlSsWDEVK1ZMt9xyi+68804NHTpU48aN0x9//GG6zwcffKDu3bsrNDTULTVcuHBBY8aM0fjx492SDwBuBr6+vgoLC1NYWJgqVqyorl276sUXX9TPP/+sjz76yPTh4T///FP9+vVT48aNnT5uTv8+y8zMvHr/+9KlSzp16pQ2bdqkGTNmKDo62nSfr7/+Wh06dFDTpk2drhNwh5vn234AgEu2bt2qn3/+2dNlOLRhwwb17ds315pt3S0zM1MvvfSSodlWkj788EPVqlXrhjk++ugjw6sPQkNDNWXKlCybvK51++236/XXXzfEY2Nj9dNPP91wfwB552aYh6+1ZcsWff/997mW31vmP2+pAwCu5+fnp8DAQIWHh6ty5cpq3769Xn/9dS1cuFCdO3c23SclJUUfffRRljmTkpL0yy+/GOKlSpXS5MmTHTbbXhEYGKiPP/5YVatWNYz9+uuvebqyGYCbkzd8Lp4+fboOHjxoiDdo0EDTp083bba9Xrly5TRt2jR16tTJMJaenq6PP/7YLbUCgCuaN29uiC1atMitx0hKStLff/9tF3PnWw+KFy9uujK6u8/jxIkT2rVrl10sL97e8M033xjeIAEA+VXRokU1duzYLB80iIuLM71v4Yq5c+dq2bJlbs0JADcbf39/Pfroo/rmm2/k7+9vus3nn3+epzX5+voqICDgamNwq1at9Nxzz2nRokXq16+f6T42m03vvfdentYJmKHhFgAKkI8//lgnT570dBkG6enp+uSTT/Too4/q/Pnzni4n2yZMmKDVq1cb4o888oi6dOlyw/0PHDigtWvXGuJPP/20KleunO06+vfvr1tvvdUQnzZtmukTagA8x1vn4eslJydr2LBhpq/XdQdvmf+8pQ4AyInw8HBNmjTJdN6RpCVLlpg+ECb9+3rGS5cuGeIvvfSSihcvnu0agoKCTF/hm5SUpBUrVmQ7D4CCy9Ofi80afsPCwjRx4sQcrabl7++v8ePHq2LFioaxBQsWZPm6RgDIKz169DDEli5dalgx1hUrVqxQcnKyXaxr165uyy+Zn4e7G27NVrd193mYycjI0BtvvOHW3xMA8HYDBw7Mco6dOXOm24/39ttv6/Lly27PCwA3mxYtWmT5hrK1a9cqJiYmjysyCg4O1siRI/Xggw+aju/evVsbNmzI46oAezTcAkA+FRwcbIglJSVpxIgRHqgma5s3b9Zdd92lyZMn51pjV25Yv369pk6daojXrl1br776arZy/Pe//zXEihQpogceeCDH9bz44ouGWGxsrFatWpXjXADc42aZh818+OGHOnbsmF3stttuc1t+b5n/vKUOAMgpHx8fjR071nTFLavVqtmzZ5vut2TJEkOsePHiTs3xHTt2VEhIiCFu9mp1AAWbt30uPnTokA4dOmSI9+/fP1tvOLheoUKFTD8L2mw2HkIA4HFdunSRn5+fXSwuLk4bN2502zGub1QNDg5Wx44d3ZZfMm+43b9/v+HehSsWLlxo93O5cuXUoEEDt+WX/n29uq+vryG+d+9eTZkyxa3HAgBv9+qrr8pisRjiR44ccenhPLN/f8TExOToNecAkJ/95z//Ufny5Q1xq9VqutiYpwwbNkyRkZGmY7NmzcrbYoDr0HALAPmU2YpT0r9PJv3+++95W4yJI0eO6IUXXtCDDz5o+hpHSapbt67+85//5HFlN3bp0iUNHTrU0CAcGBio8ePHKyAgIFt5rr+JK0k9e/Y0bVy4kerVq6thw4aG+Ny5c3OcC4B7ePs8nJV169YZXttVvXp1DRkyxG3H8Jb5z1vqAABnhIWF6aGHHjIdW7NmjSGWkZGhLVu2GOLdu3c3NGFkR0BAgOmN2ZthJXcAecvbPhcvXbrUELNYLLr//vudztmlSxfTz5CsuALA04oWLaqWLVsa4mb/HnZGYmKi/v77b7tYVg9muaJGjRqmb6Jx1yq3x48f1+7du+1iPXv2dEvua4WHh6t///6mY1988YXpAyEAkF9VqlRJtWvXNh3bs2eP03mz+vfHjBkzTN92BgAFjZ+fX5arjLsy/7qbv7+/Bg4caDpmdv8byEs03AJAPtWpUyf16dPHdOz9999XdHR0Hlf0f3bu3Kk+ffqYvqbrij59+uinn35SsWLF8rCy7Bk1apTOnDljiD/zzDOqUqVKtnIcOHDA9NWSWb0aODs6depkiK1evVqZmZlO5wTgPG+eh7OSkJCg4cOHy2azXY35+flp7Nix8vf3d8sxvGX+85Y6AMAV3bt3N43v37/fEEtJSdFTTz2lXr166ZZbbrk6rzdq1Mjp45s1UiQmJjqdD0D+5G2fi83myFtuucWl+w8BAQGqU6eOIe6Nn/kBFDxmq8MuWbLELW8bW758uVJSUuxivXr1cjmvGbPPvu5quDW7T51b5/H888+rUqVKhnh6erreeOONm+otcADgKrPP0JJrn6MffPBBNWnSxHTszTffVFJSktO5ASC/yI35Nzd069ZNPj7G1sazZ88qPj7eAxUB/6LhFgDyseHDh5t+YXT58mW98847eV/Q/5eUlJRl81FoaKjGjh2r8ePHm772xdPWrl1r+oreWrVq6fHHH892nnXr1hliFosly5sA2dG0aVNDLD4+3queRAMKGm+dh7MyduxYnTp1yi725JNPqm7dum47hrfMf95SBwC4olq1aqZvV0hISNDly5ftYqGhoRo8eLA++eQTzZ07V9u2bdPs2bNNHxbILrMHFwoXLux0PgD5lzd9Lj58+LAhlt2HZx0xO7/z58+7nFf697WOu3fv1syZMzV16lRNnjxZv/zyi5YvX65z58655Rg3cvbsWS1fvly//PKLpkyZoqlTp+r333/XsmXLTP8+AOA9unTpYnijwfnz57V582aXc1/fqBoSEqIOHTq4nNeM2Yqzu3btcssbFq4/j4oVK2bZhOCqoKAgjR492vQ16tu3b9cPP/yQK8cFAG+U1UNvCQkJTue0WCwaM2aMAgMDDWOnTp3Sxx9/7HRuAMgvcmP+zQ2hoaGqUKGC6ZjZAmlAXsn5OwMBADeNYsWKacSIEXrxxRcNY8uWLdPcuXOzXGXGE7p3767hw4erdOnSni7FVGpqqukXgT4+PhozZkyOXsW7b98+Q6xy5couNSjUqVNHPj4+hlUQdu7cqXr16jmdF4DzbqZ5eOXKlZoxY4ZdrHr16nr66afdehxvmf+8pQ4AcIWPj4+KFi1q2uiUlJTkcE7z9/dXjRo1nD52dHS06U3NqlWrOp0TQP7lTZ+LH3zwQZ04cULR0dFX/ytbtqzLec1W+L72zRFmbr31VsMDb7t37756fyEhIUHff/+9fvvtN4dNrXXq1NE999yje++917SxwFknT57U9OnTtXjxYkVFRTnctkKFCrrrrrvUt29flSpVym01AHBdeHi4WrRoYXjt6uLFi9W8eXOn8yYkJGjVqlV2sVtvvdWt89C1atasqcjISMN89Ndff+mxxx5zOm9UVJT27t1rF8ut1W2vaN68ufr166fffvvNMPbpp5+qc+fOqlixYq7WAADeIKvVZosUKeJS3sqVK+uZZ57R+PHjDWM///yzevbs6dLCCwBws0tOTjaNe+NiCsWKFdOxY8cMcd60Bk9ihVsAyOd69eqlzp07m46NHj1aFy5cyOOKjOrXr6+ff/5Zn332mdc220rS119/bfphrm/fvjle8eDgwYOGmKvNCcHBwabXz6ypDEDeuRnm4fj4eL355pt2MT8/P40bN8505URXeMv85y11AICr0tLSTOOFChXK1ePOmDHD9HW3rVu3ztXjArh5ecvn4vvuu08vv/yyPvjgA/3www9auHChhg4d6nLeo0ePGmLFixd3Ot/27dt1++23a+LEiTdcQXb37t0aOXKkevbsqSVLljh9zCsuXLigYcOGqXv37poyZcoNm20l6cSJE/rss8/UpUsXTZkyJcs3GwHwjB49ehhiixcvvuGDAY4sW7ZMqampdrHcblQ1O49Fixa5lPP61W0l89V03e3VV181feAjOTlZw4cPd+n3BgBuFmb3aCXXPkdfMWDAANPv7qxWq4YPH274OwwACpIDBw6YxkuUKJHHldxYVvN1bt//Bhyh4RYACoB33nnH9GnQixcvavTo0R6o6F/16tXThAkT9N///tf0NdzeJCYmRt98840hXrhwYT3//PM5zmf2CsvIyEhnSrNjtvKBO16rBsA13joPXzFq1CjFxMTYxZ566qlceX2it8x/3lIHALgiLS1Nly5dMsQDAgIUGhqaa8fdt2+fJk+ebIhHRkaqZcuWuXZcADc/b/9c7KzDhw8bVqqVpCpVqjiVb82aNXrkkUdMczpy6tQpDRkyxHQlr+xavXq1evfurT/++EMZGRk53j81NVXjx4/XY489luWKZQDyXpcuXQxv54qOjtY///zjdM7rG1ULFy6sdu3aOZ0vO8wabv/55x9FR0c7nfP686hSpYpq1qzpdL7sCg0N1bvvvms6tnHjRk2fPj3XawAAT4qLi9PmzZtNx+rXr+9yfl9fX7333nvy9/c3jB09elSTJk1y+RgAcLPK6mFdd8y/7nbu3DnTuDsezgCcRcMtABQApUqV0uuvv246Nm/ePC1dujRP6ylXrpx+/vlnzZgxQz179pTFYsnT4ztjwoQJpl8UPf300zn+MJeQkGCayx2r+5YsWdIQy+kXdADcz9vm4Wv99ddfmjNnjl2sRo0aevrpp91+LG+Z/7ylDgBw1datW01XvoqIiMi1Yy5btkwPPfSQ6cq6r7766k3x2R6A53jz52JX/Prrr6ZxZ17Vvm/fPj3zzDNKSUlxup4pU6Zo3LhxOd5v0aJFGjRokFtWG96wYYMGDBiQ5UrsAPJWsWLFTOckZ1eHTUhI0OrVq+1iXbp0cftbcq5Xq1YtVapUyS5ms9m0ePFip/IdPXpU+/fvt4vl9iq91+rQoYPuuOMO07EPP/xQZ8+ezbNaACCvff/990pPTzfEq1Sp4rb7GjVr1tSTTz5pOvbtt99q9+7dbjkOANxM1q1bZzr/WSwWr3t72cmTJw0LBkn/vnEyPDzcAxUB/6LhFgAKiHvvvVdt2rQxHXvnnXdMV8bKLeXLl/f6FW2vdeDAAc2cOdMQL1eunB566KEc58vqKSx3vKLBLMf58+ddzgvAdd40D19x4cIFvf3223YxPz8/jRs3zvTJf1d5y/znLXUAgKvMXn8rye2rzFqtVq1evVpPPPGEBg8erMuXLxu2efDBB9WlSxe3HhdA/uSNn4tdERsbqxkzZhjigYGB6tChQ47zvfjii4aHw+rWrauRI0dq8eLF2rFjh9asWaMffvhB/fr1U2BgoGme7777Tv/73/+yfdzdu3frlVdeMW16kP79u+Xdd9/V/PnztWnTJu3cuVNLlizR2LFjVbduXdN9tm7dqgkTJmS7BgC5y2x1WGcbVZcsWWJoqO/Zs6dTuXLKnecxf/58QyyvzuOKN954w/ReQkJCgt566608rQUA8srOnTs1depU07EHHnjArccaPHiwbrnlFkM8IyNDb7zxRpaffwEgP4qPj9eIESNMx7p06aJSpUrlcUWOmX1el6SmTZsa3uAB5CUabgGgABk5cqRCQkIM8ZiYGKdWPikoJk2aJKvVaogPHjzYqVUbsmr0Klq0aI5zXa9w4cKGWEJCgmn9APKet83D77zzjqEZdODAgapdu3auHM9b5j9vqQMAXHH06NEsG6natm3rUu6lS5fqk08+0ahRo/Tkk0+qVatWGjBggFatWmW6/f33368333zTpWMCKFi87XOxK0aNGqXk5GRDvFevXqafDW/k+PHjV38dEBCg4cOHa8aMGerXr58qVaqkwMBAlShRQi1bttTIkSP1559/Ztnw+t5772XrNetpaWl66aWXTFejLVu2rKZOnaoffvhB999/v6pWraoiRYooICBAFSpU0N13360ZM2Zo2LBh8vExft3w3Xffad++fTm4AgByS9euXeXr62sXO3XqlHbu3JnjXAsXLrT7uWjRonm2GpZZw+3mzZuz/Le+I9efR/Xq1VWtWjWna3NGWFhYlk0PK1eu1J9//pmn9QBAblu/fr2eeOIJ00bX0qVLq1+/fm49XkBAgMaMGWP6WXXfvn2aMmWKW48HAN7q5MmTevTRR3XixAnDmI+Pj5555hkPVJW1ixcv6ptvvjEdc/X+N+AqGm4BoAApX768XnrpJdOx//3vf1qzZk0eV+T9Dh06ZLpCQsWKFXXXXXc5lTMxMdE0XqhQIafyXcvsC0ubzaaEhASXcwNwnTfNw3PmzDG8OrJmzZoaPHhwrh3TW+Y/b6kDAJx14cIFDR482PTLqXLlyqljx44u5Z83b54mT56sn376SX///bfi4uJMtytRooTGjx+vd9991/SLKwDIijd9LnbF3LlzTV/H7ufnp6eeesql3AEBAZo0aZIeeeQRWSyWLLerUqWKfvjhBzVu3NgwlpCQoC+//PKGx/rtt98UFRVliFeuXFm//vqr2rVr53B/i8WiRx99VMOHDzeMWa1WffvttzesAUDuK1asmJo1a2aI53R12MuXL2v16tV2sa5du+bKm3LM1K5dWxUrVrSLWa1WLVmyJEd5Dh8+rAMHDtjF8np12yt69Oihbt26mY6NHTvWqWZiADePmTNnqkaNGrn236233urpU1RmZqY2bdqkl19+WY8++qjpfQZfX1999NFHWb7BwRUNGjRQ//79Tce+/PJLHTx40O3HBABvcejQIX344Ye67bbbtGfPHtNtnnvuOdWsWTOPK8taSkqKhgwZYvr3RUhIiO644468Lwq4BusrA0AB89BDD2n+/PnaunWrYWzEiBGaO3euabNQQfXll1/KZrMZ4kOGDHH6NQVmK8ZIUlBQkFP5spMjNTXV5dwA3MMb5uGYmBiNHj3aLubv769x48bl6hdk3jL/eUsdAJBTNptNy5cv19tvv62YmBjTbZ588kmX5/LTp0/fcBt/f3898cQTatmypUvHAlBwecPnYlfs3r07yxUJH374YVWpUsWl/G+99ZY6dOiQrW1DQ0P1+eefq1evXrp48aLd2IwZM/TMM8+Yvq5c+rf54euvvzbEAwMD9fnnn6ts2bLZrvmhhx7S0qVLtXbtWrv4/PnzNWLECKdW/AXgXj169ND69evtYosXL9bLL7+c7RxLliwxPPjVq1cvt9SXXd27dzfMXYsWLdL999+f7RwLFiwwxDzVcCv9O+9v2LBB8fHxdvG4uDiNHDlSn332mYcqA1DQzZw5Uxs3bszRPhkZGUpJSdH58+d16tQp7d+/P8tFEKR/H+B688031bx5c1fLzdILL7ygpUuX2r1RQpLS09M1fPhw/fbbbzxMDMCrbNy4URMnTszRPpmZmUpNTVVcXJyio6O1f//+Gz681adPHw0aNMiVUt1q8+bNGjFihI4cOWI6/uCDDyo8PDyPqwLs0XALAAWMxWLRmDFjdOeddxqaf06dOqXx48dn+YVRQXPy5EnTG68RERHq06eP03mzavRytoH3Wte/Fu6KjIwMl3MDcA9vmIdHjBhheCp00KBBqlWrVq4e11vmP2+pAwAcSU1NVWJioi5evKiDBw9q9+7dWrBggekrv65o2LCh7r33XpePffbs2Rtuk56ernHjxmn8+PG655579Mwzz6hkyZIuHxtAweENn4uddfz4cQ0cOFBJSUmGsSpVquiFF15wKX/r1q3Vt2/fHO1TrFgxvf766xo6dKhdPD09XfPnz9cjjzxiut/q1atNH+IYNGiQqlatmqMapH8fUL6+4TY9PV0bNmxQly5dcpwPgHt169ZNo0aNUmZm5tVYVFSU9u3bl+0Vra6/XxoeHq4WLVq4tc4b6dGjh6HhduPGjbp48WK2v3y//jxq1aqlypUru63GnCpZsqSGDRtmmMelf5uJFy1apO7du3ugMgAF3cyZM3M1f3BwsD744IMsV/p2l6CgII0ePVr9+/c3LLSzfft2ff/993r88cdztQYAyImNGzfm+IGHnBo4cKBefPFFh2/WyS1paWlKSEjQpUuXdOjQIe3du1eLFy82vIXiWhUqVNDAgQPzsErAHA23AFAAValSRc8884zGjx9vGPv555/Vs2dPNW3a1AOVeZdff/3V7ubzFQ8//LBLTVlmr/6Vsm7Syoms6qLRC/AunpyHZ8yYoRUrVtjFatWqlSf/QPWW+c9b6gBQsEyaNEmTJk3KtfxhYWGaMGGCy6vbZmZmZrl6rpn09HT99ttvWrJkiT7++OM8b7YAcHO7Ge9PnDp1So8++qhiY2MNYyEhIZowYYLLb04YMmSIU/v16dNHH3zwgS5cuGAXX7BgQZYNtwsXLjTE/P39c7RK5LWaNm2qiIgInT59Wj4+PoqIiPBoAxsAe8WLF1fTpk21YcMGu/iiRYuy1XB76dIlQ1N9t27d3PIAa07UrVtXFSpUsHsYLSMjQ0uXLs3WA2iHDh3SoUOH7GJ5vUqvmbvuukvz5s3TqlWrDGMjR45UixYtFBYWlveFAUAusFgs6tOnj1588UWVK1cuT47ZokUL9evXT7/99pth7NNPP1Xnzp1VqVKlPKkFADypUaNGeu2119S4cWO35Zw5c2auPqQREBCgzz77jLfnwCuwJj4AFFADBgxQnTp1DHGbzabhw4cX+Fdfp6amasaMGYZ4SEhIjleZuV5WT4hd/0StM6xWq2ncHU1kANzLE/Pw6dOnNXbsWLuYv7+/xo0b53KDVnZ4y/znLXUAgLsUKVJEkydPztFrv7OSnJys559/XtOmTdPff/+tnTt3av369frzzz/12muvqXr16qb7nTt3Tk8++aQ2b97scg0ACpab6f7EiRMn9PDDD+vUqVOGMR8fH3344YeqUaOGS8eoXLmy003G/v7+pm/k2blzp5KTk033MVstp23btipWrJhTNUjSZ599pjlz5mj79u1aunSppk6dyuq2gBfp0aOHIbZ48eJs7fvXX38ZHmL1VKOq2Wqv2T2P+fPnG2I9e/Z0uSZ3GDlypAoVKmSInzt3znBPBwBuRpGRkXr66ac1f/58ffTRR3nWbHvFq6++anr/JCUlRW+++aZb7hEDgDcKCwtTv3799OOPP+q3335za7NtbgsICNAnn3yi2rVre7oUQBINtwBQYPn6+uq9994zbbCKiorSxIkTPVCV95g3b57hdeuSdM8997j81FRWTW1mq+nmVFYrKAYEBLicG4B75fU8fKVhISEhwS4+aNCgbL820lXeMv95Sx0A4A7NmjXT//73PzVq1Mgt+UJDQzVw4EC1aNFCpUuXVkBAgMLDw1WrVi0NGDBAs2bN0qhRoxQYGGjYNzU1VUOGDNH58+fdUguAguFmuT9x8OBB/ec//zFttpX+bZByR1Npx44dXdrf7Auz9PR07dy50xA/d+6cTp48ma0cOVGvXj1Vr16dz8CAl+rWrZt8fOy/Hjx06JAOHz58w30XLFhg93OJEiXUvHlzt9aXXWaNw2vXrtXly5dvuO/1q3vXq1dPFSpUcFttroiIiNArr7xiOvbnn3/q77//zuOKAOS2u+66S/v378+1/5YtW+bpU1Tr1q31+eefa8WKFVq0aJGef/55ValSxSO1hIaG6t133zUd27hxo+nqtwBws6pQoYLef/99zZs3T+vWrdPIkSM99vndWdWrV9dvv/3Gg7zwKjTcAkABVrNmTT355JOmY99++63plzEFRVavO7jvvvtczp3VF07ueN04jV7AzSUv5+FffvnF8NrH2rVra+DAgW47xo14y/znLXUAgLP8/PzUsWNHTZkyRT/99JMqVqyYZ8f28fHRfffdp2nTpikkJMQwHhcXpw8//DDP6gGQP3j7/YmtW7fqwQcfVExMjOn48OHDXX4bzhVmq/3mRFarvRw7dswQi4qKMt22Vq1aLtUAwLuVKFHCdCXtRYsWOdwvPj5e69evt4v16NHD0LybV+rVq6fy5cvbxdLT02/YWLZ//35Dc7GnVunNygMPPKBmzZqZjr311luGh6kBIDdNmzYty2bevXv3avv27Vq+fLk+++wzderUyTTH2rVr9eeff3rN6rEdOnTQHXfcYTr24Ycf6syZM3lcEQAYPfPMM1nOv/v27dPOnTu1Zs0a/fDDD3rggQdMF0g4ceKEvvvuO8XHx3vsc7szLBaLmjdvro8//lizZs1y+V4J4G43z/9NAIBcMXjwYN1yyy2GeGZmpoYPH254RVhBcPbsWW3atMkQv7JCi6vMXgkmSYmJiS7nTkpKMo0HBQW5nBtA7siLefj48eP66KOP7GL+/v4aO3Zslqu95gZvmf+8pQ4AcMTPz08hISEqVaqUateura5du2rQoEH64osvtG7dOn311Vfq0KGDx+pr2LCh3nvvPdOxOXPmZNmUBgBZ8db7E4sWLdKjjz6q+Ph4w5jFYtHbb7+tRx55xG3Hq1Gjhkv7Z7VCo9lKtmYxSYYGNgD5j9nqsDdquF28eLFhLu7Zs6db68qp7t27G2I3Oo/rV+m1WCym18OTLBaLxowZY3ov4cyZMzzgBsBr+Pj4KCgoSBEREerevbsmT56sX375xfTB4L/++ku33367V6y4K0lvvPGGSpQoYYgnJiZqxIgRHqgIALLPYrEoICBAJUqUUMuWLfXOO+9owYIFatu2rWHbffv26aGHHvKaNwhd4evrq+DgYJUsWVI1atRQp06dNGDAAE2YMEFr1qzRjz/+qN69e99UjcIoOPhTCQAFXEBAgMaMGWP6QWX//v366quvPFCVZ82ZM8f0Kdu77rrLLfnDwsJM4+5o9DLLERISwsqKgBfL7XnYarVq6NChhkbQwYMHq2bNmi7lzilvmf+8pQ4ABYujFQnM/tu9e7e2bdumVatWaebMmZo0aZJefPFFde7cWUWKFPH06Uj6t8GiVatWhnhGRobmzp3rgYoA3My88f7E1KlT9fzzzys1NdUw5ufnp3Hjxuk///mPW49ZtGhRl/b38fExXYH80qVL2YpJ/75mF0D+1q1bN8N8u2/fPh0/fjzLfRYuXGj3c+nSpdWkSZNcqS+7zBplV69e7fDf99efR8OGDRUREeH22lxVqVIlPffcc6Zj06dP18aNG/O4IgDIniZNmmj69Ommb024fPmynn32WcNc7AlhYWFZNtauWrVKf/75Z94WBAAuKleunKZMmWLa02C1WjVp0iSNGTMm145/11135ej+9549e/TPP/9o9erVmj17tiZPnqzXXntNPXv2VPHixXOtTsAdaLgFAKhBgwbq37+/6djkyZN18ODBPK7Is+bPn2+IBQQEqE+fPm7JHx4ebhqPi4tzOffFixcNMT6QAt4vN+fh77//Xlu2bLGL1a5dWwMHDnQ6p7O8Zf7zljoAID947LHHTONbt27N40oA5Afecn8iPT1db7zxhj788EPTB3KDgoI0adIk3XnnnW4/duHChV3OYdYwm5ycnK1YVvsDyF9Klixp2iybVQPUxYsXtX79ertYjx49ZLFYcqW+7Kpfv77KlStnF0tNTdXKlStNt9+3b5+OHj1qF/P0Kr2OPProo6pXr54hbrPZ9OabbyolJcUDVQHAjRUrVkxTp041zNHSvw/pvvzyy1q7dq0HKrPXo0cPdevWzXRs7NixOnfuXB5XBACu8fX11ZgxY9SxY0fT8WnTpmnSpEl5WxSQD/l5ugAAgHd44YUXtHTpUsMqBle+ZPrtt9/k6+vroeryTnR0tPbs2WOIt2zZ0uVVZq4oVaqU/P39Da9gc8c/3M1ymL0SB4D3yY15OC4uThMmTDDEK1WqpC+//DJHubJa/WrmzJmmq6r079/fsAKjt8x/3lIHAOQHLVq0UEBAgNLS0uzi+/bt81BFAG52nr4/cenSJT377LOGxrIrwsPD9cUXX6hx48a5cvzAwECXc2RkZBhifn58FQDAXo8ePbRp0ya72OLFi/XUU08Ztv3rr78Mc0uvXr1ytb7s6t69u7799lu72KJFi0zrW7Bggd3PFovFdJVcb+Hr66v33ntPd999t+EexrFjx/Tpp5/q9ddf91B1AOBYiRIlNGnSJPXr189wzyAjI0PPPfecpk+frqpVq3qown+99dZb2rBhg+Lj4+3icXFxGjlypD777DMPVQYAzvH19dVHH32ke+65R8eOHTOMT5w4UZGRkW5bbAwoiLjLBgCQ9O/qLKNHj1b//v0Nq7fs2LFD33//vQYMGOCh6vJOVqsfdO3a1W3H8PHxUdmyZQ1fHp4+fdrl3KdOnTLEKlas6HJeALkvN+bhhIQE09ffXv8FkytmzpxpGr/rrrsMDbfeMv95Sx0AkB8EBQUpIiJCUVFRdnF3rBoOoGDy5P2JU6dO6YknntCRI0dMxytWrKivv/5akZGRuXJ8SUpMTDR8js6phIQEQywkJMQQy2o13cTERAUHB7tUAwDv161bN40ZM0ZWq/VqbOfOnTp16pRhRcLr7yOUK1dODRo0yJM6b6RHjx6Ghtu///5bKSkpCgoKsotfv4Jv06ZNVbp06Vyv0RXVq1fXoEGDNHHiRMPY999/rx49enjN7wUAXK927dp67bXXNHr0aMPY5cuX9cILL+i///2vRz97lixZUsOGDdPQoUMNY4sWLdLChQu9+uEMADBTuHBhffzxx7r//vsND25J0ogRI1SrVi2PP/QA3Kx8PF0AAMB7tGjRQvfdd5/p2GeffWb6BFR+s2LFCkPMx8dHt956q1uPU7lyZUPMHdf3xIkThli1atVczgsgbxSEedhb5j9vqQMA8oPw8HBDLCkpyQOVAMgvPPG5+ODBg3rggQeybLZt2LChpk+fnqvNtpJ5s2xOpKWlGVYQk6TixYsbYlm9ycfVGgDcHEqVKmW6Wvdff/1l9/OFCxcMb7bp3r27LBZLrtaXXQ0aNFBERIRdLCkpSatWrbKL7d271/CQWM+ePXO7PLcYOHCgqlevbohbrVYNHz7cdN4HAG/x8MMPq23btqZjBw4c0Pvvv5/HFRndddddateunenYqFGjeKgYwE2pbt26evbZZ03HkpKS9NJLL5k24wK4MRpuAQB2XnvtNZUpU8YQT0lJ0fDhww2ry+QnmZmZpq+MbNCggdtfB16nTh1DbP/+/S7lPHbsmGljA41ewM0lv8/D3jL/eUsdAJBX4uPjtWPHDs2bN0+TJ0/W8OHD9eH/a+8+o6yq0jQAf1VUESySESQ0SlAQAypGbB0DUcyZURe2CR2xG1FHMIABDCQRFTHbOrbamMEE2DC2ilkUBUWdBiwFFJWMVUDNj17dM3hPXSveCv08a/GD/Z29z3f5cbgU79l71KgKWXvNmjUpY7/cTQygtDL5vfjTTz+N008/PZYuXZpY79mzZzz00EOx1VZbVdg9i/Ptt9+Wa35xgeE2bdqkjBUXuP3666/L1QNQcyTt2Pfyyy9v9vtp06bFhg0bNhvr06dPpfZVWj179kwZ++Xn+OUuvdnZ2YnzqqPc3NwYOXJk1KlTJ6W2YMGCmDhxYhV0BVByI0eOLPZ0hcceeyzeeOONDHeU6rrrrou8vLyU8e+//z5GjhxZBR0BlN+5555b7GkI8+fPjzvvvDPDHUHtIHALwGYaNmwY1113XWLtnXfeiT/96U8Z7ihz5s2blxgWOPDAAyv8XrvvvnvK2NKlS4v9z72SmDNnTspYVlZWdOnSpcxrAplX25/D1eX5V136AMiUm266KU466aS45JJLYty4cTF58uR47LHHYuPGjeVee9myZSlj1f1oXqD6y9T34k8//TT69+9f7K5V/fv3j/Hjx2fsRYJ58+ZVyvwOHTqUaCyi/C+iffjhh9G/f/+46qqr4q677oqpU6fGnDlzNju2HqgeevTokbJT7YcffhjffffdP3//0ksvbVZv3bp17Lbbbhnpr6SSgsMzZ87cbOfXX36Offfdt8I3WahMu+22W/Tv3z+xds8998T8+fMz2xBAKTRr1iyuuOKKxFpRUVFcc801sX79+gx3tbkWLVrE4MGDE2vPPvtszJo1K8MdAZRfdnZ23HjjjVG3bt3E+j333BMLFizIcFdQ8wncApDikEMOiWOOOSaxNnr06Pjmm28y3FFmvPfee4nj++23X4Xfa5999onc3NyU8ddff73Ma/71r39NGevUqVNGduABKlZFPYdbtWoVn332WYX8mjFjRuI9/vjHPyZe36pVq8Trq8vzr7r0AZApSUfQrl69Oj744INyrfvZZ58lhtSKC3EBlEZl/3xi4cKFcc4558SKFStSallZWTFkyJAYMmRIRo9N//DDD8s1/5133kkZ22abbaJdu3Yp482aNUs5hj0i+UWy0nj33XfjzTffjD//+c8xbty4uOSSS+Kcc86pNsfPA/+nWbNmsddee202tmnTpn/+DODHH3+Mt99+e7N6ddvdNuLvJ5Rtv/32m42tWrUq3nrrrYj4+8sICxcu3KxeHT/Hr/n9738fO+ywQ8p4YWFhDB06tEJepgOoLCeeeGIccMABibXFixdXi10W+/XrF/vss09ibdiwYYmb9gBUd+3atYsBAwYk1goLC+Oaa66p8adrQqYJ3AKQaOjQoYlv+K9Zsybuv//+Kuio8iUFbuvWrRt77rlnhd+rYcOGietOnTq1TOutW7cu/vKXv6SMH3zwwWVaD6h6tfU5XF2ef9WlD4BM6dSpU+L4008/Xa51f3k07z9UxktrwL+myvpevHLlyjj33HNj+fLlKbWcnJwYNWpUsTsJVqYZM2bE2rVryzR33bp1KTs4RkR069at2Dm/DNpFRMyaNStWr15dph4ikl9i23PPPQVuoZpK2h122rRpERExffr02LBhw2a13r17Z6Sv0sjKyoqePXumjP/jc7z88subjefk5ET37t0z0ltFqlevXowYMSLxefrJJ5/Eq6++WgVdAZTc9ddfX+zJEffff3+V77KYlZUVI0aMSOzx22+/jT//+c9V0BVA+Z133nmJGzJERLz//vvxxBNPZLgjqNkEbgFI1LRp07j66qsTa1V9rEtlmTt3bspYhw4dij1iobyOPvrolLHXX3+9TD9QePLJJ2PlypWbjWVlZcXxxx9f5v6AqlWbn8PV5flXXfoAyIR99tknMbD2wgsvbHZkcGn8+OOP8fDDD6eM5+TkJAYeAMqisr4XX3755Sm7HUZE5Obmxvjx4+Ooo44q89rlsXbt2njmmWfKNHfy5MmJu24de+yxxc45/PDDU8bWr19f5h6WLFnyzx0l/7/999+/TOsBla9nz54pAc633norVq1aFa+88spm4zvssEOxL3JVtaTvn6+++moUFRWlfI7999+/xp5G07Vr1+jXr19irab/vAio/Vq3bh0XXHBBYq267LLYpk2buPjiixNrnrNATZWbmxvDhw8v9kXYMWPGJL6QDCQTuAWgWL169YoePXpUdRsZsWrVqsjPz08Z32WXXSrtnr179468vLzNxoqKimL48OEpO0ek880338SECRNSxvfbb79o06ZNufsEqk5tfQ5Xl+dfdekDIBPq1KmTGB5bu3ZtjB07ttTrFRUVxZAhQxJ3QDzyyCMTw70AZVXR34sff/zxxNMJIiJGjhwZRxxxRIXdqyzGjRsXS5cuLdWc/Pz8GDduXMp4mzZtij26NyKiR48eKcewR0SMHz8+li1bVqoeIiJuvfXWlCPNc3Nz04Z+garVrFmz6NKly2ZjhYWF8cILL8Sbb7652XifPn0y2Fnp7LnnntG8efPNxr777rt4+umn48svv9xsvDru0lsagwcPjpYtW1Z1GwBlcvbZZ0f79u0Ta++//3612EW2f//+sdtuu1V1GwAVau+9944TTjghsbZixYoYOXJkhjuCmkvgFoC0rrnmmmjSpElVt1HpPvvss8TxytyxoWHDhnHmmWemjL/77rtx7bXXlmiNtWvXxgUXXBA//fRTSm3gwIHlbRGoBmrjc7i6PP+qSx8AmdK/f//YYostUsafeuqpeOyxx0q8zsaNG+Paa69NDKvVr1/f8w+oFBX1vfiHH36IUaNGJdYGDBiQeApCpq1cuTIGDhwYq1atKtH1P/zwQ1xwwQWJu9sOHDiw2B1sIv6+K/npp5+e2MPFF19c4h4iIp5//vnEnXF79+5dY3eShH8VvXr1ShkbN25cFBYWbjZWnQO3WVlZiS9n3HLLLZv9Pjc3N7p3756ptipFXl5eXHfddVXdBkCZ/Noui6NHj67yXRbr1KkTI0eOjNzc3CrtA6CiXXbZZcX++3zKlCnx+uuvZ7gjqJkEbgFIa9ttt40hQ4ZUdRuV7quvvkoc33HHHSv1vr/73e8Sd/964oknYvDgwYn/WfYPixYtin79+sX8+fNTat27d4+uXbtWaK9A1aitz+Hq8vyrLn0AZELz5s2LPbrx2muvjfHjx//qDt/5+fkxYMCA+NOf/pRYHzx4cLRu3brcvQL8UkV9L540aVJiiLRz587V6oWBOXPmxBlnnBELFixIe91HH30U/fr1S3yReK+99oq+ffv+6r3OOOOM6NixY8r4Bx98EKeffnri993/r6ioKB588MEYMmRIyhHAeXl5MXjw4F/tAahavXr1Sgk+/fjjj5v9vn379tGhQ4dMtlVqScHhX36Obt261YoXmw866KA4/vjjq7oNgDLZZ5994rjjjkusVZddFnfaaacYMGBAVbcBUKGaNm0al19+ebH14cOHx88//5zBjqBmyqnqBgCo/o477riYOnVqvPbaa1XdSqXJz89PHK/so8AbN24c119/fWLwYcqUKfHuu+/GaaedFoceemi0atUqCgoKYuHChfHcc8/F008/HWvXrk2Zt+2225Z4Z0agZqiNz+Hq8vyrLn0AZMpZZ50Vs2fPTtmtYNOmTXHnnXfGc889F/369Ytu3bpFq1atom7durF8+fKYP39+TJ8+PZ577rkoKChIXPvYY49N3DkcoKKU93vx6tWr44knnkisffLJJ9G5c+fytJeouBN1SmLevHlx3HHHxdFHHx1HHnlkdOjQIZo2bRo//PBDzJs3L55//vl48cUXY9OmTSlzGzduHGPGjEm7u+0/1KtXL8aOHRsnnnhiyvfb+fPnx/HHHx+9evWKnj17RufOnWPbbbeNDRs2xJIlS2L27Nnx2GOPxeeff5649uWXX55yxDtQ/TRv3jy6dOkSH3zwQbHX9O7dO4Mdlc1ee+0VzZo1i6VLlxZ7TU34HCV1xRVXxGuvvRbfffddVbcClMPTTz8dTz/9dEbuddFFF1Wbl8wuu+yyePXVVxNPDpsyZUqccMIJceCBB2a+sf/n/PPPj5dffrnY77pA7VUZP+Ps2LFjPPvssxW+bmkdd9xx8dRTT8Xbb7+dUlu0aFHceeedMWjQoCroDGoOgVsASuS6666Lvn37pt3pryb75ptvUsZyc3Nj++23r/R7H3bYYfEf//Efcccdd6TUlixZEuPGjYtx48aVaK0GDRrErbfeGltvvXVFtwlUsdr4HK4uz7/q0gdAJuTm5saECRPizDPPjLlz56bUv/7665Rjd0uiT58+MWLEiIpoESCt8nwvfvHFFxNfmKpO2rZtu9kpPIWFhfHkk0/Gk08+WeI18vLy4u67744WLVqUeE67du3illtuiUGDBqUcIb9x48aYOnVqTJ06tcTrRUSce+65ceqpp5ZqDlB1evXqlTZw26dPnwx2UzZZWVnRo0ePePjhhxPrdevWjcMPPzzDXVWeJk2axLBhw+Kiiy6q6lYASm2rrbaKyy67LK688srE+vDhw+P555+PevXqZbiz/5ObmxsjR46MU045JTZu3FhlfQBUtOHDh8cxxxyT8u//iIj77rsvjjrqqGjfvn0VdAY1Q3ZVNwBAzdCiRYtafQTg999/nzK27bbbRnZ2Zv6qvPjii+PCCy8s1xpbbLFFTJo0yRHmUEvV1udwdXn+VZc+ADIhLy8vHnzwwejZs2e518rOzo4BAwbE2LFjIyfHe91A5SvP9+KacGJEnz594rLLLivz/JYtW8aDDz4Ye+65Z6nndu/ePe66665o2rRpme8f8fe/Gy644IK49NJLy7UOkFk9e/Ysdlfsjh07Rtu2bTPcUdn06tWr2Npvf/vbaNSoUQa7qXzdu3dP+5kBqrMTTjih2J+lLly4MO68884Md5Rqt912i/79+1d1GwAVql27dnH22Wcn1goLC2PYsGFRVFSU4a6g5hC4BaDE+vXrF/vss09Vt1Epkna4yfTuhL///e/jjjvuiG233bbUc/fcc8945plnYr/99quEzoDqorY+h6vL86+69AGQCY0aNYrbbrsthg0bVubvvZ07d45HHnkkBg0aVKIjywEqSlm/F3/66aeV0E3FO+ecc+LWW2+NLbfcssRz6tSpEyeeeGI888wzsfvuu5f53gcddFA8++yz0aNHjzLNb9WqVTzwwAPxhz/8ocw9AFVj++23jz322COxVhN2t/2HvffeO7bbbrvEWk36HKVxzTXXlPtlCYCqkJWVFcOHD4/c3NzE+n333RdffPFFhrtKdfHFF8cOO+xQ1W0AVKgLL7wwWrdunVh79913Y/LkyRnuCGoOgVsASiwrKytuuOGGqF+/flW3UuGSArfbbLNNxvs44ogjYtq0aTFkyJDYaaed0l6bk5MTBxxwQNxxxx3x6KOPRps2bTLUJVBVavNzuLo8/6pLHwCZ0q9fv3j11Vfj6quvji5dukSdOnXSXr/FFltE79694957742nnnoq9t577wx1CvB/yvq9OOl0m+qqd+/eMXXq1Dj99NMjLy+v2OuaNm0a/fr1i+effz5GjBgRjRs3Lve9mzdvHhMmTIhnnnkmTjnllF8N/ubk5MRee+0Vo0aNipdffjn233//cvcAVI3idkrt3bt3hjspu6ysrMSTHOrXrx+HHnpoFXRU+bbeeusYOnRoVbcBUCYdOnSIs846K7FWXXZZrF+/fowYMcLLxkCtUq9evRg2bFix9dGjR8cPP/yQwY6g5sgqqupvJwBAsZYuXRpz586N/Pz8WLNmTeTm5kaTJk2iVatWsccee8QWW2xR1S0CVIrq8vyrLn0AZMrq1atjzpw5sWzZsvjpp59i3bp10aBBg9huu+2ibdu2sdNOO/1qKBeAkjnssMMiPz9/s7GLLrooBg4cuNlYQUFBfPjhh/HFF1/EihUrom7durHVVltFp06dokOHDpX+XC4qKoovv/wyFixYEEuXLo1169ZFbm5ubLnlltG8efPo0qVL2lAwAAAAANQWArcAAAAAAJBhJQ3cAgAAAADVQ3ZVNwAAAAAAAAAAAAAA1ZnALQAAAAAAAAAAAACkIXALAAAAAAAAAAAAAGkI3AIAAAAAAAAAAABAGgK3AAAAAAAAAAAAAJCGwC0AAAAAAAAAAAAApCFwCwAAAAAAAAAAAABpCNwCAAAAAAAAAAAAQBoCtwAAAAAAAAAAAACQhsAtAAAAAAAAAAAAAKSRVVRUVFTVTQAAAAAAAAAAAABAdWWHWwAAAAAAAAAAAABIQ+AWAAAAAAAAAAAAANIQuAUAAAAAAAAAAACANARuAQAAAAAAAAAAACANgVsAAAAAAAAAAAAASEPgFgAAAAAAAAAAAADSELgFAAAAAAAAAAAAgDQEbgEAAAAAAAAAAAAgDYFbAAAAAAAAAAAAAEhD4BYAAAAAAAAAAAAA0hC4BQAAAAAAAAAAAIA0BG4BAAAAAAAAAAAAIA2BWwAAAAAAAAAAAABIQ+AWAAAAAAAAAAAAANIQuAUAAAAAAAAAAACANARuAQAAAAAAAAAAACANgVsAAAAAAAAAAAAASEPgFgAAAAAAAAAAAADSELgFAAAAAAAAAAAAgDQEbgEAAAAAAAAAAAAgjZyqbgAAAAAAgJptwoQJcfvtt1d1GyluvPHGOP7446u6DQAAAACgFhC4BQAAAACAfwFnnHFGvP322ynjM2bMiFatWlVBRwAAAABQc2RXdQMAAAAAAAAAAAAAUJ0J3AIAAAAAAAAAAABAGgK3AAAAAAAAAAAAAJCGwC0AAAAAAAAAAAAApJFT1Q0AAAAAAFCzDRw4MAYOHFjm+WeccUa8/fbbKePHHXdc3HTTTeVpDQAAAACgQtjhFgAAAAAAAAAAAADSELgFAAAAAAAAAAAAgDQEbgEAAAAAAAAAAAAgDYFbAAAAAAAAAAAAAEhD4BYAAAAAAAAAAAAA0hC4BQAAAAAAAAAAAIA0cqq6AQAAAAAAyKT8/Pz4+OOP44svvohvv/02li9fHj///HMUFhZG3bp1o379+rHNNttEixYtolOnTtGlS5do0qRJpfSyePHi+Oijj+Ljjz+O7777LlauXBnr16+PevXqRePGjWP77bePdu3axR577BHt2rWrlB4AAAAAgF8ncAsAAAAAQK23cOHCePzxx2PGjBnxt7/9rVRzc3JyomvXrnHaaadFjx49Iju7fIfHrVq1Kp544ol49tln47PPPivxvJYtW0bv3r3j3//936NFixbl6qG8br/99pgwYUJibbvttosHH3xQQBgAAACAWkXgFgAAAACAWis/Pz9Gjx4dL774YhQVFZVpjQ0bNsTs2bNj9uzZ0bFjxxg5cmR07ty5TGs9+eSTMXr06Pjhhx9KPTc/Pz/uvffeePDBB6Nfv37xhz/8IfLy8srUR3lMmDAhbr/99sRay5Yt46GHHorWrVtnuCsAAAAAqFzlew0fAAAAAACqqenTp8fRRx8dL7zwQpnDtr80f/78OPXUU2PWrFmlmldUVBSjRo2KoUOHlils+/9t2LAh/vjHP8Ypp5wSS5YsKddapXXbbbcVG7bdcccd49FHHxW2BQAAAKBWErgFAAAAAKDWeeGFF2LgwIGxevXqCl+7oKAgLr300vj+++9LPGfixIlx7733VmgfCxYsiHPPPTfWr19foesWZ/z48XHHHXck1nbeeef4r//6r2jevHlGegEAAACATMup6gYAAAAAAKAiffXVVzFkyJDYtGlTYr19+/bRu3fv2GuvvWLHHXeMRo0aRYMGDWLdunXx008/xeeffx7vv/9+PPvss7Fs2bLENVauXBk333xzjBo16lf7mTt3btx5552JtW222SaOOuqoOOCAA6Jt27ax5ZZb/rOX77//PubOnRsvvvhivPrqq4mf5/PPP4+JEyfGoEGDfrWP8rj11ltj4sSJibXddtst7rvvvmjSpEml9gAAAAAAVUngFgAAAACAWuWGG25I3PU1Nzc3rrrqqjj55JMjOzv1ALiGDRtGw4YNo1WrVnHYYYfFoEGD4oEHHojx48dHQUFByvUvvfRSDBs2LBo2bJi2n9GjR0dhYWHKeN++fePaa69NnP+PXnbYYYfo27dvfPjhh3HxxRfH0qVLU659+OGH45xzzolGjRql7aOsxo0bF3fddVdirWvXrjFp0qRf/TMAAAAAgJou9SeKAAAAAABQQ82ZMydef/31xNott9wSp556amLYNkmdOnXinHPOibFjxybWCwoKYubMmWnXWLJkScyePTtlfN99941Ro0aVOKjapUuXePjhhyMvLy+ltmbNmpg+fXqJ1imtdGHbgw46KO69915hWwAAAAD+JQjcAgAAAABQa0yePDlxvE+fPtGnT58yrdm9e/c48MADE2tz585NO/e9996LoqKilPHzzjuvxMHff2jTpk1cdNFFibVZs2aVaq2SGDt2bLFh28MPPzwmTpwYDRo0qPD7AgAAAEB1JHALAAAAAECtUFRUVOxOrxdeeGG51u7Ro0fi+HfffZd2Xn5+fuJ4ixYtytTHiSeeGHXq1NlsLCsrK7799tsyrVecsWPHxqRJkxJrffv2jdtuuy3q1q1bofcEAAAAgOosp6obAAAAAACAirBs2bJo2bJlbNq0KX766ad/ju+6667RoUOHcq3dsWPHxPE1a9aknZe0u23E33fGbdeuXan7aNy4cVx88cXRoEGDaN26dfzmN7+J1q1bR7169Uq9VnHGjBkTd999d2LtxBNPjOuvv77Uu/MCAAAAQE0ncAsAAAAAQK3QrFmzmDx5ckRErFy5MhYtWhSLFi2KbbbZptxrN2nSJHG8oKAg7bzi7j1hwoTo1q1bmXobMGBAqeeU1OjRo+Oee+5JrJ155pkxdOjQyMrKqrT7AwAAAEB15RV0AAAAAABqncaNG8euu+4affr0iX333bfc69WtWzdxfOPGjWnndenSJXF88eLFccopp8Qrr7xS7C64mTZq1Khiw7bnn39+XHnllcK2AAAAAPzLssMtAAAAAACk8fnnn8fUqVMTa5s2bUo7t127dtGpU6eYN29eSu3rr7+OgQMHRsuWLePQQw+Ngw8+OPbdd99o0KBBhfRdGrfcckvcd999ibWTTz45Lrnkkgx3BAAAAADVi8AtAAAAAADE38OzixcvjgULFsTnn38eH330UXz00UexfPnycq07cODAuPDCC4ut5+fnxyOPPBKPPPJI1K1bN/baa6/o1q1bdOvWLXbZZZdK31V2zJgx8cILLxRbnzlzZqxYsSKaNGlSqX0AAAAAQHUmcAsAAAAAwL+MjRs3xsKFC+N//ud/YvHixf/8tWjRosjPz4+CgoIKv+fhhx8eJ554YkyePPlXry0oKIjZs2fH7NmzY8yYMbHllltGt27d4tBDD42DDjoomjZtWuH9pQvbRkQsW7YsrrvuuhgzZkyF3xsAAAAAagqBWwAAAAAAarVly5bF1KlTY/r06TF37txYv359xnu47rrrYtOmTfHUU0+Vat6PP/4YU6ZMiSlTpkROTk7su+++0bt37+jdu3c0atSokrpNNWXKlOjRo0f07NkzY/cEAAAAgOoku6obAAAAAACAypCfnx+DBw+OQw45JG666aZ49913yxy2zc4u34/T69SpEzfeeGOMGTMmtt9++zKtsWHDhnjjjTfi6quvjoMOOiiuv/76WL58ebn6+qXs7Oxo0aJFYm348OEVfj8AAAAAqCkEbgEAAAAAqHUef/zx6NWrV0yZMiU2bdpUpjWaNGkSRxxxRNxwww0xefLkCumrb9++MW3atBg3blwcfPDBkZubW6Z11q9fH4888kgceeSR8frrr1dIb7m5uTFmzJiYNGlSYl8//PBDDBs2rELuBQAAAAA1TU5VNwAAAAAAABVp3Lhxcdddd5Vqzrbbbhtt27aNnXbaKXbZZZfYddddo3379v/c2fbrr7+usP5yc3OjT58+0adPn/jpp59i5syZMWvWrHj99ddjxYoVpVrrxx9/jPPPPz8mTZoU3bp1K3NP9erVi9tuuy3+7d/+LSIiBgwYEBMmTEi5btq0afHMM8/EscceW+Z7AQAAAEBNJHALAAAAAECt8dJLL6UN22ZlZcXOO+8cXbt2jV122SXat28f7dq1i4YNG6Zdd+PGjRXdakRENG3aNI499tg49thjY+PGjfHJJ5/E7Nmz480334z3338/1q9f/6trFBYWxiWXXBIvvfRSbLnllqXuIS8vLyZOnBj77bffP8fOP//8mDZtWsyfPz/l+hEjRsQBBxwQzZo1K/W9AAAAAKCmErgFAAAAAKBWWL58eVxzzTWJtaysrDjmmGPioosuitatW5d67ZIEX8urTp06sfvuu8fuu+8e5513XhQUFMR7770Xs2bNihkzZsSiRYuKnfvTTz/F/fffH4MHDy71fe+4447NwrYRf9+Fd+TIkXHyySfHhg0bNqutXLkyhg4dGvfdd1+p7wUAAAAANVV2VTcAAAAAAAAV4aGHHooVK1Yk1q6//vq4+eabyxS2jfh7mDfT6tatGwcccEBcccUVMW3atHj00UfjgAMOKPb65557rkz3Ke7PpHPnznH22Wcn1v7617/GY489Vqb7AQAAAEBNJHALAAAAAECtMGXKlMTxo48+Ok466aRyrb148eLE8U2bNpVr3dLYe++944EHHojjjz8+sb5kyZJYunRphd7zoosuinbt2iXWbr755mL/XAAAAACgthG4BQAAAACgxlu2bFnk5+cn1k4//fRyr//BBx8kjm/cuLHYOYWFhfHVV1/FjBkz4p577omhQ4fGaaedFi+99FKZ+8jKyoqhQ4dG/fr1E+vLli0r89pJ6tatGyNHjozs7NT/Tli7dm0MGTIko6FjAAAAAKgqOVXdAAAAAAAAlNeSJUuKrbVv375ca69bty6mT5+eWNuwYUOx85555pm46qqrUsZ32mmn6NWrV5n7adSoUbRt2zY+/fTTUvVTVl26dIkzzzwzHnzwwZTaO++8Ew899FCcddZZFX5fAAAAAKhO7HALAAAAAECNV1RUVGwt3S60JXHvvffGqlWrEmuFhYXFzttvv/0Sx19++eUoKCgoV09r165NHN96663LtW5xBg0aFG3atEmsjRs3Lr788stKuS8AAAAAVBcCtwAAAAAA1HhbbbVVsbUPPvigzOu+++67MWnSpGLrP//8c7G13/zmN9G6deuU8R9//DEeffTRMve0aNGiWLhwYcp4kyZNEu9XEerXrx833HBDZGVlpdR+/vnnuOKKK8odbAYAAACA6kzgFgAAAACAGq958+axxRZbJNYmTZqUdgfc4rz99ttxwQUXpN3Fdt26dWnXOOWUUxLHx48fH5988kmpeyoqKoqRI0cmfp4jjjgiMRBbUfbdd9847bTTEmsfffRR2mAyAAAAANR0ArcAAAAAANR4ubm58dvf/jax9t5778WwYcNiw4YNJVpr7dq1MW7cuDjrrLNi5cqVaa/9tfrJJ5+cGAReu3ZtnHXWWTFr1qwS9RQRsX79+hgyZEj85S9/SallZ2fHmWeeWeK1yurSSy+Nli1bJtbuvPPOmD9/fqX3AAAAAABVQeAWAAAAAIBaoV+/fsXWHn/88Tj55JPjhRdeiDVr1qTUCwoK4oMPPohbbrklDj300LjrrrtKFNBdu3ZtrF27tth6kyZN4tJLL02srVixIs4777w4//zzY/r06cWGd/Pz8+Ohhx6Kvn37xtNPP514zUknnRQdO3b81X7LKy8vL66//vrEWmFhYVx++eVRUFBQ6X0AAAAAQKblVHUDAAAAAABQEfbff//o0aNHvPLKK4n1Tz75JAYNGhTZ2dnRqlWraNKkSURErFq1KvLz86OwsLDYtY899tioV69ePP744ym1efPmxd57713s3H79+sXMmTPjv//7vxPrM2fOjJkzZ0ZWVlZsv/320aRJk6hbt26sXbs2li1bFitWrEj3saNjx44xZMiQtNdUpG7dusUJJ5wQTz75ZErts88+i9tvvz0uueSSjPUDAAAAAJlgh1sAAAAAAGqNG2+8MTp16pT2mk2bNsWiRYvi448/jo8//jj+9re/FRu2zcvLixtuuCFuvvnm2HPPPROveeONN9LeLysrKyZMmBAHHXRQ2uuKiorim2++iXnz5sWcOXNiwYIFvxq27dy5czzwwAPRoEGDtNdVtCFDhsR2222XWLv33ntjzpw5Ge0HAAAAACqbwC0AAAAAALVGw4YN47777osDDzyw3Gv17NkzpkyZEieddFJERHTt2jWysrJSrnv55Zd/da369evHxIkT43e/+13k5JT/8Lns7Ow444wz4tFHH42tttqq3OuVVqNGjeLaa69NrG3cuDH+8z//M9avX5/hrgAAAACg8mQVFRUVVXUTAAAAAAD863rqqaciPz8/ZbxTp05xxBFHlGnNoqKieOyxx+L++++PRYsWlXhebm5uHH744XH22WfH7rvvnlI/44wz4u23304Zv/vuu+OQQw4p0T0+++yzuOeee2L69Omxbt26EvcWEdGgQYPo0aNHnH/++dGuXbtSzS2u9xkzZkSrVq1KtdY/XHrppfH8888n1s4888y48sory7QuAAAAAFQ3ArcAAAAAANRamzZtijfffDPeeuutmDNnTnz99dexatWqWL16deTk5ETjxo2jVatWsfPOO0fXrl3jkEMOicaNGxe73rJly2L58uUp41tttVU0a9asVL2tWbMmXnvttfjkk09i3rx5sXjx4li9enWsWbMmCgoKol69etG0adNo1apVdOzYMfbZZ5/o1q1b5OXllfrPAQAAAAAoH4FbAAAAAAAAAAAAAEgju6obAAAAAAAAAAAAAIDqTOAWAAAAAAAAAAAAANIQuAUAAAAAAAAAAACANARuAQAAAAAAAAAAACANgVsAAAAAAAAAAAAASEPgFgAAAAAAAAAAAADSELgFAAAAAAAAAAAAgDQEbgEAAAAAAAAAAAAgDYFbAAAAAAAAAAAAAEhD4BYAAAAAAAAAAAAA0hC4BQAAAAAAAAAAAIA0BG4BAAAAAAAAAAAAIA2BWwAAAAAAAAAAAABIQ+AWAAAAAAAAAAAAANIQuAUAAAAAAAAAAACANARuAQAAAAAAAAAAACANgVsAAAAAAAAAAAAASEPgFgAAAAAAAAAAAADSELgFAAAAAAAAAAAAgDQEbgEAAAAAAAAAAAAgDYFbAAAAAAAAAAAAAEhD4BYAAAAAAAAAAAAA0hC4BQAAAAAAAAAAAIA0BG4BAAAAAAAAAAAAIA2BWwAAAAAAAAAAAABIQ+AWAAAAAAAAAAAAANIQuAUAAAAAAAAAAACANARuAQAAAAAAAAAAACANgVsAAAAAAAAAAAAASEPgFgAAAAAAAAAAAADSELgFAAAAAAAAAAAAgDQEbgEAAAAAAAAAAAAgDYFbAAAAAAAAAAAAAEhD4BYAAAAAAAAAAAAA0hC4BQAAAAAAAAAAAIA0BG4BAAAAAAAAAAAAIA2BWwAAAAAAAAAAAABIQ+AWAAAAAAAAAAAAANIQuAUAAAAAAAAAAACANARuAQAAAAAAAAAAACANgVsAAAAAAAAAAAAASEPgFgAAAAAAAAAAAADSELgFAAAAAAAAAAAAgDQEbgEAAAAAAAAAAAAgDYFbAAAAAAAAAAAAAEhD4BYAAAAAAAAAAAAA0hC4BQAAAAAAAAAAAIA0BG4BAAAAAAAAAAAAIA2BWwAAAAAAAAAAAABIQ+AWAAAAAAAAAAAAANIQuAUAAAAAAAAAAACANP4X1AOjhT7Sf8QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 3200x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# N170 plot, no downsampling\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(8, 4), dpi=400)\n",
    "sns.barplot(x=\"Task\", y=\"Validation Balanced Accuracy\", \n",
    "            hue=\"Model\", \n",
    "            data=df_task, ci=\"sd\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8ecbf",
   "metadata": {},
   "source": [
    "### CSP LDA Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d49ef68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.9e+02 (2.2e-16 eps * 28 dim * 4.6e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.6e+02 (2.2e-16 eps * 28 dim * 4.2e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "N170\n",
      "Classification accuracy: 0.514261 / Chance level: 0.505276\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.9e+02 (2.2e-16 eps * 28 dim * 3e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.5e+02 (2.2e-16 eps * 28 dim * 2.4e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "N400\n",
      "Classification accuracy: 0.587808 / Chance level: 0.550102\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.8e+02 (2.2e-16 eps * 28 dim * 6.1e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.6e+02 (2.2e-16 eps * 28 dim * 2.5e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "P3\n",
      "Classification accuracy: 0.819269 / Chance level: 0.822169\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.8e+02 (2.2e-16 eps * 28 dim * 4.6e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.8e+02 (2.2e-16 eps * 28 dim * 4.4e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "N2pc\n",
      "Classification accuracy: 0.498153 / Chance level: 0.503972\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6e+02 (2.2e-16 eps * 28 dim * 9.7e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.1e+02 (2.2e-16 eps * 28 dim * 5e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "MMN\n",
      "Classification accuracy: 0.800816 / Chance level: 0.796926\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.4e+02 (2.2e-16 eps * 28 dim * 8.6e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.8e+02 (2.2e-16 eps * 28 dim * 2.9e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "ERN\n",
      "Classification accuracy: 0.879128 / Chance level: 0.886715\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.5e+02 (2.2e-16 eps * 28 dim * 5.7e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.5e+02 (2.2e-16 eps * 28 dim * 5.6e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "LRP\n",
      "Classification accuracy: 0.513084 / Chance level: 0.504549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from mne.decoding import CSP\n",
    "\n",
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "preprocessing = \"medium\"\n",
    "\n",
    "for task in [\"N170\", \"N400\", \"P3\", \"N2pc\", \"MMN\", \"ERN\", \"LRP\"]:\n",
    "    df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "    data, labels = DataLoader.create_data_labels(df)\n",
    "    cv = ShuffleSplit(1, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(data, labels)\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    csp = CSP(n_components=8, reg=None, log=True, norm_trace=False)\n",
    "    clf = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "    scores = cross_val_score(clf, data, labels, cv=cv, n_jobs=1)\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(task)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6bb18fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.3e+02 (2.2e-16 eps * 28 dim * 3.7e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.3e+02 (2.2e-16 eps * 28 dim * 3.8e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "N170\n",
      "Classification accuracy: 0.503324 / Chance level: 0.500570\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.6e+02 (2.2e-16 eps * 28 dim * 2.6e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.4e+02 (2.2e-16 eps * 28 dim * 2.2e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "N400\n",
      "Classification accuracy: 0.557480 / Chance level: 0.546629\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.1e+02 (2.2e-16 eps * 28 dim * 5e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.4e+02 (2.2e-16 eps * 28 dim * 2.3e+16  max singular value)\n",
      "    Estimated rank (mag): 28\n",
      "    MAG: rank 28 computed from 28 data channels with 0 projectors\n",
      "Reducing data rank from 28 -> 28\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "P3\n",
      "Classification accuracy: 0.816143 / Chance level: 0.820574\n"
     ]
    }
   ],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "preprocessing = \"heavy\"\n",
    "\n",
    "for task in [\"N170\", \"N400\", \"P3\"]:\n",
    "    df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "    data, labels = DataLoader.create_data_labels(df)\n",
    "    cv = ShuffleSplit(1, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(data, labels)\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "    clf = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "    scores = cross_val_score(clf, data, labels, cv=cv, n_jobs=1)\n",
    "    class_balance = np.mean(labels == labels[0])\n",
    "    class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(task)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a391983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N170\n",
      "Classification accuracy: 0.277778 / Chance level: 0.500000\n",
      "N170\n",
      "Classification accuracy: 0.571429 / Chance level: 0.503597\n",
      "N170\n",
      "Classification accuracy: 0.566667 / Chance level: 0.513699\n",
      "N170\n",
      "Classification accuracy: 0.400000 / Chance level: 0.513699\n",
      "N170\n",
      "Classification accuracy: 0.741935 / Chance level: 0.512987\n",
      "N170\n",
      "Classification accuracy: 0.590909 / Chance level: 0.518182\n",
      "N170\n",
      "Classification accuracy: 0.481481 / Chance level: 0.503759\n",
      "N170\n",
      "Classification accuracy: 0.500000 / Chance level: 0.533333\n",
      "N170\n",
      "Classification accuracy: 0.387097 / Chance level: 0.509934\n",
      "N170\n",
      "Classification accuracy: 0.571429 / Chance level: 0.519608\n",
      "N170\n",
      "Classification accuracy: 0.571429 / Chance level: 0.514286\n",
      "N170\n",
      "Classification accuracy: 0.500000 / Chance level: 0.503597\n",
      "N170\n",
      "Classification accuracy: 0.482759 / Chance level: 0.503497\n",
      "N170\n",
      "Classification accuracy: 0.366667 / Chance level: 0.513333\n",
      "N170\n",
      "Classification accuracy: 0.714286 / Chance level: 0.522059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\mne\\decoding\\mixin.py\", line 33, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\mne\\decoding\\csp.py\", line 168, in fit\n",
      "    raise ValueError(\"n_classes must be >= 2.\")\n",
      "ValueError: n_classes must be >= 2.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N170\n",
      "Classification accuracy: nan / Chance level: 0.974026\n",
      "N170\n",
      "Classification accuracy: 0.656250 / Chance level: 0.509554\n",
      "N170\n",
      "Classification accuracy: 0.588235 / Chance level: 0.517647\n",
      "N170\n",
      "Classification accuracy: 0.516129 / Chance level: 0.516129\n",
      "N170\n",
      "Classification accuracy: 0.653846 / Chance level: 0.503876\n",
      "N170\n",
      "Classification accuracy: 0.523810 / Chance level: 0.552381\n",
      "N170\n",
      "Classification accuracy: 0.586207 / Chance level: 0.503448\n",
      "N170\n",
      "Classification accuracy: 0.571429 / Chance level: 0.514493\n",
      "N170\n",
      "Classification accuracy: 0.321429 / Chance level: 0.528986\n",
      "N170\n",
      "Classification accuracy: 0.461538 / Chance level: 0.503876\n",
      "N170\n",
      "Classification accuracy: 0.535714 / Chance level: 0.521739\n",
      "N170\n",
      "Classification accuracy: 0.384615 / Chance level: 0.511811\n",
      "N170\n",
      "Classification accuracy: 0.333333 / Chance level: 0.516949\n",
      "N170\n",
      "Classification accuracy: 0.481481 / Chance level: 0.507576\n",
      "N170\n",
      "Classification accuracy: 0.551724 / Chance level: 0.503448\n",
      "N170\n",
      "Classification accuracy: 0.363636 / Chance level: 0.537037\n",
      "N170\n",
      "Classification accuracy: 0.548387 / Chance level: 0.503268\n",
      "N170\n",
      "Classification accuracy: 0.448276 / Chance level: 0.506944\n",
      "N170\n",
      "Classification accuracy: 0.482759 / Chance level: 0.510345\n",
      "N170\n",
      "Classification accuracy: 0.619048 / Chance level: 0.539216\n",
      "N170\n",
      "Classification accuracy: 0.709677 / Chance level: 0.503268\n",
      "N170\n",
      "Classification accuracy: 0.433333 / Chance level: 0.506849\n",
      "N170\n",
      "Classification accuracy: 0.600000 / Chance level: 0.533784\n",
      "N170\n",
      "Classification accuracy: 0.521739 / Chance level: 0.504348\n",
      "N170\n",
      "Classification accuracy: 0.666667 / Chance level: 0.518519\n"
     ]
    }
   ],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "preprocessing = \"heavy\"\n",
    "task = \"N170\"\n",
    "\n",
    "for i in range(0,40):\n",
    "    with HelperFunctions.suppress_stdout():\n",
    "        df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "        data, labels = DataLoader.create_data_labels(df, list_of_subjects=[i])\n",
    "        cv = ShuffleSplit(1, test_size=0.2, random_state=42)\n",
    "        cv_split = cv.split(data, labels)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "        clf = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "        scores = cross_val_score(clf, data, labels, cv=cv, n_jobs=1)\n",
    "        class_balance = np.mean(labels == labels[0])\n",
    "        class_balance = max(class_balance, 1. - class_balance)\n",
    "    print(task)\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ca2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
