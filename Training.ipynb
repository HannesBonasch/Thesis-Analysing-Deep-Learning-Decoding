{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ac0425",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23faa265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, skorch, sklearn, os, json, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet, EEGNetv1, Deep4Net, TCN, EEGNetv4\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.dataset import CVSplit\n",
    "from braindecode import EEGClassifier\n",
    "from torch.utils.data import TensorDataset\n",
    "from skorch.helper import predefined_split\n",
    "from pathlib import Path\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def init_model(model_name, lr, n_epochs=25, batch_size=64, n_chan=30, \n",
    "               n_classes=2, weight_decay=0, seed=42, \n",
    "               input_window_samples=251, valid_ds=None, class_weights=None,\n",
    "               gpu=True):\n",
    "    \"\"\"\n",
    "    Initializes the model and classifier.\n",
    "    \"\"\"\n",
    "    if gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        # set seed for reproducability\n",
    "        set_random_seeds(seed=seed, cuda=True)\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        set_random_seeds(seed=seed, cuda=False)\n",
    "    \n",
    "    # load model\n",
    "    if model_name == \"eegnet\":\n",
    "        model = EEGNetv4(\n",
    "            n_chan,\n",
    "            n_classes,\n",
    "            input_window_samples=input_window_samples,\n",
    "            final_conv_length=\"auto\",\n",
    "        )\n",
    "    elif model_name == \"shallow\":\n",
    "        model = ShallowFBCSPNet(\n",
    "            n_chan,\n",
    "            n_classes,\n",
    "            input_window_samples=input_window_samples,\n",
    "            n_filters_time=40, \n",
    "            filter_time_length=25, \n",
    "            n_filters_spat=40, \n",
    "            pool_time_length=75, \n",
    "            pool_time_stride=15, \n",
    "            final_conv_length=\"auto\"\n",
    "            \n",
    "        )\n",
    "    elif model_name == \"deep\":\n",
    "        model = Deep4Net(\n",
    "            n_chan,\n",
    "            n_classes,\n",
    "            input_window_samples=input_window_samples,\n",
    "            n_filters_time=25, \n",
    "            n_filters_spat=25, \n",
    "            filter_time_length=10,\n",
    "            # changed stride to fit shorter input\n",
    "            pool_time_length=2, \n",
    "            pool_time_stride=2, \n",
    "            n_filters_2=50, \n",
    "            filter_length_2=10, \n",
    "            n_filters_3=100, \n",
    "            filter_length_3=10, \n",
    "            n_filters_4=200, \n",
    "            filter_length_4=10,\n",
    "            final_conv_length=\"auto\",\n",
    "        )\n",
    "    elif model_name == \"tcn\":\n",
    "        model = TCN(\n",
    "            n_chan,\n",
    "            n_classes,\n",
    "            n_filters=50,\n",
    "            n_blocks=7,\n",
    "            kernel_size=2,\n",
    "            drop_prob=0.3,\n",
    "            add_log_softmax=True\n",
    "        )\n",
    "    \n",
    "    # send model to gpu\n",
    "    if device == 'cuda':\n",
    "        model.cuda()\n",
    "        \n",
    "    if valid_ds==None:\n",
    "        train_split=None\n",
    "    else:\n",
    "        train_split=predefined_split(valid_ds)\n",
    "    \n",
    "    # load classifier\n",
    "    clf = EEGClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        criterion__weight=class_weights,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=train_split,\n",
    "        optimizer__lr=lr,\n",
    "        optimizer__weight_decay=weight_decay,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[\n",
    "            #\"accuracy\",\n",
    "            #\"balanced_accuracy\",\n",
    "            #\"roc_auc\",\n",
    "            (\"train_balanced_accuracy\", skorch.callbacks.EpochScoring(scoring='balanced_accuracy', on_train=True, name=\"train_balanced_accuracy\", lower_is_better=False)),\n",
    "            (\"valid_balanced_accuracy\", skorch.callbacks.EpochScoring(scoring='balanced_accuracy', on_train=False, name=\"valid_balanced_accuracy\", lower_is_better=False)),\n",
    "            (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "        ],\n",
    "        device=device,\n",
    "    )\n",
    "    clf.initialize()\n",
    "    # number of trainable parameters\n",
    "    #print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    \n",
    "    return clf, model\n",
    "\n",
    "def run_exp(data, labels, task, preprocessing, model_folder, model_name, \n",
    "            lr, n_epochs, n_splits, batch_size=64, additional_save_param=\"\"):\n",
    "    \"\"\"\n",
    "    Trains classifier using Stratified Cross Validation and saves parameters and history.\n",
    "    \"\"\"\n",
    "    # path to save to\n",
    "    model_path = os.getcwd()+\"\\\\\"+model_folder+\"\\\\\"+model_name+\"\\\\\"+task+\"\\\\\"+preprocessing+\"\\\\\"\n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # calculate class weights\n",
    "    class_weights=class_weight.compute_class_weight('balanced',np.unique(labels),labels)\n",
    "    class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "    class_weights = class_weights.to('cuda')\n",
    "    \n",
    "    # push data and labels to gpu\n",
    "    dataset = TensorDataset(torch.from_numpy(data).to('cuda'),\n",
    "                            torch.from_numpy(labels).to('cuda'))\n",
    "    \n",
    "    # create stratified splits\n",
    "    cv = sklearn.model_selection.StratifiedShuffleSplit(n_splits, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(data,labels)\n",
    "\n",
    "    # train and validate on each split, then save parameters and history\n",
    "    i = 0\n",
    "    for train_idx, test_idx in cv_split:\n",
    "        i += 1\n",
    "        #valid_ds = TensorDataset(torch.from_numpy(data[test_idx]), torch.from_numpy(labels[test_idx]))\n",
    "        clf, model = init_model(model_name, lr, n_epochs=25, batch_size=64, n_chan=30, \n",
    "               n_classes=2, weight_decay=0, seed=42, input_window_samples=251, \n",
    "               valid_ds=torch.utils.data.Subset(dataset, test_idx), \n",
    "               class_weights=class_weights, gpu=True)\n",
    "        #clf, model = init_model(model_name, lr, n_epochs, batch_size, \n",
    "        #                        valid_ds=torch.utils.data.Subset(dataset, test_idx), \n",
    "        #                        class_weights=class_weights)  \n",
    "        clf.fit(torch.utils.data.Subset(dataset, train_idx), y=None, epochs=n_epochs)\n",
    "        clf.save_params(f_params=model_path+\"split_\"+str(i)+additional_save_param+\"_model.pkl\",\n",
    "                       f_optimizer=model_path+\"split_\"+str(i)+additional_save_param+\"_optimizer.pkl\",\n",
    "                       f_history=model_path+\"split_\"+str(i)+additional_save_param+\"_history.json\")\n",
    "        \n",
    "def load_exp(model_folder, model_name, task, preprocessing, n_splits, model_path=None, additional_save_param=\"\"):\n",
    "    \"\"\"\n",
    "    Loads the history json and puts it in a dataframe.\n",
    "    \"\"\"\n",
    "    if model_path == None:\n",
    "        model_path = os.getcwd()+\"\\\\\"+model_folder+\"\\\\\"+model_name+\"\\\\\"+task+\"\\\\\"+preprocessing+\"\\\\\"\n",
    "    df_list = []\n",
    "    for i in range(1,n_splits+1):\n",
    "        df_list.append(pd.read_json(model_path+\"split_\"+str(i)+additional_save_param+\"_history.json\"))\n",
    "    df = pd.concat(df_list,axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def run_exp_per_subject(df, task, preprocessing, model_folder, model_name, \n",
    "            lr, n_epochs, batch_size=64, n_subjects=40):\n",
    "    \"\"\"\n",
    "    Trains classifier on all but one subject and saves parameters and history.\n",
    "    \"\"\"\n",
    "    # path to save to\n",
    "    model_path = os.getcwd()+\"\\\\\"+model_folder+\"\\\\\"+model_name+\"\\\\\"+task+\"\\\\\"+preprocessing+\"\\\\\"\n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    # train and validate on each subject, then save parameters and history\n",
    "    for i in range(n_subjects):\n",
    "        list_train = list(range(n_subjects))\n",
    "        list_train.remove(i)\n",
    "        data, labels = DataLoader.create_data_labels(df, list_train)\n",
    "        # calculate class weights\n",
    "        class_weights=class_weight.compute_class_weight('balanced',np.unique(labels),labels)\n",
    "        class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "        class_weights = class_weights.to('cuda')\n",
    "        # push data and labels to gpu\n",
    "        dataset = TensorDataset(torch.from_numpy(data).to('cuda'),\n",
    "                                torch.from_numpy(labels).to('cuda'))\n",
    "        \n",
    "        valid_data, valid_labels = DataLoader.create_data_labels(df, [i])\n",
    "        valid_dataset = TensorDataset(torch.from_numpy(valid_data).to('cuda'),\n",
    "                                      torch.from_numpy(valid_labels).to('cuda'))\n",
    "        \n",
    "        clf, model = init_model(model_name, lr, n_epochs=25, batch_size=64, n_chan=30, \n",
    "                               n_classes=2, weight_decay=0, seed=42, input_window_samples=251, \n",
    "                               valid_ds=valid_dataset, \n",
    "                               class_weights=class_weights, gpu=True)\n",
    "        clf.fit(dataset, y=None, epochs=n_epochs)\n",
    "        clf.save_params(f_params=model_path+\"split_\"+str(i)+\"_model.pkl\",\n",
    "                       f_optimizer=model_path+\"split_\"+str(i)+\"_optimizer.pkl\",\n",
    "                       f_history=model_path+\"split_\"+str(i)+\"_history.json\")\n",
    " \n",
    "def run_exp_single_subject(df, task, preprocessing, model_folder, model_name, \n",
    "            lr, n_epochs, n_splits, batch_size=64, n_subjects=40):\n",
    "    \"\"\"\n",
    "    Trains classifier on single subject and saves parameters and history.\n",
    "    \"\"\"\n",
    "    # path to save parameters to\n",
    "    model_path = os.getcwd()+\"\\\\\"+model_folder+\"\\\\\"+model_name+\"\\\\\"+task+\"\\\\\"+preprocessing+\"\\\\\"\n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    # train and validate on each subject, then save parameters and history\n",
    "    for subjectID in range(n_subjects):\n",
    "        data, labels = DataLoader.create_data_labels(df, [subjectID])\n",
    "        # calculate class weights\n",
    "        class_weights=class_weight.compute_class_weight('balanced',np.unique(labels),labels)\n",
    "        class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "        class_weights = class_weights.to('cuda')\n",
    "        # push data and labels to gpu\n",
    "        dataset = TensorDataset(torch.from_numpy(data).to('cuda'),\n",
    "                                torch.from_numpy(labels).to('cuda'))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # create stratified splits\n",
    "        cv = sklearn.model_selection.StratifiedShuffleSplit(n_splits, test_size=0.2, random_state=42)\n",
    "        cv_split = cv.split(data,labels)\n",
    "\n",
    "        # train and validate on each split, then save parameters and history\n",
    "        i = 0\n",
    "        for train_idx, test_idx in cv_split:\n",
    "            i += 1\n",
    "            #valid_ds = TensorDataset(torch.from_numpy(data[test_idx]), torch.from_numpy(labels[test_idx]))\n",
    "            clf, model = init_model(model_name, lr, n_epochs=25, batch_size=64, n_chan=30, \n",
    "                               n_classes=2, weight_decay=0, seed=42, input_window_samples=251, \n",
    "                               valid_ds=torch.utils.data.Subset(dataset, test_idx), \n",
    "                               class_weights=class_weights, gpu=True)\n",
    "            clf.fit(torch.utils.data.Subset(dataset, train_idx), y=None, epochs=n_epochs)\n",
    "            clf.save_params(f_params=model_path+\"subject_\"+str(subjectID)+\"_split_\"+str(i)+\"_model.pkl\",\n",
    "                           f_optimizer=model_path+\"subject_\"+str(subjectID)+\"_split_\"+str(i)+\"_optimizer.pkl\",\n",
    "                           f_history=model_path+\"subject_\"+str(subjectID)+\"_split_\"+str(i)+\"_history.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48588974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5731\u001b[0m        \u001b[32m0.7144\u001b[0m                     \u001b[35m0.6303\u001b[0m        \u001b[31m0.6468\u001b[0m  0.0100  2.0257\n"
     ]
    }
   ],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "task = \"N170\"\n",
    "preprocessing = \"medium\"\n",
    "model_name = \"eegnet\"\n",
    "lr = 0.01\n",
    "model_folder = \"test\"\n",
    "n_epochs = 1\n",
    "n_splits = 1\n",
    "df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "data, labels = DataLoader.create_data_labels(df)\n",
    "run_exp(data, labels, task, preprocessing, model_folder, model_name, \n",
    "        lr, n_epochs, n_splits, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e34d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5781, 28, 251)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "task = \"N170\"\n",
    "preprocessing = \"medium\"\n",
    "model_name = \"eegnet\"\n",
    "lr = 0.01\n",
    "model_folder = \"test_group\"\n",
    "n_epochs = 25\n",
    "n_splits = 1\n",
    "df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "run_exp_per_subject(df, task, preprocessing, model_folder, model_name, \n",
    "        lr, n_epochs, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ed9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "task = \"N170\"\n",
    "preprocessing = \"medium\"\n",
    "model_name = \"eegnet\"\n",
    "lr = 0.01\n",
    "model_folder = \"test_group\"\n",
    "n_epochs = 25\n",
    "n_splits = 1\n",
    "df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "run_exp_single_subject(df, task, preprocessing, model_folder, model_name, \n",
    "        lr, n_epochs, n_splits, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070f0174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 1 1 ... 1 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.6298\u001b[0m        \u001b[32m1.1878\u001b[0m                     \u001b[35m0.7018\u001b[0m        \u001b[31m0.7191\u001b[0m  0.0100  1.5741\n",
      "      2                     \u001b[36m0.6627\u001b[0m        \u001b[32m0.9140\u001b[0m                     0.6850        \u001b[31m0.6225\u001b[0m  0.0100  0.2819\n",
      "      3                     \u001b[36m0.6877\u001b[0m        \u001b[32m0.7679\u001b[0m                     \u001b[35m0.7166\u001b[0m        \u001b[31m0.5568\u001b[0m  0.0098  0.2823\n",
      "      4                     \u001b[36m0.6908\u001b[0m        \u001b[32m0.6481\u001b[0m                     0.7147        0.6637  0.0096  0.2798\n",
      "      5                     0.6754        0.7482                     0.7106        0.5825  0.0093  0.3048\n",
      "      6                     \u001b[36m0.7226\u001b[0m        \u001b[32m0.5717\u001b[0m                     0.7027        0.7486  0.0090  0.2832\n",
      "      7                     0.6832        0.7906                     \u001b[35m0.7216\u001b[0m        0.6123  0.0085  0.2833\n",
      "      8                     0.7136        0.5910                     0.7138        0.5967  0.0080  0.2822\n",
      "      9                     0.7111        0.6014                     \u001b[35m0.7283\u001b[0m        0.5576  0.0075  0.2791\n",
      "     10                     \u001b[36m0.7270\u001b[0m        \u001b[32m0.5628\u001b[0m                     0.7171        0.6080  0.0069  0.2817\n",
      "     11                     0.7141        0.6327                     \u001b[35m0.7289\u001b[0m        \u001b[31m0.5431\u001b[0m  0.0063  0.2822\n",
      "     12                     0.7222        0.5788                     \u001b[35m0.7323\u001b[0m        0.5620  0.0057  0.2797\n",
      "     13                     \u001b[36m0.7361\u001b[0m        \u001b[32m0.5315\u001b[0m                     \u001b[35m0.7332\u001b[0m        \u001b[31m0.5375\u001b[0m  0.0050  0.2813\n",
      "     14                     0.7348        \u001b[32m0.5278\u001b[0m                     \u001b[35m0.7333\u001b[0m        \u001b[31m0.5365\u001b[0m  0.0043  0.2792\n",
      "     15                     \u001b[36m0.7414\u001b[0m        \u001b[32m0.5194\u001b[0m                     \u001b[35m0.7404\u001b[0m        \u001b[31m0.5325\u001b[0m  0.0037  0.2814\n",
      "     16                     \u001b[36m0.7469\u001b[0m        \u001b[32m0.5149\u001b[0m                     \u001b[35m0.7489\u001b[0m        0.5356  0.0031  0.2803\n",
      "     17                     0.7458        0.5153                     0.7398        \u001b[31m0.5241\u001b[0m  0.0025  0.2794\n",
      "     18                     \u001b[36m0.7518\u001b[0m        \u001b[32m0.5011\u001b[0m                     \u001b[35m0.7508\u001b[0m        \u001b[31m0.5191\u001b[0m  0.0020  0.2820\n",
      "     19                     0.7422        0.5027                     0.7411        0.5294  0.0015  0.2812\n",
      "     20                     \u001b[36m0.7539\u001b[0m        0.5012                     \u001b[35m0.7514\u001b[0m        0.5218  0.0010  0.2832\n",
      "     21                     0.7522        0.5019                     0.7498        0.5195  0.0007  0.2807\n",
      "     22                     \u001b[36m0.7589\u001b[0m        \u001b[32m0.4941\u001b[0m                     \u001b[35m0.7536\u001b[0m        0.5219  0.0004  0.2831\n",
      "     23                     0.7506        0.5007                     0.7524        0.5195  0.0002  0.2795\n",
      "     24                     0.7556        0.4998                     0.7521        0.5196  0.0000  0.2901\n",
      "     25                     0.7542        0.4953                     0.7476        0.5228  0.0000  0.3002\n"
     ]
    }
   ],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "task = \"N400\"\n",
    "preprocessing = \"light\"\n",
    "model_name = \"deep\"\n",
    "lr = 0.01\n",
    "model_folder = \"test_reference\"\n",
    "n_epochs = 25\n",
    "n_splits = 1\n",
    "df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "data, labels = DataLoader.create_data_labels(df)\n",
    "run_exp(data, labels, task, preprocessing, model_folder, model_name, \n",
    "        lr, n_epochs, n_splits, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ff4ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "model_name = \"eegnet\"\n",
    "model_folder = \"test\"\n",
    "task = \"N170\"\n",
    "preprocessing = \"medium\"\n",
    "lr=0.01\n",
    "\n",
    "df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "data, labels = DataLoader.create_data_labels(df)\n",
    "clf, model = init_model(model_name, lr, n_epochs=25, batch_size=64, n_chan=30, \n",
    "                       n_classes=2, weight_decay=0, seed=42, input_window_samples=251, \n",
    "                       valid_ds=None, \n",
    "                       class_weights=None, gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90e8960e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rnn_torchviz.png'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "x = torch.randn(1, 30, 251)\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "199618c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50002\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba3396ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EEGNetv4(\n",
       "  (ensuredims): Ensure4d()\n",
       "  (dimshuffle): Expression(expression=_transpose_to_b_1_c_0) \n",
       "  (conv_temporal): Conv2d(1, 8, kernel_size=(1, 64), stride=(1, 1), padding=(0, 32), bias=False)\n",
       "  (bnorm_temporal): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (conv_spatial): Conv2dWithConstraint(8, 16, kernel_size=(30, 1), stride=(1, 1), groups=8, bias=False)\n",
       "  (bnorm_1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (elu_1): Expression(expression=elu) \n",
       "  (pool_1): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
       "  (drop_1): Dropout(p=0.25, inplace=False)\n",
       "  (conv_separable_depth): Conv2d(16, 16, kernel_size=(1, 16), stride=(1, 1), padding=(0, 8), groups=16, bias=False)\n",
       "  (conv_separable_point): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bnorm_2): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (elu_2): Expression(expression=elu) \n",
       "  (pool_2): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
       "  (drop_2): Dropout(p=0.25, inplace=False)\n",
       "  (conv_classifier): Conv2d(16, 2, kernel_size=(1, 8), stride=(1, 1))\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       "  (permute_back): Expression(expression=_transpose_1_0) \n",
       "  (squeeze): Expression(expression=squeeze_final_output) \n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07903c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
