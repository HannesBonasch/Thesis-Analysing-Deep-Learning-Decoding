{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ac0425",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23faa265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, skorch, sklearn, os, json, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet, EEGNetv1, Deep4Net, TCN, EEGNetv4\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.dataset import CVSplit\n",
    "from braindecode import EEGClassifier\n",
    "from torch.utils.data import TensorDataset\n",
    "from skorch.helper import predefined_split\n",
    "from pathlib import Path\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def init_model(model_name, lr, n_epochs=25, batch_size=64, n_chan=30, \n",
    "               n_classes=2, weight_decay=0, seed=42, \n",
    "               input_window_samples=251, valid_ds=None, class_weights=None,\n",
    "               gpu=True):\n",
    "    \"\"\"\n",
    "    Initializes the model and classifier.\n",
    "    \"\"\"\n",
    "    if gpu and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        # set seed for reproducability\n",
    "        set_random_seeds(seed=seed, cuda=True)\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        set_random_seeds(seed=seed, cuda=False)\n",
    "    \n",
    "    # load model\n",
    "    if model_name == \"eegnet\":\n",
    "        model = EEGNetv4(\n",
    "            n_chan,\n",
    "            n_classes,\n",
    "            input_window_samples=input_window_samples,\n",
    "            final_conv_length=\"auto\",\n",
    "        )\n",
    "    elif model_name == \"shallow\":\n",
    "        model = ShallowFBCSPNet(\n",
    "            n_chan,\n",
    "            n_classes,\n",
    "            input_window_samples=input_window_samples,\n",
    "            n_filters_time=40, \n",
    "            filter_time_length=25, \n",
    "            n_filters_spat=40, \n",
    "            pool_time_length=75, \n",
    "            pool_time_stride=15, \n",
    "            final_conv_length=\"auto\"\n",
    "            \n",
    "        )\n",
    "    elif model_name == \"deep\":\n",
    "        model = Deep4Net(\n",
    "            n_chan,\n",
    "            n_classes,\n",
    "            input_window_samples=input_window_samples,\n",
    "            n_filters_time=25, \n",
    "            n_filters_spat=25, \n",
    "            filter_time_length=10,\n",
    "            # changed stride to fit shorter input\n",
    "            pool_time_length=2, \n",
    "            pool_time_stride=2, \n",
    "            n_filters_2=50, \n",
    "            filter_length_2=10, \n",
    "            n_filters_3=100, \n",
    "            filter_length_3=10, \n",
    "            n_filters_4=200, \n",
    "            filter_length_4=10,\n",
    "            final_conv_length=\"auto\",\n",
    "        )\n",
    "    elif model_name == \"tcn\":\n",
    "        model = TCN(\n",
    "            n_chan,\n",
    "            n_classes,\n",
    "            n_filters=50,\n",
    "            n_blocks=7,\n",
    "            kernel_size=2,\n",
    "            drop_prob=0.3,\n",
    "            add_log_softmax=True\n",
    "        )\n",
    "    \n",
    "    # send model to gpu\n",
    "    if device == 'cuda':\n",
    "        model.cuda()\n",
    "        \n",
    "    # workaround to use sklearn crossvalidation splits\n",
    "    if valid_ds==None:\n",
    "        train_split=None\n",
    "    else:\n",
    "        train_split=predefined_split(valid_ds)\n",
    "    \n",
    "    # load classifier\n",
    "    clf = EEGClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        criterion__weight=class_weights,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=train_split,\n",
    "        optimizer__lr=lr,\n",
    "        optimizer__weight_decay=weight_decay,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[\n",
    "            #\"accuracy\",\n",
    "            #\"balanced_accuracy\",\n",
    "            #\"roc_auc\",\n",
    "            (\"train_balanced_accuracy\", skorch.callbacks.EpochScoring(scoring='balanced_accuracy', on_train=True, name=\"train_balanced_accuracy\", lower_is_better=False)),\n",
    "            (\"valid_balanced_accuracy\", skorch.callbacks.EpochScoring(scoring='balanced_accuracy', on_train=False, name=\"valid_balanced_accuracy\", lower_is_better=False)),\n",
    "            (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "        ],\n",
    "        device=device,\n",
    "    )\n",
    "    clf.initialize()\n",
    "    return clf, model\n",
    "\n",
    "def run_exp(data, labels, task, preprocessing, model_folder, model_name, \n",
    "            lr, n_epochs, n_splits, batch_size=64, additional_save_param=\"\"):\n",
    "    \"\"\"\n",
    "    Trains classifier using Stratified Cross Validation and saves parameters and history.\n",
    "    \"\"\"\n",
    "    # path to save to\n",
    "    model_path = os.getcwd()+\"\\\\\"+model_folder+\"\\\\\"+model_name+\"\\\\\"+task+\"\\\\\"+preprocessing+\"\\\\\"\n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # calculate class weights\n",
    "    class_weights=class_weight.compute_class_weight('balanced',np.unique(labels),labels)\n",
    "    class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "    class_weights = class_weights.to('cuda')\n",
    "    \n",
    "    # push data and labels to gpu\n",
    "    dataset = TensorDataset(torch.from_numpy(data).to('cuda'),\n",
    "                            torch.from_numpy(labels).to('cuda'))\n",
    "    \n",
    "    # create stratified splits\n",
    "    cv = sklearn.model_selection.StratifiedShuffleSplit(n_splits, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(data,labels)\n",
    "\n",
    "    # train and validate on each split, then save parameters and history\n",
    "    i = 0\n",
    "    for train_idx, test_idx in cv_split:\n",
    "        i += 1\n",
    "        #valid_ds = TensorDataset(torch.from_numpy(data[test_idx]), torch.from_numpy(labels[test_idx]))\n",
    "        clf, model = init_model(model_name, lr, n_epochs=25, batch_size=64, n_chan=30, \n",
    "               n_classes=2, weight_decay=0, seed=42, input_window_samples=251, \n",
    "               valid_ds=torch.utils.data.Subset(dataset, test_idx), \n",
    "               class_weights=class_weights, gpu=True)\n",
    "        #clf, model = init_model(model_name, lr, n_epochs, batch_size, \n",
    "        #                        valid_ds=torch.utils.data.Subset(dataset, test_idx), \n",
    "        #                        class_weights=class_weights)  \n",
    "        clf.fit(torch.utils.data.Subset(dataset, train_idx), y=None, epochs=n_epochs)\n",
    "        clf.save_params(f_params=model_path+\"split_\"+str(i)+additional_save_param+\"_model.pkl\",\n",
    "                       f_optimizer=model_path+\"split_\"+str(i)+additional_save_param+\"_optimizer.pkl\",\n",
    "                       f_history=model_path+\"split_\"+str(i)+additional_save_param+\"_history.json\")\n",
    "        \n",
    "def load_exp(model_folder, model_name, task, preprocessing, n_splits, model_path=None, additional_save_param=\"\"):\n",
    "    \"\"\"\n",
    "    Loads the history json and puts it in a dataframe.\n",
    "    \"\"\"\n",
    "    if model_path == None:\n",
    "        model_path = os.getcwd()+\"\\\\\"+model_folder+\"\\\\\"+model_name+\"\\\\\"+task+\"\\\\\"+preprocessing+\"\\\\\"\n",
    "    df_list = []\n",
    "    for i in range(1,n_splits+1):\n",
    "        df_list.append(pd.read_json(model_path+\"split_\"+str(i)+additional_save_param+\"_history.json\"))\n",
    "    df = pd.concat(df_list,axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def run_exp_per_subject(df, task, preprocessing, model_folder, model_name, \n",
    "            lr, n_epochs, batch_size=64, n_subjects=40):\n",
    "    \"\"\"\n",
    "    Trains classifier on all but one subject and saves parameters and history.\n",
    "    \"\"\"\n",
    "    # path to save to\n",
    "    model_path = os.getcwd()+\"\\\\\"+model_folder+\"\\\\\"+model_name+\"\\\\\"+task+\"\\\\\"+preprocessing+\"\\\\\"\n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    # train and validate on each subject, then save parameters and history\n",
    "    for i in range(n_subjects):\n",
    "        list_train = list(range(n_subjects))\n",
    "        list_train.remove(i)\n",
    "        data, labels = DataLoader.create_data_labels(df, list_train)\n",
    "        # calculate class weights\n",
    "        class_weights=class_weight.compute_class_weight('balanced',np.unique(labels),labels)\n",
    "        class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "        class_weights = class_weights.to('cuda')\n",
    "        # push data and labels to gpu\n",
    "        dataset = TensorDataset(torch.from_numpy(data).to('cuda'),\n",
    "                                torch.from_numpy(labels).to('cuda'))\n",
    "        \n",
    "        valid_data, valid_labels = DataLoader.create_data_labels(df, [i])\n",
    "        valid_dataset = TensorDataset(torch.from_numpy(valid_data).to('cuda'),\n",
    "                                      torch.from_numpy(valid_labels).to('cuda'))\n",
    "        \n",
    "        clf, model = init_model(model_name, lr, n_epochs=25, batch_size=64, n_chan=30, \n",
    "                               n_classes=2, weight_decay=0, seed=42, input_window_samples=251, \n",
    "                               valid_ds=valid_dataset, \n",
    "                               class_weights=class_weights, gpu=True)\n",
    "        clf.fit(dataset, y=None, epochs=n_epochs)\n",
    "        clf.save_params(f_params=model_path+\"split_\"+str(i)+\"_model.pkl\",\n",
    "                       f_optimizer=model_path+\"split_\"+str(i)+\"_optimizer.pkl\",\n",
    "                       f_history=model_path+\"split_\"+str(i)+\"_history.json\")\n",
    " \n",
    "def run_exp_single_subject(df, task, preprocessing, model_folder, model_name, \n",
    "            lr, n_epochs, n_splits, batch_size=64, n_subjects=40):\n",
    "    \"\"\"\n",
    "    Trains classifier on single subject and saves parameters and history.\n",
    "    \"\"\"\n",
    "    # path to save parameters to\n",
    "    model_path = os.getcwd()+\"\\\\\"+model_folder+\"\\\\\"+model_name+\"\\\\\"+task+\"\\\\\"+preprocessing+\"\\\\\"\n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    # train and validate on each subject, then save parameters and history\n",
    "    for subjectID in range(n_subjects):\n",
    "        data, labels = DataLoader.create_data_labels(df, [subjectID])\n",
    "        # calculate class weights\n",
    "        class_weights=class_weight.compute_class_weight('balanced',np.unique(labels),labels)\n",
    "        class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "        class_weights = class_weights.to('cuda')\n",
    "        # push data and labels to gpu\n",
    "        dataset = TensorDataset(torch.from_numpy(data).to('cuda'),\n",
    "                                torch.from_numpy(labels).to('cuda'))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # create stratified splits\n",
    "        cv = sklearn.model_selection.StratifiedShuffleSplit(n_splits, test_size=0.2, random_state=42)\n",
    "        cv_split = cv.split(data,labels)\n",
    "\n",
    "        # train and validate on each split, then save parameters and history\n",
    "        i = 0\n",
    "        for train_idx, test_idx in cv_split:\n",
    "            i += 1\n",
    "            #valid_ds = TensorDataset(torch.from_numpy(data[test_idx]), torch.from_numpy(labels[test_idx]))\n",
    "            clf, model = init_model(model_name, lr, n_epochs=25, batch_size=64, n_chan=30, \n",
    "                               n_classes=2, weight_decay=0, seed=42, input_window_samples=251, \n",
    "                               valid_ds=torch.utils.data.Subset(dataset, test_idx), \n",
    "                               class_weights=class_weights, gpu=True)\n",
    "            clf.fit(torch.utils.data.Subset(dataset, train_idx), y=None, epochs=n_epochs)\n",
    "            clf.save_params(f_params=model_path+\"subject_\"+str(subjectID)+\"_split_\"+str(i)+\"_model.pkl\",\n",
    "                           f_optimizer=model_path+\"subject_\"+str(subjectID)+\"_split_\"+str(i)+\"_optimizer.pkl\",\n",
    "                           f_history=model_path+\"subject_\"+str(subjectID)+\"_split_\"+str(i)+\"_history.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48588974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vapor\\anaconda3\\envs\\braindecode\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_balanced_accuracy    train_loss    valid_balanced_accuracy    valid_loss      lr     dur\n",
      "-------  -------------------------  ------------  -------------------------  ------------  ------  ------\n",
      "      1                     \u001b[36m0.5731\u001b[0m        \u001b[32m0.7144\u001b[0m                     \u001b[35m0.6303\u001b[0m        \u001b[31m0.6468\u001b[0m  0.0100  2.0257\n"
     ]
    }
   ],
   "source": [
    "# Location of Dataframes\n",
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "# task names: \"N170\", \"N400\", \"P3\", \"N2pc\", \"MMN\", \"ERN\", \"LRP\"\n",
    "task = \"N170\"\n",
    "# preprocessing names: \"light\", \"medium\", \"heavy\"\n",
    "preprocessing = \"medium\"\n",
    "# model names: \"eegnet\", \"shallow\", \"deep\"\n",
    "model_name = \"eegnet\"\n",
    "# learning rate, we used 0.01 for shallow and deep and 0.03 for eegnet\n",
    "lr = 0.01\n",
    "# number of epochs, we used 25\n",
    "n_epochs = 1\n",
    "# number of cross validation splits, we used 10\n",
    "n_splits = 1\n",
    "# Folder name to save model and history to\n",
    "model_folder = \"test\"\n",
    "# Load the dataframe of the desired task and preprocessing\n",
    "df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "# Get the data and labels in the shape expected by PyTorch\n",
    "data, labels = DataLoader.create_data_labels(df)\n",
    "# Train using cross-validation splits on all subjects\n",
    "run_exp(data, labels, task, preprocessing, model_folder, model_name, \n",
    "        lr, n_epochs, n_splits, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e34d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5781, 28, 251)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "task = \"N170\"\n",
    "preprocessing = \"medium\"\n",
    "model_name = \"eegnet\"\n",
    "lr = 0.01\n",
    "model_folder = \"test_cross\"\n",
    "n_epochs = 25\n",
    "df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "# Cross-subject Training\n",
    "run_exp_per_subject(df, task, preprocessing, model_folder, model_name, \n",
    "        lr, n_epochs, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ed9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "task = \"N170\"\n",
    "preprocessing = \"medium\"\n",
    "model_name = \"eegnet\"\n",
    "lr = 0.01\n",
    "model_folder = \"test_within\"\n",
    "n_epochs = 25\n",
    "n_splits = 1\n",
    "df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "# Within-subject Training\n",
    "run_exp_single_subject(df, task, preprocessing, model_folder, model_name, \n",
    "        lr, n_epochs, n_splits, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ff4ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to load an already trained model\n",
    "data_path = \"F:/Masterthesis/Data/\"\n",
    "model_name = \"eegnet\"\n",
    "model_folder = \"test\"\n",
    "task = \"N170\"\n",
    "preprocessing = \"medium\"\n",
    "lr=0.01\n",
    "\n",
    "df = DataLoader.load_df(data_path, task, preprocessing)\n",
    "data, labels = DataLoader.create_data_labels(df)\n",
    "clf, model = init_model(model_name, lr, n_epochs=25, batch_size=64, n_chan=30, \n",
    "                       n_classes=2, weight_decay=0, seed=42, input_window_samples=251, \n",
    "                       valid_ds=None, \n",
    "                       class_weights=None, gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba3396ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EEGNetv4(\n",
       "  (ensuredims): Ensure4d()\n",
       "  (dimshuffle): Expression(expression=_transpose_to_b_1_c_0) \n",
       "  (conv_temporal): Conv2d(1, 8, kernel_size=(1, 64), stride=(1, 1), padding=(0, 32), bias=False)\n",
       "  (bnorm_temporal): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (conv_spatial): Conv2dWithConstraint(8, 16, kernel_size=(30, 1), stride=(1, 1), groups=8, bias=False)\n",
       "  (bnorm_1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (elu_1): Expression(expression=elu) \n",
       "  (pool_1): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
       "  (drop_1): Dropout(p=0.25, inplace=False)\n",
       "  (conv_separable_depth): Conv2d(16, 16, kernel_size=(1, 16), stride=(1, 1), padding=(0, 8), groups=16, bias=False)\n",
       "  (conv_separable_point): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bnorm_2): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (elu_2): Expression(expression=elu) \n",
       "  (pool_2): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
       "  (drop_2): Dropout(p=0.25, inplace=False)\n",
       "  (conv_classifier): Conv2d(16, 2, kernel_size=(1, 8), stride=(1, 1))\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       "  (permute_back): Expression(expression=_transpose_1_0) \n",
       "  (squeeze): Expression(expression=squeeze_final_output) \n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "199618c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50002\n"
     ]
    }
   ],
   "source": [
    "# print number of parameters\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07903c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
